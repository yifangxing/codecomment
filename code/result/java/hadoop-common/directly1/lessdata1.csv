file_path,Name,full_name,Start Line,End Line,Comment,Pre_Comment,child Name,domain,inner_method,node_level
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileRange.java,createFileRange,"org.apache.hadoop.fs.FileRange:createFileRange(long,int)",73,75,"/**
 * Creates a file range with an offset and length.
 * @param offset starting position in the file
 * @param length number of bytes in the range
 * @return FileRange object representing the specified range
 */","* Factory method to create a FileRange object.
   * @param offset starting offset of the range.
   * @param length length of the range.
   * @return a new instance of FileRangeImpl.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileRange.java,createFileRange,"org.apache.hadoop.fs.FileRange:createFileRange(long,int,java.lang.Object)",84,86,"/**
* Creates a FileRange object.
* @param offset starting position in file
* @param length number of bytes
* @param reference associated object
* @return FileRange instance
*/","* Factory method to create a FileRange object.
   * @param offset starting offset of the range.
   * @param length length of the range.
   * @param reference nullable reference to store in the range.
   * @return a new instance of FileRangeImpl.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateRangeRequest,org.apache.hadoop.fs.VectoredReadUtils:validateRangeRequest(org.apache.hadoop.fs.FileRange),65,75,"/**
* Validates and returns a file range.
* @param range FileRange object to validate
* @return Validated FileRange object
* @throws EOFException if position is negative
*/","* Validate a single range.
   * @param range range to validate.
   * @return the range.
   * @param <T> range type
   * @throws IllegalArgumentException the range length is negative or other invalid condition
   * is met other than the those which raise EOFException or NullPointerException.
   * @throws EOFException the range offset is negative
   * @throws NullPointerException if the range is null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNull,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNull(java.lang.Object,java.lang.String)",45,47,"/**
* Validates that an object is not null.
* @param obj the object to check
* @param argName name of the argument for error message
*/","* Validates that the given reference argument is not null.
   * @param obj the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPositiveInteger,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPositiveInteger(long,java.lang.String)",54,56,"/**
* Validates that the given value is a positive integer.
* @param value the number to validate
* @param argName name of the argument for error message
*/","* Validates that the given integer argument is not zero or negative.
   * @param value the argument value to validate
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNegative,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNegative(long,java.lang.String)",63,65,"/**
* Validates that the given value is non-negative.
* @param value the value to check
* @param argName name of the argument for error message
*/","* Validates that the given integer argument is not negative.
   * @param value the argument value to validate
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkRequired,"org.apache.hadoop.fs.impl.prefetch.Validate:checkRequired(boolean,java.lang.String)",72,74,"/**
* Validates presence of an argument.
* @param isPresent flag indicating if argument is present
* @param argName name of the argument to validate
*/","* Validates that the expression (that checks a required field is present) is true.
   * @param isPresent indicates whether the given argument is present.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValid,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String)",81,83,"/**
* Logs validation failure message.
* @param isValid flag indicating validity
* @param argName name of the argument being validated
*/","* Validates that the expression (that checks a field is valid) is true.
   * @param isValid indicates whether the given argument is valid.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValid,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String,java.lang.String)",91,96,"/**
 * Validates argument and logs error if invalid.
 * @param isValid flag indicating argument validity
 * @param argName name of the argument to validate
 * @param validValues string of valid values for the argument
 */","* Validates that the expression (that checks a field is valid) is true.
   * @param isValid indicates whether the given argument is valid.
   * @param argName the name of the argument being validated.
   * @param validValues the list of values that are allowed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValuesEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValuesEqual(long,java.lang.String,long,java.lang.String)",201,213,"/**
* Asserts equality of two long values.
* @param value1 first value to compare
* @param value1Name name of the first value
* @param value2 second value to compare
* @param value2Name name of the second value
*/","* Validates that the given two values are equal.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkIntegerMultiple,"org.apache.hadoop.fs.impl.prefetch.Validate:checkIntegerMultiple(long,java.lang.String,long,java.lang.String)",222,234,"/**
* Checks if one value is an integer multiple of another.
* @param value1 the dividend
* @param value1Name name of the dividend
* @param value2 the divisor
* @param value2Name name of the divisor
*/","* Validates that the first value is an integer multiple of the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkGreater,"org.apache.hadoop.fs.impl.prefetch.Validate:checkGreater(long,java.lang.String,long,java.lang.String)",243,255,"/**
 * Ensures the first value is greater than the second.
 * @param value1 the first numeric value
 * @param value1Name name of the first value for error message
 * @param value2 the second numeric value
 * @param value2Name name of the second value for error message
 */","* Validates that the first value is greater than the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkGreaterOrEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkGreaterOrEqual(long,java.lang.String,long,java.lang.String)",264,276,"/**
* Validates that one value is greater than or equal to another.
* @param value1 the first value to compare
* @param value1Name name of the first value for error message
* @param value2 the second value to compare
* @param value2Name name of the second value for error message
*/","* Validates that the first value is greater than or equal to the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkLessOrEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkLessOrEqual(long,java.lang.String,long,java.lang.String)",285,297,"/**
* Validates that one long value is less than or equal to another.
* @param value1 first value to compare
* @param value1Name name of the first value
* @param value2 second value to compare
* @param value2Name name of the second value
*/","* Validates that the first value is less than or equal to the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkWithinRange,"org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(long,java.lang.String,long,long)",306,318,"/**
* Validates if a value is within a specified inclusive range.
* @param value the value to validate
* @param valueName name of the value for error message
* @param minValueInclusive minimum allowable value (inclusive)
* @param maxValueInclusive maximum allowable value (inclusive)
*/","* Validates that the given value is within the given range of values.
   * @param value the value to check.
   * @param valueName the name of the argument.
   * @param minValueInclusive inclusive lower limit for the value.
   * @param maxValueInclusive inclusive upper limit for the value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkWithinRange,"org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(double,java.lang.String,double,double)",327,339,"/**
* Validates if a double value is within a specified inclusive range.
* @param value the value to validate
* @param valueName name of the value for error message
* @param minValueInclusive minimum value (inclusive)
* @param maxValueInclusive maximum value (inclusive)
*/","* Validates that the given value is within the given range of values.
   * @param value the value to check.
   * @param valueName the name of the argument.
   * @param minValueInclusive inclusive lower limit for the value.
   * @param maxValueInclusive inclusive upper limit for the value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotEmpty(int,java.lang.String)",393,398,"/**
* Validates that an array has at least one element.
* @param arraySize size of the array to validate
* @param argName name of the array argument for error message
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BulkDeleteUtils.java,validateBulkDeletePaths,"org.apache.hadoop.fs.BulkDeleteUtils:validateBulkDeletePaths(java.util.Collection,int,org.apache.hadoop.fs.Path)",39,48,"/**
 * Masks paths based on size and base path constraints.
 * @param paths collection of file paths to be masked
 * @param pageSize maximum allowed number of paths
 * @param basePath root directory for path validation
 */","* Preconditions for bulk delete paths.
   * @param paths paths to delete.
   * @param pageSize maximum number of paths to delete in a single operation.
   * @param basePath base path for the delete operation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(java.io.File),177,182,"/**
* Initializes block upload data with a file.
* @param file the file to be uploaded
*/","* File constructor; input stream and byteArray will be null.
     *
     * @param file file to upload",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,requireIOStatisticsSnapshot,org.apache.hadoop.io.wrappedio.WrappedStatistics:requireIOStatisticsSnapshot(java.io.Serializable),352,356,"/**
* Validates and casts snapshot to IOStatisticsSnapshot.
* @param snapshot object to validate and cast
* @return IOStatisticsSnapshot instance
*/","* Require the parameter to be an instance of {@link IOStatisticsSnapshot}.
   * @param snapshot object to validate
   * @return cast value
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,<init>,"org.apache.hadoop.net.SocketInputWrapper:<init>(java.net.Socket,java.io.InputStream)",43,52,"/**
 * Wraps a socket and input stream.
 * @param s the socket to wrap
 * @param is the input stream associated with the socket
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfigurationHelper.java,mapEnumNamesToValues,"org.apache.hadoop.util.ConfigurationHelper:mapEnumNamesToValues(java.lang.String,java.lang.Class)",109,124,"/**
* Creates a case-insensitive map of enum constants by prefix.
* @param prefix common prefix for keys
* @param enumClass class of the enum type
* @return Map with lower-case string keys and corresponding enum values
*/","* Given an enum class, build a map of lower case names to values.
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param enumClass class of enum
   * @param <E> enum type
   * @return a mutable map of lower case names to enum values
   * @throws IllegalArgumentException if there are two entries which differ only by case.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,sortRanges,org.apache.hadoop.fs.VectoredReadUtils:sortRanges(java.util.List),358,361,"/**
* Applies mask to file ranges.
* @param input list of file ranges to process
* @return array of masked file ranges
*/","* Sort the input ranges by offset; no validation is done.
   * <p>
   * This method is used externally and must be retained with
   * the signature unchanged.
   * @param input input ranges.
   * @return a new list of the ranges, sorted by offset.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,perms,org.apache.hadoop.fs.Options$CreateOpts:perms(org.apache.hadoop.fs.permission.FsPermission),65,67,"/**
 * Converts FsPermission to Perms.
 * @param perm file system permission object
 * @return Perms object representing the same permissions
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,seekInternal,org.apache.hadoop.fs.sftp.SFTPInputStream:seekInternal(),79,96,"/**
* Adjusts stream position and handles skips or resets.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,checkNotClosed,org.apache.hadoop.fs.sftp.SFTPInputStream:checkNotClosed(),135,141,"/**
* Checks if stream is closed and throws IOException if true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,isParentOf,"org.apache.hadoop.fs.ftp.FTPFileSystem:isParentOf(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",648,657,"/**
 * Checks if a child path is within a parent directory.
 * @param parent the parent directory path
 * @param child the child path to check
 * @return true if child is within parent, false otherwise
 */","* Probe for a path being a parent of another
   * @param parent parent path
   * @param child possible child path
   * @return true if the parent's path matches the start of the child's",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,isSameFS,"org.apache.hadoop.fs.FileContext:isSameFS(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2306,2312,"/**
* Compares two paths for a specific mask condition.
* @param qualPath1 first path to compare
* @param qualPath2 second path to compare
* @return true if paths match the mask condition, false otherwise
*/","* Are qualSrc and qualDst of the same file system?
   * @param qualPath1 - fully qualified path
   * @param qualPath2 - fully qualified path
   * @return is same fs true,not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,deleteOnExit,org.apache.hadoop.fs.FileSystem:deleteOnExit(org.apache.hadoop.fs.Path),1804,1812,"/**
* Masks a file by adding it to the delete on exit list.
* @param f file path to be masked
* @return true if masking is successful, false otherwise
*/","* Mark a path to be deleted when its FileSystem is closed.
   * When the JVM shuts down cleanly, all cached FileSystem objects will be
   * closed automatically. These the marked paths will be deleted as a result.
   *
   * If a FileSystem instance is not cached, i.e. has been created with
   * {@link #createFileSystem(URI, Configuration)}, then the paths will
   * be deleted in when {@link #close()} is called on that instance.
   *
   * The path must exist in the filesystem at the time of the method call;
   * it does not have to exist at the time of JVM shutdown.
   *
   * Notes
   * <ol>
   *   <li>Clean shutdown of the JVM cannot be guaranteed.</li>
   *   <li>The time to shut down a FileSystem will depends on the number of
   *   files to delete. For filesystems where the cost of checking
   *   for the existence of a file/directory and the actual delete operation
   *   (for example: object stores) is high, the time to shutdown the JVM can be
   *   significantly extended by over-use of this feature.</li>
   *   <li>Connectivity problems with a remote filesystem may delay shutdown
   *   further, and may cause the files to not be deleted.</li>
   * </ol>
   * @param f the path to delete.
   * @return  true if deleteOnExit is successful, otherwise false.
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,processDeleteOnExit,org.apache.hadoop.fs.FileSystem:processDeleteOnExit(),1833,1848,"/**
* Deletes files marked for deletion on exit.
* Synchronizes access to the delete list and handles IOExceptions gracefully.
*/","* Delete all paths that were marked as delete-on-exit. This recursively
   * deletes all files and directories in the specified paths.
   *
   * The time to process this operation is {@code O(paths)}, with the actual
   * time dependent on the time for existence and deletion operations to
   * complete, successfully or not.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,keystoreExists,org.apache.hadoop.security.alias.KeyStoreProvider:keystoreExists(),58,61,"/**
 * Checks mask condition using file system operations.
 * @throws IOException if an I/O error occurs
 * @return true if mask condition is met, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,compareTo,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:compareTo(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode),140,151,"/**
* Compares node status modification times.
* @param other the other MRNflyNode to compare with
* @return comparison result: 0 if equal, 1 if this is newer, -1 if older
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getModificationTime,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getModificationTime(),504,507,"/**
 * Delegates call to underlying status.
 * @return result from realStatus.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getModificationTime,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getModificationTime(),80,83,"/**
 * Delegates to myFs's m1 method.
 * @return Result from myFs.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,decodeFileName,org.apache.hadoop.fs.HarFileSystem:decodeFileName(java.lang.String),261,268,"/**
* Masks function name based on metadata version.
* @param fname original function name
* @return masked function name or original if not applicable
* @throws UnsupportedEncodingException if encoding fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,compareTo,org.apache.hadoop.fs.shell.PathData:compareTo(org.apache.hadoop.fs.shell.PathData),593,596,"/**
 * Delegates call to path's m1 method.
 * @param o PathData object
 * @return Result of path.m1(o.path)
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,isChecksumFile,org.apache.hadoop.fs.ChecksumFs:isChecksumFile(org.apache.hadoop.fs.Path),98,101,"/**
* Checks if file name ends with "".crc"" and does not contain ""."".
* @param file Path object representing the file
* @return true if conditions are met, false otherwise
*/","* Return true iff file is a checksum file name.
   *
   * @param file the file path.
   * @return if is checksum file true,not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,isChecksumFile,org.apache.hadoop.fs.ChecksumFileSystem:isChecksumFile(org.apache.hadoop.fs.Path),130,133,"/**
* Checks if file name contains both '.' and '.crc'.
* @param file Path to the file
* @return true if conditions are met, false otherwise
*/","* Return true if file is a checksum file name.
   *
   * @param file the file path.
   * @return if file is a checksum file true, not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,fixBlockLocations,"org.apache.hadoop.fs.HarFileSystem:fixBlockLocations(org.apache.hadoop.fs.BlockLocation[],long,long,long)",423,457,"/**
 * Adjusts block locations based on start, length, and file offset.
 * @param locations array of BlockLocation objects to adjust
 * @param start starting position for adjustment
 * @param len length of the segment to adjust
 * @param fileOffsetInHar file offset within HAR
 * @return adjusted BlockLocation array
 */","* Fix offset and length of block locations.
   * Note that this method modifies the original array.
   * @param locations block locations of har part file
   * @param start the start of the desired range in the contained file
   * @param len the length of the desired range
   * @param fileOffsetInHar the offset of the desired file in the har part file
   * @return block locations with fixed offset and length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,compareTo,org.apache.hadoop.fs.FileStatus:compareTo(org.apache.hadoop.fs.FileStatus),411,413,"/**
 * Calls m2 on result of m1(), passing o's m1() as argument.
 * @param o FileStatus object
 * @return Result of nested method calls
 */","* Compare this FileStatus to another FileStatus based on lexicographical
   * order of path.
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,stat2Paths,org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[]),114,122,"/**
 * Extracts paths from file statuses.
 * @param stats array of FileStatus objects
 * @return array of Path objects or null if input is null
 */","* convert an array of FileStatus to an array of Path
   *
   * @param stats
   *          an array of FileStatus objects
   * @return an array of paths corresponding to the input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,addPartFileStatuses,org.apache.hadoop.fs.HarFileSystem$HarMetaData:addPartFileStatuses(org.apache.hadoop.fs.Path),1148,1152,"/**
* Masks files in specified HDFS path.
* @param path directory to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,merge,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])",1228,1242,"/**
* Filters and combines file statuses.
* @param toStatuses primary file statuses
* @param fromStatuses secondary file statuses
* @return combined array of unique file statuses
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getPath,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPath(),529,532,"/**
 * Delegates to realStatus's m1 method.
 * @return Path returned by realStatus's m1
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,merge,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])",1622,1636,"/**
* Filters and merges file statuses.
* @param toStatuses target file statuses
* @param fromStatuses source file statuses
* @return merged array of unique file statuses
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolveIntermediate,org.apache.hadoop.fs.FileContext:resolveIntermediate(org.apache.hadoop.fs.Path),2353,2361,"/**
* Resolves a file path to its status.
* @param f the input file path
* @return FileStatus object representing the resolved file
* @throws IOException if an I/O error occurs
*/","* Resolves all symbolic links in the specified path leading up 
   * to, but not including the final path component.
   * @param f path to resolve
   * @return the new path object.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getReplication,org.apache.hadoop.fs.FileSystem:getReplication(org.apache.hadoop.fs.Path),1603,1606,"/**
 * Calls m1 on the source path and returns its m2 result.
 * @param src source file path
 * @return short value from m2 of m1's result
 * @throws IOException if an I/O error occurs
 */","* Get the replication factor.
   *
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @param src file name
   * @return file replication
   * @throws FileNotFoundException if the path does not resolve.
   * @throws IOException an IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getReplication,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getReplication(),499,502,"/**
 * Delegates call to realStatus's m1 method.
 * @return result of realStatus.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getReplication,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getReplication(),75,78,"/**
 * Calls and returns result from m1 of myFs.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBlockSize,org.apache.hadoop.fs.FileSystem:getBlockSize(org.apache.hadoop.fs.Path),2742,2745,"/**
 * Deprecated method to get file size.
 * @param f Path to the file
 * @return Size of the file in bytes
 * @throws IOException if an I/O error occurs
 */","* Get the block size for a particular file.
   * @param f the filename
   * @return the number of bytes in a block
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @throws FileNotFoundException if the path is not present
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getBlockSize,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getBlockSize(),494,497,"/**
 * Delegates to realStatus's m1 method.
 * @return result of realStatus.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getBlockSize,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getBlockSize(),70,73,"/**
* Calls m1 on myFs.
* @return result of myFs.m1()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getAccessTime,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getAccessTime(),509,512,"/**
 * Delegates to the underlying realStatus instance.
 * @return Result of calling m1 on realStatus
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getAccessTime,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getAccessTime(),85,88,"/**
 * Delegates to myFs for execution of m1.
 * @return result from myFs.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getPermission,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPermission(),514,517,"/**
 * Retrieves file system permissions.
 * @return FsPermission object representing current permissions
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getPermission,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getPermission(),90,93,"/**
 * Retrieves file system permissions.
 * @return FsPermission object representing current permissions
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,stashOriginalFilePermissions,org.apache.hadoop.security.alias.KeyStoreProvider:stashOriginalFilePermissions(),73,79,"/**
* Retrieves file status and updates permissions.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,isPermissionLoaded,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:isPermissionLoaded(),927,929,"/**
 * Checks if mask condition is met.
 * @return true if condition is not met, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getOwner,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getOwner(),519,522,"/**
 * Delegates to realStatus's m1 method.
 * @return Result from realStatus's m1
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getOwner,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getOwner(),95,98,"/**
 * Calls m1 method of myFs.
 * @return Result from myFs.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getGroup,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getGroup(),524,527,"/**
 * Delegates to realStatus's m1 method.
 * @return Result of calling m1 on realStatus
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getGroup,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getGroup(),100,103,"/**
 * Delegates call to myFs's m1 method.
 * @return Result of myFs.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,msync,org.apache.hadoop.fs.HarFileSystem:msync(),661,664,"/**
 * Delegates call to underlying file system.
 * @throws IOException if an I/O error occurs
 * @throws UnsupportedOperationException if operation is not supported
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,msync,org.apache.hadoop.fs.FilterFileSystem:msync(),465,468,"/**
 * Delegates call to fs.m1().
 * Throws IOException or UnsupportedOperationException as needed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.HarFileSystem:getDefaultReplication(),1294,1298,"/**
 * Calls deprecated method m1 from fs.
 * @return result of fs.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),2785,2787,"/**
 * Calls another method m1 without parameters.
 * @param path unused parameter
 * @return result of calling m1()
 */","* Get the default replication for a path.
   * The given path will be used to locate the actual FileSystem to query.
   * The full path does not have to exist.
   * @param path of the file
   * @return default replication for the path's filesystem",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication(),431,434,"/**
 * Calls and returns result of fs.m1().
 * @return Result from fs.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,cleanUp,org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReference:cleanUp(),4148,4159,"/**
 * Updates statistics with provided data.
 * Synchronizes access to shared resources.
 */","* Performs clean-up action when the associated thread is garbage
       * collected.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,<init>,"org.apache.hadoop.fs.FsUrlConnection:<init>(org.apache.hadoop.conf.Configuration,java.net.URL)",48,53,"/**
* Constructs an FsUrlConnection with configuration and URL.
* @param conf Configuration object
* @param url URL to connect to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,validatePositionedReadArgs,"org.apache.hadoop.fs.FSInputStream:validatePositionedReadArgs(long,byte[],int,int)",102,116,"/**
 * Masks data in a buffer at a specified position.
 * @param position starting position in the buffer
 * @param buffer byte array containing data
 * @param offset offset within the buffer to start masking
 * @param length number of bytes to mask
 * @throws EOFException if position is negative
 * @throws IndexOutOfBoundsException if length exceeds available buffer space
 */","* Validation code, available for use in subclasses.
   * @param position position: if negative an EOF exception is raised
   * @param buffer destination buffer
   * @param offset offset within the buffer
   * @param length length of bytes to read
   * @throws EOFException if the position is negative
   * @throws IndexOutOfBoundsException if there isn't space for the amount of
   * data requested.
   * @throws IllegalArgumentException other arguments are invalid.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkUploadId,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkUploadId(byte[]),80,85,"/**
* Validates upload ID.
* @param uploadId unique identifier for the upload
* @throws IllegalArgumentException if uploadId is null or empty
*/","* Utility method to validate uploadIDs.
   * @param uploadId Upload ID
   * @throws IllegalArgumentException invalid ID",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPartHandles,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPartHandles(java.util.Map),92,100,"/**
 * Validates and processes part handles.
 * @param partHandles map of part indices to PartHandle objects
 */","* Utility method to validate partHandles.
   * @param partHandles handles
   * @throws IllegalArgumentException if the parts are invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/PathCapabilitiesSupport.java,validatePathCapabilityArgs,"org.apache.hadoop.fs.impl.PathCapabilitiesSupport:validatePathCapabilityArgs(org.apache.hadoop.fs.Path,java.lang.String)",42,49,"/**
* Masks a capability for the given file path.
* @param path file path to mask capability for
* @param capability capability string to be masked
* @return masked capability string
*/","* Validate the arguments to
   * {@link PathCapabilities#hasPathCapability(Path, String)}.
   * @param path path to query the capability of.
   * @param capability non-null, non-empty string to query the path for support.
   * @return the string to use in a switch statement.
   * @throws IllegalArgumentException if a an argument is invalid.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,<init>,"org.apache.hadoop.service.launcher.InterruptEscalator:<init>(org.apache.hadoop.service.launcher.ServiceLauncher,int)",74,78,"/**
 * Initializes an InterruptEscalator with a service launcher and shutdown time.
 * @param owner the ServiceLauncher instance that owns this escalator
 * @param shutdownTimeMillis the time in milliseconds before escalation
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,<init>,"org.apache.hadoop.service.launcher.IrqHandler:<init>(java.lang.String,org.apache.hadoop.service.launcher.IrqHandler$Interrupted)",78,83,"/**
 * Initializes an IrqHandler with a name and handler.
 * @param name unique identifier for the handler
 * @param handler function to handle interrupts
 */","* Create an IRQ handler bound to the specific interrupt.
   * @param name signal name
   * @param handler handler",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,partition,"org.apache.hadoop.util.Lists:partition(java.util.List,int)",269,284,"/**
* Partitions a list into sublists of a specified size.
* @param originalList the list to be partitioned
* @param pageSize the size of each sublist
* @return a list of sublists containing the partitioned elements
*/","* Returns consecutive sub-lists of a list, each of the same size
   * (the final list may be smaller).
   * @param originalList original big list.
   * @param pageSize desired size of each sublist ( last one
   *                 may be smaller)
   * @param <T> Generics Type.
   * @return a list of sub lists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,<init>,"org.apache.hadoop.util.JsonSerialization:<init>(java.lang.Class,boolean,boolean)",105,113,"/**
* Initializes JSON serialization with specified class type and configuration.
* @param classType the target class for serialization
* @param failOnUnknownProperties if true, throws exception on unknown properties
* @param pretty if true, formats output with indentation
*/","* Create an instance bound to a specific type.
   * @param classType class to marshall
   * @param failOnUnknownProperties fail if an unknown property is encountered.
   * @param pretty generate pretty (indented) output?",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,fetch,"org.apache.hadoop.fs.FileSystemStorageStatistics:fetch(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData,java.lang.String)",86,116,"/**
 * Retrieves statistics value based on key.
 * @param data StatisticsData object containing metrics
 * @param key metric identifier
 * @return Long value of the metric or null if key is invalid
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,<init>,"org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)",47,54,"/**
 * Constructs storage statistics from I/O statistics.
 * @param name unique identifier for the storage
 * @param scheme storage scheme used
 * @param ioStatistics I/O statistics to analyze
 */","* Instantiate.
   * @param name storage statistics name.
   * @param scheme FS scheme; may be null.
   * @param ioStatistics IOStatistics source.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/EmptyStorageStatistics.java,<init>,org.apache.hadoop.fs.EmptyStorageStatistics:<init>(java.lang.String),28,30,"/**
 * Initializes an EmptyStorageStatistics with a given name.
 * @param name the storage statistics name
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,<init>,"org.apache.hadoop.fs.UnionStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.StorageStatistics[])",79,92,"/**
 * Initializes UnionStorageStatistics with a name and array of StorageStatistics.
 * @param name unique identifier for the statistics
 * @param stats array of StorageStatistics objects
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getScheme,org.apache.hadoop.fs.FileSystemStorageStatistics:getScheme(),127,130,"/**
 * Delegates to stats for processing.
 * @return result of stats.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatistics,org.apache.hadoop.fs.FileSystem:getStatistics(),4560,4567,"/**
* Deprecated function to map statistics by key.
* @return Map of statistics keyed by some identifier
*/","* Get the Map of Statistics object indexed by URI Scheme.
   * @return a Map having a key as URI scheme and value as Statistics object
   * @deprecated use {@link #getGlobalStorageStatistics()}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFs:listXAttrs(org.apache.hadoop.fs.Path),846,851,"/**
 * Resolves and returns a list of file names from the given path.
 * @param path file system path to resolve
 * @return List of file names or empty list if no files found
 * @throws IOException if an I/O error occurs during resolution
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listXAttrs,org.apache.hadoop.fs.FilterFs:listXAttrs(org.apache.hadoop.fs.Path),387,390,"/**
* Reads file content from given path.
* @param path file path to read
* @return list of lines in the file
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",1275,1284,"/**
* Creates a file output stream with specified parameters.
* @param f path to the file
* @param permission file permissions
* @param flags create flags
* @param bufferSize buffer size in bytes
* @param replication replication factor
* @param blockSize block size in bytes
* @param progress progress monitor
* @return FSDataOutputStream for writing
* @throws IOException if an I/O error occurs
*/","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * @param f the file name to open
   * @param permission file permission
   * @param flags {@link CreateFlag}s to use for this stream.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,create,"org.apache.hadoop.fs.FilterFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",201,212,"/**
* Creates a file output stream for writing to a specified path.
* @param f file path
* @param permission file permissions
* @param flags options for creating the file (e.g., overwrite)
* @param bufferSize buffer size for I/O operations
* @param replication number of data replicas
* @param blockSize block size in bytes
* @param progress callback for reporting write progress
* @param checksumOpt checksum option for data integrity
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1456,1463,"/**
* Creates or overwrites a file with specified permissions and options.
* @param f file path
* @param permission file permissions
* @param overwrite flag to overwrite existing file
* @param bufferSize buffer size for I/O operations
* @param replication number of replications
* @param blockSize block size for file storage
* @param progress progress tracker
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/","* Opens an FSDataOutputStream at the indicated Path with write-progress
   * reporting. Same as create(), except fails if parent directory doesn't
   * already exist.
   * @param f the file name to open
   * @param permission file permission
   * @param overwrite if a file with this name already exists, then if true,
   * the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FilterFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",222,229,"/**
* Creates a file with specified parameters.
* @param f file path
* @param permission file permissions
* @param flags creation flags
* @param bufferSize buffer size for I/O operations
* @param replication number of data replicas
* @param blockSize block size in bytes
* @param progress progress monitor
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String),24,26,"/**
 * Constructs an exception indicating access denial to a specified path.
 * @param path the path that access is denied to
 */",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String),26,28,"/**
 * Constructs a PathPermissionException with a specified path.
 * @param path the path where the operation is not permitted
 */",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String)",34,36,"/**
 * Constructs a PathPermissionException with a specified path and error message.
 * @param path the affected file or directory path
 * @param error the error description
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String),26,28,"/**
 * Constructs a PathNotFoundException with the specified path.
 * @param path the missing file or directory path
 */",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String)",34,36,"/**
 * Constructs a PathNotFoundException with specified path and error message.
 * @param path the invalid path that caused the exception
 * @param error additional error details
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathExistsException.java,<init>,org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String),26,28,"/**
 * Constructs an exception indicating that a file already exists.
 * @param path the path of the existing file
 */",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathExistsException.java,<init>,"org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String,java.lang.String)",30,32,"/**
 * Constructs a PathExistsException with a specific path and error message.
 * @param path the file or directory path that exists
 * @param error additional error details
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,<init>,org.apache.hadoop.fs.PathIOException:<init>(java.lang.String),43,45,"/**
 * Constructs an IOException with a specified path.
 * @param path file path associated with the exception
 */","* Constructor a generic I/O error exception
   *  @param path for the exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ClosedIOException.java,<init>,"org.apache.hadoop.fs.ClosedIOException:<init>(java.lang.String,java.lang.String)",36,38,"/**
 * Constructs a ClosedIOException with the specified file path and error message.
 * @param path the file path where the exception occurred
 * @param message the detail message of the exception
 */","* Appends the custom error-message to the default error message.
   * @param path path that encountered the closed resource.
   * @param message custom error message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getThisBuilder,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getThisBuilder(),102,102,"/**
 * Abstract method to be implemented by subclasses.
 * @return an instance of type B
 */",* Return the concrete implementation of the builder instance.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.permission.FsPermission),43,47,"/**
* Converts FsPermission to FsPermissionProto.
* @param p file system permission object
* @return FsPermissionProto representation
* @throws IOException if conversion fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkReturnValue,"org.apache.hadoop.fs.FileUtil:checkReturnValue(boolean,java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1510,1518,"/**
* Throws an IOException if the permission setting failed.
* @param rv result of permission setting operation
* @param p file whose permissions are being set
* @param permission desired file permissions
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,write,org.apache.hadoop.fs.permission.FsPermission:write(java.io.DataOutput),179,183,"/**
 * Writes masked data to output.
 * @param out DataOutput stream to write to
 * @deprecated Use alternative method instead
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toExtendedShort,org.apache.hadoop.fs.permission.FsPermission:toExtendedShort(),240,243,"/**
 * Returns masked value using function m1.
 * @deprecated Use alternative method instead.
 */","* Encodes the object to a short.  Unlike {@link #toShort()}, this method may
   * return values outside the fixed range 00000 - 01777 if extended features
   * are encoded into this permission, such as the ACL bit.
   *
   * @return short extended short representation of this permission",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toOctal,org.apache.hadoop.fs.permission.FsPermission:toOctal(),251,255,"/**
* Masks and converts integer to short.
* @return masked value as short
*/","* Returns the FsPermission in an octal format.
   *
   * @return short Unlike {@link #toShort()} which provides a binary
   * representation, this method returns the standard octal style permission.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,hashCode,org.apache.hadoop.fs.permission.FsPermission:hashCode(),269,270,"/**
 * Returns the mask value by calling m1.
 * @return integer mask value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringInterner.java,internStringsInArray,org.apache.hadoop.util.StringInterner:internStringsInArray(java.lang.String[]),81,86,"/**
* Masks each string in the input array.
* @param strings array of strings to be masked
* @return array of masked strings
*/","* Interns all the strings in the given array in place,
   * returning the same array.
   *
   * @param strings strings.
   * @return internStringsInArray.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleStartProperty,org.apache.hadoop.conf.Configuration$Parser:handleStartProperty(),3267,3294,"/**
 * Resets configuration properties and reads new values from reader.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,isDir,org.apache.hadoop.fs.FileStatus:isDir(),240,243,"/**
 * Deprecated method that calls another function.
 * @deprecated Use alternative method instead.
 * @return Result of calling m1()
 */","* Old interface, instead use the explicit {@link FileStatus#isFile()},
   * {@link FileStatus#isDirectory()}, and {@link FileStatus#isSymlink()}
   * @return true if this is a directory.
   * @deprecated Use {@link FileStatus#isFile()},
   * {@link FileStatus#isDirectory()}, and {@link FileStatus#isSymlink()}
   * instead.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,isDirectory,org.apache.hadoop.fs.ChecksumFs:isDirectory(org.apache.hadoop.fs.Path),446,453,"/**
* Checks if a file exists and is accessible.
* @param f file path to check
* @return true if file exists, false otherwise
*/","True iff the named path is a directory.
   * Note: Avoid using this method. Instead reuse the FileStatus 
   * returned by getFileStatus() or listStatus() methods.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processPath,org.apache.hadoop.fs.shell.CopyCommands$Merge:processPath(org.apache.hadoop.fs.shell.PathData),133,143,"/**
* Processes PathData based on its status and mask.
* @param src source PathData object to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,isPathRecursable,org.apache.hadoop.fs.shell.Command:isPathRecursable(org.apache.hadoop.fs.shell.PathData),418,420,"/**
 * Checks mask status of a PathData item.
 * @param item PathData object to check
 * @return true if mask is set, false otherwise
 * @throws IOException if an I/O error occurs
 */","* Determines whether a {@link PathData} item is recursable. Default
   * implementation is to recurse directories but can be overridden to recurse
   * through symbolic links.
   *
   * @param item
   *          a {@link PathData} object
   * @return true if the item is recursable, false otherwise
   * @throws IOException
   *           if anything goes wrong in the user-implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,getAclEntries,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:getAclEntries(org.apache.hadoop.fs.shell.PathData),283,289,"/**
* Determines ACL entries based on conditions.
* @param item PathData object to evaluate
* @return List of AclEntry objects
*/","* Returns the ACL entries to use in the API call for the given path.  For a
     * recursive operation, returns all specified ACL entries if the item is a
     * directory or just the access ACL entries if the item is a file.  For a
     * non-recursive operation, returns all specified ACL entries.
     *
     * @param item PathData path to check
     * @return List<AclEntry> ACL entries to use in the API call",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPathArgument,org.apache.hadoop.fs.shell.FsUsage$Du:processPathArgument(org.apache.hadoop.fs.shell.PathData),209,217,"/**
* Processes PathData item conditionally.
* @param item the PathData to process
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,isDirectory,org.apache.hadoop.fs.FileSystem:isDirectory(org.apache.hadoop.fs.Path),1877,1884,"/**
 * Checks if file exists and calls m2 on its result.
 * @param f file path to check
 * @return true if file exists and m2 returns true, otherwise false
 */","True iff the named path is a directory.
   * Note: Avoid using this method. Instead reuse the FileStatus
   * returned by getFileStatus() or listStatus() methods.
   *
   * @param f path to check
   * @throws IOException IO failure
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @return if f is directory true, not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isDirectory(),484,487,"/**
 * Delegates to realStatus's m1 method.
 * @return result of realStatus.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isDirectory,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isDirectory(),60,63,"/**
 * Checks condition using delegate method.
 * @return true if condition met, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ChmodParser.java,applyNewPermission,org.apache.hadoop.fs.permission.ChmodParser:applyNewPermission(org.apache.hadoop.fs.FileStatus),48,54,"/**
* Determines mask for file permissions.
* @param file FileStatus object representing the file
* @return Short value of the determined mask
*/","* Apply permission against specified file and determine what the
   * new mode would be
   * @param file File against which to apply mode
   * @return File's new mode if applied.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,isFile,org.apache.hadoop.fs.FileStatus:isFile(),220,222,"/**
 * Checks if both m1 and m2 are false.
 * @return true if both methods return false, otherwise false
 */","* Is this a file?
   * @return true if this is a file",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,getSymlink,org.apache.hadoop.fs.FileStatus:getSymlink(),393,398,"/**
 * Returns the symlink path.
 * @throws IOException if the path is not a symbolic link
 */","* @return The contents of the symbolic link.
   *
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isSymlink(),489,492,"/**
 * Delegates to realStatus's m1 method.
 * @return result of realStatus.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isSymlink,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isSymlink(),65,68,"/**
 * Delegates call to myFs.m1().
 * @return Result of myFs.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getFileLength,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getFileLength(),270,275,"/**
* Returns the length of the file.
* @return Length of the file in bytes
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link is unresolved
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getFileLength,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getFileLength(),330,335,"/**
* Returns the file length, fetching it if not already known.
* @return Length of the file in bytes
* @throws IOException if an I/O error occurs
*/","* Calculate length of file if not already cached.
     * @return file length.
     * @throws IOException any IOE.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,totalPartsLen,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:totalPartsLen(java.util.List),168,174,"/**
* Calculates total length of file parts.
* @param partHandles list of file path handles
* @return total length in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLength,org.apache.hadoop.fs.FileSystem:getLength(org.apache.hadoop.fs.Path),1912,1915,"/**
 * Masks file using specified path.
 * @param f file path
 * @return masked value as long
 * @deprecated Use alternative method instead
 */","* The number of bytes in a file.
   * @param f the path.
   * @return the number of bytes; 0 for a directory
   * @deprecated Use {@link #getFileStatus(Path)} instead.
   * @throws FileNotFoundException if the path does not resolve
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getLen,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getLen(),474,477,"/**
 * Delegates to realStatus's m1 method.
 * @return result of realStatus.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getLen,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getLen(),50,53,"/**
 * Calls and returns the result of myFs.m1().
 * @return The result from myFs.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String,byte)",82,91,"/**
* Constructs FsServerDefaults with specified parameters.
* @param blockSize size of each block in bytes
* @param bytesPerChecksum number of bytes per checksum
* @param writePacketSize size of data packet for writing
* @param replication default replication factor
* @param fileBufferSize buffer size for file operations
* @param encryptDataTransfer flag to enable data encryption transfer
* @param trashInterval interval in milliseconds for trash cleanup
* @param checksumType type of checksum algorithm
* @param keyProviderUri URI for key provider
* @param storagepolicy default storage policy
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:getStoragePolicy(org.apache.hadoop.fs.Path),418,422,"/**
 * Retrieves block storage policy for a given path.
 * @param src source file path
 * @return BlockStoragePolicySpi object
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:getStoragePolicy(org.apache.hadoop.fs.Path),914,919,"/**
* Retrieves block storage policy for a given path.
* @param src source file path
* @return BlockStoragePolicySpi object or null if not found
*/","* Retrieve the storage policy for a given file or directory.
   *
   * @param src file or directory path.
   * @return storage policy for give file.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getStoragePolicy,org.apache.hadoop.fs.FilterFs:getStoragePolicy(org.apache.hadoop.fs.Path),432,436,"/**
 * Retrieves block storage policy for a given path.
 * @param src source file path
 * @return BlockStoragePolicySpi object
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",816,822,"/**
 * Sets extended attribute on a file.
 * @param path file path
 * @param name attribute name
 * @param value attribute value
 * @param flag flags for setting attribute
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,setXAttr,"org.apache.hadoop.fs.AbstractFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",1358,1362,"/**
 * Sets an extended attribute with specified flags.
 * @param path file path
 * @param name attribute name
 * @param value attribute value
 */","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setXAttr,"org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",365,369,"/**
 * Sets an extended attribute on a file.
 * @param path the file path
 * @param name the attribute name
 * @param value the attribute value
 * @param flag flags for setting the attribute
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,blockSize,org.apache.hadoop.fs.Options$CreateOpts:blockSize(long),46,48,"/**
 * Creates a BlockSize object with specified block size.
 * @param bs block size in bytes
 * @return BlockSize object initialized with given size
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,bufferSize,org.apache.hadoop.fs.Options$CreateOpts:bufferSize(int),49,51,"/**
 * Creates a BufferSize instance with specified size.
 * @param bs buffer size value
 * @return BufferSize object initialized with bs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,repFac,org.apache.hadoop.fs.Options$CreateOpts:repFac(short),52,54,"/**
 * Creates a ReplicationFactor instance from a short value.
 * @param rf replication factor as a short
 * @return ReplicationFactor object initialized with the given value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,bytesPerChecksum,org.apache.hadoop.fs.Options$CreateOpts:bytesPerChecksum(short),55,57,"/**
* Creates a BytesPerChecksum instance with a given CRC value.
* @param crc checksum value to be used
* @return BytesPerChecksum object initialized with the provided CRC
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,checksumParam,org.apache.hadoop.fs.Options$CreateOpts:checksumParam(org.apache.hadoop.fs.Options$ChecksumOpt),58,61,"/**
 * Creates a ChecksumParam from a ChecksumOpt.
 * @param csumOpt options for checksum calculation
 * @return ChecksumParam object initialized with provided options
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,progress,org.apache.hadoop.fs.Options$CreateOpts:progress(org.apache.hadoop.util.Progressable),62,64,"/**
 * Creates a new Progress instance based on the given Progressable.
 * @param prog Progressable object to base the new Progress on
 * @return New Progress object initialized with prog's data
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,createParent,org.apache.hadoop.fs.Options$CreateOpts:createParent(),68,70,"/**
* Creates a CreateParent instance with mask enabled.
* @return CreateParent object with mask set to true
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,donotCreateParent,org.apache.hadoop.fs.Options$CreateOpts:donotCreateParent(),71,73,"/**
* Creates a new instance of CreateParent with mask set to false.
* @return CreateParent object with mask disabled
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,"org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",32,36,"/**
 * Constructs an exception indicating access denial to a specified path.
 * @param path the denied path
 * @param error description of the error
 * @param cause underlying cause of the exception
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",38,42,"/**
 * Constructs a PathPermissionException with a specified path, error message, and cause.
 * @param path the problematic file path
 * @param error the error description
 * @param cause the underlying exception cause
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",38,42,"/**
 * Constructs a PathNotFoundException with specified path, error message, and cause.
 * @param path the missing path
 * @param error the error message
 * @param cause the underlying cause of the exception
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,<init>,"org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.Throwable)",52,54,"/**
 * Constructs an IOException with a specified file path and cause.
 * @param path the file path associated with the error
 * @param cause the underlying cause of the exception
 */","* Appends the text of a Throwable to the default error message
   * @param path for the exception
   * @param cause a throwable to extract the error message",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFs:removeAcl(org.apache.hadoop.fs.Path),794,800,"/**
* Resolves and processes a file system path.
* @param path the path to process
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeAcl,org.apache.hadoop.fs.FilterFs:removeAcl(org.apache.hadoop.fs.Path),344,347,"/**
 * Delegates file processing to another filesystem.
 * @param path file path to process
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,seek,org.apache.hadoop.fs.AvroFSInput:seek(long),75,78,"/**
 * Delegates call to stream's m1 method with the given parameter.
 * @param p long parameter passed to stream's m1
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)",924,937,"/**
 * Initializes a new HarFsInputStream.
 * @param fs FileSystem instance
 * @param path Path to the file
 * @param start Starting position in the file
 * @param length Length of the stream
 * @param bufferSize Buffer size for reading
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,skip,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:skip(long),1006,1022,"/**
 * Masks and processes data from a stream.
 * @param n amount of data to process
 * @return processed data length or 0 if none
 * @throws IOException on I/O error
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,seek,org.apache.hadoop.io.SequenceFile$Reader:seek(long),2818,2824,"/**
* Moves input stream to specified position.
* @param position new position for the stream
* @throws IOException if an I/O error occurs
*/","* Set the current byte position in the input file.
     *
     * <p>The position passed must be a position returned by {@link
     * SequenceFile.Writer#getLength()} when writing this file.  To seek to an arbitrary
     * position, use {@link SequenceFile.Reader#sync(long)}. </p>
     *
     * @param position input position.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,"org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[],int,int)",87,106,"/**
 * Reads bytes from input stream into buffer.
 * @param b byte array to store read data
 * @param off offset in the array to start storing
 * @param len number of bytes to read
 * @return number of bytes read or -1 if end of stream
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,tell,org.apache.hadoop.fs.AvroFSInput:tell(),80,83,"/**
 * Retrieves a mask value from the stream.
 * @throws IOException if an I/O error occurs
 * @return the mask value as a long
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,available,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:available(),939,946,"/**
* Returns available bytes for reading, capped at Integer.MAX_VALUE.
* @return number of bytes available or Integer.MAX_VALUE if more than that
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readRecordLength,org.apache.hadoop.io.SequenceFile$Reader:readRecordLength(),2538,2558,"/**
* Reads and validates next segment length.
* @return length of the next segment or -1 if end reached
* @throws IOException if file is corrupt
*/","* Read and return the next record length, potentially skipping over 
     * a sync block.
     * @return the length of the next record or -1 if there is no next record
     * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getPosition,org.apache.hadoop.io.SequenceFile$Reader:getPosition(),2873,2875,"/**
 * Returns masked value from input.
 * @throws IOException if an I/O error occurs
 */","* @return Return the current byte position in the input file.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setOwner,"org.apache.hadoop.fs.DelegateToFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",214,219,"/**
* Calls m1 and fsImpl.m2 to process file with user and group information.
* @param f file path to be processed
* @param username user name associated with the file
* @param groupname group name associated with the file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processPath,org.apache.hadoop.fs.FsShellPermissions$Chown:processPath(org.apache.hadoop.fs.shell.PathData),173,190,"/**
 * Updates file ownership if conditions are met.
 * @param item PathData object representing the file
 * @throws IOException if there's an error changing ownership
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setOwner,"org.apache.hadoop.fs.FilterFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",533,537,"/**
 * Delegates file operation to underlying filesystem.
 * @param p path to the file
 * @param username user performing the operation
 * @param groupname group associated with the user
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,registerExpression,org.apache.hadoop.fs.shell.find.ExpressionFactory:registerExpression(java.lang.Class),59,69,"/**
 * Registers an expression class with the factory.
 * @param expressionClass the class to be registered
 */","* Invokes ""static void registerExpression(FindExpressionFactory)"" on the
   * given class. This method abstracts the contract between the factory and the
   * expression class. Do not assume that directly invoking registerExpression
   * on the given class will have the same effect.
   *
   * @param expressionClass
   *          class to allow an opportunity to register",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,registerCommands,org.apache.hadoop.fs.shell.CommandFactory:registerCommands(java.lang.Class),64,72,"/**
* Registers commands using the provided registrar class.
* @param registrarClass class responsible for command registration
*/","* Invokes ""static void registerCommands(CommandFactory)"" on the given class.
   * This method abstracts the contract between the factory and the command
   * class.  Do not assume that directly invoking registerCommands on the
   * given class will have the same effect.
   * @param registrarClass class to allow an opportunity to register",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doResponse,"org.apache.hadoop.ipc.Server$RpcCall:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)",1308,1328,"/**
* Handles RPC error and response.
* @param t Throwable object representing the error
* @param status RpcStatusProto indicating the status
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path),436,443,"/**
 * Retrieves file status for a given path.
 * @param f the path to the file
 * @return FileStatus object representing the file's metadata
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if the file does not exist
 * @throws UnsupportedFileSystemException if the file system is unsupported
 * @throws IOException for other I/O errors
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getUri,org.apache.hadoop.fs.FilterFs:getUri(),179,182,"/**
 * Delegates to myFs's m1 method.
 * @return URI result from myFs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setSymlink(org.apache.hadoop.fs.Path),544,547,"/**
 * Delegates call to realStatus's m1 method.
 * @param p file path to process
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(),968,972,"/**
* Reads a single byte from the input source.
* @return the byte value as an integer or -1 if no more bytes are available
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(byte[]),978,982,"/**
 * Calls overloaded method to process byte array.
 * @param b byte array to process
 * @return result of processing
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,seek,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:seek(long),1029,1034,"/**
 * Updates position and delegates to underlying stream.
 * @param pos new position offset
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(long,byte[],int,int)",1058,1071,"/**
 * Reads bytes from a specified position into a byte array.
 * @param pos starting position in the stream
 * @param b destination byte array
 * @param offset offset within the buffer
 * @param length number of bytes to read
 * @return number of bytes read or -1 if no more bytes are available
 * @throws IOException if an I/O error occurs
 */",* implementing position readable.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,readFully,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:readFully(long,byte[],int,int)",1076,1087,"/**
* Reads data from a stream.
* @param pos starting position in the stream
* @param b byte array to store the read data
* @param offset offset within the byte array to start storing data
* @param length number of bytes to read
* @throws IOException if an I/O error occurs or insufficient data available
*/",* position readable again.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,setReadahead,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setReadahead(java.lang.Long),1089,1092,"/**
 * Delegates read-ahead operation to underlying stream.
 * @param readahead number of bytes to read ahead
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,setDropBehind,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setDropBehind(java.lang.Boolean),1094,1097,"/**
 * Delegates the call to m1 on the underlying stream.
 * @param dropBehind flag indicating whether to drop behind
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,getCurrentTrashDir,org.apache.hadoop.fs.Trash:getCurrentTrashDir(org.apache.hadoop.fs.Path),198,200,"/**
 * Applies trash policy to given file path.
 * @param path file path to be processed
 * @return modified path after applying trash policy
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,completed,"org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:completed(java.lang.Integer,java.lang.Integer)",365,383,"/**
* Processes read result and updates file buffers.
* @param result number of bytes read, -1 if EOF
* @param r index representing file or resource
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobExpander.java,expandLeftmost,org.apache.hadoop.fs.GlobExpander:expandLeftmost(org.apache.hadoop.fs.GlobExpander$StringWithOffset),86,146,"/**
 * Expands a file pattern with offsets into multiple alternatives.
 * @param filePatternWithOffset pattern and offset information
 * @return list of expanded patterns or null if invalid
 * @throws IOException if illegal pattern is encountered
 */","* Expand the leftmost outer curly bracket pair containing a
   * slash character (""/"") in <code>filePattern</code>.
   * @param filePatternWithOffset
   * @return expanded file patterns
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatusBatch,"org.apache.hadoop.fs.FileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])",2060,2068,"/**
* Retrieves directory entries with a mask.
* @param f file path
* @param token security token
* @return DirectoryEntries object containing listed files
* @throws FileNotFoundException if file not found
* @throws IOException on I/O errors
*/","* Given an opaque iteration token, return the next batch of entries in a
   * directory. This is a private API not meant for use by end users.
   * <p>
   * This method should be overridden by FileSystem subclasses that want to
   * use the generic {@link FileSystem#listStatusIterator(Path)} implementation.
   * @param f Path to list
   * @param token opaque iteration token returned by previous call, or null
   *              if this is the first call.
   * @return directory entries.
   * @throws FileNotFoundException when the path does not exist.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/XAttrCodec.java,encodeValue,"org.apache.hadoop.fs.XAttrCodec:encodeValue(byte[],org.apache.hadoop.fs.XAttrCodec)",109,119,"/**
* Masks byte array value using specified encoding.
* @param value byte array to be masked
* @param encoding type of encoding (HEX, BASE64, or default)
* @return masked string representation
* @throws IOException if UTF-8 conversion fails
*/","* Encode byte[] value to string representation with encoding. 
   * Values encoded as text strings are enclosed in double quotes (\""), 
   * while strings encoded as hexadecimal and base64 are prefixed with 
   * 0x and 0s, respectively.
   * @param value byte[] value
   * @param encoding encoding.
   * @return String string representation of value
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2076,2085,"/**
* Filters files in a directory based on given criteria.
* @param results list to store filtered file statuses
* @param f path to the directory being processed
* @param filter criteria for filtering files
* @throws FileNotFoundException if file not found
* @throws IOException if I/O error occurs
*/","* Filter files/directories in the given path using the user-supplied path
   * filter. Results are added to the given array <code>results</code>.
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,<init>,"org.apache.hadoop.metrics2.MetricsTag:<init>(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",43,46,"/**
* Initializes a new MetricsTag with given info and value.
* @param info MetricsInfo object containing tag details
* @param value string value of the tag
*/","* Construct the tag with name, description and value
   * @param info  of the tag
   * @param value of the tag",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounter.java,<init>,org.apache.hadoop.metrics2.lib.MutableCounter:<init>(org.apache.hadoop.metrics2.MetricsInfo),35,37,"/**
 * Initializes a new MutableCounter with given metrics information.
 * @param info MetricsInfo object containing counter details
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGauge.java,<init>,org.apache.hadoop.metrics2.lib.MutableGauge:<init>(org.apache.hadoop.metrics2.MetricsInfo),35,37,"/**
 * Initializes a new MutableGauge with given metrics information.
 * @param info MetricsInfo object containing gauge details
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,<init>,org.apache.hadoop.metrics2.lib.MutableRates:<init>(org.apache.hadoop.metrics2.lib.MetricsRegistry),48,50,"/**
 * Initializes MutableRates with a MetricsRegistry.
 * @param registry the MetricsRegistry to use
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsInfoImpl.java,<init>,"org.apache.hadoop.metrics2.lib.MetricsInfoImpl:<init>(java.lang.String,java.lang.String)",34,37,"/**
* Initializes metrics info with name and description.
* @param name unique metric identifier
* @param description brief explanation of the metric
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/AbstractMetric.java,<init>,org.apache.hadoop.metrics2.AbstractMetric:<init>(org.apache.hadoop.metrics2.MetricsInfo),41,43,"/**
 * Initializes an AbstractMetric with given MetricsInfo.
 * @param info MetricsInfo object containing metric details
 */","* Construct the metric
   * @param info  about the metric",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String,java.lang.String)",389,403,"/**
* Retrieves delegation token from specified URL.
* @param url the target URL for token retrieval
* @param token initial token object to hold the result
* @param renewer user allowed to renew the token
* @param doAsUser user on behalf of whom the operation is performed
* @return updated token with delegation token or null if failed
* @throws IOException if an I/O error occurs
* @throws AuthenticationException if authentication fails
*/","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @param doAsUser the user to do as, which will be the token owner.
   * @return a delegation token.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",433,446,"/**
 * Authenticates and retrieves a delegation token.
 * @param url target URL for authentication
 * @param token authentication token
 * @param doAsUser user to authenticate as
 * @return delegation token ID
 * @throws IOException if an I/O error occurs
 * @throws AuthenticationException if authentication fails
 */","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",472,484,"/**
* Authenticates using Kerberos delegation token.
* @param url target URL for authentication
* @param token security token containing delegation information
* @param doAsUser user to act on behalf of
* @throws IOException if an I/O error occurs during authentication
*/","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,<init>,"org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)",222,260,"/**
* Initializes a ValueQueue with specified parameters.
* @param numValues max number of values in the queue
* @param lowWatermark percentage threshold for refilling
* @param expiry time after which entries expire
* @param numFillerThreads number of threads to refill queues
* @param policy synchronization generation policy
* @param refiller strategy for filling the queue
*/","* Constructor takes the following tunable configuration parameters
   * @param numValues The number of values cached in the Queue for a
   *    particular key.
   * @param lowWatermark The ratio of (number of current entries/numValues)
   *    below which the <code>fillQueueForKey()</code> funciton will be
   *    invoked to fill the Queue.
   * @param expiry Expiry time after which the Key and associated Queue are
   *    evicted from the cache.
   * @param numFillerThreads Number of threads to use for the filler thread
   * @param policy The SyncGenerationPolicy to use when client
   *    calls ""getAtMost""
   * @param refiller implementation of the QueueRefiller",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Preconditions.java,checkNotNull,org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object),68,70,"/**
 * Validates and returns the object.
 * @param obj the object to validate
 * @return the validated object
 */","* <p>Preconditions that the specified argument is not {@code null},
   * throwing a NPE exception otherwise.
   *
   * <p>The message of the exception is
   * &quot;The validated object is null&quot;.</p>
   *
   * @param <T> the object type
   * @param obj  the object to check
   * @return the validated object
   * @throws NullPointerException if the object is {@code null}
   * @see #checkNotNull(Object, Object)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFs:getAclStatus(org.apache.hadoop.fs.Path),809,814,"/**
* Retrieves ACL status for a given path.
* @param path file system path
* @return AclStatus object representing the ACL permissions
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getAclStatus,org.apache.hadoop.fs.FilterFs:getAclStatus(org.apache.hadoop.fs.Path),354,357,"/**
 * Retrieves ACL status for a given path.
 * @param path file system path
 * @return AclStatus object representing ACLs
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobalStorageStatistics.java,put,"org.apache.hadoop.fs.GlobalStorageStatistics:put(java.lang.String,org.apache.hadoop.fs.GlobalStorageStatistics$StorageStatisticsProvider)",73,93,"/**
 * Retrieves or creates storage statistics.
 * @param name unique identifier for the storage
 * @param provider factory to create new statistics if needed
 * @return StorageStatistics object
 * @throws RuntimeException if provider returns null or incorrect stats
 */","* Create or return the StorageStatistics object with the given name.
   *
   * @param name        The storage statistics object name.
   * @param provider    An object which can create a new StorageStatistics
   *                      object if needed.
   * @return            The StorageStatistics object with the given name.
   * @throws RuntimeException  If the StorageStatisticsProvider provides a null
   *                           object or a new StorageStatistics object with the
   *                           wrong name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobalStorageStatistics.java,next,org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator:next(),127,139,"/**
* Retrieves and advances to the next storage statistics entry.
* @return current StorageStatistics or throws NoSuchElementException if no more entries
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,clearStatistics,org.apache.hadoop.fs.FileSystem:clearStatistics(),4610,4612,"/**
 * Calls m1 on GlobalStorageStatistics singleton.
 */",* Reset all statistics for all file systems.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell$UnknownCommandException:<init>(),410,410,"/**
 * Constructs an UnknownCommandException with no detail message.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,close,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:close(),258,266,"/**
* Marks the resource as closed and performs cleanup.
* @throws IOException if an I/O error occurs during cleanup
*/","* Close: closes any upload stream and byteArray provided in the
     * constructor.
     *
     * @throws IOException inherited exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,copyFileUnbuffered,"org.apache.hadoop.io.nativeio.NativeIO:copyFileUnbuffered(java.io.File,java.io.File)",1138,1161,"/**
* Masks a file by copying its contents to another file.
* @param src source file to be masked
* @param dst destination file for the masked content
* @throws IOException if an I/O error occurs during the process
*/","* Unbuffered file copy from src to dst without tainting OS buffer cache
   *
   * In POSIX platform:
   * It uses FileChannel#transferTo() which internally attempts
   * unbuffered IO on OS with native sendfile64() support and falls back to
   * buffered IO otherwise.
   *
   * It minimizes the number of FileChannel#transferTo call by passing the the
   * src file size directly instead of a smaller size as the 3rd parameter.
   * This saves the number of sendfile64() system call when native sendfile64()
   * is supported. In the two fall back cases where sendfile is not supported,
   * FileChannle#transferTo already has its own batching of size 8 MB and 8 KB,
   * respectively.
   *
   * In Windows Platform:
   * It uses its own native wrapper of CopyFileEx with COPY_FILE_NO_BUFFERING
   * flag, which is supported on Windows Server 2008 and above.
   *
   * Ideally, we should use FileChannel#transferTo() across both POSIX and Windows
   * platform. Unfortunately, the wrapper(Java_sun_nio_ch_FileChannelImpl_transferTo0)
   * used by FileChannel#transferTo for unbuffered IO is not implemented on Windows.
   * Based on OpenJDK 6/7/8 source code, Java_sun_nio_ch_FileChannelImpl_transferTo0
   * on Windows simply returns IOS_UNSUPPORTED.
   *
   * Note: This simple native wrapper does minimal parameter checking before copy and
   * consistency check (e.g., size) after copy.
   * It is recommended to use wrapper function like
   * the Storage#nativeCopyFileUnbuffered() function in hadoop-hdfs with pre/post copy
   * checks.
   *
   * @param src                  The source path
   * @param dst                  The destination path
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,closeStream,org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable),276,280,"/**
* Closes a Closeable resource safely.
* @param stream the resource to close
*/","* Closes the stream ignoring {@link Throwable}.
   * Must only be called in cleaning up from exception handlers.
   *
   * @param stream the Stream to close",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,closeStreams,org.apache.hadoop.io.IOUtils:closeStreams(java.io.Closeable[]),288,292,"/**
* Closes multiple Closeable resources.
* @param streams array of Closeable objects to close
*/","* Closes the streams ignoring {@link Throwable}.
   * Must only be called in cleaning up from exception handlers.
   *
   * @param streams the Streams to close",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,stop,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:stop(),207,218,"/**
* Stops the process and closes resources.
* @throws InterruptedException if thread is interrupted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,close,org.apache.hadoop.crypto.OpensslCtrCryptoCodec:close(),111,117,"/**
* Closes resources if applicable.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,close,org.apache.hadoop.crypto.random.OsSecureRandom:close(),120,126,"/**
* Masks the stream by logging and nullifying it.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,diskIoCheckWithoutNativeIo,org.apache.hadoop.util.DiskChecker:diskIoCheckWithoutNativeIo(java.io.File),283,302,"/**
 * Masks a file by writing a single byte and deleting it.
 * @param file the file to be masked
 * @throws IOException if an I/O error occurs
 */","* Try to perform some disk IO by writing to the given file
   * without using Native IO.
   *
   * @param file
   * @throws IOException if there was a non-retriable error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hflush,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hflush(),501,504,"/**
 * Calls method m1.
 * @throws IOException if an I/O error occurs in m1
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hsync,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hsync(),510,514,"/**
 * Calls methods to perform operations.
 * @throws IOException if an I/O error occurs
 */","* HSync calls sync on fhe file descriptor after a local flush() call.
     * @throws IOException failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,close,org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:close(),95,105,"/**
* Masks function and updates statistics.
* Uses key for naming, appends suffix on failure.
*/","* Set the finished time and then update the statistics.
   * If the operation failed then the key + .failures counter will be
   * incremented by one.
   * The operation min/mean/max values will be updated with the duration;
   * on a failure these will all be the .failures metrics.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,skip,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:skip(long),274,283,"/**
 * Skips bytes in the input stream.
 * @param n number of bytes to skip
 * @return actual number of bytes skipped
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(byte[],int,int)",479,488,"/**
* Writes bytes to output stream.
* @param b byte array containing data to write
* @param off starting offset in the byte array
* @param len number of bytes to write
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(int),490,499,"/**
 * Writes an integer to the output stream.
 * @param b integer value to write
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hasCapability,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hasCapability(java.lang.String),516,527,"/**
* Checks if a capability is masked.
* @param capability the capability to check
* @return true if capability is masked, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List,org.apache.hadoop.ipc.RemoteException)",52,58,"/**
 * Constructs a PartialListing with either a listing or an exception.
 * @param listedPath the path for which the listing is applicable
 * @param partialListing the list of items, if available
 * @param exception the exception, if any, occurred during listing
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,setCount,org.apache.hadoop.io.DataOutputBuffer$Buffer:setCount(int),78,83,"/**
* Updates and returns the old count.
* @param newCount new count value to set
* @return previous count value
*/","* Set the count for the current buf.
     * @param newCount the new count to set
     * @return the original count",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,"org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object,java.lang.Throwable,org.apache.hadoop.io.retry.CallReturn$State)",60,65,"/**
 * Initializes a call return with result or exception.
 * @param r the return value of the call
 * @param t the exception thrown during the call, if any
 * @param s the state of the call
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getConnectorAddress,org.apache.hadoop.http.HttpServer2:getConnectorAddress(int),1330,1342,"/**
* Retrieves server address by index.
* @param index connector index
* @return InetSocketAddress or null if invalid
*/","* Get the address that corresponds to a particular connector.
   *
   * @param index index.
   * @return the corresponding address for the connector, or null if there's no
   *         such connector or the connector is not bounded or was closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec:calculateIV(byte[],long,byte[],int)",63,80,"/**
* Masks initialization vector with counter.
* @param initIV initial IV bytes
* @param counter long value to mask with
* @param iv destination IV bytes
* @param blockSize size of the block in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceCtrCryptoCodec:calculateIV(byte[],long,byte[],int)",55,72,"/**
 * Masks the initialization vector with a counter.
 * @param initIV initial IV array
 * @param counter long value to mask with
 * @param iv output masked IV array
 * @param blockSize size of the block in bytes
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,<init>,"org.apache.hadoop.util.GcTimeMonitor:<init>(long,long,int,org.apache.hadoop.util.GcTimeMonitor$GcTimeAlertHandler)",125,151,"/**
 * Initializes a GcTimeMonitor with specified parameters.
 * @param observationWindowMs time window for monitoring GC activity (in ms)
 * @param sleepIntervalMs interval between checks (in ms)
 * @param maxGcTimePercentage maximum allowed GC time percentage
 * @param alertHandler handler for alerts on excessive GC times
 */","* Create an instance of GCTimeMonitor. Once it's started, it will stay alive
   * and monitor GC time percentage until shutdown() is called. If you don't
   * put a limit on the number of GCTimeMonitor instances that you create, and
   * alertHandler != null, you should necessarily call shutdown() once the given
   * instance is not needed. Otherwise, you may create a memory leak, because
   * each running GCTimeMonitor will keep its alertHandler object in memory,
   * which in turn may reference and keep in memory many more other objects.
   *
   * @param observationWindowMs the interval over which the percentage
   *   of GC time should be calculated. A practical value would be somewhere
   *   between 30 sec and several minutes.
   * @param sleepIntervalMs how frequently this thread should wake up to check
   *   GC timings. This is also a frequency with which alertHandler will be
   *   invoked if GC time percentage exceeds the specified limit. A practical
   *   value would likely be 500..1000 ms.
   * @param maxGcTimePercentage A GC time percentage limit (0..100) within
   *   observationWindowMs. Once this is exceeded, alertHandler will be
   *   invoked every sleepIntervalMs milliseconds until GC time percentage
   *   falls below this limit.
   * @param alertHandler a single method in this interface is invoked when GC
   *   time percentage exceeds the specified limit.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ServletUtil.java,getRawPath,"org.apache.hadoop.util.ServletUtil:getRawPath(javax.servlet.http.HttpServletRequest,java.lang.String)",107,110,"/**
* Masks servlet name in request path.
* @param request HTTP request object
* @param servletName name of the servlet
* @return masked servlet name from request path
*/","* Parse the path component from the given request and return w/o decoding.
   * @param request Http request to parse
   * @param servletName the name of servlet that precedes the path
   * @return path component, null if the default charset is not supported",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getMovableTypes,org.apache.hadoop.fs.StorageType:getMovableTypes(),78,80,"/**
* Returns a list of storage types.
* @return List of StorageType objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getTypesSupportingQuota,org.apache.hadoop.fs.StorageType:getTypesSupportingQuota(),82,84,"/**
 * Returns a list of storage types.
 * @return List of StorageType objects
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,parseStorageType,org.apache.hadoop.fs.StorageType:parseStorageType(java.lang.String),90,92,"/**
 * Converts string to StorageType using StringUtils.
 * @param s input string
 * @return corresponding StorageType or default if conversion fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,initMode,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initMode(),631,638,"/**
* Determines initialization mode from system property or environment variable.
* @return InitMode based on configuration settings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",838,844,"/**
 * Resolves and retrieves file data from a specified path.
 * @param path the file system path
 * @param names list of file names to retrieve
 * @return map of file names to their byte array data
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttrs,"org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",381,385,"/**
 * Retrieves files as byte arrays from specified paths.
 * @param path directory path to search
 * @param names list of file names to retrieve
 * @return map of file names to their byte array contents
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,unbuffer,org.apache.hadoop.fs.FSDataInputStream:unbuffer(),237,240,"/**
 * Invokes the static method m1 from StreamCapabilitiesPolicy with input in.
 * @param in the input parameter passed to the method
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,equals,org.apache.hadoop.fs.FileStatus:equals(java.lang.Object),435,445,"/**
* Checks equality based on m1() comparison.
* @param o object to compare
* @return true if equal, false otherwise
*/","Compare if this object is equal to another object
   * @param   o the object to be compared.
   * @return  true if two file status has the same path name; false if not.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,equals,org.apache.hadoop.fs.shell.PathData:equals(java.lang.Object),598,603,"/**
* Checks if object is non-null PathData and matches internal path.
* @param o the object to check
* @return true if conditions are met, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,hashCode,org.apache.hadoop.fs.FileStatus:hashCode(),453,456,"/**
 * Calls m2 on the result of m1().
 * @return Result of calling m2 on the object returned by m1()
 */","* Returns a hash code value for the object, which is defined as
   * the hash code of the path name.
   *
   * @return  a hash code value for the path name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,hashCode,org.apache.hadoop.fs.shell.PathData:hashCode(),605,608,"/**
* Delegates to path's m1 method.
* @return result of path.m1()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setPath,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setPath(org.apache.hadoop.fs.Path),534,537,"/**
 * Delegates call to realStatus's m1 method.
 * @param p Path object representing file path
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,calculateFolderSize,org.apache.hadoop.fs.DUHelper:calculateFolderSize(java.lang.String),38,43,"/**
 * Calculates the size of a folder.
 * @param folder path to the folder
 * @return size of the folder in bytes
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,check,org.apache.hadoop.fs.DUHelper:check(java.lang.String),45,53,"/**
* Masks folder details.
* @param folder path to the directory
* @return formatted string with folder size, file count, and usage
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",853,858,"/**
* Resolves and delegates file creation to the appropriate filesystem.
* @param path full path for the new file
* @param name name of the new file
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeXAttr,"org.apache.hadoop.fs.FilterFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",392,395,"/**
 * Delegates file creation to underlying filesystem.
 * @param path file path
 * @param name file name
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,setSamplesAndSum,"org.apache.hadoop.fs.statistics.MeanStatistic:setSamplesAndSum(long,long)",157,161,"/**
 * Updates metrics with new sample count and sum.
 * @param sampleCount number of samples
 * @param newSum total sum of samples
 */","* Set the sum and samples.
   * Synchronized.
   * @param sampleCount new sample count.
   * @param newSum new sum",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,add,org.apache.hadoop.fs.statistics.MeanStatistic:add(org.apache.hadoop.fs.statistics.MeanStatistic),212,230,"/**
 * Merges statistics from another MeanStatistic.
 * @param other the MeanStatistic to merge with
 * @return this MeanStatistic after merging
 */","* Add another MeanStatistic.
   * @param other other value
   * @return mean statistic.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,equals,org.apache.hadoop.fs.statistics.MeanStatistic:equals(java.lang.Object),254,269,"/**
* Checks equality of two MeanStatistic objects.
* @param o object to compare with
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,toString,org.apache.hadoop.fs.statistics.MeanStatistic:toString(),285,289,"/**
* Generates a formatted string with sample count, sum, and mean.
* @return Formatted string containing statistical data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,mapToString,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.lang.String)",133,149,"/**
 * Appends formatted key-value pairs from map to StringBuilder.
 * @param sb StringBuilder to append to
 * @param type type identifier for output
 * @param map source of key-value pairs
 * @param separator string to separate entries
 */","* Given a map, add its entryset to the string.
   * The entries are only sorted if the source entryset
   * iterator is sorted, such as from a TreeMap.
   * @param sb string buffer to append to
   * @param type type (for output)
   * @param map map to evaluate
   * @param separator separator
   * @param <E> type of values of the map",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,entryToString,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:entryToString(java.util.Map$Entry),136,139,"/**
 * Converts map entry to string using custom methods.
 * @param entry map entry with key as String
 * @return result of applying m3 to entry's key and value
 */","* Convert an entry to the string format used in logging.
   *
   * @param entry entry to evaluate
   * @param <E> entry type
   * @return formatted string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationTrackerFactory.java,trackDuration,"org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String,long)",48,50,"/**
 * Tracks function duration.
 * @param key identifier for the function
 * @param count number of times to execute
 * @return DurationTracker object
 */","* Initiate a duration tracking operation by creating/returning
   * an object whose {@code close()} call will
   * update the statistics.
   *
   * The statistics counter with the key name will be incremented
   * by the given count.
   *
   * The expected use is within a try-with-resources clause.
   *
   * The default implementation returns a stub duration tracker.
   * @param key statistic key prefix
   * @param count  #of times to increment the matching counter in this
   * operation.
   * @return an object to close after an operation completes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/EmptyPrefetchingStatistics.java,prefetchOperationStarted,org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:prefetchOperationStarted(),45,48,"/**
 * Returns a duration tracker.
 * @return DurationTracker instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,getLong,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLong(java.lang.String),97,104,"/**
 * Retrieves a value by key using multiple methods.
 * @param key unique identifier for the value
 * @return Long value or null if not found in any method
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,isTracked,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:isTracked(java.lang.String),106,110,"/**
* Checks if key is masked by either m1 or m3.
* @param key the key to check
* @return true if key is masked, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,toLongStatistic,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:toLongStatistic(java.util.Map$Entry),85,87,"/**
 * Creates a LongStatistic from map entry.
 * @param e map entry with string key and long value
 * @return LongStatistic object initialized with entry's values
 */","* Convert a counter/gauge entry to a long statistics.
   * @param e entry
   * @return statistic",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,next,org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:next(),70,78,"/**
* Retrieves and processes the next statistic.
* @return LongStatistic object for the current key
* @throws NoSuchElementException if no more keys are available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EvaluatingStatisticsMap.java,<init>,org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:<init>(),51,53,"/**
 * Initializes statistics map with passthrough function.
 */",* Construct with the copy function being simple passthrough.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addCounterFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addCounterFunction(java.lang.String,java.util.function.Function)",91,93,"/**
 * Updates a counter metric with a calculated value.
 * @param key identifier for the metric
 * @param eval function to compute the new metric value
 */","* add a mapping of a key to a counter function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addGaugeFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addGaugeFunction(java.lang.String,java.util.function.Function)",100,102,"/**
 * Records a gauge value using a provided evaluation function.
 * @param key identifier for the gauge metric
 * @param eval function to compute the gauge value
 */","* add a mapping of a key to a gauge function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMinimumFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMinimumFunction(java.lang.String,java.util.function.Function)",109,111,"/**
 * Updates mask with evaluated value.
 * @param key unique identifier for the mask
 * @param eval function to compute the new value
 */","* add a mapping of a key to a minimum function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMaximumFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMaximumFunction(java.lang.String,java.util.function.Function)",118,120,"/**
 * Applies a function to update the maximum value associated with a key.
 * @param key unique identifier for the value
 * @param eval function to compute the new maximum value
 */","* add a mapping of a key to a maximum function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMeanStatisticFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMeanStatisticFunction(java.lang.String,java.util.function.Function)",127,130,"/**
 * Applies a function to calculate statistics for a given key.
 * @param key identifier for the data set
 * @param eval function to compute MeanStatistic
 */","* add a mapping of a key to a meanStatistic function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,wrap,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:wrap(org.apache.hadoop.fs.statistics.IOStatistics),116,118,"/**
 * Wraps given statistics in a source wrapper.
 * @param statistics original IO statistics to be wrapped
 * @return IOStatisticsSource object wrapping the input statistics
 */","* Take an IOStatistics instance and wrap it in a source.
   * @param statistics statistics.
   * @return a source which will return the values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,getAggregator,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getAggregator(),49,52,"/**
* Returns an empty I/O statistics aggregator.
* @return IOStatisticsAggregator instance with no data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,emptyStatisticsStore,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatisticsStore(),107,109,"/**
 * Returns an empty IOStatisticsStore instance.
 * @return EmptyIOStatisticsStore object
 */","* Get the shared instance of the immutable empty statistics
   * store.
   * @return an empty statistics object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,getIOStatistics,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getIOStatistics(),54,57,"/**
 * Returns an empty I/O statistics instance.
 * @return EmptyIOStatistics object representing no I/O operations
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,emptyStatistics,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatistics(),98,100,"/**
 * Returns an empty I/O statistics instance.
 * @return IOStatistics object representing no I/O operations
 */","* Get the shared instance of the immutable empty statistics
   * object.
   * @return an empty statistics object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setCounter,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setCounter(java.lang.String,long)",172,176,"/**
* Sets a counter value for a given key.
* @param key identifier for the counter
* @param value new value to set for the counter
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMaximum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMaximum(java.lang.String,long)",199,202,"/**
 * Masks a value in the map using a key.
 * @param key unique identifier for the value
 * @param value the value to be masked
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMinimum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMinimum(java.lang.String,long)",209,212,"/**
 * Updates map with minimum value.
 * @param key unique identifier
 * @param value new value to compare against current minimum
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setGauge,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setGauge(java.lang.String,long)",235,238,"/**
 * Updates gauge value for a given key.
 * @param key identifier for the gauge
 * @param value new value to set for the gauge
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementCounter,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter(java.lang.String,long)",178,197,"/**
 * Increments a counter for a given key.
 * @param key unique identifier for the counter
 * @param value amount to increment the counter by
 * @return updated counter value or 0 if invalid input
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementMaximum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMaximum(java.lang.String,long)",204,207,"/**
 * Applies mask to value using key.
 * @param key identifier used for masking
 * @param value original numeric value
 * @return masked value as a long
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementMinimum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMinimum(java.lang.String,long)",214,217,"/**
* Applies mask to value using key.
* @param key unique identifier for masking logic
* @param value input value to be masked
* @return masked value as a long
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementGauge,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementGauge(java.lang.String,long)",240,243,"/**
* Applies mask to value based on key.
* @param key identifier for masking logic
* @param value input value to be masked
* @return masked value as a long
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMinimumSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMinimumSample(java.lang.String,long)",219,225,"/**
 * Updates the minimum value for a given key.
 * @param key unique identifier for the value
 * @param value new value to compare and possibly update
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMaximumSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMaximumSample(java.lang.String,long)",227,233,"/**
* Updates the maximum value for a given key.
* @param key the key to update
* @param value the new value to consider
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMeanStatisticSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMeanStatisticSample(java.lang.String,long)",253,259,"/**
* Updates the mean statistic for a given key with a new value.
* @param key identifier for the statistic
* @param value new value to update the statistic with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getCounterReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getCounterReference(java.lang.String),372,375,"/**
 * Retrieves an AtomicLong from counterMap by key.
 * @param key unique identifier for the counter
 * @return AtomicLong associated with the key or null if not found
 */","* Get a reference to the atomic instance providing the
   * value for a specific counter. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMaximumReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMaximumReference(java.lang.String),385,388,"/**
 * Retrieves an AtomicLong from the maximumMap using the given key.
 * @param key unique identifier for the AtomicLong
 * @return AtomicLong associated with the key, or default if not found
 */","* Get a reference to the atomic instance providing the
   * value for a specific maximum. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMinimumReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMinimumReference(java.lang.String),398,401,"/**
 * Retrieves an AtomicLong from minimumMap by key.
 * @param key unique identifier for the AtomicLong
 * @return AtomicLong associated with the key or null if not found
 */","* Get a reference to the atomic instance providing the
   * value for a specific minimum. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getGaugeReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getGaugeReference(java.lang.String),411,414,"/**
 * Retrieves an AtomicLong from gaugeMap by key.
 * @param key unique identifier for the AtomicLong
 * @return AtomicLong associated with the key or null if not found
 */","* Get a reference to the atomic instance providing the
   * value for a specific gauge. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMeanStatistic,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMeanStatistic(java.lang.String),422,425,"/**
* Retrieves MeanStatistic by key from map.
* @param key unique identifier for statistic
* @return MeanStatistic object or null if not found
*/","* Get a mean statistic.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/PairedDurationTrackerFactory.java,asDuration,org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:asDuration(),87,90,"/**
 * Returns the duration from the first element.
 * @return Duration object representing the duration of the first element
 */",* @return the global duration,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,counters,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:counters(),55,58,"/**
 * Retrieves a map from method m1 and calls its m2 method.
 * @return Map<String, Long> result of m2 method call
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,gauges,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:gauges(),79,82,"/**
 * Retrieves a map from another method.
 * @return Map containing string keys and long values
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,minimums,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:minimums(),84,87,"/**
 * Retrieves a map from another method.
 * @return Map of strings to longs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,maximums,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:maximums(),89,92,"/**
 * Retrieves a map of string keys to long values.
 * @return Map containing key-value pairs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,meanStatistics,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:meanStatistics(),94,97,"/**
 * Retrieves mean statistics.
 * @return Map of string keys to MeanStatistic objects
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,setWrapped,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:setWrapped(org.apache.hadoop.fs.statistics.IOStatistics),73,77,"/**
* Sets the wrapped statistics.
* @param wrapped IOStatistics object to wrap
*/","* Set the wrapped statistics.
   * Will fail if the field is already set.
   * @param wrapped new value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,activeInstance,org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:activeInstance(),63,66,"/**
 * Returns the built instance of DynamicIOStatistics.
 * @return DynamicIOStatistics object or throws an exception if not built
 */","* Get the statistics instance.
   * @return the instance to build/return
   * @throws IllegalStateException if the builder has already been built.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,checkMutable,org.apache.hadoop.fs.impl.FlagSet:checkMutable(),125,128,"/**
 * Ensures FlagSet is mutable.
 * Throws an exception if FlagSet is immutable.
 */","* Check for mutability before any mutating operation.
   * @throws IllegalStateException if the set is still mutable",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toByteArray,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:toByteArray(),235,250,"/**
* Reads data from a file or input stream into a byte array.
* @return byte array containing the read data
* @throws IOException if an I/O error occurs
*/","* Convert to a byte array.
     * If the data is stored in a file, it will be read and returned.
     * If the data was passed in via an input stream (which happens if the
     * data is stored in a bytebuffer) then it will be converted to a byte
     * array -which will then be cached for any subsequent use.
     *
     * @return byte[] after converting the uploadBlock.
     * @throws IOException throw if an exception is caught while reading
     *                     File/InputStream or closing InputStream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,bind,org.apache.hadoop.service.launcher.IrqHandler:bind(),89,100,"/**
* Binds a signal handler.
* @throws IllegalArgumentException if handler is already bound or JVM has -Xrs set
*/","* Bind to the interrupt handler.
   * @throws IllegalArgumentException if the exception could not be set",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CloseableReferenceCount.java,unreference,org.apache.hadoop.util.CloseableReferenceCount:unreference(),65,70,"/**
* Checks if the status is closed.
* @return true if status is closed, false otherwise
*/","* Decrement the reference count.
   *
   * @return          True if the object is closed and has no outstanding
   *                  references.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,run,org.apache.hadoop.ha.HealthMonitor$MonitorDaemon:run(),285,296,"/**
* Continuously executes tasks until shouldRun is false.
* Handles InterruptedException by checking if it's expected.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setZooKeeperRef,org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:setZooKeeperRef(org.apache.zookeeper.ZooKeeper),1226,1231,"/**
* Sets ZooKeeper instance.
* @param zk ZooKeeper object to set
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,snapshotMap,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map,java.util.function.Function)",214,221,"/**
* Copies elements from source map to destination concurrent hashmap using a transformation function.
* @param source the original map with string keys and serializable values
* @param copyFn function to transform or copy each value
* @return ConcurrentHashMap containing transformed entries
*/","* Take a snapshot of a supplied map, using the copy function
   * to replicate the source values.
   * @param source source map
   * @param copyFn function to copy the value
   * @param <E> type of values.
   * @return a concurrent hash map referencing the same values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE)",445,450,"/**
* Executes callable with tracking and error handling.
* @param factory DurationTrackerFactory for tracking execution time
* @param statistic identifier for the tracked operation
* @param input CallableRaisingIOE to be executed
* @return result of the callable execution
* @throws IOException if an I/O error occurs during execution
*/","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @param <B> return type.
   * @return the result of the operation.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,pairedTrackerFactory,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:pairedTrackerFactory(org.apache.hadoop.fs.statistics.DurationTrackerFactory,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",687,691,"/**
* Combines two DurationTrackerFactories.
* @param first  the primary factory
* @param second the secondary factory
* @return a new PairedDurationTrackerFactory combining both
*/","* Create a DurationTrackerFactory which aggregates the tracking
   * of two other factories.
   * @param first first tracker factory
   * @param second second tracker factory
   * @return a factory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableCounterLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",60,66,"/**
 * Masks metrics in the builder.
 * @param builder MetricsRecordBuilder to update
 * @param all Whether to include all metrics
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheHit,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheHit(),82,84,"/**
 * Returns mask value from cache hit.
 * @return long representing the mask value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheCleared,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheCleared(),86,88,"/**
 * Returns the result of mask operation from cache cleared.
 * @return long value representing the mask result
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheUpdated,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheUpdated(),90,92,"/**
 * Returns the mask value from cache update.
 * @return long representing the mask value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getClientBackoffDisconnected,org.apache.hadoop.ipc.metrics.RpcMetrics:getClientBackoffDisconnected(),358,360,"/**
 * Retrieves mask value from RPC client backoff.
 * @return long representing the mask value
 */","* Returns the number of disconnected backoffs.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getRpcSlowCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcSlowCalls(),420,422,"/**
 * Retrieves mask value from RPC slow calls.
 * @return long value representing the mask
 */","* Returns the number of slow calls.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getRpcRequeueCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcRequeueCalls(),428,431,"/**
 * Returns result of RPC requeue calls.
 * @return Long value from m1 method
 */","* Returns the number of requeue calls.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,counters,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:counters(),48,51,"/**
 * Retrieves a map of names to counts.
 * @return Map containing string keys and long values
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,gauges,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:gauges(),53,56,"/**
 * Calls m1 and then invokes m2 on its result.
 * @return Map from m2 of the result of m1
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,minimums,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:minimums(),58,61,"/**
 * Retrieves a map from another method.
 * @return Map containing string keys and long values
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,maximums,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:maximums(),63,66,"/**
 * Calls m2 on the result of m1().
 * @return Map from m2() or null if m1() is null
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,meanStatistics,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:meanStatistics(),68,71,"/**
 * Retrieves statistics map.
 * @return Map of string keys to MeanStatistic values
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,aggregate,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:aggregate(org.apache.hadoop.fs.statistics.IOStatistics),73,76,"/**
 * Delegates call to another method with provided statistics.
 * @param statistics optional I/O statistics
 * @return result of delegated method call
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementCounter,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementCounter(java.lang.String,long)",78,81,"/**
 * Delegates call to another method with key and value.
 * @param key unique identifier
 * @param value associated value
 * @return result of the delegated method call
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setCounter,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setCounter(java.lang.String,long)",83,86,"/**
* Delegates call to another method with key and value.
* @param key unique identifier
* @param value associated numeric value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setGauge,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setGauge(java.lang.String,long)",88,91,"/**
 * Delegates call to another method with given key and value.
 * @param key unique identifier
 * @param value associated numeric value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementGauge,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementGauge(java.lang.String,long)",93,96,"/**
 * Delegates call to another method with same parameters.
 * @param key unique identifier
 * @param value associated numeric value
 * @return result of the delegated method call
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMaximum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMaximum(java.lang.String,long)",98,101,"/**
 * Delegates call to another method with key and value.
 * @param key unique identifier
 * @param value associated numeric value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementMaximum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMaximum(java.lang.String,long)",103,106,"/**
 * Delegates to another method with the same parameters.
 * @param key unique identifier key
 * @param value associated numerical value
 * @return result of the delegated method call
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMinimum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMinimum(java.lang.String,long)",108,112,"/**
 * Delegates call to m2 of nested object.
 * @param key unique identifier
 * @param value associated numeric value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementMinimum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMinimum(java.lang.String,long)",114,118,"/**
 * Delegates call to another method with key and value.
 * @param key unique identifier
 * @param value associated value
 * @return result of the delegated method call
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMinimumSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMinimumSample(java.lang.String,long)",120,124,"/**
 * Delegates call to another method with given key and value.
 * @param key unique identifier
 * @param value associated numeric value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMaximumSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMaximumSample(java.lang.String,long)",126,129,"/**
 * Delegates to another method with the same parameters.
 * @param key unique identifier
 * @param value associated numeric value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",131,135,"/**
 * Delegates call to another method with given parameters.
 * @param key unique identifier
 * @param value statistical data
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMeanStatisticSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMeanStatisticSample(java.lang.String,long)",137,141,"/**
 * Delegates call to another method with provided parameters.
 * @param key unique identifier for the operation
 * @param value associated numerical value for the operation
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,reset,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:reset(),143,146,"/**
 * Calls method m2 on the result of m1().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getCounterReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getCounterReference(java.lang.String),148,151,"/**
 * Retrieves an AtomicLong associated with the given key.
 * @param key unique identifier for the AtomicLong
 * @return AtomicLong object or null if not found
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMaximumReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMaximumReference(java.lang.String),153,156,"/**
 * Retrieves an atomic long value associated with the given key.
 * @param key unique identifier for the atomic long
 * @return AtomicLong object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMinimumReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMinimumReference(java.lang.String),158,161,"/**
* Retrieves an AtomicLong by key using nested method calls.
* @param key unique identifier for the AtomicLong
* @return AtomicLong object associated with the key
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getGaugeReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getGaugeReference(java.lang.String),163,166,"/**
 * Retrieves an AtomicLong associated with a given key.
 * @param key unique identifier for the AtomicLong
 * @return AtomicLong object or null if not found
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMeanStatistic,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMeanStatistic(java.lang.String),168,171,"/**
 * Retrieves mean statistic by key.
 * @param key identifier for the statistic
 * @return MeanStatistic object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,long)",173,178,"/**
 * Delegates to another method with the same parameters.
 * @param prefix string prefix for processing
 * @param durationMillis time duration in milliseconds
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,java.time.Duration)",180,184,"/**
 * Delegates call to m2 with given parameters.
 * @param prefix string prefix to process
 * @param duration time duration to consider
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,withDurationTracking,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withDurationTracking(java.lang.String[]),77,93,"/**
* Configures statistics with specified prefixes.
* @param prefixes array of prefix strings for metrics
* @return IOStatisticsStoreBuilderImpl instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,withSampleTracking,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withSampleTracking(java.lang.String[]),95,105,"/**
* Applies mask functions to specified prefixes.
* @param prefixes array of prefix strings
* @return IOStatisticsStoreBuilderImpl instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,reset,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:reset(),101,105,"/**
* Clears IO statistics context by ID.
* @param id unique identifier of the context to clear
*/",* Reset the thread +.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,<init>,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>(),113,115,"/**
 * Initializes an instance and sets up necessary maps.
 */",* Construct.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setCounter,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setCounter(java.lang.String,long)",226,229,"/**
* Masks a value with a given key.
* @param key unique identifier for the mask
* @param value value to be masked
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setGauge,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setGauge(java.lang.String,long)",231,235,"/**
 * Masks a key with a value.
 * @param key unique identifier to mask
 * @param value value to apply as a mask
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMaximum,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMaximum(java.lang.String,long)",237,241,"/**
 * Masks a key with a value.
 * @param key unique identifier to mask
 * @param value value to apply as a mask
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMinimum,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMinimum(java.lang.String,long)",243,246,"/**
 * Masks a key with a value.
 * @param key unique identifier
 * @param value to mask with
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",248,251,"/**
 * Masks a statistic with a given key.
 * @param key identifier for the statistic
 * @param value MeanStatistic object to be masked
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,enabled,org.apache.hadoop.fs.statistics.IOStatisticsContext:enabled(),95,97,"/**
 * Checks integration status.
 * @return true if integrated, false otherwise
 */","* Static probe to check if the thread-level IO statistics enabled.
   *
   * @return if the thread-level IO statistics enabled.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,retrieveIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:retrieveIOStatistics(java.lang.Object),78,88,"/**
* Converts or retrieves IOStatistics from the source.
* @param source object to convert or retrieve from
* @return IOStatistics object or null if conversion is not possible
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:<init>(),123,133,"/**
* Initializes a Gzip decompressor.
* Sets initial state and resets CRC.
*/",* Creates a new (pure Java) gzip decompressor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,available,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:available(),189,192,"/**
 * Sums results of m1 from datas and superclass.
 * @return Sum of m1 results
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,available,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:available(),225,228,"/**
 * Sums results from local and superclass methods.
 * @return Sum of m1 from datas and super
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,seekToNewSource,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seekToNewSource(long),221,227,"/**
 * Updates file data at target position.
 * @param targetPos position in file to update
 * @return true if update successful or new data source is used
 * @throws IOException if I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,readChunk,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])",229,267,"/**
 * Reads data from a position with checksum validation.
 * @param pos starting position in the data stream
 * @param buf buffer to store read data
 * @param offset offset within the buffer
 * @param len number of bytes to read
 * @param checksum array for storing checksums
 * @return number of bytes actually read
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,verifySums,"org.apache.hadoop.fs.FSInputChecker:verifySums(byte[],int,int)",336,359,"/**
* Verifies checksum of data chunks.
* @param b byte array containing data
* @param off offset in the byte array to start verification
* @param read number of bytes to verify
* @throws ChecksumException if checksum mismatch occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,throwChecksumException,"org.apache.hadoop.util.DataChecksum:throwChecksumException(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.lang.String,long,int,int)",514,521,"/**
* Throws a ChecksumException indicating a mismatch.
* @param type the type of checksum
* @param algorithm the checksum algorithm used
* @param filename the name of the file being checked
* @param errPos the position where the error occurred
* @param expected the expected checksum value
* @param computed the actual computed checksum value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getCounter,org.apache.hadoop.crypto.CryptoInputStream:getCounter(long),288,290,"/**
 * Masks position by dividing it with m2 value.
 * @param position the original position to mask
 * @return masked position as a long
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getPadding,org.apache.hadoop.crypto.CryptoInputStream:getPadding(long),292,294,"/**
 * Applies mask to position.
 * @param position long value to mask
 * @return masked byte result
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,updateEncryptor,org.apache.hadoop.crypto.CryptoOutputStream:updateEncryptor(),220,228,"/**
* Masks data using encryption.
* @throws IOException if an I/O error occurs
*/",Update the {@link #encryptor}: calculate counter and {@link #padding}.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,checkBufferSize,"org.apache.hadoop.crypto.CryptoStreamUtils:checkBufferSize(org.apache.hadoop.crypto.CryptoCodec,int)",90,95,"/**
* Adjusts buffer size to be a multiple of codec's block size.
* @param codec cryptographic codec instance
* @param bufferSize initial buffer size
* @return adjusted buffer size
*/","* Check and floor buffer size.
   *
   * @param codec crypto codec.
   * @param bufferSize the size of the buffer to be used.
   * @return calc buffer size.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,link,"org.apache.hadoop.io.nativeio.NativeIO:link(java.io.File,java.io.File)",1080,1087,"/**
* Masks a file by copying or linking it.
* @param src source file to mask
* @param dst destination file for masking
* @throws IOException if an I/O error occurs
*/","* Creates a hardlink ""dst"" that points to ""src"".
   *
   * This is deprecated since JDK7 NIO can create hardlinks via the
   * {@link java.nio.file.Files} API.
   *
   * @param src source file
   * @param dst hardlink location
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,getInstance,org.apache.hadoop.fs.DelegationTokenRenewer:getInstance(),200,205,"/**
* Returns singleton instance of DelegationTokenRenewer for FileSystem.
* @return DelegationTokenRenewer instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,makeRequestIfNeeded,org.apache.hadoop.fs.BatchedRemoteIterator:makeRequestIfNeeded(),84,96,"/**
 * Handles entry processing based on index.
 * Throws IOException if an I/O error occurs.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java,<init>,"org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)",38,40,"/**
 * Constructs an MD5MD5CRC32GzipFileChecksum with specified parameters.
 * @param bytesPerCRC number of bytes per CRC block
 * @param crcPerBlock number of CRC blocks
 * @param md5 MD5 hash instance
 */","* Create a MD5FileChecksum.
   *
   * @param bytesPerCRC bytesPerCRC.
   * @param crcPerBlock crcPerBlock.
   * @param md5 md5.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:<init>(),43,45,"/**
 * Constructs an MD5-MD5-CRC32 file checksum.
 * @param param1 first parameter (purpose unknown)
 * @param param2 second parameter (purpose unknown)
 * @param param3 third parameter (purpose unknown)
 */","Same as this(0, 0, null)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java,<init>,"org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)",38,40,"/**
* Constructs an MD5MD5CRC32CastagnoliFileChecksum.
* @param bytesPerCRC number of bytes per CRC block
* @param crcPerBlock number of CRC blocks
* @param md5 MD5Hash instance
*/","* Create a MD5FileChecksum.
   *
   * @param bytesPerCRC bytesPerCRC.
   * @param crcPerBlock crcPerBlock.
   * @param md5 md5.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,accept,org.apache.hadoop.fs.GlobFilter:accept(org.apache.hadoop.fs.Path),79,82,"/**
 * Checks if a path matches criteria.
 * @param path file system path to check
 * @return true if path meets both pattern and filter conditions, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,set,org.apache.hadoop.fs.GlobPattern:set(java.lang.String),74,157,"/**
 * Converts a glob pattern to a regex pattern.
 * @param glob the input glob pattern string
 */","* Set and compile a glob pattern
   * @param glob  the glob pattern string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFsStatus,org.apache.hadoop.fs.FilterFs:getFsStatus(org.apache.hadoop.fs.Path),146,150,"/**
* Delegates file status retrieval to underlying filesystem.
* @param f file path
* @return file status
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws UnresolvedLinkException if symbolic link cannot be resolved
* @throws IOException for other I/O errors
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listStatusIterator,org.apache.hadoop.fs.FilterFileSystem:listStatusIterator(org.apache.hadoop.fs.Path),291,295,"/**
 * Retrieves file status iterator for a given path.
 * @param f file path to query
 * @return iterator of FileStatus objects
 * @throws IOException if an I/O error occurs
 */",Return a remote iterator for listing in a directory,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,isRegularFile,org.apache.hadoop.fs.FileUtil:isRegularFile(java.io.File),632,634,"/**
 * Checks if a file exists.
 * @param file the File to check
 * @return true if the file exists, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeShellPath,"org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File,boolean)",696,703,"/**
* Recursively gets file path.
* @param file the File object
* @param makeCanonicalPath flag to use canonical path
* @return file path as String
* @throws IOException if I/O error occurs
*/","* Convert a os-native filename to a path that works for the shell.
   * @param file The filename to convert
   * @param makeCanonicalPath
   *          Whether to make canonical path for the file passed
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,permissionsFromMode,org.apache.hadoop.fs.FileUtil:permissionsFromMode(int),783,793,"/**
 * Converts file mode to POSIX permissions.
 * @param mode numeric file mode
 * @return set of POSIX file permissions
 */","* The permission operation of this method only involves users, user groups, and others.
   * If SUID is set, only executable permissions are reserved.
   * @param mode Permissions are represented by numerical values
   * @return The original permissions for files are stored in collections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unpackEntries,"org.apache.hadoop.fs.FileUtil:unpackEntries(org.apache.commons.compress.archivers.tar.TarArchiveInputStream,org.apache.commons.compress.archivers.tar.TarArchiveEntry,java.io.File)",1130,1186,"/**
 * Extracts a tar archive entry to the specified directory.
 * @param tis TarArchiveInputStream containing the archive
 * @param entry The current TarArchiveEntry being processed
 * @param outputDir Directory where the entry should be extracted
 * @throws IOException if extraction fails or paths are invalid
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,join,"org.apache.hadoop.util.StringUtils:join(char,java.lang.String[])",1084,1086,"/**
 * Joins strings using a separator.
 * @param separator character used to separate strings
 * @param strings array of strings to join
 * @return concatenated string with separators
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execute,org.apache.hadoop.util.Shell$ShellCommandExecutor:execute(),1275,1283,"/**
* Checks for null entries in command list and throws IOException if found.
* Calls m2() after validation.
*/","* Execute the shell command.
     * @throws IOException if the command fails, or if the command is
     * not well constructed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkWindowsCommandLineLength,org.apache.hadoop.util.Shell:checkWindowsCommandLineLength(java.lang.String[]),127,140,"/**
* Checks if the total length of shell commands exceeds Windows limit.
* @param commands array of command strings to be executed
* @throws IOException if combined command length exceeds maximum allowed
*/","* Checks if a given command (String[]) fits in the Windows maximum command
   * line length Note that the input is expected to already include space
   * delimiters, no extra count will be added for delimiters.
   *
   * @param commands command parts, including any space delimiters
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/PowerShellFencer.java,buildPSScript,"org.apache.hadoop.ha.PowerShellFencer:buildPSScript(java.lang.String,java.lang.String)",115,158,"/**
* Generates a PowerShell script to kill a process on a remote host.
* @param processName name of the process to terminate
* @param host target host where the process runs
* @return path to the generated PowerShell script or null if failed
*/","* Build a PowerShell script to kill a java.exe process in a remote machine.
   *
   * @param processName Name of the process to kill. This is an attribute in
   *                    CommandLine.
   * @param host Host where the process is.
   * @return Path of the PowerShell script.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toString,org.apache.hadoop.fs.permission.FsPermission:toString(),272,283,"/**
* Constructs a symbol string with optional sticky bit modification.
* @return modified or original symbol string based on stickyBit
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,join,"org.apache.hadoop.util.StringUtils:join(char,java.lang.Iterable)",1058,1060,"/**
 * Joins elements of an iterable into a single string with a separator.
 * @param separator character used to separate elements
 * @param strings iterable collection of objects to join
 * @return concatenated string of elements separated by the given character
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,close,org.apache.hadoop.fs.sftp.SFTPFileSystem$2:close(),710,722,"/**
 * Calls superclass method and checks resource status.
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,close,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:close(),644,653,"/**
 * Executes methods m1, sums.m2, and datas.m2.
 * Marks the operation as closed in the finally block.
 * @throws IOException if any I/O error occurs during execution
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,close,org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:close(),391,400,"/**
* Executes methods m1(), sums.m2(), and datas.m2().
* Marks the resource as closed in a finally block.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,<init>,"org.apache.hadoop.fs.DU:<init>(java.io.File,long,long,long)",36,41,"/**
* Constructs a Disk Usage (DU) object.
* @param path file path to monitor
* @param interval time interval for checks
* @param jitter random delay added to interval
* @param initialUsed initial disk usage value
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,refresh,org.apache.hadoop.fs.DU:refresh(),50,58,"/**
 * Executes shell command to get disk usage.
 * @throws IOException if command execution fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,connect,"org.apache.hadoop.fs.sftp.SFTPConnectionPool:connect(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)",123,183,"/**
 * Establishes an SFTP connection to a remote host.
 * @param host the hostname or IP address of the server
 * @param port the port number, default is typically 22
 * @param user the username for authentication
 * @param password the password for authentication
 * @param keyFile path to the private key file for authentication
 * @return ChannelSftp object representing the SFTP channel
 * @throws IOException if connection fails or other I/O errors occur
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,disconnect,org.apache.hadoop.fs.sftp.SFTPConnectionPool:disconnect(com.jcraft.jsch.ChannelSftp),185,211,"/**
* Manages SFTP channel lifecycle, closing excess connections.
* @param channel SFTP channel to manage
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,<init>,"org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics,long)",86,90,"/**
* Initializes a new FSDataOutputStream.
* @param out underlying output stream
* @param stats file system statistics
* @param startPosition initial position in the stream
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,getChecksumSize,org.apache.hadoop.fs.FSOutputSummer:getChecksumSize(),197,199,"/**
* Calls m1 on sum object.
* @return result of sum.m1()
*/",@return the size for a checksum.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,getChecksumSize,org.apache.hadoop.util.DataChecksum:getChecksumSize(int),345,347,"/**
* Calculates adjusted size based on data size and constants.
* @param dataSize original size of data
* @return adjusted size as an integer
*/","* the required checksum size given the data length.
   * @param dataSize data size.
   * @return the required checksum size given the data length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,convertToByteStream,"org.apache.hadoop.fs.FSOutputSummer:convertToByteStream(java.util.zip.Checksum,int)",237,239,"/**
* Applies mask to checksum.
* @param sum Checksum object to process
* @param checksumSize size of the checksum array
* @return masked checksum as byte array
*/","* Converts a checksum integer value to a byte stream
   *
   * @param sum check sum.
   * @param checksumSize check sum size.
   * @return byte stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesRead,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesRead(long),4206,4208,"/**
 * Updates total bytes read.
 * @param newBytes number of bytes to add
 */","* Increment the bytes read in the statistics.
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesWritten,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesWritten(long),4214,4216,"/**
 * Adds bytes written to the current count.
 * @param newBytes number of bytes to add
 */","* Increment the bytes written in the statistics.
     * @param newBytes the additional bytes written",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementReadOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementReadOps(int),4222,4224,"/**
 * Increments read operations count.
 * @param count number of operations to add
 */","* Increment the number of read operations.
     * @param count number of read operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementLargeReadOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementLargeReadOps(int),4230,4232,"/**
 * Increments large read operations count.
 * @param count number of operations to increment
 */","* Increment the number of large read operations.
     * @param count number of large read operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementWriteOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementWriteOps(int),4238,4240,"/**
* Increments write operation count by specified amount.
* @param count number of operations to increment
*/","* Increment the number of write operations.
     * @param count number of write operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesReadErasureCoded,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadErasureCoded(long),4246,4248,"/**
 * Updates bytesReadErasureCoded with new byte count.
 * @param newBytes number of bytes to add
 */","* Increment the bytes read on erasure-coded files in the statistics.
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesReadByDistance,"org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadByDistance(int,long)",4258,4275,"/**
* Updates byte count based on distance.
* @param distance network distance of the data source
* @param newBytes number of bytes to add
*/","* Increment the bytes read by the network distance in the statistics
     * In the common network topology setup, distance value should be an even
     * number such as 0, 2, 4, 6. To make it more general, we group distance
     * by {1, 2}, {3, 4} and {5 and beyond} for accounting.
     * @param distance the network distance
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,increaseRemoteReadTime,org.apache.hadoop.fs.FileSystem$Statistics:increaseRemoteReadTime(long),4281,4283,"/**
 * Adds specified duration to remote read time.
 * @param durationMS time in milliseconds to add
 */","* Increment the time taken to read bytes from remote in the statistics.
     * @param durationMS time taken in ms to read bytes from remote",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,visitAll,org.apache.hadoop.fs.FileSystem$Statistics:visitAll(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsAggregator),4295,4302,"/**
* Aggregates statistics using a visitor pattern.
* @param visitor object to process statistics data
* @return result of aggregation
*/","* Apply the given aggregator to all StatisticsData objects associated with
     * this Statistics object.
     *
     * For each StatisticsData object, we will call accept on the visitor.
     * Finally, at the end, we will call aggregate to get the final total.
     *
     * @param         visitor to use.
     * @return        The total.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.FilterFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),603,606,"/**
 * Delegates file system operation to underlying implementation.
 * @param path file path to operate on
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary$Builder:<init>(),47,48,"/**
 * Constructs a new Builder instance.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeConsumed,org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(long[]),108,112,"/**
 * Overrides parent's m1 method with additional logic.
 * @param typeConsumed array of consumed types
 * @return current Builder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeQuota,"org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(org.apache.hadoop.fs.StorageType,long)",114,118,"/**
 * Sets storage type and quota.
 * @param type StorageType to be set
 * @param quota Quota value to be set
 * @return This builder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeConsumed,"org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(org.apache.hadoop.fs.StorageType,long)",120,124,"/**
 * Sets storage type and consumption.
 * @param type StorageType to be set
 * @param consumed Amount of storage consumed
 * @return This Builder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeQuota,org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(long[]),126,130,"/**
* Sets type quotas and returns builder instance.
* @param typeQuota array of type quotas to set
* @return current Builder instance for chaining
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,build,org.apache.hadoop.fs.QuotaUsage$Builder:build(),93,95,"/**
 * Creates a new QuotaUsage instance based on this object.
 * @return QuotaUsage object initialized with current state
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary:<init>(org.apache.hadoop.fs.ContentSummary$Builder),194,204,"/**
* Constructs a ContentSummary from a Builder.
* @param builder the Builder containing summary data
*/","* Constructor for ContentSummary.Builder.
   *
   * @param builder builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getAlgorithmName,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getAlgorithmName(),60,64,"/**
* Generates a masked string using CRC and MD5.
* @return Masked string combining CRC, byte count, and method results
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getChecksumOpt,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getChecksumOpt(),94,97,"/**
 * Returns a ChecksumOpt with mask and CRC settings.
 * @return ChecksumOpt object configured with mask and bytes per CRC
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$ChecksumOpt:<init>(),255,257,"/**
 * Constructs a ChecksumOpt with default settings.
 * Initializes with DEFAULT checksum type and -1 as size limit.
 */",* Create a uninitialized one,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,createDisabled,org.apache.hadoop.fs.Options$ChecksumOpt:createDisabled(),287,289,"/**
 * Creates a NULL checksum option.
 * @return ChecksumOpt object configured for NULL checksum type
 */","* Create a ChecksumOpts that disables checksum.
     *
     * @return ChecksumOpt.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,getChecksumOpt,org.apache.hadoop.fs.CompositeCrcFileChecksum:getChecksumOpt(),69,72,"/**
 * Creates a new ChecksumOpt instance.
 * @return ChecksumOpt object configured with crcType and bytesPerCrc
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,write,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:write(java.io.DataOutput),106,111,"/**
 * Writes configuration data to output stream.
 * @param out DataOutput stream for writing
 * @throws IOException if I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,hasPattern,org.apache.hadoop.fs.GlobFilter:hasPattern(),75,77,"/**
 * Checks a condition using pattern's m1 method.
 * @return true if condition is met, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validate,org.apache.hadoop.fs.CreateFlag:validate(java.util.EnumSet),149,162,"/**
* Validates create flags.
* @param flag set of creation flags
* @throws HadoopIllegalArgumentException if no options specified or both append and overwrite are enabled
*/","* Validate the CreateFlag and throw exception if it is invalid
   * @param flag set of CreateFlag
   * @throws HadoopIllegalArgumentException if the CreateFlag is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/XAttrSetFlag.java,validate,"org.apache.hadoop.fs.XAttrSetFlag:validate(java.lang.String,boolean,java.util.EnumSet)",53,70,"/**
 * Masks an extended attribute with given flags.
 * @param xAttrName name of the extended attribute
 * @param xAttrExists whether the attribute exists
 * @param flag set of operation flags
 * @throws IOException if conditions are not met for the specified flags
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,checkScheme,"org.apache.hadoop.fs.AbstractFileSystem:checkScheme(java.net.URI,java.lang.String)",291,300,"/**
* Validates URI scheme against a supported scheme.
* @param uri the URI to validate
* @param supportedScheme the expected scheme
* @throws HadoopIllegalArgumentException if URI is missing or has an unsupported scheme
*/","* Check that the Uri's scheme matches.
   *
   * @param uri name URI of the FS.
   * @param supportedScheme supported scheme.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InvalidPathException.java,<init>,org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String),38,40,"/**
 * Constructs an exception for invalid path names.
 * @param path the invalid path name
 */","* Constructs exception with the specified detail message.
   * 
   * @param path invalid path.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InvalidPathException.java,<init>,"org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String,java.lang.String)",48,51,"/**
 * Constructs an InvalidPathException with a specified path and reason.
 * @param path the invalid path
 * @param reason optional additional information about the error
 */","* Constructs exception with the specified detail message.
   * 
   * @param path invalid path.
   * @param reason Reason <code>path</code> is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,findFirstValidInput,org.apache.hadoop.io.erasurecode.coder.util.HHUtil:findFirstValidInput(java.lang.Object[]),210,219,"/**
 * Returns the first non-null element from the array.
 * @param inputs array of elements to check
 * @return first non-null element or throws exception if all are null
 */","* Find the valid input from all the inputs.
   *
   * @param <T> Generics Type T.
   * @param inputs input buffers to look for valid input
   * @return the first valid input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,findFirstValidInput,org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:findFirstValidInput(java.lang.Object[]),163,172,"/**
* Returns the first non-null element from the array.
* @param inputs array of elements to check
* @throws HadoopIllegalArgumentException if all elements are null
*/","* Find the valid input from all the inputs.
   * @param inputs input buffers to look for valid input
   * @return the first valid input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,checkBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:checkBuffers(byte[][]),91,103,"/**
* Validates byte buffers for non-null and correct length.
* @param buffers array of byte arrays to validate
*/","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,checkOutputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkOutputBuffers(java.nio.ByteBuffer[]),129,145,"/**
 * Validates an array of ByteBuffers.
 * @param buffers array of ByteBuffer objects to validate
 */","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,checkOutputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkOutputBuffers(byte[][]),121,133,"/**
* Validates input buffers for decoding.
* @param buffers array of byte arrays to validate
*/","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,checkBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:checkBuffers(java.nio.ByteBuffer[]),91,107,"/**
* Validates an array of ByteBuffers.
* @param buffers array of ByteBuffers to validate
* @throws HadoopIllegalArgumentException if any buffer is null or invalid
*/","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkPrimitive,org.apache.hadoop.io.ArrayPrimitiveWritable:checkPrimitive(java.lang.Class),71,79,"/**
* Validates component type for processing.
* @param componentType the class type of the component to validate
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkDeclaredComponentType,org.apache.hadoop.io.ArrayPrimitiveWritable:checkDeclaredComponentType(java.lang.Class),81,88,"/**
* Validates input array component type against declared type.
* @param componentType the runtime type of array components
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkArray,org.apache.hadoop.io.ArrayPrimitiveWritable:checkArray(java.lang.Object),90,98,"/**
* Validates that the input is a non-null array.
* @param value object to validate
* @throws HadoopIllegalArgumentException if value is null or not an array
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseGetLevelArgs,"org.apache.hadoop.log.LogLevel$CLI:parseGetLevelArgs(java.lang.String[],int)",172,188,"/**
* Parses and sets operation for getting level.
* @param args command line arguments array
* @param index current argument index
* @return next argument index after parsing
* @throws HadoopIllegalArgumentException if parameters are invalid or operation is redundant
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseSetLevelArgs,"org.apache.hadoop.log.LogLevel$CLI:parseSetLevelArgs(java.lang.String[],int)",190,207,"/**
 * Parses and sets logging level from command-line arguments.
 * @param args command-line arguments array
 * @param index current argument index
 * @return next argument index after processing
 * @throws HadoopIllegalArgumentException if operation is redundant or parameters are missing
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,stopProxy,org.apache.hadoop.ipc.RPC:stopProxy(java.lang.Object),792,821,"/**
* Closes the given proxy if it's Closeable.
* @param proxy object to be closed
*/","* Stop the proxy. Proxy must either implement {@link Closeable} or must have
   * associated {@link RpcInvocationHandler}.
   * 
   * @param proxy
   *          the RPC proxy object to be stopped
   * @throws HadoopIllegalArgumentException
   *           if the proxy does not implement {@link Closeable} interface or
   *           does not have closeable {@link InvocationHandler}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,<init>,org.apache.hadoop.util.ZKUtil$BadAclFormatException:<init>(java.lang.String),206,208,"/**
 * Constructs a new BadAclFormatException with the specified detail message.
 * @param message the detail message explaining the reason for this exception
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,<init>,org.apache.hadoop.util.ZKUtil$BadAuthFormatException:<init>(java.lang.String),216,218,"/**
 * Constructs a BadAuthFormatException with a specified detail message.
 * @param message the detail message
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,processChecksumOpt,"org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt,int)",302,328,"/**
* Determines checksum options based on default and user settings.
* @param defaultOpt default checksum option
* @param userOpt user-defined checksum option
* @param userBytesPerChecksum user-specified bytes per checksum
* @return configured ChecksumOpt instance
*/","* A helper method for processing user input and default value to 
     * create a combined checksum option. This is a bit complicated because
     * bytesPerChecksum is kept for backward compatibility.
     *
     * @param defaultOpt Default checksum option
     * @param userOpt User-specified checksum option. Ignored if null.
     * @param userBytesPerChecksum User-specified bytesPerChecksum
     *                Ignored if {@literal <} 0.
     * @return ChecksumOpt.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setPermission,"org.apache.hadoop.fs.DelegateToFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",221,226,"/**
* Sets file permissions.
* @param f file path
* @param permission new file permissions
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setPermission,"org.apache.hadoop.fs.FilterFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",545,549,"/**
 * Sets file permissions on specified path.
 * @param p Path to the file or directory
 * @param permission Desired file permissions
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",877,882,"/**
* Resolves and deletes a snapshot in the specified file system path.
* @param path the file system path to resolve
* @param snapshotName the name of the snapshot to delete
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,deleteSnapshot,"org.apache.hadoop.fs.FilterFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",409,413,"/**
* Creates a snapshot of the specified file system path.
* @param path the file system path to snapshot
* @param snapshotName name of the snapshot
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getDefaultPortIfDefined,org.apache.hadoop.fs.DelegateToFileSystem:getDefaultPortIfDefined(org.apache.hadoop.fs.FileSystem),69,72,"/**
* Retrieves default port from FileSystem, falling back to delegate port.
* @param theFsImpl FileSystem implementation instance
* @return default port or DELEGATE_TO_FS_DEFAULT_PORT if not set
*/","* Returns the default port if the file system defines one.
   * {@link FileSystem#getDefaultPort()} returns 0 to indicate the default port
   * is undefined.  However, the logic that consumes this value expects to
   * receive -1 to indicate the port is undefined, which agrees with the
   * contract of {@link URI#getPort()}.
   *
   * @param theFsImpl file system to check for default port
   * @return default port, or -1 if default port is undefined",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,canonicalizeUri,org.apache.hadoop.fs.FileSystem:canonicalizeUri(java.net.URI),402,417,"/**
* Modifies URI by setting query if path is empty.
* @param uri original URI to be modified
* @return modified URI with updated query
*/","* Canonicalize the given URI.
   *
   * This is implementation-dependent, and may for example consist of
   * canonicalizing the hostname using DNS and adding the default
   * port if not specified.
   *
   * The default implementation simply fills in the default port if
   * not specified and if {@link #getDefaultPort()} returns a
   * default port.
   *
   * @param uri url.
   * @return URI
   * @see NetUtils#getCanonicalUri(URI, int)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.DelegateToFileSystem:getInitialWorkingDirectory(),74,77,"/**
 * Delegates to the underlying file system implementation.
 * @return Path object from the file system
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.FilterFileSystem:getInitialWorkingDirectory(),324,327,"/**
 * Returns a file system path.
 * @return Path object from file system operations
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.FilterFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),484,488,"/**
* Checks file status.
* @param f file path to check
* @return FileStatus object
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws UnsupportedFileSystemException if file system is unsupported
* @throws IOException for other I/O errors
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.LocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),162,165,"/**
 * Retrieves file status.
 * @param f file path
 * @return FileStatus object
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),136,146,"/**
* Retrieves and updates file status.
* @param f file path
* @return FileStatus object with updated metadata
* @throws IOException on I/O errors
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getLinkTarget,org.apache.hadoop.fs.DelegateToFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),257,260,"/**
 * Delegates file processing to the filesystem implementation.
 * @param f file path to process
 * @return processed file path
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getLinkTarget,org.apache.hadoop.fs.FilterFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),494,496,"/**
 * Delegates file processing to another service.
 * @param f file path to process
 * @return processed file path
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,getLinkTarget,org.apache.hadoop.fs.LocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),167,170,"/**
 * Delegates file processing to underlying filesystem.
 * @param f file path to process
 * @return processed file path
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,truncate,"org.apache.hadoop.fs.DelegateToFileSystem:truncate(org.apache.hadoop.fs.Path,long)",200,204,"/**
* Sets file length to specified value.
* @param f file path
* @param newLength new file length in bytes
* @return true if operation successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,truncate,"org.apache.hadoop.fs.FilterFileSystem:truncate(org.apache.hadoop.fs.Path,long)",260,263,"/**
 * Sets file length to specified value.
 * @param f Path to the file
 * @param newLength New length of the file
 * @return true if operation is successful, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setReplication,"org.apache.hadoop.fs.DelegateToFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",228,233,"/**
* Sets file replication factor.
* @param f file path
* @param replication desired replication level
* @return true if successful, false otherwise
* @throws IOException on I/O errors
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setReplication,"org.apache.hadoop.fs.FilterFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",240,243,"/**
 * Checks file existence and sets replication factor.
 * @param src file path to check
 * @param replication desired replication factor
 * @return true if file exists, false otherwise
 * @throws IOException on I/O errors
 */","* Set replication for an existing file.
   * 
   * @param src file name
   * @param replication new replication
   * @throws IOException raised on errors performing I/O.
   * @return true if successful;
   *         false if file does not exist or is a directory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setTimes,"org.apache.hadoop.fs.DelegateToFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",235,239,"/**
 * Updates file timestamps and calls helper method.
 * @param f file path to update
 * @param mtime modified time in milliseconds
 * @param atime accessed time in milliseconds
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,updateTime,org.apache.hadoop.fs.shell.TouchCommands$Touch:updateTime(org.apache.hadoop.fs.shell.PathData),177,195,"/**
* Updates file timestamps based on configuration.
* @param item PathData object containing file path and metadata
* @throws IOException if an I/O error occurs during timestamp update
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setTimes,"org.apache.hadoop.fs.FilterFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",539,543,"/**
* Sets modification and access times for a file.
* @param p path to the file
* @param mtime new modification time in milliseconds
* @param atime new access time in milliseconds
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setVerifyChecksum,org.apache.hadoop.fs.DelegateToFileSystem:setVerifyChecksum(boolean),241,244,"/**
 * Delegates checksum verification to file system implementation.
 * @param verifyChecksum flag to enable or disable checksum verification
 * @throws IOException if an I/O error occurs during the operation
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setVerifyChecksum,org.apache.hadoop.fs.FilterFileSystem:setVerifyChecksum(boolean),512,515,"/**
 * Delegates checksum verification to file system.
 * @param verifyChecksum flag to enable or disable checksum verification
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,supportsSymlinks,org.apache.hadoop.fs.DelegateToFileSystem:supportsSymlinks(),246,249,"/**
 * Delegates call to fsImpl's m1 method.
 * @return result of fsImpl.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,supportsSymlinks,org.apache.hadoop.fs.FilterFileSystem:supportsSymlinks(),490,492,"/**
 * Delegates to fs.m1().
 * @return Result of fs.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,createSymlink,"org.apache.hadoop.fs.DelegateToFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",251,255,"/**
* Creates a symbolic link.
* @param target path to the target file or directory
* @param link path where the symbolic link should be created
* @param createParent if true, creates parent directories as needed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createSymlink,"org.apache.hadoop.fs.FilterFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",476,482,"/**
 * Creates a symbolic link.
 * @param target the path where the link will point to
 * @param link the path of the new symbolic link
 * @param createParent if true, creates parent directories if necessary
 * @throws AccessControlException if access is denied
 * @throws FileAlreadyExistsException if the link already exists
 * @throws FileNotFoundException if target does not exist
 * @throws ParentNotDirectoryException if a parent is not a directory
 * @throws UnsupportedFileSystemException if operation is unsupported
 * @throws IOException for other I/O errors
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,createSymlink,"org.apache.hadoop.fs.LocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",156,160,"/**
 * Creates a symbolic link at the specified target.
 * @param target path where the symbolic link will be created
 * @param link path to the target file or directory
 * @param createParent if true, creates parent directories if they do not exist
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFs:truncate(org.apache.hadoop.fs.Path,long)",559,566,"/**
* Sets file length.
* @param f path to the file
* @param newLength new length of the file
* @return true if successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,create,"org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",69,75,"/**
* Throws UnsupportedOperationException as this method is not implemented.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,create,"org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",69,75,"/**
* Throws UnsupportedOperationException as this method is not implemented.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,append,"org.apache.hadoop.fs.http.HttpsFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",77,81,"/**
 * Throws UnsupportedOperationException as the operation is not supported.
 * @param path file system path
 * @param i integer parameter
 * @param progressable progress tracking object
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,append,"org.apache.hadoop.fs.http.HttpFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",77,81,"/**
 * Throws UnsupportedOperationException as this method is not implemented.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,rename,"org.apache.hadoop.fs.http.HttpsFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,86,"/**
 * Throws UnsupportedOperationException.
 * @param path first file path
 * @param path1 second file path
 * @throws IOException if operation fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,rename,"org.apache.hadoop.fs.http.HttpFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,86,"/**
 * Throws UnsupportedOperationException as the method is not implemented.
 * @param path first file path
 * @param path1 second file path
 * @return never returns a value
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,delete,"org.apache.hadoop.fs.http.HttpsFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",88,91,"/**
 * Throws an UnsupportedOperationException as this method is not implemented.
 * @param path Path to be masked
 * @param b Boolean flag
 * @return Always throws exception, no return value
 * @throws IOException if operation fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,delete,"org.apache.hadoop.fs.http.HttpFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",88,91,"/**
 * Throws UnsupportedOperationException as method is not implemented.
 * @param path file path to be masked
 * @param b boolean flag indicating whether to mask or unmask
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,listStatus,org.apache.hadoop.fs.http.HttpsFileSystem:listStatus(org.apache.hadoop.fs.Path),93,96,"/**
 * Masks file status by path.
 * @param path file path to mask
 * @throws IOException if an I/O error occurs
 * @return array of FileStatus objects
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,listStatus,org.apache.hadoop.fs.http.HttpFileSystem:listStatus(org.apache.hadoop.fs.Path),93,96,"/**
 * Throws an unsupported operation exception.
 * @param path file path to be checked
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,mkdirs,"org.apache.hadoop.fs.http.HttpsFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",107,111,"/**
 * Masks file permissions.
 * @param path file path to modify permissions
 * @param fsPermission new permissions to apply
 * @return true if successful, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,mkdirs,"org.apache.hadoop.fs.http.HttpFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",107,111,"/**
 * Masks file permissions at specified path.
 * @param path file system path to modify
 * @param fsPermission new file permission settings
 * @return true if operation successful, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.http.HttpsFileSystem:getWorkingDirectory(),102,105,"/**
 * Returns the working directory path.
 * @return Path representing the working directory
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.http.HttpFileSystem:getWorkingDirectory(),102,105,"/**
 * Returns the working directory path.
 * @return Path representing the working directory
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.http.HttpsFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),98,100,"/**
 * Masks content at specified file path.
 * @param path file path to be masked
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.http.HttpFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),98,100,"/**
* Masks a file at the given path.
* @param path location of the file to be masked
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getUri,org.apache.hadoop.fs.http.HttpsFileSystem:getUri(),56,59,"/**
 * Returns the URI associated with the function.
 * @return URI of the function
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getUri,org.apache.hadoop.fs.http.HttpFileSystem:getUri(),56,59,"/**
 * Returns the URI associated with the function.
 * @return URI representing the function's endpoint
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,skip,org.apache.hadoop.fs.BufferedFSInputStream:skip(long),70,78,"/**
 * Applies a mask to a number.
 * @param n the input number
 * @return the masked number or 0 if input is non-positive
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,minSeekForVectorReads,org.apache.hadoop.fs.BufferedFSInputStream:minSeekForVectorReads(),169,172,"/**
 * Delegates to m1 of PositionedReadable.
 * @return result from delegate's m1
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,minSeekForVectorReads,org.apache.hadoop.fs.FSDataInputStream:minSeekForVectorReads(),294,297,"/**
* Delegates to m1 of PositionedReadable interface.
* @return result of m1 call on input stream
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,maxReadSizeForVectorReads,org.apache.hadoop.fs.BufferedFSInputStream:maxReadSizeForVectorReads(),174,177,"/**
 * Calls m1 on the input stream.
 * @return result of m1 from PositionedReadable
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,maxReadSizeForVectorReads,org.apache.hadoop.fs.FSDataInputStream:maxReadSizeForVectorReads(),299,302,"/**
 * Delegates call to m1 on PositionedReadable input.
 * @return result of m1 from PositionedReadable
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBPartHandle.java,from,org.apache.hadoop.fs.BBPartHandle:from(java.nio.ByteBuffer),40,42,"/**
 * Creates a PartHandle from a ByteBuffer.
 * @param byteBuffer source data buffer
 * @return PartHandle instance wrapping the ByteBuffer
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBPartHandle.java,equals,org.apache.hadoop.fs.BBPartHandle:equals(java.lang.Object),54,62,"/**
* Compares this handle with another object.
* @param other the object to compare with
* @return true if other is a PartHandle and their inner parts match, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",891,897,"/**
* Applies a storage policy to a file path.
* @param path the file path to apply the policy to
* @param policyName the name of the storage policy
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setStoragePolicy,"org.apache.hadoop.fs.FilterFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",420,424,"/**
 * Delegates file system operation to underlying implementation.
 * @param path file path to operate on
 * @param policyName security policy name
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,disconnect,org.apache.hadoop.fs.ftp.FTPFileSystem:disconnect(org.apache.commons.net.ftp.FTPClient),248,260,"/**
* Disconnects the FTP client and logs any logout failure.
* @param client FTPClient instance to be disconnected
*/","* Logout and disconnect the given FTPClient. *
   * 
   * @param client
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,close,org.apache.hadoop.fs.ftp.FTPFileSystem$1:close(),104,107,"/**
 * Calls method m1 on the out object.
 * @throws IOException if an I/O error occurs
 */",* Close the underlying output stream.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,close,org.apache.hadoop.fs.ftp.FTPInputStream:close(),103,121,"/**
* Executes a synchronized method with error handling.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFsAction,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFsAction(int,org.apache.commons.net.ftp.FTPFile)",440,453,"/**
 * Determines file permissions for a given access group.
 * @param accessGroup the group to check permissions for
 * @param ftpFile the FTP file to evaluate permissions on
 * @return FsAction representing the combined permissions for the group
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hasNext,org.apache.hadoop.fs.FileSystem$DirListingIterator:hasNext(),2320,2324,"/**
 * Checks conditions involving entry count and mask status.
 * @return true if index is within bounds or mask is active, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary:<init>(),149,150,"/**
 * Deprecated default constructor for ContentSummary.
 */",Constructor deprecated by ContentSummary.Builder,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,"org.apache.hadoop.fs.ContentSummary:<init>(long,long,long,long,long,long)",177,187,"/**
* Constructs a ContentSummary object.
* @param length total content length
* @param fileCount number of files
* @param directoryCount number of directories
* @param quota storage quota
* @param spaceConsumed current space consumed
* @param spaceQuota space quota limit
*/","* Constructor, deprecated by ContentSummary.Builder.
   *
   * @param length length.
   * @param fileCount file count.
   * @param directoryCount directory count.
   * @param quota quota.
   * @param spaceConsumed space consumed.
   * @param spaceQuota space quota.
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,equals,org.apache.hadoop.fs.ContentSummary:equals(java.lang.Object),257,275,"/**
* Compares this object with another for equality.
* @param to the object to compare
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,hashCode,org.apache.hadoop.fs.ContentSummary:hashCode(),277,284,"/**
* Computes a combined hash using XOR of multiple methods' results.
* @return int value representing the computed hash
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",824,829,"/**
* Reads file content from given path.
* @param path file path
* @param name file name
* @return byte array of file content
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttr,"org.apache.hadoop.fs.FilterFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",371,374,"/**
 * Reads file content as bytes.
 * @param path file path
 * @param name file name
 * @return file content in byte array
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,getDelay,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:getDelay(java.util.concurrent.TimeUnit),82,86,"/**
 * Converts remaining time to specified unit.
 * @param unit target time unit
 * @return time in the specified unit until renewal
 */",Get the delay until this event should happen.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,updateRenewalTime,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:updateRenewalTime(long),116,118,"/**
 * Updates renewal time with a masked delay.
 * @param delay the original delay value
 */","* Set a new time for the renewal.
     * It can only be called when the action is not in the queue or any
     * collection because the hashCode may change
     * @param delay the renewal time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,touch,org.apache.hadoop.ipc.Client$Connection:touch(),465,467,"/**
 * Updates the last activity timestamp.
 */",Update lastActivity with the current time.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ThreadUtil.java,sleepAtLeastIgnoreInterrupts,org.apache.hadoop.util.ThreadUtil:sleepAtLeastIgnoreInterrupts(long),39,50,"/**
* Pauses execution for a specified duration.
* @param millis duration to pause in milliseconds
*/","* Cause the current thread to sleep as close as possible to the provided
   * number of milliseconds. This method will log and ignore any
   * {@link InterruptedException} encountered.
   * 
   * @param millis the number of milliseconds for the current thread to sleep",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,now,org.apache.hadoop.util.Timer:now(),39,41,"/**
 * Returns the current time in milliseconds.
 * @return Current time in milliseconds
 */","* Current system time.  Do not use this to calculate a duration or interval
   * to sleep, because it will be broken by settimeofday.  Instead, use
   * monotonicNow.
   * @return current time in msec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AsyncDiskService.java,awaitTermination,org.apache.hadoop.util.AsyncDiskService:awaitTermination(long),131,147,"/**
* Waits for all thread pools to terminate within a specified time.
* @param milliseconds maximum wait time in milliseconds
* @return true if all thread pools terminated, false otherwise
*/","* Wait for the termination of the thread pools.
   * 
   * @param milliseconds  The number of milliseconds to wait
   * @return   true if all thread pools are terminated without time limit
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,ceiling,"org.apache.hadoop.fs.TrashPolicyDefault$Emptier:ceiling(long,long)",322,324,"/**
 * Masks and adjusts time by interval.
 * @param time original timestamp
 * @param interval adjustment period
 * @return adjusted timestamp
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,readChunk,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])",266,307,"/**
 * Reads data from a position with checksum validation.
 * @param pos starting position in the data stream
 * @param buf buffer to store read data
 * @param offset offset within the buffer
 * @param len number of bytes to read
 * @param checksum array for storing checksums
 * @return number of bytes actually read
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,seekToNewSource,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:seekToNewSource(long),258,264,"/**
* Checks if data at target position is valid.
* @param targetPos position to check in file
* @return true if data is valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,checkBytes,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:checkBytes(java.nio.ByteBuffer,long,java.nio.ByteBuffer,long,int,org.apache.hadoop.fs.Path)",376,426,"/**
* Validates checksums for data against provided sums.
* @param sumsBytes ByteBuffer containing checksum values
* @param sumsOffset offset in sumsBytes
* @param data ByteBuffer containing the actual data
* @param dataOffset offset in data
* @param bytesPerSum number of bytes per checksum chunk
* @param file path to the data file
* @return original data ByteBuffer if validation passes
* @throws CompletionException with ChecksumException if validation fails
*/","* Check the data against the checksums.
     * @param sumsBytes the checksum data
     * @param sumsOffset where from the checksum file this buffer started
     * @param data the file data
     * @param dataOffset where the file data started (must be a multiple of
     *                  bytesPerSum)
     * @param bytesPerSum how many bytes per a checksum
     * @param file the path of the filename
     * @return the data buffer
     * @throws CompletionException if the checksums don't match",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getSumBufferSize,"org.apache.hadoop.fs.ChecksumFs:getSumBufferSize(int,int,org.apache.hadoop.fs.Path)",124,131,"/**
* Determines optimal buffer size for processing file.
* @param bytesPerSum number of bytes per sum operation
* @param bufferSize initial buffer size
* @param file path to the file being processed
* @return adjusted buffer size based on calculations
* @throws IOException if an I/O error occurs while reading file properties
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,open,org.apache.hadoop.fs.AbstractFileSystem:open(org.apache.hadoop.fs.Path),721,724,"/**
* Opens an input stream for reading from a file.
* @param f file path
* @return FSDataInputStream for the file
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws UnresolvedLinkException if link cannot be resolved
* @throws IOException for other I/O errors
*/","* The specification of this method matches that of
   * {@link FileContext#open(Path)} except that Path f must be for this
   * file system.
   *
   * @param f the path.
   * @throws AccessControlException access control exception.
   * @throws FileNotFoundException file not found exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.
   * @return input stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getServerDefaults,org.apache.hadoop.fs.FilterFs:getServerDefaults(org.apache.hadoop.fs.Path),163,166,"/**
 * Retrieves file system defaults for a given path.
 * @param f file path
 * @return FsServerDefaults object
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getChecksumFileLength,"org.apache.hadoop.fs.ChecksumFs:getChecksumFileLength(org.apache.hadoop.fs.Path,long)",111,113,"/**
* Masks file size using specified path.
* @param file file path
* @param fileSize original file size
* @return masked file size
*/","* Return the length of the checksum file given the size of the
   * actual file.
   *
   * @param file the file path.
   * @param fileSize file size.
   * @return check sum file length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,listLocatedStatus,org.apache.hadoop.fs.ChecksumFs:listLocatedStatus(org.apache.hadoop.fs.Path),582,614,"/**
* Filters files in a directory.
* @param f path to the directory
* @return iterator over filtered file statuses
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,useStatIfAvailable,org.apache.hadoop.fs.RawLocalFileSystem:useStatIfAvailable(),96,99,"/**
* Toggles the use of deprecated file status.
* @VisibleForTesting indicates this method is for testing purposes only.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createOutputStream,"org.apache.hadoop.fs.RawLocalFileSystem:createOutputStream(org.apache.hadoop.fs.Path,boolean)",570,573,"/**
 * Returns an OutputStream for file operations.
 * @param f Path to the file
 * @param append true to append data to the file, false to overwrite
 * @return OutputStream for writing to the file
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatus,org.apache.hadoop.fs.FileSystem:getStatus(org.apache.hadoop.fs.Path),3041,3043,"/**
 * Returns a status with maximum values.
 * @param p file system path (unused)
 * @return FsStatus object with max capacity and no free space
 */","* Returns a status object describing the use and capacity of the
   * filesystem. If the filesystem has multiple partitions, the
   * use and capacity of the partition pointed to by the specified
   * path is reflected.
   * @param p Path for which status should be obtained. null means
   * the default partition.
   * @return a FsStatus object
   * @throws IOException
   *           see specific implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFsStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFsStatus(),1130,1133,"/**
 * Returns an instance of FsStatus with all values set to 0.
 * @return FsStatus object initialized with zeros
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFsStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFsStatus(),445,449,"/**
* Returns file system status with zero values.
* @return FsStatus object initialized to zeros
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,registerCommands,org.apache.hadoop.fs.FsShellPermissions:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),50,54,"/**
* Registers Chmod, Chown, and Chgrp commands with the factory.
* @param factory CommandFactory instance to register commands with
*/","* Register the permission related commands with the factory
   * @param factory the command factory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,registerCommands,org.apache.hadoop.fs.shell.Test:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),37,39,"/**
 * Registers a command with the factory.
 * @param factory CommandFactory instance to register the command with
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,registerCommands,org.apache.hadoop.fs.shell.SnapshotCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),41,45,"/**
* Registers snapshot command classes with the factory.
* @param factory CommandFactory instance to register commands with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,registerCommands,org.apache.hadoop.fs.shell.find.Find:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),49,51,"/**
 * Registers Find command with CommandFactory.
 * @param factory instance of CommandFactory to register command with
 */","* Register the names for the count command
   * 
   * @param factory the command factory that will instantiate this class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,registerCommands,org.apache.hadoop.fs.shell.Head:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),40,42,"/**
* Registers Head command with factory.
* @param factory CommandFactory instance to register with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,registerCommands,org.apache.hadoop.fs.shell.Ls:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),45,48,"/**
 * Configures command factory with LS and LSR commands.
 * @param factory CommandFactory instance to configure
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,registerCommands,org.apache.hadoop.fs.shell.Tail:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),42,44,"/**
 * Registers the Tail command with the given factory.
 * @param factory CommandFactory instance to register the command with
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,registerCommands,org.apache.hadoop.fs.shell.FsUsage:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),46,50,"/**
* Registers command classes with factory.
* @param factory CommandFactory instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,registerCommands,org.apache.hadoop.fs.shell.XAttrCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),42,45,"/**
* Registers Getfattr and Setfattr commands with the factory.
* @param factory CommandFactory instance to register commands with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,registerCommands,org.apache.hadoop.fs.shell.Delete:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),50,55,"/**
* Registers command classes with their respective aliases.
* @param factory CommandFactory instance to register commands
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,registerCommands,org.apache.hadoop.fs.shell.Count:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),46,48,"/**
 * Registers Count command with factory.
 * @param factory CommandFactory instance to register command
 */","* Register the names for the count command
   * @param factory the command factory that will instantiate this class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,registerCommands,org.apache.hadoop.fs.shell.TouchCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),43,46,"/**
 * Registers touch commands with the factory.
 * @param factory CommandFactory instance to register commands with
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,registerCommands,org.apache.hadoop.fs.shell.Mkdir:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),39,41,"/**
 * Registers Mkdir command with factory.
 * @param factory CommandFactory instance to register command with
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Concat.java,registerCommands,org.apache.hadoop.fs.shell.Concat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),38,40,"/**
 * Registers Concat command with factory.
 * @param factory CommandFactory instance to register command
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,registerCommands,org.apache.hadoop.fs.shell.CopyCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),44,52,"/**
* Registers command classes with their respective flags.
* @param factory CommandFactory instance to register commands
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,registerCommands,org.apache.hadoop.fs.shell.MoveCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),35,39,"/**
* Registers command classes with a factory.
* @param factory CommandFactory instance to register commands with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,registerCommands,org.apache.hadoop.fs.shell.Stat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),53,55,"/**
 * Registers Stat command with factory.
 * @param factory CommandFactory instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,registerCommands,org.apache.hadoop.fs.shell.Display:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),62,66,"/**
* Registers command classes with factory.
* @param factory CommandFactory instance to register commands
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,registerCommands,org.apache.hadoop.fs.shell.AclCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),47,50,"/**
* Registers Getfacl and Setfacl commands with the factory.
* @param factory CommandFactory instance to register commands with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,registerCommands,org.apache.hadoop.fs.shell.Truncate:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),35,37,"/**
 * Registers Truncate command with CommandFactory.
 * @param factory CommandFactory instance to register the command with
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,registerCommands,org.apache.hadoop.fs.shell.SetReplication:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),37,39,"/**
 * Registers SetReplication command with factory.
 * @param factory CommandFactory instance to register command
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat:<init>(java.lang.String,int,int,java.lang.String[])",45,48,"/**
* Deprecated constructor for CommandFormat.
* @param name command name (unused)
* @param min minimum number of arguments
* @param max maximum number of arguments
* @param possibleOpt possible options (varargs)
*/","* @deprecated use replacement since name is an unused parameter
   * @param name of command, but never used
   * @param min see replacement
   * @param max see replacement
   * @param possibleOpt see replacement
   * @see #CommandFormat(int, int, String...)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,"org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(org.apache.hadoop.fs.Path,boolean)",361,403,"/**
 * Deletes old checkpoints from the trash.
 * @param trashRoot root directory of the trash
 * @param deleteImmediately if true, deletes all checkpoints immediately
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystemPathHandle.java,verify,org.apache.hadoop.fs.LocalFileSystemPathHandle:verify(org.apache.hadoop.fs.FileStatus),54,61,"/**
* Validates file status.
* @param stat FileStatus object to validate
* @throws InvalidPathHandleException if file is null or content has changed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",802,807,"/**
* Applies ACL to a file path.
* @param path target file path
* @param aclSpec list of ACL entries to apply
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setAcl,"org.apache.hadoop.fs.FilterFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",349,352,"/**
* Applies ACL entries to a file system path.
* @param path file system path to modify
* @param aclSpec list of ACL entries to apply
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,startUpload,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:startUpload(org.apache.hadoop.fs.Path),98,110,"/**
* Uploads file to storage.
* @param filePath path of the file to upload
* @return CompletableFuture with UploadHandle or throws IOException
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,putPart,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:putPart(org.apache.hadoop.fs.UploadHandle,int,org.apache.hadoop.fs.Path,java.io.InputStream,long)",112,122,"/**
* Uploads a file part.
* @param uploadId unique identifier for the upload
* @param partNumber part number to be uploaded
* @param filePath path to the file
* @param inputStream input stream of the file
* @param lengthInBytes length of the file in bytes
* @return CompletableFuture containing PartHandle or null if upload fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,complete,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:complete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)",176,185,"/**
* Processes file upload and returns a path handle.
* @param uploadId unique identifier for the upload
* @param filePath path to the uploaded file
* @param handleMap mapping of part handles
* @return CompletableFuture containing PathHandle result
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,eval,org.apache.hadoop.fs.impl.FutureIOSupport:eval(org.apache.hadoop.util.functional.CallableRaisingIOE),179,182,"/**
* Executes a Callable and returns a CompletableFuture.
* @param callable task to be executed
* @return CompletableFuture containing the result of the callable
*/","* Evaluate a CallableRaisingIOE in the current thread,
   * converting IOEs to RTEs and propagating.
   * See {@link FutureIO#eval(CallableRaisingIOE)}.
   *
   * @param callable callable to invoke
   * @param <T> Return type.
   * @return the evaluated result.
   * @throws UnsupportedOperationException fail fast if unsupported
   * @throws IllegalArgumentException invalid argument",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,concat,"org.apache.hadoop.fs.FilterFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",188,191,"/**
 * Delegates file operation to underlying filesystem.
 * @param f target file path
 * @param psrcs source paths array
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,rejectUnknownMandatoryKeys,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:rejectUnknownMandatoryKeys(java.util.Collection,java.lang.String)",358,362,"/**
* Validates keys with optional additional error text.
* @param knownKeys collection of known keys
* @param extraErrorText additional error message for validation
* @throws IllegalArgumentException if validation fails
*/","* Reject a configuration if one or more mandatory keys are
   * not in the set of mandatory keys.
   * The first invalid key raises the exception; the order of the
   * scan and hence the specific key raising the exception is undefined.
   * @param knownKeys a possibly empty collection of known keys
   * @param extraErrorText extra error text to include.
   * @throws IllegalArgumentException if any key is unknown.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileRangeImpl.java,toString,org.apache.hadoop.fs.impl.FileRangeImpl:toString(),54,58,"/**
* Generates formatted string with range and reference details.
* @return Formatted string describing range and reference
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,pathCapabilities,org.apache.hadoop.fs.impl.FlagSet:pathCapabilities(),209,213,"/**
* Masks names based on capability check.
* @return List of filtered names with capabilities
*/","* Generate the list of capabilities.
   * @return a possibly empty list.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,buildHttpReferrer,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:buildHttpReferrer(),190,223,"/**
 * Constructs a masked URI for auditing purposes.
 * @return Masked URI as a String or empty string if failed
 */","* Build the referrer string.
   * This includes dynamically evaluating all of the evaluated
   * attributes.
   * If there is an error creating the string it will be logged once
   * per entry, and """" returned.
   * @return a referrer string or """"",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,<init>,"org.apache.hadoop.util.WeakReferenceMap:<init>(java.util.function.Function,java.util.function.Consumer)",100,106,"/**
* Initializes a WeakReferenceMap with a value factory and an optional reference lost callback.
* @param factory function to create values for keys
* @param referenceLost consumer called when a key's reference is lost, may be null
*/","* instantiate.
   * @param factory supplier of new instances
   * @param referenceLost optional callback on lost references.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/StoreImplementationUtils.java,hasCapability,"org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.OutputStream,java.lang.String)",80,82,"/**
 * Masks output stream with specified capability.
 * @param out OutputStream to be masked
 * @param capability masking capability as string
 * @return true if masking successful, false otherwise
 */","* Probe for an output stream having a capability; returns true
   * if the stream implements {@link StreamCapabilities} and its
   * {@code hasCapabilities()} method returns true for the capability.
   * @param out output stream
   * @param capability capability to probe for
   * @return true if the stream declares that it supports the capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/StoreImplementationUtils.java,hasCapability,"org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.InputStream,java.lang.String)",92,94,"/**
 * Checks if input stream has specified capability.
 * @param in input stream to check
 * @param capability capability to verify
 * @return true if capability is present, false otherwise
 */","* Probe for an input stream having a capability; returns true
   * if the stream implements {@link StreamCapabilities} and its
   * {@code hasCapabilities()} method returns true for the capability.
   * @param in input stream
   * @param capability capability to probe for
   * @return true if the stream declares that it supports the capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/ExecutorServiceFuturePool.java,shutdown,"org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:shutdown(org.slf4j.Logger,long,java.util.concurrent.TimeUnit)",81,83,"/**
* Executes a task with a timeout using Hadoop executors.
* @param logger Logger instance for logging
* @param timeout Timeout value
* @param unit TimeUnit for the timeout
*/","* Utility to shutdown the {@link ExecutorService} used by this class. Will wait up to a
   * certain timeout for the ExecutorService to gracefully shutdown.
   *
   * @param logger Logger
   * @param timeout the maximum time to wait
   * @param unit the time unit of the timeout argument",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,<init>,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:<init>(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),128,131,"/**
 * Initializes an End operation.
 * @param op Operation object to be ended
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getSummary(java.lang.StringBuilder),133,137,"/**
 * Appends ""E"" to StringBuilder and calls superclass's m2.
 * @param sb StringBuilder instance to modify
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getDebugInfo,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getDebugInfo(),139,142,"/**
 * Modifies and returns string from superclass method.
 * @return modified string with prefix ""***""
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,add,org.apache.hadoop.fs.impl.prefetch.BlockOperations:add(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),160,166,"/**
* Processes an operation.
* @param op the operation to process
* @return the processed operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,canRelease,org.apache.hadoop.fs.impl.prefetch.BufferPool:canRelease(org.apache.hadoop.fs.impl.prefetch.BufferData),318,322,"/**
 * Checks if buffer state is DONE or READY.
 * @param data buffer data object
 * @return true if state is DONE or READY, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:get(),411,420,"/**
* Handles block processing with error logging.
* @return null always
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,distance,"org.apache.hadoop.fs.impl.prefetch.BufferPool:distance(org.apache.hadoop.fs.impl.prefetch.BufferData,int)",226,228,"/**
 * Applies mask to buffer data.
 * @param data BufferData object containing mask and value
 * @param blockNumber number to subtract from mask
 * @return masked result as integer
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,find,org.apache.hadoop.fs.impl.prefetch.BufferPool:find(int),305,316,"/**
* Retrieves buffer data by block number.
* @param blockNumber the block identifier to search for
* @return BufferData object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,close,org.apache.hadoop.fs.impl.prefetch.BufferPool:close(),257,275,"/**
* Processes buffer data, updates pool, and records statistics.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,acquire,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:acquire(),70,73,"/**
 * Calls m1 with true flag.
 * @return result of m1(true)
 */",* Acquires a resource blocking if necessary until one becomes available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,tryAcquire,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:tryAcquire(),78,81,"/**
 * Calls m1 with false flag.
 * @return result of m1(false)
 */",* Acquires a resource blocking if one is immediately available. Otherwise returns null.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,close,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:close(),113,124,"/**
* Processes and clears created items.
* Calls m1 on each item, then processes and clears both items and createdItems lists.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:numAvailable(),148,150,"/**
 * Calculates a masked value based on size and item count.
 * @return The calculated masked integer value.
 */","* Number of items available to be acquired. Mostly for testing purposes.
   * @return the number available.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,duration,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:duration(),144,146,"/**
 * Computes the difference between m1 and op's m1, scaled by 1e9.
 * @return Scaled difference as a double
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,analyze,org.apache.hadoop.fs.impl.prefetch.BlockOperations:analyze(java.lang.StringBuilder),297,367,"/**
 * Analyzes operations and appends issues to StringBuilder.
 * @param sb StringBuilder to append results
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,<init>,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:<init>(org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics,int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",223,234,"/**
 * Initializes a cache with block management and prefetching statistics.
 * @param prefetchingStatistics object to track prefetching stats
 * @param maxBlocksCount maximum number of blocks to cache, must be > 0
 * @param trackerFactory factory for creating duration trackers, can be null
 */","* Constructs an instance of a {@code SingleFilePerBlockCache}.
   *
   * @param prefetchingStatistics statistics for this stream.
   * @param maxBlocksCount max blocks count to be kept in cache at any time.
   * @param trackerFactory tracker with statistics to update",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,<init>,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",71,82,"/**
 * Constructs a SemaphoredDelegatingExecutor.
 * @param executorDelegatee the underlying ExecutorService
 * @param permitCount number of permits for concurrency control
 * @param fair true if semaphore should use fair ordering policy
 * @param trackerFactory factory for creating duration trackers, may be null
 */","* Instantiate.
   * @param executorDelegatee Executor to delegate to
   * @param permitCount number of permits into the queue permitted
   * @param fair should the semaphore be ""fair""
   * @param trackerFactory duration tracker factory.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,addToLinkedListHead,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListHead(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),314,321,"/**
* Masks an entry by locking and processing it.
* @param entry the Entry object to be masked
*/","* Helper method to add the given entry to the head of the linked list.
   *
   * @param entry Block entry to add.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,validateEntry,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:validateEntry(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry,java.nio.ByteBuffer)",551,566,"/**
* Validates entry size and checksum against buffer.
* @param entry the data entry to validate
* @param buffer the ByteBuffer containing data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setDone,org.apache.hadoop.fs.impl.prefetch.BufferData:setDone(),223,231,"/**
* Marks the operation as done and validates checksum.
* @throws IllegalStateException if checksum has changed
*/",* Indicates that this block is no longer of use and can be reclaimed.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,close,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:close(),502,508,"/**
* Masks functionality and logs status.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,toString,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:toString(),540,549,"/**
* Constructs a stats string with block information.
* @return formatted stats string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,toString,org.apache.hadoop.fs.impl.prefetch.BufferData:toString(),289,299,"/**
* Generates a formatted string with block details.
* @return Formatted string containing block information
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,throwIfInvalidBuffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:throwIfInvalidBuffer(),298,300,"/**
 * Validates that 'buffer' is not null.
 * Throws an exception if validation fails.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,bufferSize,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:bufferSize(int),133,136,"/**
 * Sets buffer size and returns builder.
 * @param bufSize desired buffer size
 * @return FutureDataInputStreamBuilder instance
 */","* Set the size of the buffer to be used.
   *
   * @param bufSize buffer size.
   * @return FutureDataInputStreamBuilder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,builder,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:builder(),146,148,"/**
 * Returns a FutureDataInputStreamBuilder instance.
 * @return FutureDataInputStreamBuilder object
 */","* Get the builder.
   * This must be used after the constructor has been invoked to create
   * the actual builder: it allows for subclasses to do things after
   * construction.
   *
   * @return FutureDataInputStreamBuilder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,getForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:getForCurrentThread(),45,47,"/**
 * Applies two transformations to input.
 * @return transformed value
 */","* Get the value for the current thread, creating if needed.
   * @return an instance.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,removeForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:removeForCurrentThread(),53,55,"/**
 * Applies two transformations to an input value.
 * @return transformed value after applying m1 and m2
 */","* Remove the reference for the current thread.
   * @return any reference value which existed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,setForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:setForCurrentThread(java.lang.Object),70,90,"/**
 * Updates value associated with ID.
 * @param newVal new value to set
 * @return updated or existing value
 */","* Set the new value for the current thread.
   * @param newVal new reference to set for the active thread.
   * @return the previously set value, possibly null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,<init>,"org.apache.hadoop.fs.impl.CombinedFileRange:<init>(long,long,org.apache.hadoop.fs.FileRange)",44,47,"/**
* Constructs a CombinedFileRange from an offset, end, and an original FileRange.
* @param offset starting position in the file
* @param end ending position in the file
* @param original original FileRange to be appended
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,merge,"org.apache.hadoop.fs.impl.CombinedFileRange:merge(long,long,org.apache.hadoop.fs.FileRange,int,int)",79,89,"/**
 * Checks and updates file range based on given parameters.
 * @param otherOffset starting offset of the other range
 * @param otherEnd ending offset of the other range
 * @param other FileRange object to compare with
 * @param minSeek minimum seek distance required
 * @param maxSize maximum size allowed for update
 * @return true if updated successfully, false otherwise
 */","* Merge this input range into the current one, if it is compatible.
   * It is assumed that otherOffset is greater or equal the current offset,
   * which typically happens by sorting the input ranges on offset.
   * @param otherOffset the offset to consider merging
   * @param otherEnd the end to consider merging
   * @param other the underlying FileRange to add if we merge
   * @param minSeek the minimum distance that we'll seek without merging the
   *                ranges together
   * @param maxSize the maximum size that we'll merge into a single range
   * @return true if we have merged the range into this one",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createBulkDelete,org.apache.hadoop.fs.FileSystem:createBulkDelete(org.apache.hadoop.fs.Path),5002,5006,"/**
 * Creates and returns a bulk delete operation.
 * @param path file system path to perform deletion on
 * @return BulkDelete instance for the specified path
 * @throws IllegalArgumentException if path is invalid
 * @throws IOException if an I/O error occurs
 */","* Create a bulk delete operation.
   * The default implementation returns an instance of {@link DefaultBulkDeleteOperation}.
   * @param path base path for the operation.
   * @return an instance of the bulk delete.
   * @throws IllegalArgumentException any argument is invalid.
   * @throws IOException if there is an IO problem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getBufferSize,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBufferSize(),64,67,"/**
 * Calls superclass's m1 method.
 * @return result of superclass's m1 method
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getReplication,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getReplication(),69,72,"/**
 * Calls superclass method m1.
 * @return result of superclass's m1 method
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getFlags,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFlags(),74,77,"/**
 * Returns the set of creation flags.
 * @return EnumSet containing CreateFlag values
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getChecksumOpt,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getChecksumOpt(),79,82,"/**
 * Returns checksum option.
 * @return ChecksumOpt object from superclass
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getBlockSize,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBlockSize(),84,87,"/**
 * Calls superclass method m1.
 * @return result of superclass's m1 method
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processOptions,org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processOptions(java.util.LinkedList),153,177,"/**
* Parses command-line arguments for setting extended attributes.
* @param args LinkedList of command-line arguments
* @throws IOException if argument parsing fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeXAttr,"org.apache.hadoop.fs.FilterFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",656,659,"/**
 * Delegates file operation to underlying filesystem.
 * @param path file path
 * @param name file name
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processOptions,org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processOptions(java.util.LinkedList),53,59,"/**
* Processes command-line arguments, throwing an exception for unrecognized options.
* @param args list of command-line arguments
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/And.java,registerExpression,org.apache.hadoop.fs.shell.find.And:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),31,35,"/**
* Configures expression factory with AND operators.
* @param factory ExpressionFactory instance to configure
* @throws IOException if an I/O error occurs during configuration
*/",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,registerExpression,org.apache.hadoop.fs.shell.find.Print:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),30,34,"/**
 * Registers print and print0 expressions with the factory.
 * @param factory ExpressionFactory instance to register expressions with
 */",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,registerExpression,org.apache.hadoop.fs.shell.find.Name:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),33,37,"/**
 * Masks expressions using specified factories.
 * @param factory ExpressionFactory instance to use
 * @throws IOException if an I/O error occurs during masking
 */",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,<init>,org.apache.hadoop.fs.shell.find.Print:<init>(),45,47,"/**
 * Constructs a Print object with a newline character.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name:<init>(boolean),57,62,"/**
 * Constructs a Name object with case sensitivity option.
 * @param caseSensitive true if name comparison is case-sensitive, false otherwise
 */","* Construct a Name {@link Expression} with a specified case sensitivity.
   *
   * @param caseSensitive if true the comparisons are case sensitive.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,apply,"org.apache.hadoop.fs.shell.find.Name:apply(org.apache.hadoop.fs.shell.PathData,int)",82,93,"/**
* Filters PathData based on a pattern.
* @param item PathData to be filtered
* @param depth current depth in the hierarchy
* @return PASS if matches pattern, FAIL otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,addCodec,org.apache.hadoop.io.compress.CompressionCodecFactory:addCodec(org.apache.hadoop.io.compress.CompressionCodec),64,75,"/**
* Registers a compression codec with various mappings.
* @param codec the CompressionCodec to register
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodec,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodec(org.apache.hadoop.fs.Path),199,217,"/**
* Determines compression codec for a file.
* @param file path to the file
* @return CompressionCodec object or null if not found
*/","* Find the relevant compression codec for the given file based on its
   * filename suffix.
   * @param file the filename to check
   * @return the codec object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsConfig:<init>(org.apache.commons.configuration2.Configuration,java.lang.String)",94,96,"/**
 * Initializes metrics configuration.
 * @param c Configuration object
 * @param prefix Prefix string to be converted to lowercase
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,isLocalhost,org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:isLocalhost(java.lang.String),491,501,"/**
 * Checks if the given host is a local address.
 * @param host IP address or hostname to check
 * @return true if host is localhost, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,resolvePropertyName,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:resolvePropertyName(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String)",216,221,"/**
* Masks a template string based on SSL mode.
* @param mode SSL configuration mode
* @param template input string to be masked
* @return masked string
*/","* Resolves a property name to its client/server version if applicable.
   * <p>
   * NOTE: This method is public for testing purposes.
   *
   * @param mode client/server mode.
   * @param template property name template.
   * @return the resolved property name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,hasCapability,org.apache.hadoop.crypto.CryptoInputStream:hasCapability(java.lang.String),860,880,"/**
* Checks if a capability is supported.
* @param capability the capability to check
* @return true if supported, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherSuite.java,getConfigSuffix,org.apache.hadoop.crypto.CipherSuite:getConfigSuffix(),98,106,"/**
 * Masks the name by replacing each segment with a dot and its masked form.
 * @return Masked name as a string
 */","* Returns suffix of cipher suite configuration.
   * @return String configuration suffix",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,combine,org.apache.hadoop.fs.shell.find.Result:combine(org.apache.hadoop.fs.shell.find.Result),59,62,"/**
* Masks two results by logical AND operation.
* @param other another Result object to mask with
* @return a new Result with masked values
*/","* Returns the combination of this and another result.
   * @param other other.
   * @return result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,negate,org.apache.hadoop.fs.shell.find.Result:negate(),68,70,"/**
 * Returns a Result based on negation of m1 and value of m2.
 * @return Result object with boolean values from m1 and m2
 */","* Negate this result.
   * @return Result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,toString,org.apache.hadoop.fs.shell.find.Result:toString(),72,75,"/**
 * Constructs a status string with success and recursion information.
 * @return Formatted status string combining results from m1() and m2()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name$Iname:<init>(),97,99,"/**
* Constructs an Iname object with a Name instance.
* Initializes with false flag indicating no specific name type.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,<init>,org.apache.hadoop.fs.shell.find.Print$Print0:<init>(),72,74,"/**
 * Initializes a new Print0 object with null character.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,createOptions,org.apache.hadoop.fs.shell.find.Find:createOptions(),244,252,"/**
* Configures and returns a FindOptions object.
* @return FindOptions with specified input/output settings
*/",Create a new set of find options.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isExpression,org.apache.hadoop.fs.shell.find.Find:isExpression(java.lang.String),445,448,"/**
 * Checks if an expression exists by name.
 * @param expressionName the name of the expression to check
 * @return true if the expression exists, false otherwise
 */",Asks the factory whether an expression is recognized.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,setOptions,org.apache.hadoop.fs.shell.find.BaseExpression:setOptions(org.apache.hadoop.fs.shell.find.FindOptions),67,73,"/**
* Sets options and processes expressions.
* @param options configuration options for processing
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,prepare,org.apache.hadoop.fs.shell.find.BaseExpression:prepare(),75,80,"/**
 * Calls m1 on each Expression from m2().
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,finish,org.apache.hadoop.fs.shell.find.BaseExpression:finish(),82,87,"/**
 * Iterates over expressions and calls m1 on each.
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,isAction,org.apache.hadoop.fs.shell.find.BaseExpression:isAction(),147,155,"/**
* Checks if any child expression evaluates to true.
* @return true if any child expression returns true, otherwise false
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,toString,org.apache.hadoop.fs.shell.find.BaseExpression:toString(),119,145,"/**
 * Constructs a string representation of the object.
 * @return formatted string with concatenated values from m4 and m6
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,addChildren,"org.apache.hadoop.fs.shell.find.BaseExpression:addChildren(java.util.Deque,int)",219,223,"/**
 * Applies mask operation to expressions.
 * @param exprs deque of expressions to process
 * @param count number of expressions to mask
 */","* Add a specific number of children to this expression. The children are
   * popped off the head of the expressions.
   *
   * @param exprs
   *          deque of expressions from which to take the children
   * @param count
   *          number of children to be added",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,addArguments,"org.apache.hadoop.fs.shell.find.BaseExpression:addArguments(java.util.Deque,int)",250,254,"/**
 * Processes elements from deque.
 * @param args deque containing strings to process
 * @param count number of elements to process
 */","* Add a specific number of arguments to this expression. The children are
   * popped off the head of the expressions.
   *
   * @param args
   *          deque of arguments from which to take the argument
   * @param count
   *          number of children to be added",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:validate(),244,274,"/**
* Handles credential deletion logic.
* @return true if deletion is confirmed or skipped, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:validate(),354,381,"/**
 * Checks and prompts for key deletion confirmation.
 * @return true if deletion should proceed, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,confirmForceManual,org.apache.hadoop.ha.HAAdmin:confirmForceManual(),466,479,"/**
* Prompts user for confirmation before proceeding with a dangerous operation.
* @return true if user confirms, false otherwise
* @throws IOException if an I/O error occurs during input
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,relativize,"org.apache.hadoop.fs.shell.PathData:relativize(java.net.URI,java.net.URI,boolean)",410,435,"/**
* Generates relative path from cwdUri to srcUri.
* @param cwdUri current working directory URI
* @param srcUri source URI
* @param isDir true if source is a directory
* @return relative path as String
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,stringToUri,org.apache.hadoop.fs.shell.PathData:stringToUri(java.lang.String),550,591,"/**
 * Constructs a URI from a given path string.
 * @param pathString the input path string to be processed
 * @return a URI object constructed from the input path
 * @throws IOException if an I/O error occurs during processing
 */","Construct a URI from a String with unescaped special characters
   *  that have non-standard semantics. e.g. /, ?, #. A custom parsing
   *  is needed to prevent misbehavior.
   *  @param pathString The input path in string form
   *  @return URI",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setHumanReadable,org.apache.hadoop.fs.shell.FsUsage$Df:setHumanReadable(boolean),69,71,"/**
 * Sets the readability mode.
 * @param humanReadable true for human-readable output, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setHumanReadable,org.apache.hadoop.fs.shell.FsUsage$Du:setHumanReadable(boolean),69,71,"/**
* Sets the format preference.
* @param humanReadable true for human-readable output, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Df:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder),65,67,"/**
 * Sets the TableBuilder instance.
 * @param usagesTable the TableBuilder to be set
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Du:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder),65,67,"/**
 * Sets the table builder for usage records.
 * @param usagesTable TableBuilder instance to manage usage data
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,getUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Df:getUsagesTable(),61,63,"/**
 * Returns the usages table builder.
 * @return TableBuilder instance representing the usages table
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,getUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Du:getUsagesTable(),61,63,"/**
 * Returns the usages table builder.
 * @return TableBuilder instance representing the usages table
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,isSorted,org.apache.hadoop.fs.shell.Ls:isSorted(),248,254,"/**
 * Determines if any of the conditions are met.
 * @return true if at least one condition is satisfied, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,initialiseOrderComparator,org.apache.hadoop.fs.shell.Ls:initialiseOrderComparator(),374,402,"/**
* Sets order comparator based on conditions.
*/","* Initialise the comparator to be used for sorting files. If multiple options
   * are selected then the order is chosen in the following precedence: -
   * Modification time (or access time if requested) - File size - File name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getAdditionalTokenIssuers,org.apache.hadoop.fs.FileSystem:getAdditionalTokenIssuers(),718,723,"/**
 * Returns token issuers.
 * @return array of DelegationTokenIssuer objects
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:<init>(int,int)",218,220,"/**
 * Constructs an exception indicating mismatched argument count.
 * @param expected number of expected arguments
 * @param actual number of provided arguments
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:<init>(int,int)",202,204,"/**
 * Constructs an exception indicating too many arguments provided.
 * @param expected number of expected arguments
 * @param actual number of actual arguments
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,getMessage,org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:getMessage(),222,225,"/**
 * Returns an error message indicating insufficient arguments.
 * @return Error message string
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,getMessage,org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:getMessage(),206,209,"/**
 * Overrides parent method to prepend error message.
 * @return Error message indicating too many arguments
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processOptions,org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processOptions(java.util.LinkedList),70,100,"/**
 * Parses command-line arguments for mask operation.
 * @param args list of command-line arguments
 * @throws IOException if an I/O error occurs or invalid arguments are provided
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttrs,org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path),640,643,"/**
 * Reads file contents into a map.
 * @param path file path to read
 * @return map of file names to byte arrays
 * @throws IOException if reading fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttr,"org.apache.hadoop.fs.FilterFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",635,638,"/**
 * Reads file content as bytes.
 * @param path file path
 * @param name file name
 * @return byte array of file content
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,popPreserveOption,org.apache.hadoop.fs.shell.CopyCommands$Cp:popPreserveOption(java.util.List),191,210,"/**
 * Processes command-line arguments to set file attributes.
 * @param args list of command-line arguments
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processArguments(java.util.LinkedList),159,171,"/**
* Renames a snapshot in the specified path.
* @param items list of PathData objects
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.FilterFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",579,583,"/**
 * Renames a snapshot in the specified file system path.
 * @param path the file system path containing the snapshot
 * @param snapshotOldName current name of the snapshot
 * @param snapshotNewName new name for the snapshot
 * @throws IOException if an I/O error occurs during renaming
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,isDeprecated,org.apache.hadoop.fs.shell.Command:isDeprecated(),557,559,"/**
* Checks if m1() returns a non-null value.
* @return true if m1() is not null, false otherwise
*/","* Is the command deprecated?
   * @return boolean",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getName,org.apache.hadoop.fs.shell.Command:getName(),519,523,"/**
* Masks function name or command if applicable.
* @return masked name or original if not applicable
*/","* The name of the command.  Will first try to use the assigned name
   * else fallback to the command's preferred name
   * @return name of the command",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getAclStatus,org.apache.hadoop.fs.FilterFileSystem:getAclStatus(org.apache.hadoop.fs.Path),618,621,"/**
 * Retrieves ACL status for a given path.
 * @param path file system path
 * @return AclStatus object representing ACL permissions
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,setPreserve,org.apache.hadoop.fs.shell.CommandWithDestination:setPreserve(boolean),125,133,"/**
* Masks file attributes based on preserve flag.
* @param preserve true to mask timestamps, ownership, and permission; false otherwise
*/","* If true, the last modified time, last access time,
   * owner, group and permission information of the source
   * file will be preserved as far as target {@link FileSystem}
   * implementation allows.
   *
   * @param preserve preserve.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setAcl,"org.apache.hadoop.fs.FilterFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",613,616,"/**
* Applies ACL entries to a file system path.
* @param path the file system path
* @param aclSpec list of ACL entries to apply
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,<init>,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:<init>(java.lang.String,java.lang.String)",41,45,"/**
* Initializes an MBeanInfoBuilder with a name and description.
* @param name MBean name
* @param desc MBean description
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:<init>(org.apache.hadoop.metrics2.MetricsCollector,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,boolean)",58,68,"/**
 * Initializes a new MetricsRecordBuilderImpl.
 * @param parent the parent MetricsCollector
 * @param info MetricsInfo for the record
 * @param rf record filter
 * @param mf metric filter
 * @param acceptable flag indicating if the record is acceptable
 */","* @param parent {@link MetricsCollector} using this record builder
   * @param info metrics information
   * @param rf
   * @param mf
   * @param acceptable",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,<init>,"org.apache.hadoop.util.ChunkedArrayList:<init>(int,int)",103,107,"/**
* Initializes a new ChunkedArrayList with specified capacities.
* @param initialChunkCapacity the initial capacity of each chunk
* @param maxChunkSize the maximum size of any single chunk
*/","* @param initialChunkCapacity the capacity of the first chunk to be
   * allocated
   * @param maxChunkSize the maximum size of any chunk allocated",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ScopedAclEntries.java,calculatePivotOnDefaultEntries,org.apache.hadoop.fs.permission.ScopedAclEntries:calculatePivotOnDefaultEntries(java.util.List),87,94,"/**
* Finds index of default scope entry in ACL.
* @param aclBuilder list of access control entries
* @return index of default scope or PIVOT_NOT_FOUND if not found
*/","* Returns the pivot point in the list between the access entries and the
   * default entries.  This is the index of the first element in the list that
   * is a default entry.
   *
   * @param aclBuilder ArrayList<AclEntry> containing entries to build
   * @return int pivot point, or -1 if list contains no default entries",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeAcl,org.apache.hadoop.fs.FilterFileSystem:removeAcl(org.apache.hadoop.fs.Path),608,611,"/**
 * Delegates file operation to underlying filesystem.
 * @param path file path to operate on
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.FilterFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",591,595,"/**
* Applies ACL specifications to a file system path.
* @param path the target file path
* @param aclSpec list of access control entries
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.FilterFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",597,601,"/**
 * Delegates ACL modification to underlying file system.
 * @param path file path to modify ACLs
 * @param aclSpec list of ACL entries to apply
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processArguments(java.util.LinkedList),77,88,"/**
* Creates a snapshot for the given path data.
* @param items list of path data items
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createSnapshot,org.apache.hadoop.fs.FileSystem:createSnapshot(org.apache.hadoop.fs.Path),3089,3091,"/**
 * Calls overloaded method with null as default value.
 * @param path file system path
 * @return processed path
 * @throws IOException if an I/O error occurs
 */","* Create a snapshot with a default name.
   * @param path The directory where snapshots will be taken.
   * @return the snapshot path.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createSnapshot,"org.apache.hadoop.fs.FilterFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",573,577,"/**
 * Creates a snapshot of a file system path.
 * @param path the path to be snapshotted
 * @param snapshotName name of the snapshot
 * @return Path object representing the snapshot
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,addOptionWithValue,org.apache.hadoop.fs.shell.CommandFormat:addOptionWithValue(java.lang.String),73,78,"/**
* Adds an option if not already present.
* @param option unique configuration identifier
* @throws DuplicatedOptionException if option is already added
*/","* add option with value
   *
   * @param option option name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processArguments(java.util.LinkedList),117,128,"/**
* Processes linked list of path data and deletes a snapshot.
* @param items list containing PathData objects
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.FilterFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",585,589,"/**
 * Creates a snapshot of the specified file system path.
 * @param path the file system path to snapshot
 * @param snapshotName the name of the snapshot
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,<init>,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:<init>(java.lang.Object[]),274,278,"/**
 * Initializes a TableBuilder with headers and adds them as the first row.
 * @param headers variable number of header objects
 */","* Create a table with headers
     * @param headers list of headers",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,isEmpty,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:isEmpty(),348,350,"/**
 * Checks if mask function is active.
 * @return true if mask is inactive (m1() == 0), false otherwise
 */","* Does table have any rows 
     * @return boolean",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path),831,836,"/**
* Reads file content from a given path.
* @param path file path to read
* @return map of file names to byte arrays or null if not found
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttrs,org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path),376,379,"/**
 * Reads files from a given path.
 * @param path file system path to read from
 * @return map of file names to their byte content
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$HandleOpt$Location:<init>(boolean),496,498,"/**
 * Constructs a Location with change allowance.
 * @param allowChanged flag indicating if location can be changed
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$HandleOpt$Data:<init>(boolean),471,473,"/**
 * Constructs a new Data instance.
 * @param allowChanged flag indicating if changes are allowed
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,toString,org.apache.hadoop.fs.CompositeCrcFileChecksum:toString(),84,87,"/**
* Generates a masked string with CRC value.
* @return Formatted string combining mask and CRC in hexadecimal format
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell:<init>(long),910,912,"/**
 * Constructs a Shell with a specified interval.
 * @param interval time interval in milliseconds
 */","* Create an instance with a minimum interval between executions; stderr is
   * not merged with stdout.
   * @param interval interval in milliseconds between command executions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,equals,org.apache.hadoop.fs.RawPathHandle:equals(java.lang.Object),72,79,"/**
* Checks equality with another PathHandle.
* @param other object to compare
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,hashCode,org.apache.hadoop.fs.RawPathHandle:hashCode(),81,84,"/**
 * Calls m2 on the result of m1().
 * @return integer value returned by nested m2()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,toString,org.apache.hadoop.fs.RawPathHandle:toString(),86,89,"/**
 * Calls m2 on the result of m1().
 * @return Result of calling m2 on m1's output
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,run,org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:run(),209,236,"/**
 * Continuously refreshes disk usage info with jitter.
 * @throws InterruptedException if thread is interrupted while sleeping
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setOwner,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setOwner(org.apache.hadoop.io.Text),93,99,"/**
* Sets or initializes the owner text.
* @param owner the owner's text, can be null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setRealUser,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRealUser(org.apache.hadoop.io.Text),122,128,"/**
* Sets the real user text, defaulting to empty if null.
* @param realUser the user text to set
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,"org.apache.hadoop.security.token.Token:<init>(byte[],byte[],org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",86,91,"/**
* Initializes a Token with specified parameters.
* @param identifier unique token identifier
* @param password associated password
* @param kind type of token
* @param service service associated with the token
*/","* Construct a token from the components.
   * @param identifier the token identifier
   * @param password the token's password
   * @param kind the kind of token
   * @param service the service for this token",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,org.apache.hadoop.security.token.Token:<init>(),96,101,"/**
* Initializes a new Token with empty identifier and password.
*/",* Default constructor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",65,72,"/**
* Initializes a new Globber with specified context, pattern, and filter.
* @param fc FileContext for file operations
* @param pathPattern Path pattern to match files
* @param filter Filter to apply to matched paths
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)",100,110,"/**
 * Initializes a new Globber instance.
 * @param fc FileContext for file operations
 * @param pathPattern Path pattern to match files
 * @param filter Filter to apply to matched paths
 * @param resolveSymlinks Whether to resolve symbolic links
 */","* File Context constructor for use by {@link GlobBuilder}.
   * @param fc file context
   * @param pathPattern path pattern
   * @param filter optional filter
   * @param resolveSymlinks should symlinks be resolved.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getMessage,org.apache.hadoop.fs.PathIOException:getMessage(),86,104,"/**
 * Constructs a detailed operation message.
 * @return String representation of the operation details
 */","Format:
   * cmd: {operation} `path' {to `target'}: error string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,hasNext,org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:hasNext(),49,52,"/**
 * Checks if m1() returns a non-null value.
 * @return true if m1() is not null, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,next,org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:next(),64,71,"/**
* Retrieves a LongStatistic from an iterator returned by m1().
* @throws NoSuchElementException if the iterator is null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAndIncrDirNumLastAccessed,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$Context:getAndIncrDirNumLastAccessed(),282,284,"/**
 * Calls helper method with default value 1.
 * @return result of m1(1)
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",860,866,"/**
* Resolves and creates a snapshot for a given path.
* @param path the file system path
* @param snapshotName name of the snapshot to create
* @return Path to the created snapshot
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createSnapshot,"org.apache.hadoop.fs.FilterFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",397,401,"/**
* Creates a snapshot of the specified file system path.
* @param path the file system path to snapshot
* @param snapshotName name for the snapshot
* @return Path object representing the snapshot
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAbstractFileSystem,"org.apache.hadoop.fs.FileContext:getAbstractFileSystem(org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration)",339,363,"/**
 * Retrieves an AbstractFileSystem instance for a given URI and configuration.
 * @param user user group information
 * @param uri file system URI
 * @param conf configuration settings
 * @return AbstractFileSystem object
 * @throws UnsupportedFileSystemException if the file system is not supported
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsUser,"org.apache.hadoop.security.SecurityUtil:doAsUser(org.apache.hadoop.security.UserGroupInformation,java.security.PrivilegedExceptionAction)",552,559,"/**
* Executes an action with user group information.
* @param ugi UserGroupInformation object
* @param action PrivilegedExceptionAction to be executed
* @return Result of the action
* @throws IOException if operation is interrupted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleSaslConnectionFailure,"org.apache.hadoop.ipc.Client$Connection:handleSaslConnectionFailure(int,int,java.io.IOException,java.util.Random,org.apache.hadoop.security.UserGroupInformation)",705,757,"/**
* Handles retries and exceptions for server connection setup.
* @param currRetries current retry count
* @param maxRetries maximum allowed retries
* @param ex exception encountered during connection
* @param rand random number generator for backoff
* @param ugi user group information for privileged action execution
* @throws IOException if connection cannot be established
* @throws InterruptedException if thread is interrupted
*/","* If multiple clients with the same principal try to connect to the same
     * server at the same time, the server assumes a replay attack is in
     * progress. This is a feature of kerberos. In order to work around this,
     * what is done is that the client backs off randomly and tries to initiate
     * the connection again. The other problem is to do with ticket expiry. To
     * handle that, a relogin is attempted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,isViewFileSystem,org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:isViewFileSystem(org.apache.hadoop.fs.FileSystem),50,52,"/**
* Checks if FileSystem supports VIEWFS scheme.
* @param fileSystem target FileSystem instance
* @return true if supported, false otherwise
*/","* Check if the FileSystem is a ViewFileSystem.
   *
   * @param fileSystem file system.
   * @return true if the fileSystem is ViewFileSystem",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,open,"org.apache.hadoop.fs.FilterFileSystem:open(org.apache.hadoop.fs.PathHandle,int)",171,175,"/**
 * Retrieves an input stream for reading data from a file.
 * @param fd PathHandle representing the file descriptor
 * @param bufferSize size of the buffer used for reading
 * @return FSDataInputStream for reading file data
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,primitiveMkdir,"org.apache.hadoop.fs.FilterFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",561,566,"/**
* Checks file permissions.
* @param f file path
* @param abdolutePermission required permission
* @return true if permissions match, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setQuota,"org.apache.hadoop.fs.FileSystem:setQuota(org.apache.hadoop.fs.Path,long,long)",1965,1968,"/**
* Masks a file path with specified quotas.
* @param src source file path
* @param namespaceQuota maximum number of files allowed
* @param storagespaceQuota maximum storage space allowed in bytes
* @throws IOException if an I/O error occurs
*/","* Set quota for the given {@link Path}.
   *
   * @param src the target path to set quota for
   * @param namespaceQuota the namespace quota (i.e., # of files/directories)
   *                       to set
   * @param storagespaceQuota the storage space quota to set
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setQuotaByStorageType,"org.apache.hadoop.fs.FileSystem:setQuotaByStorageType(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.StorageType,long)",1978,1981,"/**
 * Masks a file with specified storage type and quota.
 * @param src source file path
 * @param type storage type to apply
 * @param quota storage quota limit
 * @throws IOException if an I/O error occurs
 */","* Set per storage type quota for the given {@link Path}.
   *
   * @param src the target path to set storage type quota for
   * @param type the storage type to set
   * @param quota the quota to set for the given storage type
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createMultipartUploader,org.apache.hadoop.fs.FileSystem:createMultipartUploader(org.apache.hadoop.fs.Path),4987,4992,"/**
* Initializes multipart uploader builder.
* @param basePath base directory path
* @return MultipartUploaderBuilder instance or null
*/","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listCorruptFileBlocks,org.apache.hadoop.fs.FilterFileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path),277,281,"/**
 * Retrieves an iterator over file paths.
 * @param path directory path to iterate over
 * @return RemoteIterator of Path objects
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listLocatedStatus,org.apache.hadoop.fs.FileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),2261,2264,"/**
 * Returns an iterator over file statuses in the specified path.
 * @param f directory path to search
 * @return iterator of LocatedFileStatus objects
 * @throws FileNotFoundException if the path does not exist
 * @throws IOException for other I/O errors
 */","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   * Return the file's status and block locations If the path is a file.
   *
   * If a returned status is a file, it contains the file's block locations.
   *
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories
   *         in the given path
   *
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws IOException If an I/O error occurred",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.ChecksumFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),980,984,"/**
* Retrieves file statuses from a path.
* @param f directory path to search
* @return iterator of file statuses matching the default filter
* @throws IOException if an I/O error occurs
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   *
   * @param f
   *          given path
   * @return the statuses of the files/directories in the given patch
   * @throws IOException if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listLocatedStatus,"org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",214,219,"/**
* Retrieves files matching the filter in the specified path.
* @param f directory path to search
* @param filter criteria for file selection
* @return iterator over matching LocatedFileStatus objects
* @throws FileNotFoundException if the directory does not exist
* @throws IOException on I/O errors
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,java.lang.String)",97,102,"/**
* Throws an AccessControlException indicating a read-only directory.
* @param operation the attempted operation
* @param p the path on which the operation was attempted
* @return always throws an AccessControlException
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,java.lang.String)",175,180,"/**
* Throws an AccessControlException indicating a read-only directory.
* @param operation the attempted operation (e.g., write)
* @param p the file path where the operation was attempted
* @return never returns, always throws an exception
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.String),41,43,"/**
 * Constructs an AuthorizationException with a specified message.
 * @param message detail message explaining the exception
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,resolveLink,org.apache.hadoop.fs.FilterFileSystem:resolveLink(org.apache.hadoop.fs.Path),498,500,"/**
 * Delegates file processing to another method.
 * @param f input file path
 * @return processed file path
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileChecksum,org.apache.hadoop.fs.FileSystem:getFileChecksum(org.apache.hadoop.fs.Path),2980,2982,"/**
 * Computes checksum for a file up to a specified size.
 * @param f file path
 * @return FileChecksum object
 * @throws IOException if an I/O error occurs
 */","* Get the checksum of a file, if the FS supports checksums.
   *
   * @param f The file path
   * @return The file checksum.  The default return value is null,
   *  which indicates that no checksum algorithm is implemented
   *  in the corresponding FileSystem.
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",507,510,"/**
* Computes checksum for file.
* @param f file path
* @param length file length
* @return FileChecksum object
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setXAttr,"org.apache.hadoop.fs.FileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",3244,3248,"/**
* Sets an extended attribute on a file.
* @param path file path
* @param name attribute name
* @param value attribute value
* @throws IOException if I/O error occurs
*/","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default outcome).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setXAttr,"org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",629,633,"/**
 * Sets an extended attribute on a file.
 * @param path the file path
 * @param name the attribute name
 * @param value the attribute value
 * @param flag the set of flags for setting the attribute
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttrs,"org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",645,649,"/**
* Retrieves files as byte arrays from specified paths.
* @param path directory path to search in
* @param names list of file names to retrieve
* @return map of file names to their byte array contents
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listXAttrs,org.apache.hadoop.fs.FilterFileSystem:listXAttrs(org.apache.hadoop.fs.Path),651,654,"/**
 * Reads file content from the specified path.
 * @param path file path to read from
 * @return list of strings representing file lines
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),661,664,"/**
 * Delegates file processing to another method.
 * @param src source file path
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.FilterFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",666,670,"/**
* Delegates file operation to underlying filesystem.
* @param src source file path
* @param policyName storage policy name
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),672,675,"/**
 * Delegates file operation to underlying filesystem.
 * @param src source path for the file operation
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),677,681,"/**
 * Retrieves block storage policy for a given path.
 * @param src source file path
 * @return BlockStoragePolicySpi object
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAllStoragePolicies(),1925,1939,"/**
 * Retrieves block storage policies from all file systems.
 * @return Collection of BlockStoragePolicySpi objects
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.FilterFileSystem:getAllStoragePolicies(),683,687,"/**
 * Retrieves block storage policies.
 * @return collection of block storage policy SPIs
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",4806,4816,"/**
* Opens a file for reading.
* @param path file path to open
* @param parameters file opening parameters
* @return CompletableFuture with FSDataInputStream or throws IOException
*/","* Execute the actual open file operation.
   *
   * This is invoked from {@code FSDataInputStreamBuilder.build()}
   * and from {@link DelegateToFileSystem} and is where
   * the action of opening the file should begin.
   *
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)} in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.ChecksumFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",1090,1101,"/**
* Opens a file for input.
* @param path file path to open
* @param parameters file opening parameters
* @return CompletableFuture of FSDataInputStream or throws IOException
*/","* Open the file as a blocking call to {@link #open(Path, int)}.
   *
   * {@inheritDoc}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.AbstractFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",1603,1612,"/**
* Opens a file for input asynchronously.
* @param path the file path to open
* @param parameters file opening parameters
* @return CompletableFuture containing FSDataInputStream or throws IOException
*/","* Open a file with the given set of options.
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)}in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)",4834,4852,"/**
* Asynchronously opens a file for reading.
* @param pathHandle handle to the file path
* @param parameters file opening parameters
* @return CompletableFuture with FSDataInputStream or exception
*/","* Execute the actual open file operation.
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)} in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param pathHandle path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key
   * @throws UnsupportedOperationException PathHandles are not supported.
   * This may be deferred until the future is evaluated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,isValidName,org.apache.hadoop.fs.FilterFs:isValidName(java.lang.String),322,325,"/**
 * Checks if a source string exists.
 * @param src the source string to check
 * @return true if the source exists, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",770,776,"/**
* Applies ACL specifications to a file path.
* @param path the file path to apply ACLs
* @param aclSpec list of ACL entries to set
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,modifyAclEntries,"org.apache.hadoop.fs.FilterFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",327,331,"/**
 * Applies ACL entries to a specified file path.
 * @param path the file path to apply ACLs to
 * @param aclSpec list of ACL entries to be applied
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",778,784,"/**
* Applies ACL to a file path.
* @param path the file path to apply ACL
* @param aclSpec list of ACL entries to set
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeAclEntries,"org.apache.hadoop.fs.FilterFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",333,337,"/**
* Applies ACL entries to a file system path.
* @param path the target file path
* @param aclSpec list of ACL entries to apply
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),786,792,"/**
* Resolves and processes file path.
* @param path file system path to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeDefaultAcl,org.apache.hadoop.fs.FilterFs:removeDefaultAcl(org.apache.hadoop.fs.Path),339,342,"/**
 * Delegates file processing to another FileSystem implementation.
 * @param path Path of the file to process
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",868,875,"/**
* Renames a snapshot in the specified path.
* @param path file system path containing the snapshot
* @param snapshotOldName current name of the snapshot
* @param snapshotNewName new name for the snapshot
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameSnapshot,"org.apache.hadoop.fs.FilterFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",403,407,"/**
 * Renames a snapshot in the specified file system path.
 * @param path directory containing the snapshots
 * @param snapshotOldName current name of the snapshot
 * @param snapshotNewName new name for the snapshot
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),401,404,"/**
 * Delegates file processing to another filesystem.
 * @param path file path to process
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),884,889,"/**
* Resolves and processes a file path.
* @param path the file path to process
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.FilterFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),415,418,"/**
 * Delegates file system operation to underlying implementation.
 * @param path the file path to operate on
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),899,905,"/**
* Resolves and processes a file path.
* @param src source file path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,unsetStoragePolicy,org.apache.hadoop.fs.FilterFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),426,430,"/**
 * Delegates file processing to another filesystem.
 * @param src source file path
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ChRootedFs:getAllStoragePolicies(),424,428,"/**
 * Retrieves block storage policies.
 * @return collection of block storage policy implementations
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAllStoragePolicies,org.apache.hadoop.fs.FileContext:getAllStoragePolicies(),2916,2919,"/**
* Retrieves block storage policies.
* @return collection of block storage policy implementations
* @throws IOException if an I/O error occurs
*/","* Retrieve all the storage policies supported by this file system.
   *
   * @return all storage policies supported by this filesystem.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getAllStoragePolicies,org.apache.hadoop.fs.FilterFs:getAllStoragePolicies(),438,442,"/**
 * Retrieves block storage policies.
 * @return collection of block storage policy implementations
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,supportsSymlinks,org.apache.hadoop.fs.viewfs.ChRootedFs:supportsSymlinks(),436,439,"/**
 * Delegates to myFs's m1 method.
 * @return result of myFs.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,supportsSymlinks,org.apache.hadoop.fs.FilterFs:supportsSymlinks(),296,299,"/**
 * Delegates to myFs.m1().
 * @return result of myFs.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createSymlink,"org.apache.hadoop.fs.FilterFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",301,305,"/**
* Creates a symbolic link.
* @param target path to the target file or directory
* @param link path where the symbolic link will be created
* @param createParent if true, creates parent directories if they don't exist
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if the target is not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFs:getLinkTarget(org.apache.hadoop.fs.Path),669,674,"/**
* Resolves and processes a file path.
* @param f input file path
* @return processed file path
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getLinkTarget,org.apache.hadoop.fs.FilterFs:getLinkTarget(org.apache.hadoop.fs.Path),307,310,"/**
 * Delegates to myFs's m1 method.
 * @param f file path to process
 * @return result from myFs's m1 method
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getDelegationTokens,org.apache.hadoop.fs.viewfs.ChRootedFs:getDelegationTokens(java.lang.String),459,462,"/**
 * Retrieves tokens for the specified renewer.
 * @param renewer user or entity requesting token renewal
 * @return list of Token objects
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getDelegationTokens,org.apache.hadoop.fs.FilterFs:getDelegationTokens(java.lang.String),317,320,"/**
 * Retrieves tokens for the specified renewer.
 * @param renewer user identifier for token renewal
 * @return list of Token objects
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path),1073,1078,"/**
 * Throws FileNotFoundException as path is not a file.
 * @param f file path to check
 * @throws FileNotFoundException if path is directory or does not exist
 * @throws IOException for other I/O errors
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,open,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)",1301,1306,"/**
* Throws an exception as directories are not supported.
* @param f path to the file or directory
* @param bufferSize buffer size for reading
* @throws FileNotFoundException always thrown since directories are not allowed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,initializeMountedFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:initializeMountedFileSystems(java.util.List),943,959,"/**
* Maps mount points to FileSystems.
* @param mountPoints list of mount points
* @return map from mount source to FileSystem
*/","* Initialize the target filesystem for all mount points.
   * @param mountPoints The mount points
   * @return Mapping of mount point and the initialized target filesystems
   * @throws RuntimeException when the target file system cannot be initialized",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getDelegationTokens,org.apache.hadoop.fs.viewfs.ViewFs:getDelegationTokens(java.lang.String),732,761,"/**
* Retrieves tokens for a given renewer across all mount points and root fallback.
* @param renewer user or entity requesting token renewal
* @return List of Token objects
* @throws IOException if an I/O error occurs during token retrieval
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1422,1425,"/**
* Throws NotInMountpointException indicating getXAttr is unsupported.
* @param path file path to check
* @param name attribute name
* @throws IOException if operation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path),1427,1430,"/**
* Throws exception as masking is not supported.
* @param path file or directory path
* @throws NotInMountpointException always thrown
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",1432,1436,"/**
 * Throws exception indicating path is not in mountpoint.
 * @param path file system path to check
 * @param names attribute names (unused)
 * @throws IOException if path is not in mountpoint
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path),1438,1441,"/**
* Throws NotInMountpointException as masking is not supported.
* @param path file system path to check
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path),1785,1788,"/**
 * Throws an exception indicating the file is not within a mount point.
 * @param f file path to check
 * @throws NotInMountpointException if file is not in a mount point
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultBlockSize(org.apache.hadoop.fs.Path),1790,1793,"/**
 * Throws an exception indicating the file is not in a mount point.
 * @param f file path to check
 * @throws NotInMountpointException if file is not within a recognized mount point
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultReplication(org.apache.hadoop.fs.Path),1795,1798,"/**
 * Throws exception indicating file is not in mountpoint.
 * @param f file path
 * @throws NotInMountpointException always thrown
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1848,1851,"/**
* Throws NotInMountpointException indicating getXAttr operation not supported.
* @param path file system path
* @param name attribute name
* @throws IOException if operation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path),1853,1856,"/**
 * Throws an exception indicating the path is not within a mount point.
 * @param path file system path to check
 * @throws NotInMountpointException if path is outside any mount point
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",1858,1862,"/**
* Throws NotInMountpointException indicating getXAttrs operation is not supported.
* @param path file system path to check
* @param names list of attribute names (unused)
* @throws IOException if operation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path),1864,1867,"/**
* Throws NotInMountpointException indicating path is not in mountpoint.
* @param path file system path to check
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getQuotaUsage(org.apache.hadoop.fs.Path),1896,1899,"/**
* Throws NotInMountpointException indicating quota usage cannot be fetched.
* @param f file path to check
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStoragePolicy(org.apache.hadoop.fs.Path),1920,1923,"/**
* Throws NotInMountpointException indicating policy not applicable.
* @param src source path to check
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java,serializeToString,org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:serializeToString(),111,116,"/**
 * Constructs a masked string using resolved path and regex strings.
 * @return Concatenated string with internal separators
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize(),961,964,"/**
 * Throws an exception indicating the function is not in a mountpoint.
 * @throws NotInMountpointException always thrown with ""getDefaultBlockSize"" message
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication(),966,969,"/**
 * Throws an exception indicating the function is not applicable in this context.
 * @throws NotInMountpointException always thrown when called
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults(),971,974,"/**
 * Throws an exception indicating that the operation is not in a mountpoint.
 * @throws IOException if operation cannot be performed
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,delete,"org.apache.hadoop.fs.viewfs.ViewFs:delete(org.apache.hadoop.fs.Path,boolean)",366,378,"/**
 * Deletes a file or directory.
 * @param f path to the file or directory
 * @param recursive true to delete directories recursively
 * @return true if deletion is successful
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if file does not exist
 * @throws UnresolvedLinkException if link cannot be resolved
 * @throws IOException for other I/O errors
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatusIterator,org.apache.hadoop.fs.viewfs.ViewFs:listStatusIterator(org.apache.hadoop.fs.Path),451,469,"/**
 * Retrieves file statuses for a given path.
 * @param f the path to query
 * @return iterator over file statuses
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if file not found
 * @throws UnresolvedLinkException if link cannot be resolved
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ViewFs:listLocatedStatus(org.apache.hadoop.fs.Path),471,490,"/**
* Retrieves file status iterator for a given path.
* @param f target path
* @return iterator of LocatedFileStatus objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,buildResolveResultForRegexMountPoint,"org.apache.hadoop.fs.viewfs.InodeTree:buildResolveResultForRegexMountPoint(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)",1053,1081,"/**
* Resolves a path and initializes the target file system.
* @param resultKind kind of resolution result
* @param resolvedPathStr resolved path string
* @param targetOfResolvedPathStr target of resolved path string
* @param remainingPath remaining path after resolution
* @return ResolveResult object or null if initialization fails
*/","* Build resolve result.
   * Here's an example
   * Mountpoint: fs.viewfs.mounttable.mt
   *     .linkRegex.replaceresolveddstpath:_:-#.^/user/(??&lt;username&gt;\w+)
   * Value: /targetTestRoot/$username
   * Dir path to test:
   * viewfs://mt/user/hadoop_user1/hadoop_dir1
   * Expect path: /targetTestRoot/hadoop-user1/hadoop_dir1
   * resolvedPathStr: /user/hadoop_user1
   * targetOfResolvedPathStr: /targetTestRoot/hadoop-user1
   * remainingPath: /hadoop_dir1
   *
   * @param resultKind resultKind.
   * @param resolvedPathStr resolvedPathStr.
   * @param targetOfResolvedPathStr targetOfResolvedPathStr.
   * @param remainingPath remainingPath.
   * @return targetFileSystem or null on exceptions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,fsGetter,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:fsGetter(),216,219,"/**
 * Returns a new ChildFsGetter with m1 as its parent.
 * @return ChildFsGetter instance
 */","* This method is overridden because in ViewFileSystemOverloadScheme if
   * overloaded scheme matches with mounted target fs scheme, file system
   * should be created without going into {@literal fs.<scheme>.impl} based
   * resolution. Otherwise it will end up in an infinite loop as the target
   * will be resolved again to ViewFileSystemOverloadScheme as
   * {@literal fs.<scheme>.impl} points to ViewFileSystemOverloadScheme.
   * So, below method will initialize the
   * {@literal fs.viewfs.overload.scheme.target.<scheme>.impl}.
   * Other schemes can follow fs.newInstance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,getBlockLocations,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getBlockLocations(),112,115,"/**
 * Retrieves block locations.
 * @return array of BlockLocation objects
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",167,169,"/**
* Constructs an INodeDir instance.
* @param pathToNode path to the node directory
* @param aUgi user group information
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.Object,java.lang.String[])",344,349,"/**
 * Constructs an INodeLink object.
 * @param pathToNode path to the node
 * @param aUgi user group information
 * @param targetMergeFs target file system for merging
 * @param aTargetDirLinkList list of target directory links
 */",* Construct a mergeLink or nfly.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.util.function.Function,java.lang.String)",354,362,"/**
* Initializes an INodeLink with a path, user info, and file system creation method.
* @param pathToNode node path
* @param aUgi user group information
* @param createFileSystemMethod function to create file system
* @param aTargetDirLink target directory link
* @throws URISyntaxException if URI is invalid
*/",* Construct a simple link (i.e. not a mergeLink).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addLink,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addLink(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",222,228,"/**
* Adds a child node with the specified path component.
* @param pathComponent unique identifier for the child node
* @param link reference to the child node
* @throws FileAlreadyExistsException if the path component already exists
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,buildLinkRegexEntry,"org.apache.hadoop.fs.viewfs.InodeTree:buildLinkRegexEntry(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)",818,845,"/**
* Creates a LinkEntry from configuration settings.
* @param config Configuration object
* @param ugi UserGroupInformation object
* @param mntEntryStrippedKey Stripped mount entry key
* @param mntEntryValue Mount entry value
* @return LinkEntry object representing the link
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,processThrowable,"org.apache.hadoop.fs.viewfs.NflyFSystem:processThrowable(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode,java.lang.String,java.lang.Throwable,java.util.List,org.apache.hadoop.fs.Path[])",917,933,"/**
 * Handles file operation exceptions and logs them.
 * @param nflyNode node representing the file system
 * @param op operation being performed
 * @param t original throwable exception
 * @param ioExceptions list to store IOExceptions
 * @param f variable arguments for file paths
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getWorkingDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem:getWorkingDirectory(),861,864,"/**
* Returns file system path from the first node.
* @return Path object representing the file system path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,replaceRegexCaptureGroupInPath,"org.apache.hadoop.fs.viewfs.RegexMountPoint:replaceRegexCaptureGroupInPath(java.lang.String,java.util.regex.Matcher,java.lang.String,java.util.Set)",246,261,"/**
 * Replaces regex groups in destination path.
 * @param parsedDestPath original destination path
 * @param srcMatcher source matcher with regex groups
 * @param regexGroupNameOrIndexStr group name or index to replace
 * @param groupRepresentationStrSetInDest set of variable names representing the group
 * @return updated destination path after replacements
 */","* Use capture group named regexGroupNameOrIndexStr in mather to replace
   * parsedDestPath.
   * E.g. link: ^/user/(?<username>\\w+) => s3://$user.apache.com/_${user}
   * srcMatcher is from /user/hadoop.
   * Then the params will be like following.
   * parsedDestPath: s3://$user.apache.com/_${user},
   * regexGroupNameOrIndexStr: user
   * groupRepresentationStrSetInDest: {user:$user; user:${user}}
   * return value will be s3://hadoop.apache.com/_hadoop
   * @param parsedDestPath
   * @param srcMatcher
   * @param regexGroupNameOrIndexStr
   * @param groupRepresentationStrSetInDest
   * @return return parsedDestPath while ${var},$var replaced or
   * parsedDestPath nothing found.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootDir,org.apache.hadoop.fs.viewfs.InodeTree:getRootDir(),520,523,"/**
* Returns the root directory node after validation.
* @return INodeDir object representing the root directory
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootLink,org.apache.hadoop.fs.viewfs.InodeTree:getRootLink(),525,528,"/**
 * Returns the root node as an INodeLink.
 * @return root node cast to INodeLink
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootFallbackLink,org.apache.hadoop.fs.viewfs.InodeTree:getRootFallbackLink(),543,546,"/**
 * Returns the fallback link node.
 * @return INodeLink object representing the fallback link
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,tryStart,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStart(),156,184,"/**
* Initializes and starts an asynchronous call processor.
* Creates a daemon thread to handle async calls.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,kill,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:kill(org.apache.hadoop.util.Daemon),192,198,"/**
* Marks a daemon as running.
* @param d the Daemon to mark
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,offer,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:offer(java.lang.Object),101,104,"/**
 * Adds an item to the queue.
 * @param c item to be added
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,java.nio.ByteBuffer,byte)",246,265,"/**
* Decrypts data from input buffer to output buffer.
* @param decryptor decryption utility
* @param inBuffer source ByteBuffer containing encrypted data
* @param outBuffer destination ByteBuffer for decrypted data
* @param padding number of padding bytes to handle
* @throws IOException if I/O error occurs during decryption
*/","* Do the decryption using inBuffer as input and outBuffer as output.
   * Upon return, inBuffer is cleared; the decrypted data starts at 
   * outBuffer.position() and ends at outBuffer.limit();",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,checkState,org.apache.hadoop.crypto.OpensslCipher:checkState(),289,291,"/**
* Validates that context is not zero.
* Throws an exception if context is invalid.
*/",Check whether context is initialized.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,parentZNodeExists,org.apache.hadoop.ha.ActiveStandbyElector:parentZNodeExists(),341,350,"/**
* Checks if the specified znode exists in ZooKeeper.
* @return true if znode exists, false otherwise
*/","* @return true if the configured parent znode exists
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getConfigViewFsPrefix,org.apache.hadoop.fs.viewfs.ConfigUtil:getConfigViewFsPrefix(),43,46,"/**
 * Calls m1 with default configuration prefix.
 * @return result of m1 with Constants.CONFIG_VIEWFS_PREFIX_DEFAULT_MOUNT_TABLE
 */","* Get the config variable prefix for the default mount table
   * @return the config variable prefix for the default mount table",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path),1537,1542,"/**
* Computes checksum for a file.
* @param f file path
* @throws FileNotFoundException if path is not a file
* @throws IOException on I/O errors
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)",1732,1737,"/**
* Throws FileNotFoundException as path is directory.
* @param f file path
* @param bufferSize buffer size for input stream
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java,deserializeFromString,org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:deserializeFromString(java.lang.String),125,136,"/**
 * Parses a serialized string to create a RegexMountPointResolvedDstPathReplaceInterceptor.
 * @param serializedString the input string in serialized form
 * @return an interceptor object or null if parsing fails
 */","* Create interceptor from config string. The string should be in
   * replaceresolvedpath:wordToReplace:replaceString
   * Note that we'll assume there's no ':' in the regex for the moment.
   *
   * @return Interceptor instance or null on bad config.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,getReturnValue,org.apache.hadoop.io.retry.CallReturn:getReturnValue(),71,77,"/**
* Returns function's result or rethrows exception.
* @return method's return value
* @throws Throwable if an exception was thrown during execution
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,getReadableByteChannel,org.apache.hadoop.net.SocketInputWrapper:getReadableByteChannel(),81,86,"/**
* Returns the readable byte channel from the socket.
* @throws IllegalStateException if the socket does not have a channel
*/","* @return an underlying ReadableByteChannel implementation.
   * @throws IllegalStateException if this socket does not have a channel",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getChecksumFileLength,"org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFileLength(org.apache.hadoop.fs.Path,long)",143,145,"/**
* Applies mask operations on file size.
* @param file Path to the file
* @param fileSize original file size in bytes
* @return masked file size as a long value
*/","* Return the length of the checksum file given the size of the
   * actual file.
   *
   * @param file the file path.
   * @param fileSize file size.
   * @return checksum length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getFilesystem,org.apache.hadoop.fs.DF:getFilesystem(),72,82,"/**
 * Masks file system based on OS.
 * @return masked filesystem path or null if not applicable
 */","* @return a string indicating which filesystem volume we're checking.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getMount,org.apache.hadoop.fs.DF:getMount(),110,127,"/**
* Masks directory path for mounting.
* @throws IOException if file operations fail
* @return masked mount path or throws exception
*/","* @return the filesystem mount point for the indicated volume.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DFCachingGetSpaceUsed.java,refresh,org.apache.hadoop.fs.DFCachingGetSpaceUsed:refresh(),44,47,"/**
 * Calls m2 on used with result of df's m1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getPercentUsed,org.apache.hadoop.fs.DF:getPercentUsed(),100,104,"/**
* Calculates percentage of remaining capacity.
* @return integer percentage of available capacity
*/","@return the amount of the volume full, as a percent.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,startPositionWithoutWindowsDrive,org.apache.hadoop.fs.Path:startPositionWithoutWindowsDrive(java.lang.String),324,330,"/**
 * Determines mask value based on path conditions.
 * @param path input file path
 * @return integer mask value (0, 2, or 3)
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,toString,org.apache.hadoop.fs.Path:toString(),476,503,"/**
* Constructs a URI string from components.
* @return constructed URI as String
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,<init>,"org.apache.hadoop.fs.FSInputChecker:<init>(org.apache.hadoop.fs.Path,int,boolean,java.util.zip.Checksum,int,int)",87,91,"/**
* Initializes input checker with specified parameters.
* @param file path to the file to check
* @param numOfRetries number of retry attempts
* @param verifyChecksum flag to enable checksum verification
* @param sum checksum object for validation
* @param chunkSize size of data chunks
* @param checksumSize size of checksums
*/","Constructor
   * 
   * @param file The name of the file to be read
   * @param numOfRetries Number of read retries when ChecksumError occurs
   * @param sum the type of Checksum engine
   * @param chunkSize maximun chunk size
   * @param checksumSize the number byte of each checksum
   * @param verifyChecksum verify check sum.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,seek,org.apache.hadoop.fs.FSInputChecker:seek(long),428,451,"/**
* Moves file pointer to specified position.
* @param pos target position in file
* @throws IOException if an I/O error occurs or position is negative
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ByteBufferUtil.java,streamHasByteBufferRead,org.apache.hadoop.fs.ByteBufferUtil:streamHasByteBufferRead(java.io.InputStream),37,46,"/**
* Checks if input stream is compatible with ByteBufferReadable.
* @param stream the InputStream to check
* @return true if stream is ByteBufferReadable, false otherwise
*/",* Determine if a stream can do a byte buffer read via read(ByteBuffer buf),,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,init,org.apache.hadoop.fs.audit.CommonAuditContext:init(),193,197,"/**
 * Masks function with thread ID.
 * @param PARAM_THREAD1 parameter for masking
 */",* Initialize.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,noteEntryPoint,org.apache.hadoop.fs.audit.CommonAuditContext:noteEntryPoint(java.lang.Object),278,288,"/**
* Masks tool by command if conditions met.
* @param tool the tool object to be masked
*/","* Add the entry point as a context entry with the key
   * {@link AuditConstants#PARAM_COMMAND}
   * if it has not  already been recorded.
   * This is called via ToolRunner but may be used at any
   * other entry point.
   * @param tool object loaded/being launched.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,<init>,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:<init>(),404,405,"/**
 * Private constructor to prevent instantiation from outside.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,int)",93,95,"/**
 * Masks and processes a string key with an integer value.
 * @param key non-null string key to be processed
 * @param value integer value associated with the key
 * @return result of processing by method m1
 */","* Set optional int parameter for the Builder.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,float)",108,111,"/**
 * Deprecated function to mask a string with a float value.
 * @param key unique identifier key
 * @param value float value to be masked
 * @return result of masking operation
 */","* This parameter is converted to a long and passed
   * to {@link #optLong(String, long)} -all
   * decimal precision is lost.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)
   * @deprecated use {@link #optDouble(String, double)}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,long)",121,123,"/**
 * Applies mask to value using provided key.
 * @param key unique identifier for masking logic
 * @param value original numeric value to be masked
 * @return masked value of type B
 */","* Set optional long parameter for the Builder.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @deprecated use  {@link #optLong(String, long)} where possible.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,double)",136,139,"/**
 * Deprecated. Converts and maps a string key to a long value using another function.
 * @param key unique identifier string
 * @param value numeric value to be converted to long
 * @return B result of the mapping function
 */","* Pass an optional double parameter for the Builder.
   * This parameter is converted to a long and passed
   * to {@link #optLong(String, long)} -all
   * decimal precision is lost.
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)
   * @deprecated use {@link #optDouble(String, double)}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,int)",207,209,"/**
 * Applies mask operation using specified key and value.
 * @param key unique identifier string
 * @param value integer value to be masked
 * @return result of the mask operation
 */","* Set mandatory int option.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,float)",221,224,"/**
 * Deprecated. Converts float to long and calls m1.
 * @param key unique identifier key
 * @param value float value to be converted
 * @return result of m1 call
 */","* This parameter is converted to a long and passed
   * to {@link #mustLong(String, long)} -all
   * decimal precision is lost.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @deprecated use {@link #mustDouble(String, double)} to set floating point.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,long)",234,237,"/**
* Deprecated function to mask a value with a key.
* @param key unique identifier string
* @param value long value to be masked
* @return result of masking operation
*/","* Set mandatory long option.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,double)",248,251,"/**
 * Deprecated method to mask a string with a double value.
 * @param key unique identifier
 * @param value numeric value to be masked
 * @return result of masking operation
 */","* Set mandatory long option, despite passing in a floating
   * point value.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,getRow,org.apache.hadoop.tools.TableListing$Column:getRow(int),100,116,"/**
* Masks and formats a row from the dataset.
* @param idx index of the row to process
* @return formatted string array of the masked row
*/","* Return the ith row of the column as a set of wrapped strings, each at
     * most wrapWidth in length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBUploadHandle.java,from,org.apache.hadoop.fs.BBUploadHandle:from(java.nio.ByteBuffer),40,42,"/**
* Creates an UploadHandle from a ByteBuffer.
* @param byteBuffer source data buffer
* @return UploadHandle instance wrapping the ByteBuffer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBUploadHandle.java,equals,org.apache.hadoop.fs.BBUploadHandle:equals(java.lang.Object),54,61,"/**
* Checks equality with another UploadHandle.
* @param other object to compare
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,startLocalOutput,"org.apache.hadoop.fs.FilterFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",396,400,"/**
 * Moves file from temporary local path to final output path.
 * @param fsOutputFile destination path in the filesystem
 * @param tmpLocalFile source path of the temporary local file
 * @return Path object representing the moved file
 * @throws IOException if an I/O error occurs during the move operation
 */","* Returns a local File that the user can write output to.  The caller
   * provides both the eventual FS target name and the local working
   * file.  If the FS is local, we write directly into the target.  If
   * the FS is remote, we write into the tmp local area.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setWriteChecksum,org.apache.hadoop.fs.FilterFileSystem:setWriteChecksum(boolean),517,520,"/**
 * Delegates checksum writing to file system.
 * @param writeChecksum flag to enable or disable checksum writing
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,"org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)",86,88,"/**
* Constructs a FsPermission with specified actions and sticky bit.
* @param u user action permissions
* @param g group action permissions
* @param o other action permissions
* @param sb sticky bit flag
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,fromShort,org.apache.hadoop.fs.permission.FsPermission:fromShort(short),174,177,"/**
* Masks and processes bits from input.
* @param n input short value to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,getEffectivePermission,"org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry,org.apache.hadoop.fs.permission.FsPermission)",247,277,"/**
* Calculates effective permission for an ACL entry.
* @param entry ACL entry to process
* @param permArg optional file system permission argument
* @return calculated FsAction based on ACL rules
*/","* Get the effective permission for the AclEntry. <br>
   * Recommended to use this API ONLY if client communicates with the old
   * NameNode, needs to pass the Permission for the path to get effective
   * permission, else use {@link AclStatus#getEffectivePermission(AclEntry)}.
   * @param entry AclEntry to get the effective action
   * @param permArg Permission for the path. However if the client is NOT
   *          communicating with old namenode, then this argument will not have
   *          any preference.
   * @return Returns the effective permission for the entry.
   * @throws IllegalArgumentException If the client communicating with old
   *           namenode and permission is not passed as an argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,<init>,"org.apache.hadoop.fs.permission.PermissionStatus$2:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",72,76,"/**
 * Constructs a PermissionStatus object.
 * @param user username associated with the permission
 * @param group group name associated with the permission
 * @param permission file system permissions
 */","* Constructor.
   *
   * @param user user.
   * @param group group.
   * @param permission permission.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,parseAclEntry,"org.apache.hadoop.fs.permission.AclEntry:parseAclEntry(java.lang.String,boolean)",263,323,"/**
 * Parses an ACL string and creates an AclEntry.
 * @param aclStr the ACL specification string
 * @param includePermission whether to include permission in parsing
 * @return AclEntry object or throws exception for invalid input
 */","* Parses a string representation of an ACL into a AclEntry object.<br>
   * The expected format of ACL entries in the string parameter is the same
   * format produced by the {@link #toStringStable()} method.
   * 
   * @param aclStr
   *          String representation of an ACL.<br>
   *          Example: ""user:foo:rw-""
   * @param includePermission
   *          for setAcl operations this will be true. i.e. Acl should include
   *          permissions.<br>
   *          But for removeAcl operation it will be false. i.e. Acl should not
   *          contain permissions.<br>
   *          Example: ""user:foo,group:bar,mask::""
   * @return Returns an {@link AclEntry} object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,<init>,"org.apache.hadoop.fs.permission.FsCreateModes:<init>(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",65,70,"/**
* Constructs a FsCreateModes instance with masked and unmasked permissions.
* @param masked the masked file system permission
* @param unmasked the unmasked file system permission
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,equals,org.apache.hadoop.fs.permission.FsCreateModes:equals(java.lang.Object),88,101,"/**
* Compares current object with another for equality.
* @param o the object to compare
* @return true if objects are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,toStringStable,org.apache.hadoop.fs.permission.AclEntry:toStringStable(),119,136,"/**
* Constructs a formatted string representation.
* @return formatted string based on object properties
*/","* Returns a string representation guaranteed to be stable across versions to
   * satisfy backward compatibility requirements, such as for shell command
   * output or serialization.  The format of this string representation matches
   * what is expected by the {@link #parseAclSpec(String, boolean)} and
   * {@link #parseAclEntry(String, boolean)} methods.
   *
   * @return stable, backward compatible string representation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntryType.java,toString,org.apache.hadoop.fs.permission.AclEntryType:toString(),59,65,"/**
 * Returns masked function name.
 * @return Masked function name as String
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStrings,"org.apache.hadoop.util.StringUtils:getStrings(java.lang.String,java.lang.String)",418,424,"/**
* Splits input string by delimiter and masks the result.
* @param str input string to split
* @param delim delimiter for splitting
* @return array of masked strings or null if no elements
*/","* Returns an arraylist of strings.
   * @param str the string values
   * @param delim delimiter to separate the values
   * @return the arraylist of the separated string values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStringCollection,org.apache.hadoop.util.StringUtils:getStringCollection(java.lang.String),431,434,"/**
 * Splits input string by commas.
 * @param str input string to split
 * @return collection of substrings
 */","* Returns a collection of strings.
   * @param str comma separated string values
   * @return an <code>ArrayList</code> of string values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionParser.java,<init>,"org.apache.hadoop.fs.permission.PermissionParser:<init>(java.lang.String,java.util.regex.Pattern,java.util.regex.Pattern)",51,62,"/**
 * Initializes a PermissionParser with mode string and patterns.
 * @param modeStr the permission mode string to parse
 * @param symbolic pattern for symbolic permissions
 * @param octal pattern for octal permissions
 * @throws IllegalArgumentException if modeStr doesn't match any pattern
 */","* Begin parsing permission stored in modeStr
   * 
   * @param modeStr Permission mode, either octal or symbolic
   * @param symbolic Use-case specific symbolic pattern to match against
   * @throws IllegalArgumentException if unable to parse modeStr",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionParser.java,combineModes,"org.apache.hadoop.fs.permission.PermissionParser:combineModes(int,boolean)",175,183,"/**
* Applies permission masks based on user, group, and other permissions.
* @param existing current permission bits
* @param exeOk flag for executable permission
* @return new permission bits after applying masks
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",649,651,"/**
 * Constructs a ByteBufferBlockFactory.
 * @param keyToBufferDir directory for buffer storage
 * @param conf configuration settings
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",526,528,"/**
 * Constructs an ArrayBlockFactory with specified directory and configuration.
 * @param keyToBufferDir directory path for key-to-buffer mapping
 * @param conf configuration settings for the factory
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,requestBuffer,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:requestBuffer(int),659,663,"/**
* Requests a buffer from the pool.
* @param limit requested buffer size
* @return ByteBuffer instance of specified size
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,releaseBuffer,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:releaseBuffer(java.nio.ByteBuffer),665,669,"/**
* Releases a ByteBuffer back to the pool.
* @param buffer the ByteBuffer to release
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$DiskBlock:<init>(java.io.File,int,long,org.apache.hadoop.fs.store.BlockUploadStatistics)",853,863,"/**
* Initializes a DiskBlock with a file buffer.
* @param bufferFile the file used for buffering data
* @param limit the maximum size of the block
* @param index the block's index
* @param statistics upload statistics tracker
* @throws FileNotFoundException if the buffer file cannot be opened
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:<init>(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",574,581,"/**
 * Constructs a ByteArrayBlock with specified index, limit, and statistics.
 * @param index the block's index
 * @param limit the maximum size of the block
 * @param statistics upload statistics for tracking
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:hasCapacity(long),869,872,"/**
 * Checks if adding bytes exceeds the limit.
 * @param bytes amount of bytes to add
 * @return true if within limit, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toString,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:toString(),945,955,"/**
* Generates a string representation of FileBlock.
* @return formatted string with block details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,innerClose,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:innerClose(),905,933,"/**
* Handles closing operations based on the current state.
* @throws IOException if an I/O error occurs
*/","* The close operation will delete the destination file if it still
     * exists.
     *
     * @throws IOException IO problems",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,checkOpenState,org.apache.hadoop.fs.store.ByteBufferInputStream:checkOpenState(),89,92,"/**
 * Checks stream status and throws an exception if closed.
 */","* Check the open state.
   * @throws IllegalStateException if the stream is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,enterState,"org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState,org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)",349,355,"/**
* Transitions to a new state.
* @param current current state object
* @param next target state object
* @throws IllegalStateException if transition is invalid
*/","* Atomically enter a state, verifying current state.
     *
     * @param current current state. null means ""no check""
     * @param next    next state
     * @throws IllegalStateException if the current state is not as expected",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$DataBlock:write(byte[],int,int)",424,433,"/**
* Masks a buffer with specified parameters.
* @param buffer the byte array to process
* @param offset starting position in the buffer
* @param length number of bytes to mask
* @return always returns 0
* @throws IOException if an I/O error occurs
*/","* Write a series of bytes from the buffer, from the offset.
     * Returns the number of bytes written.
     * Only valid in the state {@code Writing}.
     * Base class verifies the state but does no writing.
     *
     * @param buffer buffer.
     * @param offset offset.
     * @param length length of write.
     * @return number of bytes written.
     * @throws IOException trouble",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,flush,org.apache.hadoop.fs.store.DataBlocks$DataBlock:flush(),442,444,"/**
 * Masks function by writing to output.
 * @throws IOException if an I/O error occurs
 */","* Flush the output.
     * Only valid in the state {@code Writing}.
     * In the base class, this is a no-op
     *
     * @throws IOException any IO problem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,set,"org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:set(java.lang.String,java.lang.String)",246,248,"/**
 * Masks a key with a value.
 * @param key unique identifier to mask
 * @param value value to apply as a mask
 */","* Set an attribute. If the value is non-null/empty,
   * it will be used as a query parameter.
   *
   * @param key key to set
   * @param value value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,extractQueryParameters,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:extractQueryParameters(java.lang.String),331,347,"/**
* Parses URI from header and extracts query parameters.
* @param header HTTP request header containing the URI
* @return Map of query parameters with empty values for nulls
*/","* Split up the string. Uses httpClient: make sure it is on the classpath.
   * Any query param with a name but no value, e.g ?something is
   * returned in the map with an empty string as the value.
   * @param header URI to parse
   * @return a map of parameters.
   * @throws URISyntaxException failure to build URI from header.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:hasCapacity(long),602,605,"/**
 * Checks if adding bytes exceeds the limit.
 * @param bytes number of bytes to add
 * @return true if within limit, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,remainingCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:remainingCapacity(),607,610,"/**
 * Calculates mask value.
 * @return result of subtracting m1() from limit
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,dataSize,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:dataSize(),718,720,"/**
 * Returns data size if available, otherwise calls m1().
 * @return Integer value representing data size or result of m1()
 */","* Get the amount of data; if there is no buffer then the size is 0.
       *
       * @return the amount of data available to upload.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:hasCapacity(long),733,736,"/**
 * Checks if input bytes are within mask limit.
 * @param bytes number of bytes to check
 * @return true if bytes are less than or equal to mask value, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hashCode,org.apache.hadoop.fs.FileSystem$Cache$Key:hashCode(),3891,3894,"/**
* Calculates sum of scheme+authority and UGI identifiers.
* @return Integer sum of identifiers
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hashCode,org.apache.hadoop.security.UserGroupInformation$RealUser:hashCode(),492,495,"/**
 * Delegates call to realUser's m1 method.
 * @return result of realUser.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,hashCode,org.apache.hadoop.ipc.Client$ConnectionId:hashCode(),1843,1859,"/**
* Generates a hash code based on various configuration parameters.
* @return computed hash code
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,equals,org.apache.hadoop.fs.FileSystem$Cache$Key:equals(java.lang.Object),3900,3913,"/**
* Checks equality with another object.
* @param obj the object to compare
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,equals,org.apache.hadoop.security.UserGroupInformation$RealUser:equals(java.lang.Object),481,490,"/**
* Compares this object with another for equality.
* @param o the object to compare
* @return true if objects are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getInitialWorkingDirectory,org.apache.hadoop.fs.FilterFs:getInitialWorkingDirectory(),78,81,"/**
 * Returns a path from the file system.
 * @return Path object representing a file system location
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolvePath,org.apache.hadoop.fs.FileContext:resolvePath(org.apache.hadoop.fs.Path),616,619,"/**
* Applies mask to file path.
* @param f input file path
* @return masked file path
* @throws FileNotFoundException if file not found
* @throws UnresolvedLinkException for unresolved links
* @throws AccessControlException for access issues
* @throws IOException for other I/O errors
*/","* Resolve the path following any symlinks or mount points
   * @param f to be resolved
   * @return fully qualified resolved path
   * 
   * @throws FileNotFoundException  If <code>f</code> does not exist
   * @throws AccessControlException if access denied
   * @throws IOException If an IO Error occurred
   * @throws UnresolvedLinkException If unresolved link occurred.
   *
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server
   *
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is not valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,msync,org.apache.hadoop.fs.FileContext:msync(),1267,1269,"/**
 * Calls method m1 on defaultFS.
 * Throws IOException or UnsupportedOperationException as needed.
 */","* Synchronize client metadata state.
   *
   * @throws IOException If an I/O error occurred.
   * @throws UnsupportedOperationException If file system for <code>f</code> is
   *                                       not supported.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,msync,org.apache.hadoop.fs.FilterFs:msync(),127,130,"/**
 * Delegates call to myFs's m1 method.
 * @throws IOException if an I/O error occurs
 * @throws UnsupportedOperationException if operation is not supported
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,printStatistics,org.apache.hadoop.fs.FileContext:printStatistics(),2412,2414,"/**
 * Calls the static method m1 of AbstractFileSystem.
 */","* Prints the statistics to standard output. File System is identified by the
   * scheme and authority.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getStatistics,org.apache.hadoop.fs.AbstractFileSystem:getStatistics(java.net.URI),191,204,"/**
* Retrieves or creates statistics for a given URI.
* @param uri the input URI
* @return Statistics object associated with the base URI
*/","* Get the statistics for a particular file system.
   * 
   * @param uri
   *          used as key to lookup STATISTICS_TABLE. Only scheme and authority
   *          part of the uri are used.
   * @return a statistics object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listCorruptFileBlocks,org.apache.hadoop.fs.FilterFs:listCorruptFileBlocks(org.apache.hadoop.fs.Path),209,213,"/**
 * Returns an iterator over file paths.
 * @param path directory path to iterate over
 * @return RemoteIterator of Path objects
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,createMultipartUploader,org.apache.hadoop.fs.AbstractFileSystem:createMultipartUploader(org.apache.hadoop.fs.Path),1634,1639,"/**
 * Initializes a multipart uploader builder.
 * @param basePath base directory path
 * @return MultipartUploaderBuilder instance or null
 * @throws IOException if an I/O error occurs
 */","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,obtainContext,org.apache.hadoop.fs.LocalDirAllocator:obtainContext(java.lang.String),107,117,"/**
* Retrieves or creates an AllocatorPerContext for a given configuration item.
* @param contextCfgItemName name of the configuration item
* @return AllocatorPerContext instance associated with the config item
*/","This method must be used to obtain the dir allocation context for a 
   * particular value of the context name. The context name must be an item
   * defined in the Configuration object for which we want to control the 
   * dir allocations (e.g., <code>mapred.local.dir</code>). The method will
   * create a context for that name if it doesn't already exist.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getStatistics,org.apache.hadoop.fs.FilterFs:getStatistics(),68,71,"/**
 * Retrieves statistics.
 * @return Statistics object from myFs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,getPos,org.apache.hadoop.fs.FSDataOutputStream:getPos(),97,99,"/**
 * Retrieves a value from PositionCache.
 * @return long value from PositionCache's m1 method
 */","* Get the current position in the output stream.
   *
   * @return the current position in the output stream",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,syncFs,org.apache.hadoop.io.SequenceFile$Writer:syncFs(),1382,1387,"/**
 * Masks functionality by invoking m1 on output stream.
 * @throws IOException if an I/O error occurs
 */","* flush all currently written data to the file system.
     * @deprecated Use {@link #hsync()} or {@link #hflush()} instead
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hflush,org.apache.hadoop.io.SequenceFile$Writer:hflush(),1396,1401,"/**
 * Calls m1 on the output stream if it's not null.
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hsync,org.apache.hadoop.io.SequenceFile$Writer:hsync(),1389,1394,"/**
 * Calls m1 on the output stream if it's not null.
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,quota,org.apache.hadoop.fs.ContentSummary$Builder:quota(long),90,94,"/**
 * Sets the quota and returns the builder.
 * @param quota the quota value to set
 * @return the current builder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,spaceConsumed,org.apache.hadoop.fs.ContentSummary$Builder:spaceConsumed(long),96,100,"/**
 * Sets space consumed and returns builder.
 * @param spaceConsumed amount of space consumed
 * @return this Builder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,spaceQuota,org.apache.hadoop.fs.ContentSummary$Builder:spaceQuota(long),102,106,"/**
 * Sets space quota and returns builder instance.
 * @param spaceQuota quota value to set
 * @return current Builder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,build,org.apache.hadoop.fs.ContentSummary$Builder:build(),132,136,"/**
 * Generates content summary.
 * @return ContentSummary object with file and directory counts
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,<init>,"org.apache.hadoop.util.ApplicationClassLoader:<init>(java.net.URL[],java.lang.ClassLoader,java.util.List)",87,100,"/**
* Constructs an ApplicationClassLoader.
* @param urls array of URLs for the classpath
* @param parent parent ClassLoader
* @param systemClasses list of system classes, uses default if null or empty
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getTrimmedStringCollection,org.apache.hadoop.util.StringUtils:getTrimmedStringCollection(java.lang.String),490,495,"/**
* Masks characters in input string.
* @param str input string to mask
* @return collection of masked strings
*/","* Splits a comma separated value <code>String</code>, trimming leading and
   * trailing whitespace on each value. Duplicate and empty values are removed.
   *
   * @param str a comma separated <code>String</code> with values, may be null
   * @return a <code>Collection</code> of <code>String</code> values, empty
   *         Collection if null String input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/LoggingStateChangeListener.java,<init>,org.apache.hadoop.service.LoggingStateChangeListener:<init>(),51,53,"/**
 * Constructs a new LoggingStateChangeListener with default logging.
 */",* Log events to the static log for this class,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,<init>,org.apache.hadoop.service.ServiceStateException:<init>(java.lang.String),48,50,"/**
 * Constructs a new ServiceStateException with a message.
 * @param message detailed error message
 */","* Instantiate
   * @param message error message",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,<init>,"org.apache.hadoop.service.ServiceStateException:<init>(int,java.lang.String,java.lang.Throwable)",78,83,"/**
* Constructs a ServiceStateException with an exit code.
* @param exitCode error code indicating service state
* @param message descriptive error message
* @param cause underlying exception causing the error
*/","* Instantiate, using the specified exit code as the exit code
   * of the exception, irrespetive of any exit code supplied by any inner
   * cause.
   *
   * @param exitCode exit code to declare
   * @param message exception message
   * @param cause inner cause",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,convert,"org.apache.hadoop.service.ServiceStateException:convert(java.lang.String,java.lang.Throwable)",120,126,"/**
* Wraps a throwable in a RuntimeException.
* @param text error message
* @param fault original exception
* @return RuntimeException or ServiceStateException
*/","* Convert any exception into a {@link RuntimeException}.
   * If the caught exception is already of that type, it is typecast to a
   * {@link RuntimeException} and returned.
   *
   * All other exception types are wrapped in a new instance of
   * {@code ServiceStateException}.
   * @param text text to use if a new exception is created
   * @param fault exception or throwable
   * @return a {@link RuntimeException} to rethrow",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,convert,org.apache.hadoop.service.ServiceStateException:convert(java.lang.Throwable),101,107,"/**
* Wraps a Throwable in a RuntimeException.
* @param fault original exception to be wrapped
* @return RuntimeException, either the original or wrapped in ServiceStateException
*/","* Convert any exception into a {@link RuntimeException}.
   * All other exception types are wrapped in a new instance of
   * {@code ServiceStateException}.
   * @param fault exception or throwable
   * @return a {@link RuntimeException} to rethrow",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,<init>,org.apache.hadoop.service.ServiceStateModel:<init>(java.lang.String),60,62,"/**
 * Constructs a new ServiceStateModel with the specified name and default state.
 * @param name service name
 */","* Create the service state model in the {@link Service.STATE#NOTINITED}
   * state.
   *
   * @param name input name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,isInState,org.apache.hadoop.service.AbstractService:isInState(org.apache.hadoop.service.Service$STATE),451,454,"/**
 * Checks if the current service state matches the expected state.
 * @param expected the desired state to compare against
 * @return true if the current state matches the expected state, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,isValidStateTransition,"org.apache.hadoop.service.ServiceStateModel:isValidStateTransition(org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)",149,153,"/**
* Checks if state transition is valid.
* @param current current service state
* @param proposed new proposed state
* @return true if transition is allowed, false otherwise
*/","* Is a state transition valid?
   * There are no checks for current==proposed
   * as that is considered a non-transition.
   *
   * using an array kills off all branch misprediction costs, at the expense
   * of cache line misses.
   *
   * @param current current state
   * @param proposed proposed new state
   * @return true if the transition to a new state is valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,toString,org.apache.hadoop.service.ServiceStateModel:toString(),159,163,"/**
* Constructs a string based on name and state conditions.
* @return formatted string combining name and state
*/","* return the state text as the toString() value
   * @return the current state's description",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/HadoopUncaughtExceptionHandler.java,<init>,org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:<init>(),71,73,"/**
 * Constructs a new HadoopUncaughtExceptionHandler with no specific handler.
 */","* Basic exception handler -logs simple exceptions, then continues.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,handle,org.apache.hadoop.service.launcher.IrqHandler:handle(sun.misc.Signal),125,131,"/**
* Handles a signal by logging and processing it.
* @param s the incoming signal to process
*/","* Handler for the JVM API for signal handling.
   * @param s signal raised",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,getService,org.apache.hadoop.service.launcher.InterruptEscalator:getService(),84,87,"/**
* Retrieves service instance using launcher.
* @return Service object or null if launcher is null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,<init>,"org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:<init>(org.apache.hadoop.service.Service,int)",188,191,"/**
* Initializes a forced shutdown for a given service.
* @param service the service to be shut down
* @param shutdownTimeMillis time in milliseconds before shutdown
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,lookup,org.apache.hadoop.service.launcher.InterruptEscalator:lookup(java.lang.String),153,160,"/**
 * Retrieves IRQ handler by signal name.
 * @param signalName name of the signal
 * @return IrqHandler object or null if not found
 */","* Look up the handler for a signal.
   * @param signalName signal name
   * @return a handler if found",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable)",49,51,"/**
 * Constructs a ServiceLaunchException with an exit code and cause.
 * @param exitCode numeric code representing the error
 * @param cause underlying exception causing the service launch failure
 */","* Create an exception with the specific exit code.
   * @param exitCode exit code
   * @param cause cause of the exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String)",58,60,"/**
 * Constructs a new ServiceLaunchException with an exit code and message.
 * @param exitCode numeric code representing the error
 * @param message descriptive error message
 */","* Create an exception with the specific exit code and text.
   * @param exitCode exit code
   * @param message message to use in exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String,java.lang.Object[])",74,79,"/**
* Constructs a ServiceLaunchException with an exit code and formatted message.
* @param exitCode numeric error code
* @param format message format string
* @param args arguments for the message format
*/","* Create a formatted exception.
   * <p>
   * This uses {@link String#format(String, Object...)}
   * to build the formatted exception in the ENGLISH locale.
   * <p>
   * If the last argument is a throwable, it becomes the cause of the exception.
   * It will also be used as a parameter for the format.
   * @param exitCode exit code
   * @param format format for message to use in exception
   * @param args list of arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String)",1083,1086,"/**
* Constructs a Kerberos diagnostic failure with category and message.
* @param category type of failure category
* @param message detailed error message
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable,java.lang.String,java.lang.Object[])",91,94,"/**
* Constructs a ServiceLaunchException with an exit code, cause, and formatted message.
* @param exitCode numeric exit status code
* @param cause underlying Throwable causing the exception
* @param format message format string
* @param args arguments for the message format
*/","* Create a formatted exception.
   * <p>
   * This uses {@link String#format(String, Object...)}
   * to build the formatted exception in the ENGLISH locale.
   * @param exitCode exit code
   * @param cause inner cause
   * @param format format for message to use in exception
   * @param args list of arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,run,org.apache.hadoop.service.launcher.ServiceShutdownHook:run(),82,85,"/**
 * Calls method m1 to perform masking operation.
 */","* Shutdown handler.
   * Query the service hook reference -if it is still valid the 
   * {@link Service#stop()} operation is invoked.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,<init>,org.apache.hadoop.service.launcher.ServiceLauncher:<init>(java.lang.String),184,186,"/**
 * Constructs a ServiceLauncher with default configuration.
 * @param serviceClassName name of the service class to launch
 */","* Create an instance of the launcher.
   * @param serviceClassName classname of the service",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,toString,org.apache.hadoop.service.launcher.ServiceLauncher:toString(),253,264,"/**
* Constructs a string representation of the ServiceLauncher.
* @return formatted string with service details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,noteException,org.apache.hadoop.service.launcher.ServiceLauncher:noteException(org.apache.hadoop.util.ExitUtil$ExitException),331,345,"/**
* Handles exit exception by logging details and setting service status.
* @param exitException the exception to handle
*/","* Record that an Exit Exception has been raised.
   * Save it to {@link #serviceException}, with its exit code in
   * {@link #serviceExitCode}
   * @param exitException exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,bindCommandOptions,org.apache.hadoop.service.launcher.ServiceLauncher:bindCommandOptions(),321,323,"/**
* Sets command options using mask configuration.
* @param none
* @return void
*/","* Set the {@link #commandOptions} field to the result of
   * {@link #createOptions()}; protected for subclasses and test access.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,loadConfigurationClasses,org.apache.hadoop.service.launcher.ServiceLauncher:loadConfigurationClasses(),422,449,"/**
* Loads and counts Configuration instances from a list of class names.
* @return number of successfully loaded Configuration instances
*/","* @return This creates all the configurations defined by
   * {@link #getConfigurationsToCreate()} , ensuring that
   * the resources have been pushed in.
   * If one cannot be loaded it is logged and the operation continues
   * except in the case that the class does load but it isn't actually
   * a subclass of {@link Configuration}.
   * @throws ExitUtil.ExitException if a loaded class is of the wrong type",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,registerServiceListener,org.apache.hadoop.service.AbstractService:registerServiceListener(org.apache.hadoop.service.ServiceStateChangeListener),354,357,"/**
 * Adds a listener to track service state changes.
 * @param l listener to be added
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,registerGlobalListener,org.apache.hadoop.service.AbstractService:registerGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener),369,371,"/**
 * Registers a listener for service state changes.
 * @param l ServiceStateChangeListener to be added
 */","* Register a global listener, which receives notifications
   * from the state change events of all services in the JVM
   * @param l listener",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,unregisterServiceListener,org.apache.hadoop.service.AbstractService:unregisterServiceListener(org.apache.hadoop.service.ServiceStateChangeListener),359,362,"/**
 * Adds a listener to track service state changes.
 * @param l ServiceStateChangeListener to be added
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,unregisterGlobalListener,org.apache.hadoop.service.AbstractService:unregisterGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener),378,380,"/**
 * Adds a service state change listener.
 * @param l listener to be added
 * @return true if addition is successful, false otherwise
 */","* unregister a global listener.
   * @param l listener to unregister
   * @return true if the listener was found (and then deleted)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,resetGlobalListeners,org.apache.hadoop.service.AbstractService:resetGlobalListeners(),385,388,"/**
 * Invokes method m1 on globalListeners.
 */",* Package-scoped method for testing -resets the global listener list,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,notifyListeners,org.apache.hadoop.service.AbstractService:notifyListeners(),409,416,"/**
* Notifies all registered listeners about an event.
* Handles exceptions during notification and logs them.
*/","* Notify local and global listeners of state changes.
   * Exceptions raised by listeners are NOT passed up.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,getServiceState,org.apache.hadoop.service.AbstractService:getServiceState(),118,121,"/**
 * Returns the current state from the model.
 * @return STATE representing the current state
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,serviceInit,org.apache.hadoop.service.AbstractService:serviceInit(org.apache.hadoop.conf.Configuration),312,317,"/**
* Updates configuration if it differs from the current one.
* @param conf new configuration settings
*/","* All initialization code needed by a service.
   *
   * This method will only ever be called once during the lifecycle of
   * a specific service instance.
   *
   * Implementations do not need to be synchronized as the logic
   * in {@link #init(Configuration)} prevents re-entrancy.
   *
   * The base implementation checks to see if the subclass has created
   * a new configuration instance, and if so, updates the base class value
   * @param conf configuration
   * @throws Exception on a failure -these will be caught,
   * possibly wrapped, and will trigger a service stop",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceStop,org.apache.hadoop.util.JvmPauseMonitor:serviceStop(),88,100,"/**
* Stops monitoring and calls superclass method.
* @throws Exception if an error occurs during execution
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceStart,org.apache.hadoop.service.CompositeService:serviceStart(),115,126,"/**
* Starts all available services.
* Logs the start process if logging is enabled.
* Calls superclass method after starting services.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,addIfService,org.apache.hadoop.service.CompositeService:addIfService(java.lang.Object),88,95,"/**
* Checks if object is a Service and processes it.
* @param object the object to check and process
* @return true if object is processed, false otherwise
*/","* If the passed object is an instance of {@link Service},
   * add it to the list of services managed by this {@link CompositeService}
   * @param object object.
   * @return true if a service is added, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,"org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.commons.logging.Log,org.apache.hadoop.service.Service)",79,88,"/**
* Masks exceptions during service stop.
* @param log logger for logging errors
* @param service the service to be stopped
* @return caught exception or null if no error occurs
*/","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param log the log to warn at
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.
   * @deprecated to be removed with 3.4.0. Use {@link #stopQuietly(Logger, Service)} instead.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,"org.apache.hadoop.service.ServiceOperations:stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)",100,108,"/**
* Masks exceptions by logging and returning them.
* @param log Logger instance for logging errors
* @param service Service to be operated on
* @return Exception if caught, otherwise null
*/","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param log the log to warn at
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.
   * @see ServiceOperations#stopQuietly(Service)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ProgressableOption:<init>(org.apache.hadoop.util.Progressable),969,971,"/**
 * Constructs a ProgressableOption with a given value.
 * @param value the Progressable instance to encapsulate
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ShortWritable.java,<init>,org.apache.hadoop.io.ShortWritable:<init>(short),37,39,"/**
 * Constructs a new ShortWritable with the specified short value.
 * @param value the short value to be wrapped
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$LengthOption:<init>(long),1874,1876,"/**
 * Constructs a LengthOption with a specified value.
 * @param value the length value to set
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$StartOption:<init>(long),1867,1869,"/**
* Constructs a StartOption with a given value.
* @param value numeric representation of the option
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$BlockSizeOption:<init>(long),925,927,"/**
* Constructs a BlockSizeOption with a specified value.
* @param value the block size option value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,read,org.apache.hadoop.io.DataInputByteBuffer$Buffer:read(),31,37,"/**
* Calls helper method and returns processed byte.
* @return processed byte value or -1 on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,reset,org.apache.hadoop.io.DataInputByteBuffer:reset(java.nio.ByteBuffer[]),83,85,"/**
 * Delegates processing of ByteBuffers to another method.
 * @param input variable number of ByteBuffer objects
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,byteBufferPositionedReadable_readFullyAvailable,org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream),233,246,"/**
* Checks if input stream supports positioned reading.
* @param in input stream to check
* @return true if supported, false otherwise
*/","* Probe to see if the input stream is an instance of ByteBufferPositionedReadable.
   * If the stream is an FSDataInputStream, the wrapped stream is checked.
   * @param in input stream
   * @return true if the stream implements the interface (including a wrapped stream)
   * and that it declares the stream capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,isAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:isAvailable(),433,435,"/**
 * Checks if a condition is met by calling two methods.
 * @return true if condition is met, false otherwise
 */","* Is the wrapped IO class loaded?
   * @return true if the instance is loaded.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,toString,org.apache.hadoop.util.JsonSerialization:toString(java.lang.Object),352,359,"/**
* Masks an object by converting it to a string.
* @param instance the object to be masked
* @return masked string representation or error message if conversion fails
*/","* Convert an instance to a string form for output. This is a robust
   * operation which will convert any JSON-generating exceptions into
   * error text.
   * @param instance non-null instance
   * @return a JSON string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,toUncheckedFunction,org.apache.hadoop.util.functional.FunctionalIO:toUncheckedFunction(org.apache.hadoop.util.functional.FunctionRaisingIOE),84,86,"/**
* Converts a FunctionRaisingIOE to a standard Function.
* @param fun function that may raise an IOException
* @return Function without checked exceptions
*/","* Convert a {@link FunctionRaisingIOE} as a {@link Supplier}.
   * @param fun function to wrap
   * @param <T> type of input
   * @param <R> type of return value.
   * @return a new function which invokes the inner function and wraps
   * exceptions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,fromInstance,org.apache.hadoop.util.JsonSerialization:fromInstance(java.lang.Object),236,238,"/**
 * Applies two transformations to the input instance.
 * @param <T> generic type of the instance
 * @param instance the object to be transformed
 * @return transformed instance after applying both methods
 * @throws IOException if an I/O error occurs during transformation
 */","* clone by converting to JSON and back again.
   * This is much less efficient than any Java clone process.
   * @param instance instance to duplicate
   * @return a new instance
   * @throws IOException IO problems.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,fromBytes,org.apache.hadoop.util.JsonSerialization:fromBytes(byte[]),331,333,"/**
 * Converts byte array to string and processes it.
 * @param bytes input byte array
 * @return processed object of type T
 * @throws IOException if an I/O error occurs
 */","* Deserialize from a byte array.
   * @param bytes byte array
   * @throws IOException IO problems
   * @throws EOFException not enough data
   * @return byte array.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,<init>,org.apache.hadoop.io.VIntWritable:<init>(int),38,38,"/**
 * Constructs a new VIntWritable with the specified integer value.
 * @param value the integer value to be stored
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,equals,org.apache.hadoop.io.ElasticByteBufferPool$Key:equals(java.lang.Object),57,68,"/**
* Compares this object with another for equality.
* @param rhs the other object to compare
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$SyncIntervalOption:<init>(int),1009,1013,"/**
* Constructs a SyncIntervalOption with a specified value.
* @param val desired sync interval, falls back to default if negative
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ReplicationOption:<init>(int),932,934,"/**
 * Initializes a new instance of ReplicationOption with the specified value.
 * @param value integer representing the replication option
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$BufferSizeOption:<init>(int),1881,1883,"/**
 * Constructs a BufferSizeOption with a specified value.
 * @param value integer representing the buffer size option
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$BufferSizeOption:<init>(int),919,921,"/**
 * Constructs a BufferSizeOption with a specified value.
 * @param value integer representing buffer size option
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8:<init>(org.apache.hadoop.io.UTF8),78,80,"/**
 * Constructs a UTF8 object by copying from another UTF8 instance.
 * @param utf8 source UTF8 object to copy from
 */","* Construct from a given string.
   * @param utf8 input utf8.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,writeString,"org.apache.hadoop.io.UTF8:writeString(java.io.DataOutput,java.lang.String)",351,365,"/**
* Writes string to DataOutput with length mask.
* @param out destination for writing data
* @param s input string to be written
* @return length of the written string
* @throws IOException if string is too long or IO error occurs
*/","* @return Write a UTF-8 encoded string.
   *
   * @see DataOutput#writeUTF(String)
   * @param out input out.
   * @param s input s.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,skip,org.apache.hadoop.io.UTF8:skip(java.io.DataInput),144,147,"/**
* Masks data input by reading length and skipping bytes.
* @param in DataInput source
* @throws IOException if I/O error occurs
*/","* Skips over one UTF8 in the input.
   * @param in datainput.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,skipCompressedByteArray,org.apache.hadoop.io.WritableUtils:skipCompressedByteArray(java.io.DataInput),54,59,"/**
* Masks data from input stream.
* @param in DataInput source
* @throws IOException on read error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compare,"org.apache.hadoop.io.SequenceFile$Sorter$SortPass$SeqFileComparator:compare(org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable)",3258,3263,"/**
* Compares two keys using a comparator.
* @param I first key identifier
* @param J second key identifier
* @return comparison result as integer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,org.apache.hadoop.io.SetFile:<init>(),35,35,"/**
 * Protected constructor to prevent instantiation from outside.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,org.apache.hadoop.io.ArrayFile:<init>(),35,35,"/**
 * Private constructor to prevent instantiation from outside.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,<init>,org.apache.hadoop.io.LongWritable:<init>(long),37,37,"/**
 * Constructs a LongWritable with the specified long value.
 * @param value the long value to initialize the object with
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,seek,org.apache.hadoop.io.ArrayFile$Reader:seek(long),112,115,"/**
 * Calls m1 on key with n and then calls m2 with key.
 * @param n long value to pass to m1
 * @throws IOException if an I/O error occurs
 */","* Positions the reader before its <code>n</code>th value.
     *
     * @param n n key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,get,"org.apache.hadoop.io.ArrayFile$Reader:get(long,org.apache.hadoop.io.Writable)",147,151,"/**
* Updates a writable value associated with a key.
* @param n unique identifier for the key
* @param value new value to be written
* @return updated Writable object
* @throws IOException if an I/O error occurs
*/","* Return the <code>n</code>th value in the file.
     * @param n n key.
     * @param value value.
     * @throws IOException raised on errors performing I/O.
     * @return writable.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ValueClassOption:<init>(java.lang.Class),952,954,"/**
 * Constructs a ValueClassOption with the specified class.
 * @param value the class to be used in this option
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,org.apache.hadoop.io.MapFile$Writer$KeyClassOption:<init>(java.lang.Class),270,272,"/**
 * Constructs a KeyClassOption with the specified class.
 * @param value the class to be used as an option
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$KeyClassOption:<init>(java.lang.Class),945,947,"/**
 * Constructs a KeyClassOption with the specified class.
 * @param value the class to be used as the option value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ByteWritable.java,<init>,org.apache.hadoop.io.ByteWritable:<init>(byte),34,34,"/**
 * Constructs a ByteWritable with the specified byte value.
 * @param value the byte value to be stored
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,<init>,org.apache.hadoop.io.OutputBuffer:<init>(),71,73,"/**
 * Constructs an OutputBuffer using a default Buffer.
 */",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,getData,org.apache.hadoop.io.OutputBuffer:getData(),86,86,"/**
 * Retrieves data from buffer.
 * @return byte array containing buffer data
 */","* Returns the current contents of the buffer.
   *  Data is only valid to {@link #getLength()}.
   *
   * @return the current contents of the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,getLength,org.apache.hadoop.io.OutputBuffer:getLength(),93,93,"/**
 * Delegates call to buffer's m1 method.
 * @return result of buffer's m1 method
 */","* Returns the length of the valid data currently in the buffer.
   * @return the length of the valid data
   *          currently in the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,reset,org.apache.hadoop.io.OutputBuffer:reset(),96,99,"/**
 * Calls m1 on buffer and returns current instance.
 * @return current OutputBuffer instance
 */",@return Resets the buffer to empty.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compare,"org.apache.hadoop.io.WritableComparator:compare(java.lang.Object,java.lang.Object)",215,218,"/**
 * Compares two objects using WritableComparable interface.
 * @param a first object to compare
 * @param b second object to compare
 * @return comparison result as integer
 */","* Compare two Object.
   *
   * @param a the first object to be compared.
   * @param b the second object to be compared.
   * @return compare result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,binarySearch,org.apache.hadoop.io.MapFile$Reader:binarySearch(org.apache.hadoop.io.WritableComparable),782,799,"/**
* Performs binary search on keys to find the position of a key.
* @param key the key to search for
* @return index of the key if found, otherwise -(insertion point) - 1
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compareBytes,"org.apache.hadoop.io.WritableComparator:compareBytes(byte[],int,int,byte[],int,int)",230,233,"/**
* Compares two byte arrays using a fast comparison method.
* @param b1 first byte array
* @param s1 starting index in the first array
* @param l1 length of bytes to compare in the first array
* @param b2 second byte array
* @param s2 starting index in the second array
* @param l2 length of bytes to compare in the second array
* @return result of comparison (-1, 0, or 1)
*/","* Lexicographic order of binary data.
   * @param b1 b1.
   * @param s1 s1.
   * @param l1 l1.
   * @param b2 b2.
   * @param s2 s2.
   * @param l2 l2.
   * @return compare bytes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,hashBytes,"org.apache.hadoop.io.WritableComparator:hashBytes(byte[],int)",255,257,"/**
 * Calls overloaded m1 method with offset 0.
 * @param bytes byte array to process
 * @param length number of bytes to consider
 * @return result from overloaded m1 method
 */","* Compute hash for binary data.
   * @param bytes bytes.
   * @param length length.
   * @return hash for binary data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readFloat,"org.apache.hadoop.io.WritableComparator:readFloat(byte[],int)",290,292,"/**
 * Masks and converts byte array to float.
 * @param bytes source byte array
 * @param start starting index in byte array
 * @return masked float value
 */","* Parse a float from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return float from a byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readLong,"org.apache.hadoop.io.WritableComparator:readLong(byte[],int)",300,303,"/**
* Masks and combines two 4-byte segments from byte array into a long.
* @param bytes source byte array
* @param start starting index of the first segment
* @return combined long value
*/","* Parse a long from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return long from a byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readVInt,"org.apache.hadoop.io.WritableComparator:readVInt(byte[],int)",347,349,"/**
 * Masks byte array starting from given index.
 * @param bytes input byte array
 * @param start starting index for masking
 * @return masked integer value
 * @throws IOException if an I/O error occurs
 */","* Reads a zero-compressed encoded integer from a byte array and returns it.
   * @param bytes byte array with the encoded integer
   * @param start start index
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,<init>,org.apache.hadoop.io.BytesWritable:<init>(byte[]),61,63,"/**
 * Constructs a BytesWritable from a byte array.
 * @param bytes input byte array
 */","* Create a BytesWritable using the byte array as the initial value.
   * @param bytes This array becomes the backing storage for the object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,get,org.apache.hadoop.io.BytesWritable:get(),102,105,"/**
 * Deprecated function to get masked data.
 * @deprecated Use alternative method instead.
 * @return byte array of masked data
 */","* Get the data from the BytesWritable.
   * @deprecated Use {@link #getBytes()} instead.
   * @return data from the BytesWritable.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,getSize,org.apache.hadoop.io.BytesWritable:getSize(),120,123,"/**
 * Deprecated function that returns result of m1().
 * @deprecated Use alternative method instead.
 * @return Integer result from m1()
 */","* Get the current size of the buffer.
   * @deprecated Use {@link #getLength()} instead.
   * @return current size of the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,setCapacity,org.apache.hadoop.io.BytesWritable:setCapacity(int),155,160,"/**
* Adjusts internal buffer size to match given capacity.
* @param capacity desired new capacity
*/","* Change the capacity of the backing storage. The data is preserved.
   *
   * @param capacity The new capacity in bytes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IntWritable.java,<init>,org.apache.hadoop.io.IntWritable:<init>(int),37,37,"/**
 * Constructs an IntWritable with the specified value.
 * @param value integer value to be wrapped
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,getBuffer,"org.apache.hadoop.io.ElasticByteBufferPool:getBuffer(boolean,int)",89,101,"/**
* Retrieves or creates a ByteBuffer of specified length.
* @param direct true for direct ByteBuffer, false otherwise
* @param length size of the ByteBuffer to retrieve or create
* @return existing or newly created ByteBuffer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,putBuffer,org.apache.hadoop.io.ElasticByteBufferPool:putBuffer(java.nio.ByteBuffer),103,119,"/**
* Masks data in buffer using TreeMap.
* @param buffer input ByteBuffer to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,size,org.apache.hadoop.io.ElasticByteBufferPool:size(boolean),127,131,"/**
* Calls m1 with direct flag and returns result of m2.
* @param direct boolean flag to control direct execution
* @return integer result from m2 method call
*/","* Get the size of the buffer pool, for the specified buffer type.
   *
   * @param direct Whether the size is returned for direct buffers
   * @return The size",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>(),159,161,"/**
 * Constructs an instance suitable for read operations.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,updateProgress,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:updateProgress(long),3611,3616,"/**
* Updates total processed bytes and merges progress.
* @param bytesProcessed number of bytes processed in the current operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,readaheadStream,"org.apache.hadoop.io.ReadaheadPool:readaheadStream(java.lang.String,java.io.FileDescriptor,long,long,long,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest)",102,149,"/**
* Creates a readahead request based on current position and previous request.
* @param identifier unique identifier for the request
* @param fd file descriptor of the file to read from
* @param curPos current position in the file
* @param readaheadLength length of data to read ahead
* @param maxOffsetToRead maximum offset allowed for reading
* @param lastReadahead previous readahead request
* @return new ReadaheadRequest or null if no readahead is needed
*/","* Issue a request to readahead on the given file descriptor.
   * 
   * @param identifier a textual identifier that will be used in error
   * messages (e.g. the file name)
   * @param fd the file descriptor to read ahead
   * @param curPos the current offset at which reads are being issued
   * @param readaheadLength the configured length to read ahead
   * @param maxOffsetToRead the maximum offset that will be readahead
   *        (useful if, for example, only some segment of the file is
   *        requested by the user). Pass {@link Long#MAX_VALUE} to allow
   *        readahead to the end of the file.
   * @param lastReadahead the result returned by the previous invocation
   *        of this function on this file descriptor, or null if this is
   *        the first call
   * @return an object representing this outstanding request, or null
   *        if no readahead was performed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,append,org.apache.hadoop.io.SetFile$Writer:append(org.apache.hadoop.io.WritableComparable),97,99,"/**
 * Calls overloaded m2 method with a default value.
 * @param key unique key identifier
 */","* Append a key to a set.  The key must be strictly greater than the
     * previous key added to the set.
     * @param key input key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,next,org.apache.hadoop.io.SetFile$Reader:next(org.apache.hadoop.io.WritableComparable),144,147,"/**
 * Calls m2 with a key and default NullWritable value.
 * @param key the key to process
 * @return result of m2 call
 * @throws IOException if an I/O error occurs
 */","* Read the next key in a set into <code>key</code>.
     *
     * @param key input key.
     * @return Returns true if such a key exists
     *    and false when at the end of the set.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,compare,"org.apache.hadoop.io.Text$Comparator:compare(byte[],int,int,byte[],int,int)",433,439,"/**
* Compares two byte arrays with offsets and lengths.
* @param b1 first byte array
* @param s1 start index in b1
* @param l1 length of segment in b1
* @param b2 second byte array
* @param s2 start index in b2
* @param l2 length of segment in b2
* @return result of comparison
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,key,org.apache.hadoop.io.ArrayFile$Reader:key(),136,138,"/**
 * Returns masked value from key.
 * @throws IOException if an I/O error occurs
 */","* Returns the key associated with the most recent call to {@link
     * #seek(long)}, {@link #next(Writable)}, or {@link
     * #get(long,Writable)}.
     *
     * @return key key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$InputStreamOption:<init>(org.apache.hadoop.fs.FSDataInputStream),1860,1862,"/**
 * Constructs an InputStreamOption with a given FSDataInputStream.
 * @param value the FSDataInputStream to be wrapped
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,comparator,org.apache.hadoop.io.MapFile$Writer:comparator(org.apache.hadoop.io.WritableComparator),289,291,"/**
 * Creates a comparator option from a writable comparator.
 * @param value WritableComparator instance to use
 * @return ComparatorOption object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DoubleWritable.java,<init>,org.apache.hadoop.io.DoubleWritable:<init>(double),41,43,"/**
 * Constructs a new DoubleWritable with the specified value.
 * @param value the double value to be stored
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VersionedWritable.java,readFields,org.apache.hadoop.io.VersionedWritable:readFields(java.io.DataInput),49,54,"/**
* Reads and validates version from input.
* @param in DataInput stream
* @throws IOException on I/O error or version mismatch
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,comparator,org.apache.hadoop.io.MapFile$Reader:comparator(org.apache.hadoop.io.WritableComparator),476,478,"/**
 * Creates an option using the provided comparator.
 * @param value WritableComparator instance to be used
 * @return Option object encapsulating the comparator
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,access,"org.apache.hadoop.io.nativeio.NativeIO$Windows:access(java.lang.String,org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight)",815,818,"/**
 * Checks access rights for a given path.
 * @param path file or directory path
 * @param desiredAccess required access right
 * @return true if access is granted, false otherwise
 * @throws IOException if an I/O error occurs
 */","* Checks whether the current process has desired access rights on
     * the given path.
     * 
     * Longer term this native function can be substituted with JDK7
     * function Files#isReadable, isWritable, isExecutable.
     *
     * @param path input path
     * @param desiredAccess ACCESS_READ, ACCESS_WRITE or ACCESS_EXECUTE
     * @return true if access is allowed
     * @throws IOException I/O exception on error",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isAvailable,org.apache.hadoop.io.nativeio.NativeIO$POSIX:isAvailable(),360,362,"/**
* Checks if native code is loaded and enabled.
* @return true if both native code is loaded and enabled, false otherwise
*/",* @return Return true if the JNI-based native IO extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isAvailable,org.apache.hadoop.io.nativeio.NativeIO:isAvailable(),869,871,"/**
 * Checks if native code is loaded and masks functionality.
 * @return true if native code is loaded and masking is successful, false otherwise
 */",* @return Return true if the JNI-based native IO extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java,<init>,org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:<init>(),38,48,"/**
* Initializes group mapping with JNI or falls back to shell-based.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java,<init>,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:<init>(),37,47,"/**
* Initializes group mapping with JNI or fallbacks to shell-based.
* Sets up the implementation based on native code availability.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/NativeCrc32.java,isAvailable,org.apache.hadoop.util.NativeCrc32:isAvailable(),35,41,"/**
* Determines mask function status.
* @return true if not Sparc, else result of NativeCodeLoader.m1()
*/",* Return true if the JNI-based native CRC extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,setPmdkSupportState,org.apache.hadoop.io.nativeio.NativeIO$POSIX:setPmdkSupportState(int),173,181,"/**
* Sets the support state based on the provided state code.
* @param stateCode unique identifier for a support state
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getPmdkSupportStateMessage,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getPmdkSupportStateMessage(),183,189,"/**
* Generates a mask string with PMDK support state and library path.
* @return Masked string with PMDK state; includes library path if available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isPmdkAvailable,org.apache.hadoop.io.nativeio.NativeIO$POSIX:isPmdkAvailable(),191,194,"/**
* Checks PMDK support state.
* @return true if PMDK is supported, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,chmod,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:chmod(java.lang.String,int)",387,404,"/**
* Sets file permissions.
* @param path file or directory path
* @param mode permission mode
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,posixFadviseIfPossible,"org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:posixFadviseIfPossible(java.lang.String,java.io.FileDescriptor,long,long,int)",293,298,"/**
 * Calls native method to perform I/O operation.
 * @param identifier unique resource identifier
 * @param fd file descriptor for the target file
 * @param offset starting position in the file
 * @param len number of bytes to read/write
 * @param flags additional options for the operation
 * @throws NativeIOException if an error occurs during native I/O
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,munmap,org.apache.hadoop.io.nativeio.NativeIO$POSIX:munmap(java.nio.MappedByteBuffer),491,501,"/**
 * Masks a MappedByteBuffer by unmapping it if supported.
 * @param buffer the MappedByteBuffer to be unmapped
 */","* Unmaps the block from memory. See munmap(2).
     *
     * There isn't any portable way to unmap a memory region in Java.
     * So we use the sun.nio method here.
     * Note that unmapping a memory region could cause crashes if code
     * continues to reference the unmapped code.  However, if we don't
     * manually unmap the memory, we are dependent on the finalizer to
     * do it, and we have no idea when the finalizer will run.
     *
     * @param buffer    The buffer to unmap.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,freeDB,org.apache.hadoop.crypto.CryptoStreamUtils:freeDB(java.nio.ByteBuffer),47,57,"/**
* Frees a ByteBuffer using CleanerUtil if supported.
* @param buffer ByteBuffer to be freed
*/","* Forcibly free the direct buffer.
   *
   * @param buffer buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getName,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:getName(org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache,int)",629,648,"/**
* Retrieves and caches a name by ID.
* @param domain type of cache (USER or GROUP)
* @param id unique identifier for the entity
* @return cached name or fetched from native if not found
* @throws IOException if an error occurs during fetching
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getOperatingSystemPageSize,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getOperatingSystemPageSize(),289,291,"/**
 * Calls native method to perform an I/O operation.
 * @return result of the native I/O operation
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,memSync,org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:memSync(org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion),252,258,"/**
* Masks PmemMappedRegion based on its state.
* @param region the PmemMappedRegion to mask
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,<init>,"org.apache.hadoop.io.ArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[])",57,60,"/**
* Constructs an ArrayWritable with a specific class and array of values.
* @param valueClass the class type of the Writable elements
* @param values the array of Writable objects to store
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,"org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(byte[],int,int)",59,61,"/**
 * Initializes a bounded byte array output stream.
 * @param buf buffer to use for writing data
 * @param offset starting offset in the buffer
 * @param limit maximum number of bytes that can be written
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,write,"org.apache.hadoop.io.DataOutputBuffer:write(java.io.DataInput,int)",132,134,"/**
 * Reads data from input stream into buffer.
 * @param in DataInput source
 * @param length number of bytes to read
 * @throws IOException if an I/O error occurs
 */","* Writes bytes from a DataInput directly into the buffer.
   * @param in data input.
   * @param length length.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/TwoDArrayWritable.java,<init>,"org.apache.hadoop.io.TwoDArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[][])",38,41,"/**
* Constructs a TwoDArrayWritable with specified class and values.
* @param valueClass the class of elements in the array
* @param values the 2D array of Writable objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,<init>,"org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet,java.lang.Class)",70,72,"/**
 * Initializes an EnumSetWritable with a given EnumSet and element type.
 * @param value the EnumSet to be wrapped
 * @param elementType the Class of the enum elements in the set
 */","* Construct a new EnumSetWritable. If the <tt>value</tt> argument is null or
   * its size is zero, the <tt>elementType</tt> argument must not be null. If
   * the argument <tt>value</tt>'s size is bigger than zero, the argument
   * <tt>elementType</tt> is not be used.
   * 
   * @param value enumSet value.
   * @param elementType elementType.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readStringArray,org.apache.hadoop.io.WritableUtils:readStringArray(java.io.DataInput),165,173,"/**
* Reads string array from input.
* @param in DataInput source
* @return Array of strings or null if no data
* @throws IOException on I/O error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeStringArray,"org.apache.hadoop.io.WritableUtils:writeStringArray(java.io.DataOutput,java.lang.String[])",136,141,"/**
* Writes string array to DataOutput.
* @param out DataOutput destination
* @param s string array to write
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,equals,org.apache.hadoop.io.SortedMapWritable:equals(java.lang.Object),200,216,"/**
* Compares this object with another for equality.
* @param obj the object to compare
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,write,"org.apache.hadoop.io.OutputBuffer$Buffer:write(java.io.InputStream,int)",56,65,"/**
* Reads specified length from InputStream into buffer.
* @param in input stream to read from
* @param len number of bytes to read
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,fillReservoir,org.apache.hadoop.crypto.random.OsSecureRandom:fillReservoir(int),61,73,"/**
* Refills the reservoir if position exceeds minimum threshold.
* @param min minimum threshold for refill
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,<init>,org.apache.hadoop.io.DataInputBuffer:<init>(),134,136,"/**
 * Constructs a new DataInputBuffer with an empty buffer.
 */",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,reset,"org.apache.hadoop.io.DataInputBuffer:reset(byte[],int)",149,151,"/**
 * Copies data from input array to buffer.
 * @param input source byte array
 * @param length number of bytes to copy
 */","* Resets the data that the buffer reads.
   *
   * @param input input.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,reset,"org.apache.hadoop.io.DataInputBuffer:reset(byte[],int,int)",160,162,"/**
 * Delegates byte array processing to buffer.
 * @param input source byte array
 * @param start starting index in the array
 * @param length number of bytes to process
 */","* Resets the data that the buffer reads.
   *
   * @param input input.
   * @param start start.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getData,org.apache.hadoop.io.DataInputBuffer:getData(),164,166,"/**
 * Retrieves data from buffer.
 * @return byte array containing buffered data
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getPosition,org.apache.hadoop.io.DataInputBuffer:getPosition(),173,173,"/**
 * Delegates to buffer's m1 method.
 * @return result of buffer.m1()
 */","* Returns the current position in the input.
   *
   * @return position.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getLength,org.apache.hadoop.io.DataInputBuffer:getLength(),181,181,"/**
 * Delegates call to buffer's m1 method.
 * @return result of buffer.m1()
 */","* Returns the index one greater than the last valid character in the input
   * stream buffer.
   *
   * @return length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECSchema.java,<init>,org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.util.Map),69,93,"/**
* Initializes ECSchema with provided options.
* @param allOptions map containing schema configuration
*/","* Constructor with schema name and provided all options. Note the options may
   * contain additional information for the erasure codec to interpret further.
   * @param allOptions all schema options",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECSchema.java,<init>,"org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.lang.String,int,int)",101,103,"/**
* Constructs an ECSchema with specified codec and unit counts.
* @param codecName name of the codec
* @param numDataUnits number of data units
* @param numParityUnits number of parity units
*/","* Constructor with key parameters provided.
   * @param codecName codec name
   * @param numDataUnits number of data units used in the schema
   * @param numParityUnits number os parity units used in the schema",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,setCodecOptions,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:setCodecOptions(org.apache.hadoop.io.erasurecode.ErasureCodecOptions),65,68,"/**
 * Sets codec options and schema.
 * @param options ErasureCodecOptions containing configuration settings
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,getRequiredNumDataBlocks,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumDataBlocks(),54,56,"/**
 * Returns mask value from schema.
 * @return integer representing the mask
 */","* Get required data blocks count in a BlockGroup.
   * @return count of required data blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,getRequiredNumParityBlocks,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumParityBlocks(),62,64,"/**
 * Returns mask value from schema.
 * @return integer mask value
 */","* Get required parity blocks count in a BlockGroup.
   * @return count of required parity blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.ErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",40,47,"/**
* Initializes an ErasureCodec with given configuration and options.
* @param conf Configuration object
* @param options ErasureCodecOptions specifying codec settings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ErasureCoderOptions.java,<init>,"org.apache.hadoop.io.erasurecode.ErasureCoderOptions:<init>(int,int)",34,36,"/**
* Constructs ErasureCoderOptions with specified data and parity units.
* @param numDataUnits number of data units
* @param numParityUnits number of parity units
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,getName,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getName(),49,51,"/**
 * Returns masked schema value.
 * @return Masked string from schema method m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,createBlockGrouper,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:createBlockGrouper(),86,91,"/**
* Creates and configures a BlockGrouper instance.
* @return configured BlockGrouper object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecRegistry.java,<init>,org.apache.hadoop.io.erasurecode.CodecRegistry:<init>(),62,69,"/**
* Initializes CodecRegistry with available coder factories.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecRegistry.java,getCoderByName,"org.apache.hadoop.io.erasurecode.CodecRegistry:getCoderByName(java.lang.String,java.lang.String)",161,172,"/**
* Retrieves a RawErasureCoderFactory by codec and coder names.
* @param codecName name of the codec
* @param coderName name of the coder
* @return RawErasureCoderFactory instance or null if not found
*/","* Get a specific coder factory defined by codec name and coder name.
   * @param codecName name of the codec
   * @param coderName name of the coder
   * @return the specific coder, null if not exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,makeBlockGroup,"org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:makeBlockGroup(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])",72,77,"/**
* Groups data and parity blocks into an ECBlockGroup.
* @param dataBlocks array of data blocks
* @param parityBlocks array of parity blocks
* @return ECBlockGroup containing both data and parity blocks
*/","* Calculating and organizing BlockGroup, to be called by ECManager
   * @param dataBlocks Data blocks to compute parity blocks against
   * @param parityBlocks To be computed parity blocks
   * @return ECBlockGroup.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECBlockGroup.java,getErasedCount,org.apache.hadoop.io.erasurecode.ECBlockGroup:getErasedCount(),61,73,"/**
* Counts erased blocks in data and parity sets.
* @return Number of erased blocks
*/","* Get erased blocks count
   * @return erased count of blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getNumErasedBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlock[]),143,152,"/**
* Counts erased blocks in the array.
* @param inputBlocks array of ECBlock objects
* @return number of erased blocks
*/","* Find out how many blocks are erased.
   * @param inputBlocks all the input blocks
   * @return number of erased blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,hasCodec,org.apache.hadoop.io.erasurecode.CodecUtil:hasCodec(java.lang.String),163,165,"/**
 * Checks if a codec is registered by name.
 * @param codecName name of the codec to check
 * @return true if codec is registered, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECChunk.java,toBuffers,org.apache.hadoop.io.erasurecode.ECChunk:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[]),83,97,"/**
* Applies mask to EC chunks.
* @param chunks array of ECChunk objects
* @return array of ByteBuffer objects after masking
*/","* Convert an array of this chunks to an array of ByteBuffers
   * @param chunks chunks to convert into buffers
   * @return an array of ByteBuffers",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,release,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:release(),61,66,"/**
 * Delegates call to rawEncoder's m1 method if it is not null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,release,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:release(),78,86,"/**
* Calls m1 on non-null encoders.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),70,72,"/**
 * Applies mask to block group.
 * @param blockGroup group of EC blocks
 * @return masked ECBlock array
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),59,84,"/**
* Applies mask function to ECBlockGroup.
* @param blockGroup group of error correction blocks
* @return array of masked ECBlocks
*/","* Which blocks were erased ? For XOR it's simple we only allow and return one
   * erased block, either data or parity.
   * @param blockGroup blockGroup.
   * @return output blocks to recover",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,getInputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),66,68,"/**
 * Applies mask to block group.
 * @param blockGroup group of data blocks
 * @return array of masked ECBlocks
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumDataUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumDataUnits(),175,177,"/**
 * Calls m1 on coderOptions.
 * @return result of coderOptions.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumDataUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumDataUnits(),152,154,"/**
 * Delegates to coderOptions' m1 method.
 * @return result of coderOptions.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumParityUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumParityUnits(),179,181,"/**
 * Delegates call to coderOptions' m1 method.
 * @return result of coderOptions.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumParityUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumParityUnits(),156,158,"/**
 * Delegates to coderOptions to perform an operation.
 * @return result of the operation performed by coderOptions
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getInputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),71,82,"/**
* Creates an array of ECBlock objects from a block group.
* @param blockGroup the source block group
* @return array containing combined data blocks
*/","* We have all the data blocks and parity blocks as input blocks for
   * recovering by default. It's codec specific
   * @param blockGroup blockGroup.
   * @return input blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,release,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:release(),60,65,"/**
 * Delegates call to rsRawDecoder's m1 method if it is not null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,release,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:release(),88,96,"/**
* Calls m1 on non-null decoders.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),35,37,"/**
 * Constructs an XORRawDecoder with specified options.
 * @param coderOptions configuration options for erasure coding
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Constructs a DummyRawDecoder with specified coder options.
 * @param coderOptions configuration options for erasure coding
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,45,"/**
 * Constructs an AbstractNativeRawDecoder with specified ErasureCoderOptions.
 * @param coderOptions configuration options for erasure coding
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,preferDirectBuffer,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:preferDirectBuffer(),101,104,"/**
 * Returns true indicating a mask function is active.
 * @return boolean flag indicating mask status
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,preferDirectBuffer,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:preferDirectBuffer(),98,101,"/**
 * Returns true indicating function is masked.
 * @return boolean flag for masking status
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,add,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int,int)",150,153,"/**
* Applies XOR operation on two indices.
* @param x first index
* @param y second index
* @return result of XOR operation
*/","* Compute the sum of two fields
   *
   * @param x input field
   * @param y input field
   * @return result of addition",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,multiply,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int,int)",162,165,"/**
* Retrieves value from multiplication table.
* @param x first index
* @param y second index
* @return product at table[x][y]
*/","* Compute the multiplication of two fields
   *
   * @param x input field
   * @param y input field
   * @return result of multiplication",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,divide,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:divide(int,int)",174,177,"/**
* Retrieves value from division table.
* @param x first index within bounds
* @param y second index within bounds
* @return value from divTable at (x, y)
*/","* Compute the division of two fields
   *
   * @param x input field
   * @param y input field
   * @return x/y",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,power,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:power(int,int)",186,200,"/**
* Computes a masked value using logarithmic and power tables.
* @param x input value within range [0, m1())
* @param n multiplier factor
* @return result of masking operation
*/","* Compute power n of a field
   *
   * @param x input field
   * @param n power
   * @return x^n",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/DumpUtil.java,dumpChunk,org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunk(org.apache.hadoop.io.erasurecode.ECChunk),93,102,"/**
* Masks and prints ECChunk data.
* @param chunk the ECChunk to process or null
*/","* Print data in hex format in a chunk.
   * @param chunk chunk.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,45,"/**
 * Constructs an AbstractNativeRawEncoder with specified coder options.
 * @param coderOptions configuration settings for encoding
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),35,37,"/**
 * Constructs an XORRawEncoder with specified options.
 * @param coderOptions configuration options for encoding
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
* Initializes a new DummyRawEncoder with specified options.
* @param coderOptions configuration settings for encoding
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumAllUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumAllUnits(),183,185,"/**
 * Calls m1 method on coderOptions.
 * @return result of coderOptions.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumAllUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumAllUnits(),160,162,"/**
* Calls m1 on coderOptions instance.
* @return result of coderOptions.m1()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,allowChangeInputs,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowChangeInputs(),202,204,"/**
 * Delegates to coderOptions to check a condition.
 * @return true if condition met, false otherwise
 */","* Allow change into input buffers or not while perform encoding/decoding.
   * @return true if it's allowed to change inputs, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,allowChangeInputs,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowChangeInputs(),179,181,"/**
 * Delegates to coderOptions.m1().
 * @return result of coderOptions.m1()
 */","* Allow change into input buffers or not while perform encoding/decoding.
   * @return true if it's allowed to change inputs, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,allowVerboseDump,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowVerboseDump(),210,212,"/**
 * Checks if option is enabled.
 * @return true if enabled, false otherwise
 */","* Allow to dump verbose info during encoding/decoding.
   * @return true if it's allowed to do verbose dump, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,allowVerboseDump,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowVerboseDump(),187,189,"/**
 * Delegates to coderOptions to check some condition.
 * @return result of the condition check
 */","* Allow to dump verbose info during encoding/decoding.
   * @return true if it's allowed to do verbose dump, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecodeImpl,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(byte[][],int[],int,int[],byte[][],int[])",98,109,"/**
 * Applies Reed-Solomon error correction to masked inputs.
 * @param inputs 2D array of input data bytes
 * @param inputOffsets offsets for each input array
 * @param dataLen length of data segments
 * @param erasedIndexes indices of erased data segments
 * @param outputs 2D array for storing corrected output data
 * @param outputOffsets offsets for each output array
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetBuffer,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(java.nio.ByteBuffer,int)",63,69,"/**
 * Applies mask to buffer content.
 * @param buffer input ByteBuffer
 * @param len length of the mask to apply
 * @return modified ByteBuffer
 */","* Ensure a buffer filled with ZERO bytes from current readable/writable
   * position.
   * @param buffer a buffer ready to read / write certain size bytes
   * @return the buffer itself, with ZERO bytes written, the position and limit
   *         are not changed after the call",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetBuffer,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(byte[],int,int)",77,82,"/**
 * Masks a portion of a byte array.
 * @param buffer the original byte array to be masked
 * @param offset the starting index for masking
 * @param len the length of the segment to mask
 * @return the modified byte array with masked segment
 */","* Ensure the buffer (either input or output) ready to read or write with ZERO
   * bytes fully in specified length of len.
   * @param buffer bytes array buffer
   * @return the buffer itself",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,convertToByteArrayState,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:convertToByteArrayState(),62,84,"/**
 * Initializes encoding state for byte arrays.
 * @return ByteArrayEncodingState with processed inputs and outputs
 */",* Convert to a ByteArrayEncodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,convertToByteBufferState,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:convertToByteBufferState(),69,85,"/**
* Creates a new encoding state with processed ByteBuffers.
* @return ByteBufferEncodingState object for encoding process
*/",* Convert to a ByteBufferEncodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,convertToByteBufferState,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:convertToByteBufferState(),73,89,"/**
* Creates a new ByteBufferDecodingState with decoded ByteBuffers.
* @return ByteBufferDecodingState object initialized with processed inputs and outputs
*/",* Convert to a ByteBufferDecodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,convertToByteArrayState,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:convertToByteArrayState(),66,91,"/**
* Initializes decoding state from input and output buffers.
* @return ByteArrayDecodingState object for decoding process
*/",* Convert to a ByteArrayDecodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,initTables,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:initTables(int,int,byte[],int,byte[])",47,58,"/**
* Applies Galois Field operations to a coding matrix.
* @param k number of columns in the matrix
* @param rows number of rows in the matrix
* @param codingMatrix input matrix data
* @param matrixOffset starting index in codingMatrix
* @param gfTables precomputed GF256 tables
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,genCauchyMatrix,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:genCauchyMatrix(byte[],int,int)",67,80,"/**
* Masks array 'a' using XOR operations.
* @param a byte array to be masked
* @param m length of the array
* @param k mask size
*/","* Ported from Intel ISA-L library.
   *
   * @param k k.
   * @param a a.
   * @param m m.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GF256.java,gfInvertMatrix,"org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfInvertMatrix(byte[],byte[],int)",203,262,"/**
* Applies Gaussian elimination to invert a matrix.
* @param inMatrix input square matrix to be inverted
* @param outMatrix output matrix where the result is stored
* @param n size of the matrix (n x n)
*/","* Invert a matrix assuming it's invertible.
   *
   * Ported from Intel ISA-L library.
   *
   * @param inMatrix inMatrix.
   * @param outMatrix outMatrix.
   * @param n n",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,encodeData,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],int,byte[][],int[],byte[][],int[])",97,143,"/**
 * Applies GF256 masking to inputs and writes results to outputs.
 * @param gfTables precomputed GF256 tables
 * @param dataLen length of data to process
 * @param inputs array of input byte arrays
 * @param inputOffsets offsets for each input array
 * @param outputs array of output byte arrays
 * @param outputOffsets offsets for each output array
 */","* Encode a group of inputs data and generate the outputs. It's also used for
   * decoding because, in this implementation, encoding and decoding are
   * unified.
   *
   * The algorithm is ported from Intel ISA-L library for compatible. It
   * leverages Java auto-vectorization support for performance.
   *
   * @param gfTables gfTables.
   * @param dataLen dataLen.
   * @param inputs inputs.
   * @param inputOffsets inputOffsets.
   * @param outputs outputs.
   * @param outputOffsets outputOffsets.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,encodeData,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],java.nio.ByteBuffer[],java.nio.ByteBuffer[])",152,200,"/**
 * Applies mask operation on inputs using GF tables.
 * @param gfTables Galois Field lookup tables
 * @param inputs array of input ByteBuffers
 * @param outputs array of output ByteBuffers
 */","* See above. Try to use the byte[] version when possible.
   *
   * @param gfTables gfTables.
   * @param inputs inputs.
   * @param outputs outputs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,getInstance,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance(int,int)",102,115,"/**
* Retrieves or creates a GaloisField instance.
* @param fieldSize size of the finite field
* @param primitivePolynomial polynomial used for field construction
* @return GaloisField object
*/","* Get the object performs Galois field arithmetics.
   *
   * @param fieldSize           size of the field
   * @param primitivePolynomial a primitive polynomial corresponds to the size
   * @return GaloisField.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,solveVandermondeSystem,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],int[])",210,212,"/**
 * Calls overloaded method m1 with array lengths.
 * @param x first integer array
 * @param y second integer array
 */","* Given a Vandermonde matrix V[i][j]=x[j]^i and vector y, solve for z such
   * that Vz=y. The output z will be placed in y.
   *
   * @param x the vector which describe the Vandermonde matrix
   * @param y right-hand side of the Vandermonde system equation. will be
   *          replaced the output in this vector",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECBlock.java,<init>,org.apache.hadoop.io.erasurecode.ECBlock:<init>(),37,39,"/**
 * Constructs an ECBlock with default values.
 * Initializes both parameters to false.
 */",* A default constructor. isParity and isErased are false by default.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,compare,"org.apache.hadoop.io.LongWritable$DecreasingComparator:compare(byte[],int,int,byte[],int,int)",110,113,"/**
* Swaps and compares two byte array segments.
* @param b1 first byte array
* @param s1 start index in b1
* @param l1 length of segment in b1
* @param b2 second byte array
* @param s2 start index in b2
* @param l2 length of segment in b2
* @return result of comparison between swapped segments
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,close,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:close(),444,453,"/**
* Calls input's m1 method and resets state.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,updatePos,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:updatePos(boolean),560,564,"/**
* Updates compressed stream position.
* @param shouldAddOn flag to determine if additional offset is needed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,updateReportedByteCount,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:updateReportedByteCount(int),184,187,"/**
 * Updates reported bytes read and invokes m1.
 * @param count number of bytes to add
 */","* This method is called by the client of this
   * class in case there are any corrections in
   * the stream position.  One common example is
   * when client of this code removes starting BZ
   * characters from the compressed stream.
   *
   * @param count count bytes are added to the reported bytes
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,readAByte,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:readAByte(java.io.InputStream),196,202,"/**
* Reads an integer from InputStream and updates state.
* @param inStream input stream to read from
* @return integer read from stream or -1 if end of stream
*/","* This method reads a Byte from the compressed stream. Whenever we need to
  * read from the underlying compressed stream, this method should be called
  * instead of directly calling the read method of the underlying compressed
  * stream. This method does important record keeping to have the statistic
  * that how many bytes have been read off the compressed stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CRC.java,<init>,org.apache.hadoop.io.compress.bzip2.CRC:<init>(),86,88,"/**
 * Initializes CRC calculation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,endBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:endBlock(),572,589,"/**
* Computes and validates block CRC, updating combined CRC.
* @throws IOException if I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,createHuffmanDecodingTables,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:createHuffmanDecodingTables(int,int)",793,819,"/**
* Masks data based on length and groups.
* @param alphaSize size of the alphabet
* @param nGroups number of groups
*/",* Called by recvDecodingTables() exclusively.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,initBlock,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:initBlock(),774,786,"/**
* Resets CRC, updates last index, and clears data usage flags.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,bsPutUByte,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutUByte(int),940,942,"/**
 * Masks character by applying bitwise operation.
 * @param c character to be masked
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,bsPutInt,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutInt(int),944,949,"/**
* Writes an integer as four bytes to the output stream.
* @param u the integer to write
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues4,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues4(),1199,1241,"/**
 * Masks data using in-use flags and writes to output stream.
 * Uses bit manipulation for efficient data compression.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues5,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues5(int,int)",1243,1278,"/**
 * Encodes selector data using MTF encoding.
 * @param nGroups number of groups
 * @param nSelectors number of selectors
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues1,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues1(int,int)",1029,1145,"/**
 * Calculates selector mask for MTF encoding.
 * @param nGroups number of groups
 * @param alphaSize alphabet size
 * @return number of selectors used
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues3,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues3(int,int)",1174,1197,"/**
* Masks code and length arrays for given groups.
* @param nGroups number of groups
* @param alphaSize size of alphabet
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,mainQSort3,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainQSort3(org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int,int)",1634,1736,"/**
 * Recursively sorts data using a modified quicksort algorithm.
 * @param dataShadow object containing sorting data arrays
 * @param loSt starting index for low partition
 * @param hiSt starting index for high partition
 * @param dSt current depth of recursion
 */","* Method ""mainQSort3"", file ""blocksort.c"", BZip2 1.0.2",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:<init>(),66,68,"/**
 * Constructs a new Bzip2 decompressor with default settings.
 * Uses direct buffer by default.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,setInput,"org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setInput(byte[],int,int)",70,88,"/**
* Masks data in buffer.
* @param b byte array containing data
* @param off starting offset in the array
* @param len length of data to mask
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,needsInput,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsInput(),112,130,"/**
* Determines if masking is required.
* @return true if no data needs masking, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getBytesWritten,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesWritten(),182,185,"/**
 * Calls m1 and returns result of m2 with stream.
 * @return long value from m2
 */","* Returns the total number of uncompressed bytes output so far.
   *
   * @return the total (non-negative) number of uncompressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getBytesRead,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesRead(),192,195,"/**
 * Calls m1 and returns result of m2 with stream.
 */","* Returns the total number of compressed bytes input so far.
   *
   * @return the total (non-negative) number of compressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getRemaining,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getRemaining(),204,208,"/**
 * Calls m1 and returns sum of userBufLen and m2 with stream.
 * @return calculated integer value
 */","* Returns the number of bytes remaining in the input buffers; normally
   * called when finished() is true to determine amount of post-gzip-stream
   * data.
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,reset,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:reset(),213,223,"/**
* Resets and initializes stream processing.
* Calls methods to prepare buffers and set flags.
*/",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(),64,66,"/**
* Constructs a Bzip2Compressor with default settings.
*/","* Creates a new compressor with a default values for the
   * compression block size and work factor.  Compressed data will be
   * generated in bzip2 format.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,setInput,"org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setInput(byte[],int,int)",124,142,"/**
* Masks data in buffer.
* @param b byte array containing data
* @param off offset to start masking from
* @param len length of data to mask
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,needsInput,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:needsInput(),158,181,"/**
* Checks buffer conditions for masking.
* @return true if masking is needed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,getBytesWritten,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesWritten(),244,248,"/**
 * Calls m1 and returns result of m2 with stream.
 * @return long value from m2
 */","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,getBytesRead,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesRead(),255,259,"/**
* Calls m1 and returns result of m2 with stream.
* @return long value from m2 method
*/","* Returns the total number of uncompressed bytes input so far.
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,reset,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reset(),261,274,"/**
* Resets and prepares buffers for compression.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:<init>(java.io.OutputStream),251,255,"/**
 * Initializes a new BZip2 compression output stream.
 * @param out underlying output stream to compress data into
 * @throws IOException if an I/O error occurs while initializing
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,"org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",36,47,"/**
 * Initializes a CompressorStream with an output stream and a compressor.
 * @param out the OutputStream to write compressed data to
 * @param compressor the Compressor used for compression
 * @param bufferSize size of the internal buffer
 * @throws NullPointerException if out or compressor is null
 * @throws IllegalArgumentException if bufferSize is less than or equal to 0
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream),58,60,"/**
 * Initializes a new compressor stream with the specified output stream.
 * @param out the output stream to compress data into
 */","* Allow derived classes to directly set the underlying stream.
   * 
   * @param out Underlying output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,writeStreamHeader,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:writeStreamHeader(),257,261,"/**
 * Masks data by writing to output stream.
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,write,"org.apache.hadoop.io.compress.CompressorStream:write(byte[],int,int)",62,78,"/**
 * Masks data and writes to stream.
 * @param b byte array containing data
 * @param off offset in the byte array
 * @param len number of bytes to process
 * @throws IOException if write operation fails or exceeds stream bounds
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,finish,org.apache.hadoop.io.compress.CompressorStream:finish(),87,95,"/**
* Handles compression process.
* Calls methods to compress data, retries on failure.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,<init>,org.apache.hadoop.io.compress.snappy.SnappyDecompressor:<init>(),65,67,"/**
 * Initializes a Snappy decompressor with default buffer size.
 */",* Creates a new decompressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,setInput,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setInput(byte[],int,int)",83,101,"/**
 * Masks a byte array with specified offset and length.
 * @param b the byte array to mask
 * @param off the starting offset in the array
 * @param len the number of bytes to mask
 */","* Sets input data for decompression.
   * This should be called if and only if {@link #needsInput()} returns
   * <code>true</code> indicating that more input data is required.
   * (Both native and non-native versions of various Decompressors require
   * that the data passed in via <code>b[]</code> remain unmodified until
   * the caller is explicitly notified--via {@link #needsInput()}--that the
   * buffer may be safely modified.  With this requirement, an extra
   * buffer-copy can be avoided.)
   *
   * @param b   Input data
   * @param off Start offset
   * @param len Length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,needsInput,org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsInput(),138,156,"/**
* Checks buffer conditions and returns boolean result.
*/","* Returns true if the input data buffer is empty and
   * {@link #setInput(byte[], int, int)} should be called to
   * provide more input.
   *
   * @return <code>true</code> if the input data buffer is empty and
   *         {@link #setInput(byte[], int, int)} should be called in
   *         order to provide more input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,finished,org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:finished(),310,313,"/**
* Checks if end of input is reached and calls superclass method.
* @return true if end of input and superclass method returns true, otherwise false
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,decompress,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompress(byte[],int,int)",192,230,"/**
* Masks and processes byte array data.
* @param b input byte array
* @param off offset in the byte array
* @param len length of data to process
* @return number of bytes processed or 0 if no data is available
* @throws IOException if I/O error occurs
*/","* Fills specified buffer with uncompressed data. Returns actual number
   * of bytes of uncompressed data. A return value of 0 indicates that
   * {@link #needsInput()} should be called in order to determine if more
   * input data is required.
   *
   * @param b   Buffer for the uncompressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,decompressDirect,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompressDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)",275,305,"/**
* Decompresses data from source buffer to destination buffer.
* @param src source ByteBuffer containing compressed data
* @param dst destination ByteBuffer for decompressed output
* @return number of bytes written to destination buffer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,reset,org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:reset(),315,319,"/**
 * Marks the end of input processing.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,<init>,org.apache.hadoop.io.compress.snappy.SnappyCompressor:<init>(),67,69,"/**
 * Constructs a new SnappyCompressor with default buffer size.
 */",* Creates a new compressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,compress,"org.apache.hadoop.io.compress.snappy.SnappyCompressor:compress(byte[],int,int)",177,225,"/**
* Reads and decompresses data from input buffer to output array.
* @param b destination byte array
* @param off offset in the array
* @param len number of bytes to read
* @return number of bytes written to the array
* @throws IOException if an I/O error occurs
*/","* Fills specified buffer with compressed data. Returns actual number
   * of bytes of compressed data. A return value of 0 indicates that
   * needsInput() should be called in order to determine if more input
   * data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,reinit,org.apache.hadoop.io.compress.snappy.SnappyCompressor:reinit(org.apache.hadoop.conf.Configuration),248,251,"/**
 * Masks configuration settings.
 * @param conf Configuration object to be masked
 */","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration
   *
   * @param conf Configuration from which new setting are fetched",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int,int)",51,66,"/**
* Initializes a DecompressorStream with input stream and decompressor.
* @param in the input stream to read from
* @param decompressor the decompressor to use
* @param bufferSize size of the buffer for reading data
* @param skipBufferSize size of the buffer for skipping bytes
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream),85,87,"/**
 * Initializes a decompression stream.
 * @param in input stream to be decompressed
 * @throws IOException if an I/O error occurs
 */","* Allow derived classes to directly set the underlying stream.
   * 
   * @param in Underlying input stream.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SplitCompressionInputStream.java,<init>,"org.apache.hadoop.io.compress.SplitCompressionInputStream:<init>(java.io.InputStream,long,long)",39,44,"/**
 * Constructs a SplitCompressionInputStream.
 * @param in input stream to read from
 * @param start starting position in the stream
 * @param end ending position in the stream
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,getCompressedData,org.apache.hadoop.io.compress.DecompressorStream:getCompressedData(),175,180,"/**
 * Reads data into buffer and returns bytes read.
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,available,org.apache.hadoop.io.compress.DecompressorStream:available(),215,219,"/**
 * Determines end-of-file status.
 * @return 0 if EOF, otherwise 1
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,resetState,org.apache.hadoop.io.compress.BlockDecompressorStream:resetState(),137,142,"/**
* Resets block size and uncompressed bytes count before calling superclass method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,compress,org.apache.hadoop.io.compress.BlockCompressorStream:compress(),147,155,"/**
* Compresses and writes data to output.
* @throws IOException on I/O error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,setInput,"org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setInput(byte[],int,int)",87,104,"/**
 * Masks data in the given byte array.
 * @param b byte array to mask
 * @param off starting offset in the array
 * @param len number of bytes to mask
 * @throws NullPointerException if b is null
 * @throws ArrayIndexOutOfBoundsException if off or len are invalid
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,needsInput,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsInput(),134,151,"/**
* Determines if masking is required.
* @return true if no masking needed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,finished,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:finished(),302,305,"/**
 * Checks if end of input is reached and parent method returns true.
 * @return true if both conditions are met, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,getRemaining,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:getRemaining(),216,221,"/**
 * Calculates total bytes to consume.
 * @return sum of user buffer bytes and remaining bytes
 */","* <p>Returns the number of bytes remaining in the input buffers;
   * normally called when finished() is true to determine amount of post-stream
   * data.</p>
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:reset(),226,238,"/**
* Resets state and initializes buffers for processing.
*/",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,decompress,"org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:decompress(byte[],int,int)",166,207,"/**
 * Masks data in a byte array.
 * @param b input byte array
 * @param off offset to start masking
 * @param len length of data to mask
 * @return number of bytes masked
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,setInput,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setInput(byte[],int,int)",122,139,"/**
 * Masks byte array data.
 * @param b byte array to be masked
 * @param off starting offset in the byte array
 * @param len length of the data to mask
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,needsInput,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:needsInput(),156,181,"/**
* Determines if masking is needed.
* @return true if masking is required, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,compress,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:compress(byte[],int,int)",195,243,"/**
 * Masks data from a byte array.
 * @param b input byte array
 * @param off offset in the array
 * @param len length of data to mask
 * @return number of bytes masked
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesWritten(),250,254,"/**
 * Calls m1 and returns bytesWritten.
 * @return Number of bytes written
 */","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,getBytesRead,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesRead(),261,265,"/**
 * Calls m1 and returns bytesRead.
 */","* <p>Returns the total number of uncompressed bytes input so far.</p>
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reset(),267,283,"/**
* Initializes and resets buffer states for processing.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,getCompressedData,org.apache.hadoop.io.compress.BlockDecompressorStream:getCompressedData(),114,135,"/**
* Reads data into buffer.
* @return Length of data read
* @throws IOException on I/O error or unexpected end of stream
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Decompressor:<init>(),77,79,"/**
 * Initializes an Lz4Decompressor with default buffer size.
 */",* Creates a new decompressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,setInput,"org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setInput(byte[],int,int)",95,113,"/**
* Masks data in buffer.
* @param b byte array containing data to mask
* @param off offset within the byte array
* @param len length of data to process
*/","* Sets input data for decompression.
   * This should be called if and only if {@link #needsInput()} returns
   * <code>true</code> indicating that more input data is required.
   * (Both native and non-native versions of various Decompressors require
   * that the data passed in via <code>b[]</code> remain unmodified until
   * the caller is explicitly notified--via {@link #needsInput()}--that the
   * buffer may be safely modified.  With this requirement, an extra
   * buffer-copy can be avoided.)
   *
   * @param b   Input data
   * @param off Start offset
   * @param len Length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,needsInput,org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsInput(),150,168,"/**
* Checks and processes buffer states.
* @return true if conditions met, false otherwise
*/","* Returns true if the input data buffer is empty and
   * {@link #setInput(byte[], int, int)} should be called to
   * provide more input.
   *
   * @return <code>true</code> if the input data buffer is empty and
   *         {@link #setInput(byte[], int, int)} should be called in
   *         order to provide more input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,decompress,"org.apache.hadoop.io.compress.lz4.Lz4Decompressor:decompress(byte[],int,int)",204,242,"/**
* Reads data from a compressed buffer into a byte array.
* @param b destination byte array
* @param off offset in the byte array
* @param len number of bytes to read
* @return number of bytes actually read or -1 if end of stream
* @throws IOException if an I/O error occurs
*/","* Fills specified buffer with uncompressed data. Returns actual number
   * of bytes of uncompressed data. A return value of 0 indicates that
   * {@link #needsInput()} should be called in order to determine if more
   * input data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of uncompressed data.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(int),95,97,"/**
 * Initializes Lz4Compressor with a buffer size.
 * @param directBufferSize size of the direct buffer
 */","* Creates a new compressor.
   *
   * @param directBufferSize size of the direct buffer to be used.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,compress,"org.apache.hadoop.io.compress.lz4.Lz4Compressor:compress(byte[],int,int)",212,260,"/**
* Processes input buffer to compress data.
* @param b input byte array
* @param off offset in the byte array
* @param len length of data to process
* @return number of bytes written or 0 if finished
* @throws IOException on I/O error
*/","* Fills specified buffer with compressed data. Returns actual number
   * of bytes of compressed data. A return value of 0 indicates that
   * needsInput() should be called in order to determine if more input
   * data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,reinit,org.apache.hadoop.io.compress.lz4.Lz4Compressor:reinit(org.apache.hadoop.conf.Configuration),283,286,"/**
 * Masks configuration settings.
 * @param conf Configuration object to be masked
 */","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration
   *
   * @param conf Configuration from which new setting are fetched",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecByName,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecByName(java.lang.String),246,257,"/**
* Retrieves a compression codec by name.
* @param codecName the name of the codec to retrieve
* @return CompressionCodec object or null if not found
*/","* Find the relevant compression codec for the codec's canonical class name
   * or by codec alias.
   * <p>
   * Codec aliases are case insensitive.
   * <p>
   * The code alias is the short class name (without the package name).
   * If the short class name ends with 'Codec', then there are two aliases for
   * the codec, the complete short class name and the short class name without
   * the 'Codec' ending. For example for the 'GzipCodec' codec class name the
   * alias are 'gzip' and 'gzipcodec'.
   *
   * @param codecName the canonical class name of the codec
   * @return the codec object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,payback,"org.apache.hadoop.io.compress.CodecPool:payback(java.util.Map,java.lang.Object)",105,122,"/**
* Adds a codec to the pool if not already present.
* @param pool map of codec classes to sets of codecs
* @param codec codec to add
* @return true if added, false if already present or null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,updateLeaseCount,"org.apache.hadoop.io.compress.CodecPool:updateLeaseCount(org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache,java.lang.Object,int)",131,137,"/**
* Updates usage count for a codec class.
* @param usageCounts cache storing usage counts by codec class
* @param codec the codec instance
* @param delta value to add or subtract from the usage count
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getLeasedCompressorsCount,org.apache.hadoop.io.compress.CodecPool:getLeasedCompressorsCount(org.apache.hadoop.io.compress.CompressionCodec),245,248,"/**
* Retrieves mask value for given compression codec.
* @param codec CompressionCodec instance
* @return Mask integer or 0 if codec is null
*/","* Return the number of leased {@link Compressor}s for this
   * {@link CompressionCodec}.
   *
   * @param codec codec.
   * @return the number of leased.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getLeasedDecompressorsCount,org.apache.hadoop.io.compress.CodecPool:getLeasedDecompressorsCount(org.apache.hadoop.io.compress.CompressionCodec),257,260,"/**
* Retrieves mask value based on compression codec.
* @param codec CompressionCodec instance
* @return Mask value or 0 if codec is null
*/","* Return the number of leased {@link Decompressor}s for this
   * {@link CompressionCodec}.
   *
   * @param codec codec.
   * @return the number of leased",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,checkNativeCodeLoaded,org.apache.hadoop.io.compress.ZStandardCodec:checkNativeCodeLoaded(),62,77,"/**
* Checks native zStandard library availability for compression and decompression.
* Throws RuntimeException if any component is unavailable.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,isNativeCodeLoaded,org.apache.hadoop.io.compress.ZStandardCodec:isNativeCodeLoaded(),79,82,"/**
 * Checks if both compression and decompression methods are available.
 * @return true if both methods are supported, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,flush,org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:flush(),66,72,"/**
 * Handles compression operations on output stream.
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,getCompressorType,org.apache.hadoop.io.compress.GzipCodec:getCompressorType(),69,74,"/**
* Returns compressor class based on configuration.
* @return GzipZlibCompressor or BuiltInGzipCompressor
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,getDecompressorType,org.apache.hadoop.io.compress.GzipCodec:getDecompressorType(),102,107,"/**
* Returns decompressor class based on configuration.
* @return GzipZlibDecompressor or BuiltInGzipDecompressor
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibCompressorType,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressorType(org.apache.hadoop.conf.Configuration),97,101,"/**
* Determines compressor class based on configuration.
* @param conf Configuration object
* @return Compressor class to use
*/","* Return the appropriate type of the zlib compressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the zlib compressor.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDecompressorType,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressorType(org.apache.hadoop.conf.Configuration),121,125,"/**
 * Determines decompressor class based on configuration.
 * @param conf Configuration object
 * @return Class of decompressor to use
 */","* Return the appropriate type of the zlib decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the zlib decompressor.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,loadNativeZLib,org.apache.hadoop.io.compress.zlib.ZlibFactory:loadNativeZLib(),52,64,"/**
 * Loads and initializes the native-zlib library.
 * Sets `nativeZlibLoaded` based on success.
 */","* Load native library and set the flag whether to use native library. The
   * method is also used for reset the flag modified by setNativeZlibLoaded",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,init,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:init(org.apache.hadoop.conf.Configuration),156,165,"/**
* Initializes zlib compression settings.
* @param conf configuration object for compression
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInZlibDeflater.java,reinit,org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:reinit(org.apache.hadoop.conf.Configuration),65,83,"/**
* Initializes compressor with configuration.
* @param conf Configuration object for compression settings
*/","* reinit the compressor with the given configuration. It will reset the
   * compressor's compression level and compression strategy. Different from
   * <tt>ZlibCompressor</tt>, <tt>BuiltInZlibDeflater</tt> only support three
   * kind of compression strategy: FILTERED, HUFFMAN_ONLY and DEFAULT_STRATEGY.
   * It will use DEFAULT_STRATEGY as default if the configured compression
   * strategy is not supported.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader,int)",261,274,"/**
 * Initializes a ZlibCompressor with specified parameters.
 * @param level compression level
 * @param strategy compression strategy
 * @param header compression header
 * @param directBufferSize size of direct buffer
 */","* Creates a new compressor using the specified compression level.
   * Compressed data will be generated in ZLIB format.
   * 
   * @param level Compression level #CompressionLevel
   * @param strategy Compression strategy #CompressionStrategy
   * @param header Compression header #CompressionHeader
   * @param directBufferSize Size of the direct buffer to be used.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,setInput,"org.apache.hadoop.io.compress.zlib.ZlibCompressor:setInput(byte[],int,int)",300,318,"/**
* Masks a byte array segment.
* @param b the byte array to process
* @param off starting offset in the array
* @param len length of the segment to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,needsInput,org.apache.hadoop.io.compress.zlib.ZlibCompressor:needsInput(),340,367,"/**
* Determines if masking is required.
* @return true if masking should be applied, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesWritten(),432,436,"/**
 * Calls m1 and returns result of m2 with stream.
 */","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,getBytesRead,org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesRead(),443,447,"/**
 * Calls m1 and returns result of m2 with stream.
 */","* Returns the total number of uncompressed bytes input so far.
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibCompressor:reset(),449,461,"/**
* Calls methods to process and reset buffers.
* Sets flags indicating completion status.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,compress,"org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:compress(byte[],int,int)",81,132,"/**
 * Compresses data using GZIP algorithm.
 * @param b byte array containing input data
 * @param off offset in the byte array to start compression
 * @param len length of data to compress
 * @return number of bytes written after compression
 * @throws IOException if compressor is finished or closed
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,finished,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:finished(),360,363,"/**
 * Checks if end of input is reached and superclass condition is met.
 * @return true if both conditions are satisfied, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)",107,115,"/**
* Initializes a Zlib decompressor with given header and buffer size.
* @param header compression header for the zlib stream
* @param directBufferSize size of direct buffers for compressed and uncompressed data
*/","* Creates a new decompressor.
   * @param header header.
   * @param directBufferSize directBufferSize.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,setInput,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setInput(byte[],int,int)",121,139,"/**
* Masks data in buffer.
* @param b byte array containing data
* @param off offset to start masking
* @param len length of data to mask
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,needsInput,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:needsInput(),170,188,"/**
* Determines if masking is necessary.
* @return true if no masking needed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesWritten(),242,245,"/**
 * Calls m1 and returns result of m2 with stream.
 * @return result from m2 method
 */","* Returns the total number of uncompressed bytes output so far.
   *
   * @return the total (non-negative) number of uncompressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getBytesRead,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesRead(),252,255,"/**
 * Calls m1 and returns result of m2 with stream.
 */","* Returns the total number of compressed bytes input so far.
   *
   * @return the total (non-negative) number of compressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getRemaining,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getRemaining(),264,268,"/**
 * Calls m1 and returns sum of userBufLen and recursive call to m2 with stream.
 */","* Returns the number of bytes remaining in the input buffers; normally
   * called when finished() is true to determine amount of post-gzip-stream
   * data.
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:reset(),273,283,"/**
* Resets state and processes stream.
* Calls m1() and m2(stream), then initializes buffer states.
*/",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,finalize,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:finalize(),293,296,"/**
 * Calls m1 method to perform masking operation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,executeTrailerState,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeTrailerState(),367,416,"/**
* Processes gzip trailer to validate CRC and size.
* @throws IOException if validation fails or input issues occur
*/","* Parse the gzip trailer (assuming we're in the appropriate state).
   * In order to deal with degenerate cases (e.g., user buffer is one byte
   * long), we copy trailer bytes (all 8 of 'em) to a local buffer.</p>
   *
   * See http://www.ietf.org/rfc/rfc1952.txt for the gzip spec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,processBasicHeader,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:processBasicHeader(),509,524,"/**
* Validates gzip file header and sets flags.
* Throws IOException if format is invalid.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readCompressedString,org.apache.hadoop.io.WritableUtils:readCompressedString(java.io.DataInput),87,91,"/**
* Reads masked data from input stream.
* @param in DataInput source
* @return Decoded string or null if no data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeVInt,"org.apache.hadoop.io.WritableUtils:writeVInt(java.io.DataOutput,int)",257,259,"/**
 * Masks data output with an integer.
 * @param stream DataOutput stream to write to
 * @param i integer value to mask
 */","* Serializes an integer to a binary stream with zero-compressed encoding.
   * For -112 {@literal <=} i {@literal <=} 127, only one byte is used with the
   * actual value.
   * For other values of i, the first byte value indicates whether the
   * integer is positive or negative, and the number of bytes that follow.
   * If the first byte value v is between -113 and -116, the following integer
   * is positive, with number of bytes that follow are -(v+112).
   * If the first byte value v is between -121 and -124, the following integer
   * is negative, with number of bytes that follow are -(v+120). Bytes are
   * stored in the high-non-zero-byte-first order.
   *
   * @param stream Binary output stream
   * @param i Integer to be serialized
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,write,org.apache.hadoop.io.VLongWritable:write(java.io.DataOutput),54,57,"/**
 * Writes value to DataOutput using m1 method.
 * @param out the DataOutput stream to write to
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVLong,org.apache.hadoop.io.WritableUtils:readVLong(java.io.DataInput),313,326,"/**
 * Reads and processes a variable-length integer from the input stream.
 * @param stream DataInput source for reading bytes
 * @return decoded long value
 * @throws IOException if an I/O error occurs
 */","* Reads a zero-compressed encoded long from input stream and returns it.
   * @param stream Binary input stream
   * @throws IOException raised on errors performing I/O.
   * @return deserialized long from stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WeakReferencedElasticByteBufferPool.java,getBuffer,"org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getBuffer(boolean,int)",82,107,"/**
 * Retrieves or creates a ByteBuffer of specified length.
 * @param direct true for direct ByteBuffer, false otherwise
 * @param length size of the ByteBuffer to allocate
 * @return ByteBuffer instance
 */","* {@inheritDoc}
   *
   * @param direct whether we want a direct byte buffer or a heap one.
   * @param length length of requested buffer.
   * @return returns equal or next greater than capacity buffer from
   * pool if already available and not garbage collected else creates
   * a new buffer and return it.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WeakReferencedElasticByteBufferPool.java,putBuffer,org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:putBuffer(java.nio.ByteBuffer),113,130,"/**
* Masks buffer and inserts it into a TreeMap.
* @param buffer input ByteBuffer to be processed
*/","* Return buffer to the pool.
   * @param buffer buffer to be returned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,charAt,org.apache.hadoop.io.Text:charAt(int),163,169,"/**
* Masks a byte position in a buffer.
* @param position index to mask
* @return masked value or -1 if invalid position
*/","* Returns the Unicode Scalar Value (32-bit integer value)
   * for the character at <code>position</code>. Note that this
   * method avoids using the converter or doing String instantiation.
   *
   * @param position input position.
   * @return the Unicode scalar value at position or -1
   *          if the position is invalid or points to a
   *          trailing byte",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(java.lang.String),228,237,"/**
* Masks a string using ByteBuffer operations.
* @param string input string to be masked
*/","* Set to contain the contents of a string.
   *
   * @param string input string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,encode,org.apache.hadoop.io.Text:encode(java.lang.String),514,517,"/**
 * Converts string to ByteBuffer using default charset.
 * @param string input string to convert
 * @return ByteBuffer representation of the string
 * @throws CharacterCodingException if encoding fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,buildCacheKey,org.apache.hadoop.security.token.Token:buildCacheKey(),449,452,"/**
 * Generates a masked string using UUID and other inputs.
 * @return Masked string result
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,"org.apache.hadoop.io.Text:set(byte[],int,int)",272,277,"/**
* Masks UTF-8 data and updates length.
* @param utf8 input UTF-8 byte array
* @param start starting index in the array
* @param len number of bytes to process
*/","* Set the Text to range of bytes.
   *
   * @param utf8 the data to copy from
   * @param start the first position of the new string
   * @param len the number of bytes of the new string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,append,"org.apache.hadoop.io.Text:append(byte[],int,int)",286,294,"/**
* Masks UTF-8 data into byte array.
* @param utf8 source UTF-8 data
* @param start starting index in utf8 array
* @param len number of bytes to mask
*/","* Append a range of bytes to the end of the given text.
   *
   * @param utf8 the data to copy from
   * @param start the first position to append from utf8
   * @param len the number of bytes to append",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readWithKnownLength,"org.apache.hadoop.io.Text:readWithKnownLength(java.io.DataInput,int)",383,388,"/**
* Masks data from input stream.
* @param in DataInput source
* @param len Length of data to mask
* @throws IOException if I/O error occurs
*/","* Read a Text object whose length is already known.
   * This allows creating Text from a stream which uses a different serialization
   * format.
   *
   * @param in input in.
   * @param len input len.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,org.apache.hadoop.io.Text:decode(byte[]),457,459,"/**
 * Converts UTF-8 byte array to string.
 * @param utf8 byte array containing UTF-8 encoded data
 * @return decoded string
 * @throws CharacterCodingException if decoding fails
 */","* @return Converts the provided byte array to a String using the
   * UTF-8 encoding. If the input is malformed,
   * replace by a default value.
   *
   * @param utf8 input utf8.
   * @throws CharacterCodingException when a character
   *                                  encoding or decoding error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,"org.apache.hadoop.io.Text:decode(byte[],int,int)",461,464,"/**
 * Converts UTF-8 byte array to string.
 * @param utf8 input byte array
 * @param start starting index in the byte array
 * @param length number of bytes to convert
 * @return decoded string
 * @throws CharacterCodingException if decoding fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,"org.apache.hadoop.io.Text:decode(byte[],int,int,boolean)",480,483,"/**
 * Converts byte array to string.
 * @param utf8 input byte array in UTF-8 encoding
 * @param start starting index of the array
 * @param length number of bytes to convert
 * @param replace flag for replacing malformed characters
 * @return resulting string or throws CharacterCodingException
 */","* @return Converts the provided byte array to a String using the
   * UTF-8 encoding. If <code>replace</code> is true, then
   * malformed input is replaced with the
   * substitution character, which is U+FFFD. Otherwise the
   * method throws a MalformedInputException.
   *
   * @param utf8 input utf8.
   * @param start input start.
   * @param length input length.
   * @param replace input replace.
   * @throws CharacterCodingException when a character
   *                                  encoding or decoding error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,validateUTF8,org.apache.hadoop.io.Text:validateUTF8(byte[]),626,628,"/**
 * Calls overloaded method with full byte array.
 * @param utf8 UTF-8 encoded byte array
 * @throws MalformedInputException if input is not valid UTF-8
 */","* Check if a byte array contains valid UTF-8.
   *
   * @param utf8 byte array
   * @throws MalformedInputException if the byte array contains invalid UTF-8",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,addToMap,org.apache.hadoop.io.AbstractMapWritable:addToMap(java.lang.Class),91,101,"/**
* Registers a class with a unique ID.
* @param clazz Class to register
*/","* Add a Class to the maps if it is not already present.
   * @param clazz clazz.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,<init>,org.apache.hadoop.io.AbstractMapWritable:<init>(),145,166,"/**
* Initializes a new AbstractMapWritable with default mappings.
*/",constructor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,readFields,org.apache.hadoop.io.AbstractMapWritable:readFields(java.io.DataInput),194,216,"/**
* Reads class data from input and loads classes using a class loader.
* @param in DataInput stream containing class information
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,write,org.apache.hadoop.io.AbstractMapWritable:write(java.io.DataOutput),180,192,"/**
* Writes masked data to output stream.
* @param out DataOutput stream to write to
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Metadata:<init>(),735,737,"/**
* Initializes Metadata with an empty TreeMap.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,<init>,org.apache.hadoop.io.VLongWritable:<init>(long),38,38,"/**
 * Constructs a VLongWritable with the specified long value.
 * @param value the long value to be stored
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,getSerializer,org.apache.hadoop.io.serializer.SerializationFactory:getSerializer(java.lang.Class),81,87,"/**
* Retrieves a serializer for a given class.
* @param c the Class object of the type to serialize
* @return Serializer instance or null if not available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,getDeserializer,org.apache.hadoop.io.serializer.SerializationFactory:getDeserializer(java.lang.Class),89,95,"/**
* Retrieves a deserializer for the specified class.
* @param c the target class type
* @return Deserializer object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerializationComparator.java,compare,"org.apache.hadoop.io.serializer.JavaSerializationComparator:compare(java.lang.Object,java.lang.Object)",47,51,"/**
* Applies mask operation on two objects.
* @param o1 first object
* @param o2 second object
* @return result of mask operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerialization.java,deserialize,org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:deserialize(java.lang.Object),54,63,"/**
* Deserializes an object from input stream.
* @param object template type of the object to deserialize
* @return deserialized object of type T
* @throws IOException if deserialization fails or class not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,filesystem,org.apache.hadoop.io.SequenceFile$Writer:filesystem(org.apache.hadoop.fs.FileSystem),1002,1005,"/**
 * Deprecated: Creates an option for writing sequence files to a filesystem.
 * @param fs FileSystem instance
 * @return Option object for file system configuration
 */","* @deprecated only used for backwards-compatibility in the createWriter methods
     * that take FileSystem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,<init>,org.apache.hadoop.io.DataInputByteBuffer:<init>(),74,76,"/**
 * Constructs a new instance with an empty buffer.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getData,org.apache.hadoop.io.DataInputByteBuffer:getData(),87,89,"/**
* Retrieves an array of ByteBuffers.
* @return array of ByteBuffer objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getPosition,org.apache.hadoop.io.DataInputByteBuffer:getPosition(),91,93,"/**
 * Delegates to buffers' m1 method.
 * @return Result of buffers.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getLength,org.apache.hadoop.io.DataInputByteBuffer:getLength(),95,97,"/**
 * Delegates to buffers' m1 method.
 * @return result of buffers.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,<init>,"org.apache.hadoop.util.bloom.Key:<init>(byte[],double)",98,100,"/**
 * Constructs a Key with the given value and weight.
 * @param value byte array representing the key's value
 * @param weight numeric weight associated with the key
 */","* Constructor.
   * <p>
   * Builds a key with a specified weight.
   * @param value The value of <i>this</i> key.
   * @param weight The weight associated to <i>this</i> key.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cleanup,org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:cleanup(),3916,3921,"/**
* Calls superclass methods and container's m3.
* @throws IOException from superclass method call
*/","The default cleanup. Subclasses can override this with a custom 
       * cleanup",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,grow,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow(),3193,3200,"/**
 * Resizes arrays to accommodate more elements.
 * Doubles the size of keyOffsets and related arrays.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:<init>(org.apache.hadoop.io.SequenceFile$CompressionType),977,979,"/**
 * Initializes a CompressionOption with type and default settings.
 * @param value the compression type to use
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compression,"org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",1056,1059,"/**
 * Creates a compression option.
 * @param value type of compression
 * @param codec codec used for compression
 * @return CompressionOption object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericsUtil.java,toArray,org.apache.hadoop.util.GenericsUtil:toArray(java.util.List),86,88,"/**
* Recursively processes a list to return an array.
* @param list input list of generic type T
* @return array of processed elements of type T
*/","* Converts the given <code>List&lt;T&gt;</code> to a an array of 
   * <code>T[]</code>. 
   * @param list the list to convert
   * @param <T> Generics Type T.
   * @throws ArrayIndexOutOfBoundsException if the list is empty. 
   * Use {@link #toArray(Class, List)} if the list may be empty.
   * @return T Array.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,<init>,org.apache.hadoop.io.InputBuffer:<init>(),69,71,"/**
 * Constructs an InputBuffer using a default Buffer.
 */",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,reset,"org.apache.hadoop.io.InputBuffer:reset(byte[],int)",83,85,"/**
 * Copies bytes from input array to buffer.
 * @param input source byte array
 * @param length number of bytes to copy
 */","* Resets the data that the buffer reads.
   * @param input input.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,reset,"org.apache.hadoop.io.InputBuffer:reset(byte[],int,int)",93,95,"/**
 * Forwards byte array to buffer.
 * @param input source byte array
 * @param start starting index in array
 * @param length number of bytes to process
 */","* Resets the data that the buffer reads.
   * @param input input.
   * @param start start.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,getPosition,org.apache.hadoop.io.InputBuffer:getPosition(),101,101,"/**
 * Delegates to buffer's m1 method.
 * @return result of buffer.m1()
 */","* Returns the current position in the input.
   * @return the current position in the input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,getLength,org.apache.hadoop.io.InputBuffer:getLength(),107,107,"/**
 * Calls method m1 on buffer.
 * @return result of buffer's m1 method
 */","* Returns the length of the input.
   * @return length of the input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,read,org.apache.hadoop.io.MD5Hash:read(java.io.DataInput),87,91,"/**
 * Computes MD5 hash from input data.
 * @param in DataInput stream containing data to hash
 * @return MD5Hash object representing the computed hash
 * @throws IOException if an I/O error occurs while reading input
 */","* Constructs, reads and returns an instance.
   * @param in in.
   * @throws IOException raised on errors performing I/O.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(java.io.InputStream),138,147,"/**
* Computes MD5 hash of an input stream.
* @param in InputStream to read data from
* @return MD5Hash object representing the computed hash
*/","* Construct a hash value for the content from the InputStream.
   * @param in input stream.
   * @return MD5Hash.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,"org.apache.hadoop.io.MD5Hash:digest(byte[],int,int)",156,162,"/**
 * Computes the MD5 hash of a portion of data.
 * @param data input byte array
 * @param start starting index in the array
 * @param len length of the data to hash
 * @return MD5Hash object representing the computed hash
 */","* Construct a hash value for a byte array.
   * @param data data.
   * @param start start.
   * @param len len.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,"org.apache.hadoop.io.MD5Hash:digest(byte[][],int,int)",171,179,"/**
* Computes MD5 hash of byte array slices.
* @param dataArr array of byte arrays to hash
* @param start starting index for hashing
* @param len length of the slice to hash
* @return MD5Hash object containing the computed hash
*/","* Construct a hash value for an array of byte array.
   * @param dataArr dataArr.
   * @param start start.
   * @param len len.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,hashCode,org.apache.hadoop.io.MD5Hash:hashCode(),234,237,"/**
 * Returns the result of calling m1().
 */","Returns a hash code value for this object.
   * Only uses the first 4 bytes, since md5s are evenly distributed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,setDigest,org.apache.hadoop.io.MD5Hash:setDigest(java.lang.String),283,293,"/**
 * Converts hex string to byte array and validates length.
 * @param hex hexadecimal string representation
 */","* Sets the digest value from a hex string.
   * @param hex hex.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,metadata,org.apache.hadoop.io.SequenceFile$Writer:metadata(org.apache.hadoop.io.SequenceFile$Metadata),1048,1050,"/**
* Wraps Metadata in an Option.
* @param value Metadata to be wrapped
* @return Option containing the Metadata
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$StreamOption:<init>(org.apache.hadoop.fs.FSDataOutputStream),912,914,"/**
 * Initializes a new StreamOption with the given output stream.
 * @param stream the file system data output stream to use
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,org.apache.hadoop.io.ObjectWritable:<init>(java.lang.Object),48,50,"/**
 * Constructs an ObjectWritable with the given instance.
 * @param instance the object to be wrapped
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,tryInstantiateProtobuf,"org.apache.hadoop.io.ObjectWritable:tryInstantiateProtobuf(java.lang.Class,java.io.DataInput)",351,389,"/**
* Deserializes a Protocol Buffers message from DataInput.
* @param protoClass the Protocol Buffers class type
* @param dataIn input stream or byte array source
* @return deserialized Message object
* @throws IOException if I/O error occurs during parsing
*/","* Try to instantiate a protocol buffer of the given message class
   * from the given input stream.
   * 
   * @param protoClass the class of the generated protocol buffer
   * @param dataIn the input stream to read from
   * @return the instantiated Message instance
   * @throws IOException if an IO problem occurs",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,fsync,org.apache.hadoop.io.IOUtils:fsync(java.io.File),392,413,"/**
* Synchronizes a file or directory.
* @param fileToSync the file or directory to sync
* @throws IOException if an I/O error occurs
*/","* Ensure that any writes to the given file is written to the storage device
   * that contains it. This method opens channel on given File and closes it
   * once the sync is done.<br>
   * Borrowed from Uwe Schindler in LUCENE-5588
   * @param fileToSync the file to fsync
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,equals,org.apache.hadoop.io.MapWritable:equals(java.lang.Object),78,94,"/**
* Checks equality with another object.
* @param obj the object to compare
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,putAll,org.apache.hadoop.io.MapWritable:putAll(java.util.Map),123,128,"/**
* Processes key-value pairs from the input map.
* @param t map containing writable key-value pairs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$AppendIfExistsOption:<init>(boolean),939,941,"/**
 * Sets append option based on boolean value.
 * @param value true to enable appending, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$OnlyHeaderOption:<init>(),1889,1891,"/**
 * Constructs a new OnlyHeaderOption with headerOnly set to true.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$FileOption:<init>(org.apache.hadoop.fs.Path),1852,1854,"/**
 * Constructs a FileOption with the given path.
 * @param value the file path
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$FileOption:<init>(org.apache.hadoop.fs.Path),890,892,"/**
 * Constructs a new FileOption with the specified file path.
 * @param path the file path to be used
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,<init>,org.apache.hadoop.io.BooleanWritable:<init>(boolean),41,43,"/**
 * Constructs a BooleanWritable with the specified boolean value.
 * @param value the boolean value to be stored
 */",* @param value value.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,toString,org.apache.hadoop.io.BooleanWritable:toString(),102,105,"/**
 * Calls m1 and converts its result using Boolean.m2.
 * @return String representation of m1's result processed by Boolean.m2
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,waitAsyncValue,"org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:waitAsyncValue(long,java.util.concurrent.TimeUnit)",205,217,"/**
 * Waits for a value with a specified timeout.
 * @param timeout the maximum time to wait
 * @param unit the time unit of the timeout parameter
 * @return the value if available within timeout, otherwise throws TimeoutException
 * @throws InterruptedException if interrupted while waiting
 * @throws TimeoutException if the value is not available before timeout
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:shouldRetry(java.lang.Exception,int,int,boolean)",225,230,"/**
* Returns a retry action to fail after one attempt.
* @param e exception encountered
* @param retries number of retries made so far
* @param failovers number of failovers attempted
* @param isIdempotentOrAtMostOnce flag indicating operation type
* @return RetryAction with decision to fail
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicy.java,<init>,org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision),49,51,"/**
 * Constructs a RetryAction with a specified retry decision.
 * @param action the retry decision to be executed
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicy.java,<init>,"org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long)",53,55,"/**
 * Constructs a RetryAction with specified decision and delay.
 * @param action the retry decision logic
 * @param delayTime the time to wait before retrying (in milliseconds)
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithFixedSleep:<init>(int,long,java.util.concurrent.TimeUnit)",331,333,"/**
* Initializes retry mechanism with fixed sleep.
* @param maxRetries maximum number of retries
* @param sleepTime duration to sleep between retries
* @param timeUnit unit of time for sleep duration
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry:<init>(int,long,java.util.concurrent.TimeUnit)",627,638,"/**
* Initializes an ExponentialBackoffRetry with specified parameters.
* @param maxRetries maximum number of retries allowed
* @param sleepTime base time to wait before retrying
* @param timeUnit unit of time for the sleep duration
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep:<init>(int,long,java.util.concurrent.TimeUnit)",367,369,"/**
* Constructs a retry policy with a maximum count and proportional sleep.
* @param maxRetries maximum number of retries
* @param sleepTime base sleep duration between retries
* @param timeUnit unit for sleep duration
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int)",669,672,"/**
 * Constructs a retry policy with network exception failover.
 * @param fallbackPolicy the policy to use on failure
 * @param maxFailovers maximum number of failover attempts
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)",674,677,"/**
 * Constructs a retry policy with network exception failover.
 * @param fallbackPolicy the base policy to use for retries
 * @param maxFailovers maximum number of failover attempts
 * @param delayMillis initial delay between retries in milliseconds
 * @param maxDelayBase maximum delay factor for exponential backoff
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,int,long,long)",217,222,"/**
* Creates a retry policy with network exception failover.
* @param fallbackPolicy policy to use after all retries
* @param maxFailovers maximum number of failover attempts
* @param maxRetries maximum number of retry attempts per failover
* @param delayMillis initial delay between retries in milliseconds
* @param maxDelayBase maximum base delay for exponential backoff
* @return RetryPolicy configured with network exception handling
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryByRemoteException,"org.apache.hadoop.io.retry.RetryPolicies:retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",177,181,"/**
* Creates a retry policy based on exceptions.
* @param defaultPolicy base retry policy to use
* @param exceptionToPolicyMap mapping of exceptions to specific policies
* @return RetryPolicy configured with remote exception handling
*/","* <p>
   * A retry policy for RemoteException
   * Set a default policy with some explicit handlers for specific exceptions.
   * </p>
   *
   * @param defaultPolicy defaultPolicy.
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$RemoteExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)",582,594,"/**
* Determines retry action based on exception and policy.
* @param e the exception encountered
* @param retries number of retry attempts
* @param failovers number of failover attempts
* @param isIdempotentOrAtMostOnce flag indicating operation type
* @return RetryAction to be taken
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,<init>,"org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:<init>(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler,org.apache.hadoop.io.retry.AsyncCallHandler)",237,243,"/**
* Initializes an asynchronous call.
* @param method the method to be invoked
* @param args arguments for the method
* @param isRpc flag indicating if it's an RPC call
* @param callId unique identifier for the call
* @param retryInvocationHandler handler for retries
* @param asyncCallHandler handler for asynchronous processing
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,processWaitTimeAndRetryInfo,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:processWaitTimeAndRetryInfo(),268,277,"/**
 * Determines retry action based on wait time.
 * @return CallReturn.WAIT_RETRY if wait time is positive, otherwise CallReturn.RETRY
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:<init>(long,org.apache.hadoop.io.retry.RetryPolicy$RetryAction,long,java.lang.Exception)",250,257,"/**
 * Initializes retry info with specified parameters.
 * @param delay time to wait before retrying (in milliseconds)
 * @param action the retry action to perform
 * @param expectedFailoverCount expected number of failovers
 * @param failException exception that caused the failure
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isEmpty,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:isEmpty(long),96,99,"/**
 * Checks if elapsed time exceeds given threshold and queue condition is met.
 * @param time threshold in milliseconds
 * @return true if conditions are satisfied, false otherwise
 */",Is the queue empty for more than the given time in millisecond?,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,checkEmpty,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:checkEmpty(),106,110,"/**
 * Masks function logic.
 * Checks queue status and records start time if condition met.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,clearNameMaps,org.apache.hadoop.security.ShellBasedIdMapping:clearNameMaps(),154,159,"/**
 * Updates user and group maps with latest data.
 * Synchronizes access to ensure thread safety.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,isExpired,org.apache.hadoop.security.ShellBasedIdMapping:isExpired(),161,163,"/**
 * Checks if the function mask should be updated.
 * @return true if the timeout has expired since the last update, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$MetricsUpdateRunner:run(),4207,4223,"/**
* Updates request rate metrics.
* Calculates and updates the number of requests per second.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,now,org.apache.hadoop.util.SysInfoWindows:now(),61,64,"/**
 * Returns a timestamp mask value.
 * @return Current time in milliseconds as a long.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,monotonicNow,org.apache.hadoop.util.Timer:monotonicNow(),50,50,"/**
 * Returns the current time in milliseconds.
 * @return Current time in milliseconds
 */","* Current time from some arbitrary time base in the past, counting in
   * milliseconds, and not affected by settimeofday or similar system clock
   * changes.  This is appropriate to use when computing how much longer to
   * wait for an interval to expire.
   * @return a monotonic clock that counts in milliseconds.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryOtherThanRemoteAndSaslException,"org.apache.hadoop.io.retry.RetryPolicies:retryOtherThanRemoteAndSaslException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",194,199,"/**
* Creates a retry policy excluding remote and SASL exceptions.
* @param defaultPolicy base retry policy
* @param exceptionToPolicyMap specific policies for certain exceptions
* @return specialized retry policy
*/","* <p>
   * A retry policy where RemoteException and SaslException are not retried, other individual
   * exception types can have RetryPolicy overrides, and any other exception type without an
   * override is not retried.
   * </p>
   *
   * @param defaultPolicy defaultPolicy.
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/DefaultFailoverProxyProvider.java,getProxy,org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:getProxy(),45,48,"/**
 * Creates a ProxyInfo instance with a proxy and no handler.
 * @return ProxyInfo object initialized with given proxy and null handler
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",332,338,"/**
* Initializes a RetryInvocationHandler with proxy provider and retry policies.
* @param proxyProvider provider for failover proxies
* @param defaultPolicy default retry policy for methods
* @param methodNameToPolicyMap specific retry policies by method name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,close,org.apache.hadoop.io.retry.RetryInvocationHandler:close(),457,460,"/**
 * Delegates call to proxy descriptor.
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/FailoverProxyProvider.java,getString,org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:getString(java.lang.String),50,52,"/**
 * Constructs a masked function name.
 * @param methodName original method name
 * @return masked function name string
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/FailoverProxyProvider.java,toString,org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:toString(),54,57,"/**
* Constructs a masked string with additional info.
* @return Concatenated string from m1() and proxyInfo
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,getFailoverCount,org.apache.hadoop.io.retry.RetryInvocationHandler:getFailoverCount(),345,347,"/**
 * Calls the m1 method on the proxy descriptor.
 * @return Result of the m1 method call
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeMethod,"org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])",432,443,"/**
* Invokes a method with arguments, handling exceptions.
* @param method the Method to invoke
* @param args array of arguments for the method
* @return result of the method invocation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getCallId,org.apache.hadoop.ipc.Client:getCallId(),137,139,"/**
 * Returns result of m1 if not null, otherwise returns result of m2.
 * @return integer value from either m1 or m2
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$Call:<init>(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable)",287,307,"/**
* Initializes a new RPC call with specified kind and parameters.
* Sets unique call ID and retry count, assigns external handler.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,getConnectionId,org.apache.hadoop.io.retry.RetryInvocationHandler:getConnectionId(),462,465,"/**
 * Retrieves connection ID.
 * @return ConnectionId object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProxyCombiner.java,getConnectionId,org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:getConnectionId(),122,125,"/**
 * Returns a connection ID using the first proxy.
 * @return ConnectionId object from the RPC call
 */","* Since this is incapable of returning multiple connection IDs, simply
     * return the first one. In most cases, the connection ID should be the same
     * for all proxies.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isDone,org.apache.hadoop.io.retry.AsyncCallHandler$1:isDone(),226,228,"/**
 * Checks if the value is not null.
 * @return true if value is not null, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,hashCode,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:hashCode(),451,454,"/**
 * Calls m2 on the result of m1().
 * @return result of calling m2 on the object returned by m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,equals,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:equals(java.lang.Object),456,464,"/**
* Compares objects based on m1 and m2 methods.
* @param that object to compare with
* @return true if both m1 and m2 are equal, else false
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,parseCommaSeparatedString,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parseCommaSeparatedString(java.lang.String),483,514,"/**
 * Parses a string to create a MultipleLinearRandomRetry object.
 * @param s comma-separated string of sleep and retries pairs
 * @return MultipleLinearRandomRetry object or null if input is invalid
 */","* Parse the given string as a MultipleLinearRandomRetry object.
     * The format of the string is ""t_1, n_1, t_2, n_2, ..."",
     * where t_i and n_i are the i-th pair of sleep time and number of retries.
     * Note that the white spaces in the string are ignored.
     *
     * @param s input string.
     * @return the parsed object, or null if the parsing fails.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryByException,"org.apache.hadoop.io.retry.RetryPolicies:retryByException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",162,165,"/**
 * Creates an exception-dependent retry policy.
 * @param defaultPolicy fallback retry policy
 * @param exceptionToPolicyMap mapping of exceptions to specific retry policies
 * @return ExceptionDependentRetry instance
 */","* <p>
   * Set a default policy with some explicit handlers for specific exceptions.
   * </p>
   *
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @param defaultPolicy defaultPolicy.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,calculateExponentialTime,"org.apache.hadoop.io.retry.RetryPolicies:calculateExponentialTime(long,int)",769,771,"/**
 * Calls overloaded method with max value as limit.
 * @param time initial time in milliseconds
 * @param retries number of retry attempts
 * @return result from overloaded method
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getReason,org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:getReason(),353,356,"/**
 * Returns masked function name based on max time and unit.
 * @return Masked function name as String
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getReason,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:getReason(),293,295,"/**
 * Returns masked function result.
 * @return masked string from maxRetries
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,hashCode,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:hashCode(),305,308,"/**
 * Calls m2 on the result of m1().
 * @return result of m2() from the object returned by m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,equals,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:equals(java.lang.Object),310,318,"/**
* Compares objects for equality based on specific conditions.
* @param that the object to compare with
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MultipleIOException.java,createIOException,org.apache.hadoop.io.MultipleIOException:createIOException(java.util.List),50,58,"/**
 * Masks a list of IOExceptions.
 * @param exceptions list of IO exceptions
 * @return single IOException or null if none
 */","* A convenient method to create an {@link IOException}.
   * @param exceptions IOException List.
   * @return IOException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCompressionAlgorithmByName,org.apache.hadoop.io.file.tfile.Compression:getCompressionAlgorithmByName(java.lang.String),359,370,"/**
* Retrieves an Algorithm by its compression name.
* @param compressName the name of the compression algorithm
* @return the corresponding Algorithm object
* @throws IllegalArgumentException if the compression name is unsupported
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getSupportedAlgorithms,org.apache.hadoop.io.file.tfile.Compression:getSupportedAlgorithms(),372,382,"/**
* Retrieves and filters algorithm names.
* @return Array of filtered algorithm names
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getCompressionName(),524,526,"/**
 * Returns compressed data using algorithm m1.
 * @return Compressed string result
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getBlockCount,org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockCount(),687,689,"/**
 * Returns mask value from data index.
 * @return integer mask value
 */","* Get the number of data blocks.
     * 
     * @return the number of data blocks.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,readAndVerify,org.apache.hadoop.io.file.tfile.BCFile$Magic:readAndVerify(java.io.DataInput),920,929,"/**
* Validates BCFile header.
* @param in input stream containing file data
* @throws IOException if validation fails or I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFileDumper.java,format,"org.apache.hadoop.io.file.tfile.TFileDumper$Align:format(long,int,org.apache.hadoop.io.file.tfile.TFileDumper$Align)",73,78,"/**
 * Formats a long number as a string with specified alignment.
 * @param l the number to format
 * @param width the desired width of the formatted string
 * @param align the alignment type (e.g., ZERO_PADDED)
 * @return formatted string representation of the number
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,addEntry,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:addEntry(org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry),782,784,"/**
 * Masks an index entry.
 * @param indexEntry the entry to be masked
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDefaultCompressionAlgorithm,org.apache.hadoop.io.file.tfile.BCFile$Writer:getDefaultCompressionAlgorithm(),341,343,"/**
 * Retrieves an algorithm from the data index.
 * @return Algorithm object from the data index
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDefaultCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader:getDefaultCompressionName(),652,654,"/**
 * Retrieves masked data from dataIndex.
 * @return Masked data string
 */","* Get the name of the default compression algorithm.
     * 
     * @return the name of the default compression algorithm.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,hashCode,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:hashCode(),1973,1976,"/**
 * Returns mask value using key buffer and comparator.
 * @return integer mask value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/CompareUtils.java,compare,"org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:compare(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",45,49,"/**
* Compares two RawComparable objects.
* @param o1 first RawComparable object
* @param o2 second RawComparable object
* @return result of comparison based on m1, m2, and m3 methods
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:<init>(java.io.DataInput),948,952,"/**
* Initializes a BlockRegion from DataInput.
* @param in source of block region data
* @throws IOException if reading fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,readVInt,org.apache.hadoop.io.file.tfile.Utils:readVInt(java.io.DataInput),177,184,"/**
* Reads a long value and casts it to an integer.
* @param in DataInput source
* @return casted integer value
* @throws IOException if reading fails
* @throws RuntimeException if the number is out of int range
*/","* Decoding the variable-length integer. Synonymous to
   * <code>(int)Utils#readVLong(in)</code>.
   * 
   * @param in
   *          input stream
   * @return the decoded integer
   * @throws IOException raised on errors performing I/O.
   * 
   * @see Utils#readVLong(DataInput)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,makeComparator,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:makeComparator(java.lang.String),2070,2096,"/**
 * Creates a BytesComparator based on the provided string.
 * @param comparator configuration string for the comparator
 * @return BytesComparator instance or throws exception if unsupported
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:write(java.io.DataOutput),960,964,"/**
 * Writes mask data to output stream.
 * @param out DataOutput stream to write to
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,writeVInt,"org.apache.hadoop.io.file.tfile.Utils:writeVInt(java.io.DataOutput,int)",55,57,"/**
 * Masks data by writing to output.
 * @param out DataOutput object to write to
 * @param n number of bytes to mask
 * @throws IOException if an I/O error occurs
 */","* Encoding an integer into a variable-length encoding format. Synonymous to
   * <code>Utils#writeVLong(out, n)</code>.
   * 
   * @param out
   *          output stream
   * @param n
   *          The integer to be encoded
   * @throws IOException raised on errors performing I/O.
   * @see Utils#writeVLong(DataOutput, long)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,isSorted,org.apache.hadoop.io.file.tfile.TFile$Reader:isSorted(),864,866,"/**
 * Checks file metadata validity.
 * @return true if valid, false otherwise
 */","* Is the TFile sorted?
     * 
     * @return true if TFile is sorted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCodec,org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:getCodec(),273,273,"/**
 * Returns a compression codec instance.
 * @throws IOException if an I/O error occurs during codec creation
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,equals,org.apache.hadoop.io.file.tfile.Utils$Version:equals(java.lang.Object),395,400,"/**
* Checks equality with another version.
* @param other object to compare
* @return true if versions are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getEntryCount,org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryCount(),873,875,"/**
 * Retrieves mask value from file metadata.
 * @return long representing the mask value
 */","* Get the number of key-value pair entries in TFile.
     * 
     * @return the number of key-value pairs in TFile",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Reader:close(),824,827,"/**
 * Calls m1 on readerBCF.
 * @throws IOException if an I/O error occurs
 */","* Close the reader. The state of the Reader object is undefined after
     * close. Calling close() for multiple times has no effect.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getComparatorName,org.apache.hadoop.io.file.tfile.TFile$Reader:getComparatorName(),855,857,"/**
 * Calls m1 on tfileMeta.
 * @return Result of tfileMeta.m1()
 */","* Get the string representation of the comparator.
     * 
     * @return If the TFile is not sorted by keys, an empty string will be
     *         returned. Otherwise, the actual comparator string that is
     *         provided during the TFile creation time will be returned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/ByteArray.java,<init>,org.apache.hadoop.io.file.tfile.ByteArray:<init>(org.apache.hadoop.io.BytesWritable),40,42,"/**
 * Initializes a new ByteArray from another BytesWritable.
 * @param other the source BytesWritable to copy data from
 */","* Constructing a ByteArray from a {@link BytesWritable}.
   * 
   * @param other other.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/ByteArray.java,<init>,org.apache.hadoop.io.file.tfile.ByteArray:<init>(byte[]),50,52,"/**
 * Constructs a ByteArray from a byte array.
 * @param buffer source byte array
 */","* Wrap a whole byte array as a RawComparable.
   * 
   * @param buffer
   *          the byte array buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockEntryCount,org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockEntryCount(int),2031,2033,"/**
 * Applies mask to current bid and returns result.
 * @param curBid current bid value
 * @return masked bid value as long
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,addEntry,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:addEntry(org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry),2262,2266,"/**
* Updates index and record count using file entry.
* @param keyEntry file index entry to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,register,"org.apache.hadoop.io.file.tfile.BCFile$Writer$DataBlockRegister:register(long,long,long)",468,471,"/**
 * Masks data by applying a block region.
 * @param raw raw data value
 * @param begin start position of the mask
 * @param end end position of the mask
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getBlockIndexNear,org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockIndexNear(long),745,756,"/**
 * Finds index of block region containing offset.
 * @param offset the position to search for
 * @return index in list or -1 if not found
 */","* Find the smallest Block index whose starting offset is greater than or
     * equal to the specified offset.
     * 
     * @param offset
     *          User-specific offset.
     * @return the index to the data Block if such block exists; or -1
     *         otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:lowerBound(org.apache.hadoop.io.file.tfile.RawComparable),2187,2201,"/**
* Searches for a key in the TFile.
* @param key the RawComparable key to search for
* @return index of the key or -1 if not found
*/","* @param key
     *          input key.
     * @return the ID of the first block that contains key >= input key. Or -1
     *         if no such block exists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,write,org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(int),45,51,"/**
* Writes a byte to buffer, calls m1() if buffer is full.
* @param b byte to write
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,write,"org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(byte[],int,int)",53,65,"/**
* Writes bytes from array to output stream.
* @param b byte array containing data
* @param off offset in the byte array
* @param len number of bytes to write
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,flush,org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:flush(),67,71,"/**
 * Calls m1 and out.m2 in a synchronized manner.
 * @throws IOException if an I/O error occurs during execution
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:upperBound(org.apache.hadoop.io.file.tfile.RawComparable),2209,2223,"/**
 * Searches for a key in the TFile using binary search.
 * @param key the key to search for
 * @return index of the key or -1 if not found
 * @throws RuntimeException if comparator is null
 */","* @param key
     *          input key.
     * @return the ID of the first block that contains key > input key. Or -1
     *         if no such block exists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumByLocation,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),2244,2248,"/**
 * Calculates the masked record number based on location.
 * @param location Reader.Location object containing block index and offset
 * @return Masked record number
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),741,744,"/**
 * Compares this location to another based on block and record indices.
 * @param other Location object to compare with
 * @return result of comparison
 */",* @see java.lang.Comparable#compareTo(java.lang.Object),,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(int,long)",705,707,"/**
 * Initializes a Location with specified block and record indices.
 * @param blockIndex index of the block
 * @param recordIndex index of the record within the block
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,set,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:set(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),734,736,"/**
 * Calls overloaded method with block and record indices from another Location.
 * @param other Location object containing block and record indices
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(byte[]),1775,1777,"/**
 * Calls overloaded method starting from index 0.
 * @param buf byte array to process
 * @return result of processing
 * @throws IOException if an I/O error occurs
 */","* Copy the key into user supplied buffer.
         * 
         * @param buf
         *          The buffer supplied by user. The length of the buffer must
         *          not be shorter than the key length.
         * @return The length of the key.
         * 
         * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[],int)",1859,1890,"/**
 * Masks buffer with data from input stream.
 * @param buf byte array to mask
 * @param offset starting position in the buffer
 * @return number of bytes processed or throws IOException if error occurs
 */","* Copy value into user-supplied buffer. User supplied buffer must be
         * large enough to hold the whole value (starting from the offset). The
         * value part of the key-value pair pointed by the current cursor is not
         * cached and can only be examined once. Calling any of the following
         * functions more than once without moving the cursor will result in
         * exception: {@link #getValue(byte[])}, {@link #getValue(byte[], int)},
         * {@link #getValueStream}.
         *
         * @param buf buf.
         * @param offset offset.
         * @return the length of the value. Does not require
         *         isValueLengthKnown() to be true.
         * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:<init>(org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState),549,552,"/**
* Initializes BlockReader with RBlockState.
* @param rbs RBlockState containing input stream and block data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getRawSize,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getRawSize(),585,587,"/**
 * Retrieves a state value from an internal block.
 * @return The state value as a long.
 */","* Get the uncompressed size of the block.
       * 
       * @return uncompressed size of the block.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressedSize(),594,596,"/**
 * Retrieves a value from nested objects.
 * @return long value obtained from nested method calls
 */","* Get the compressed size of the block.
       * 
       * @return compressed size of the block.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getStartPos,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getStartPos(),603,605,"/**
 * Returns the masked state value.
 * @return Masked state as a long
 */","* Get the starting position of the block in the file.
       * 
       * @return the starting position of the block in the file.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(byte[]),394,397,"/**
 * Calls overloaded method with full byte array.
 * @param b byte array to process
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputOutputStream.java,constructOutputStream,org.apache.hadoop.io.DataOutputOutputStream:constructOutputStream(java.io.DataOutput),43,49,"/**
* Wraps DataOutput in OutputStream.
* @param out DataOutput instance to wrap
* @return OutputStream wrapper for the given DataOutput
*/","* Construct an OutputStream from the given DataOutput. If 'out'
   * is already an OutputStream, simply returns it. Otherwise, wraps
   * it in an OutputStream.
   * @param out the DataOutput to wrap
   * @return an OutputStream instance that outputs to 'out'",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FloatWritable.java,<init>,org.apache.hadoop.io.FloatWritable:<init>(float),34,34,"/**
 * Constructs a new FloatWritable with the specified value.
 * @param value the float value to be stored
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FastByteComparisons.java,compareTo,"org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:compareTo(byte[],int,int,byte[],int,int)",188,242,"/**
* Compares two byte arrays for lexicographical order.
* @param buffer1 first byte array
* @param offset1 starting index in buffer1
* @param length1 length of bytes to compare in buffer1
* @param buffer2 second byte array
* @param offset2 starting index in buffer2
* @param length2 length of bytes to compare in buffer2
* @return negative if buffer1 < buffer2, positive if buffer1 > buffer2, 0 if equal
*/","* Lexicographically compare two arrays.
       *
       * @param buffer1 left operand
       * @param buffer2 right operand
       * @param offset1 Where to start comparing in the left buffer
       * @param offset2 Where to start comparing in the right buffer
       * @param length1 How much to compare from the left buffer
       * @param length2 How much to compare from the right buffer
       * @return 0 if equal, < 0 if left is less than right, etc.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,<init>,org.apache.hadoop.io.DataOutputBuffer:<init>(),89,91,"/**
 * Constructs a new DataOutputBuffer with a default buffer.
 */",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,<init>,org.apache.hadoop.io.DataOutputBuffer:<init>(int),93,95,"/**
 * Constructs a DataOutputBuffer with specified buffer size.
 * @param size initial buffer capacity in bytes
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,getData,org.apache.hadoop.io.DataOutputBuffer:getData(),108,108,"/**
* Returns raw data from buffer.
* @return byte array containing buffer's content
*/","* Returns the current contents of the buffer.
   *  Data is only valid to {@link #getLength()}.
   *
   * @return data byte.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,getLength,org.apache.hadoop.io.DataOutputBuffer:getLength(),114,114,"/**
 * Calls m1 on the buffer and returns its result.
 * @return The integer result of calling buffer.m1()
 */","* Returns the length of the valid data currently in the buffer.
   * @return length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,writeInt,"org.apache.hadoop.io.DataOutputBuffer:writeInt(int,int)",154,164,"/**
 * Writes an integer value to a buffer at a specified offset.
 * @param v the integer value to write
 * @param offset the position in the buffer to start writing
 * @throws IOException if I/O error occurs
 */","* Overwrite an integer into the internal buffer. Note that this call can only
   * be used to overwrite existing data in the buffer, i.e., buffer#count cannot
   * be increased, and DataOutputStream#written cannot be increased.
   *
   * @param v v.
   * @param offset offset.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNSDomainNameResolver.java,getHostnameByIP,org.apache.hadoop.net.DNSDomainNameResolver:getHostnameByIP(java.net.InetAddress),44,62,"/**
* Masks IP address by reversing DNS lookup if necessary.
* @param address InetAddress object representing the IP address
* @return Masked hostname or original IP address as String
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDistance,"org.apache.hadoop.net.NetworkTopology:getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",324,365,"/**
* Calculates distance between two nodes in a hierarchy.
* @param node1 first node
* @param node2 second node
* @return distance or Integer.MAX_VALUE if nodes are invalid
*/","Return the distance between two nodes
   * It is assumed that the distance from one node to its parent is 1
   * The distance between two nodes is calculated by summing up their distances
   * to their closest common ancestor.
   * @param node1 one node
   * @param node2 another node
   * @return the distance between node1 and node2 which is zero if they are the same
   *  or {@link Integer#MAX_VALUE} if node1 or node2 do not belong to the cluster",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,isNodeInScope,"org.apache.hadoop.net.NetworkTopology:isNodeInScope(org.apache.hadoop.net.Node,java.lang.String)",1023,1029,"/**
* Checks if a node's location matches the given scope.
* @param node the node to check
* @param scope the scope to compare against
* @return true if node location starts with scope, false otherwise
*/","* Checks whether a node belongs to the scope.
   * @param node  the node to check.
   * @param scope scope to check.
   * @return true if node lies within the scope",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,getPathComponents,org.apache.hadoop.net.NodeBase:getPathComponents(org.apache.hadoop.net.Node),124,126,"/**
 * Masks node path using specified separator.
 * @param node input Node object
 * @return masked path as String array
 */","* Get the path components of a node.
   * @param node a non-null node
   * @return the path of a node",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,equals,org.apache.hadoop.net.NodeBase:equals(java.lang.Object),128,137,"/**
* Checks equality with another object.
* @param to object to compare
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,hashCode,org.apache.hadoop.net.NodeBase:hashCode(),139,142,"/**
 * Calls m1 with this instance and invokes m2 on its result.
 * @return Result of calling m2 on the object returned by m1
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,toString,org.apache.hadoop.net.NodeBase:toString(),145,148,"/**
 * Masks the function name.
 * @return Masked function name as a string
 */",@return this node's path as its string representation,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,remove,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:remove(org.apache.hadoop.net.Node),228,254,"/**
 * Removes a node from the network topology.
 * @param node the node to be removed
 */","Remove a node
   * Update node counter and rack counter if necessary
   * @param node node to be removed; can be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDatanodesInRack,org.apache.hadoop.net.NetworkTopology:getDatanodesInRack(java.lang.String),202,215,"/**
* Retrieves nodes from a specified location.
* @param loc location string identifier
* @return list of Node objects or empty list if none found
*/","* Given a string representation of a rack, return its children
   * @param loc a path-like string representation of a rack
   * @return a newly allocated list with all the node's children",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getNode,org.apache.hadoop.net.NetworkTopology:getNode(java.lang.String),271,281,"/**
* Retrieves node from cluster map based on location.
* @param loc node location string
* @return Node object or null if not found
*/","Given a string representation of a node, return its reference
   * 
   * @param loc
   *          a path-like string representation of a node
   * @return a reference to the node; null if the node is not in the tree",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,locationToDepth,org.apache.hadoop.net.NodeBase:locationToDepth(java.lang.String),209,219,"/**
* Calculates the depth of a location path.
* @param location the input location string
* @return integer representing the depth of the path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,toString,org.apache.hadoop.net.NetworkTopology:toString(),714,732,"/**
* Generates a string representation of the system's rack and leaf configuration.
* @return String detailing the number of racks and leaves, along with node information.
*/",convert a network tree to a string.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,isOnSameRack,"org.apache.hadoop.net.NetworkTopology:isOnSameRack(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",409,415,"/**
* Checks if two nodes are masked.
* @param node1 first node to compare
* @param node2 second node to compare
* @return true if nodes are masked, false otherwise
*/","Check if two nodes are on the same rack
   * @param node1 one node (can be null)
   * @param node2 another node (can be null)
   * @return true if node1 and node2 are on the same rack; false otherwise
   * @exception IllegalArgumentException when either node1 or node2 is null, or
   * node1 or node2 do not belong to the cluster",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int)",569,637,"/**
 * Randomly selects a node within scope, excluding specified nodes.
 * @param parentNode parent node to select from
 * @param excludedScopeNode node to exclude from selection
 * @param excludedNodes collection of additional nodes to exclude
 * @param totalInScopeNodes total nodes in scope
 * @param availableNodes available nodes for selection
 * @return selected Node or null if none found
 */","* Randomly choose one node under <i>parentNode</i>, considering the exclude
   * nodes and scope. Should be called with {@link #netlock}'s readlock held.
   *
   * @param parentNode        the parent node
   * @param excludedScopeNode the node corresponding to the exclude scope.
   * @param excludedNodes     a collection of nodes to be excluded from
   * @param totalInScopeNodes total number of nodes under parentNode, excluding
   *                          the excludedScopeNode
   * @param availableNodes    number of available nodes under parentNode that
   *                          could be chosen, excluding excludedNodes
   * @return the chosen node, or null if none can be chosen",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getWeightUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:getWeightUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",807,845,"/**
 * Calculates a mask value based on node paths.
 * @param reader Node representing the reader's path
 * @param node Node representing the target node's path
 * @return Weight indicating similarity or difference between paths
 */","* Returns an integer weight which specifies how far away <i>node</i> is
   * from <i>reader</i>. A lower value signifies that a node is closer.
   * It uses network location to calculate the weight
   *
   * @param reader Node where data will be read
   * @param node Replica of data
   * @return weight",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,interAddNodeWithEmptyRack,org.apache.hadoop.net.NetworkTopology:interAddNodeWithEmptyRack(org.apache.hadoop.net.Node),1083,1097,"/**
* Processes a node to update rack map.
* @param node the node to process
*/","* Internal function for update empty rack number
   * for add or recommission a node.
   * @param node node to be added; can be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,<init>,"org.apache.hadoop.net.SocketIOWithTimeout:<init>(java.nio.channels.SelectableChannel,long)",58,66,"/**
* Initializes a socket with a specified timeout.
* @param channel the SelectableChannel to configure
* @param timeout the timeout duration in milliseconds
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,write,"org.apache.hadoop.net.SocketOutputStream:write(byte[],int,int)",111,129,"/**
* Processes byte array data using ByteBuffer.
* @param b byte array to process
* @param off starting offset in the array
* @param len length of data to process
* @throws IOException if an I/O error occurs or stream is closed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,transferToFully,"org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.LongWritable)",202,251,"/**
 * Transfers data from a file channel to another channel.
 * @param fileCh source file channel
 * @param position starting position in the source file
 * @param count number of bytes to transfer
 * @param waitForWritableTime time spent waiting for writability (optional)
 * @param transferToTime time spent on actual transfer (optional)
 * @throws IOException if an I/O error occurs
 */","* Transfers data from FileChannel using 
   * {@link FileChannel#transferTo(long, long, WritableByteChannel)}.
   * Updates <code>waitForWritableTime</code> and <code>transferToTime</code>
   * with the time spent blocked on the network and the time spent transferring
   * data from disk to network respectively.
   * 
   * Similar to readFully(), this waits till requested amount of 
   * data is transfered.
   * 
   * @param fileCh FileChannel to transfer data from.
   * @param position position within the channel where the transfer begins
   * @param count number of bytes to transfer.
   * @param waitForWritableTime nanoseconds spent waiting for the socket 
   *        to become writable
   * @param transferToTime nanoseconds spent transferring data
   * 
   * @throws EOFException 
   *         If end of input file is reached before requested number of 
   *         bytes are transfered.
   *
   * @throws SocketTimeoutException 
   *         If this channel blocks transfer longer than timeout for 
   *         this stream.
   *          
   * @throws IOException Includes any exception thrown by 
   *         {@link FileChannel#transferTo(long, long, WritableByteChannel)}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,normalizeHostNames,org.apache.hadoop.net.NetUtils:normalizeHostNames(java.util.Collection),662,668,"/**
* Masks collection of names.
* @param names list of original names
* @return list of masked names
*/","* Given a collection of string representation of hosts, return a list of
   * corresponding IP addresses in the textual representation.
   * 
   * @param names a collection of string representations of hosts
   * @return a list of corresponding IP addresses in the string format
   * @see #normalizeHostName(String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getHostDetailsAsString,"org.apache.hadoop.net.NetUtils:getHostDetailsAsString(java.lang.String,int,java.lang.String)",977,988,"/**
* Masks and formats host details.
* @param destHost destination host address
* @param destPort destination port number
* @param localHost local host address
* @return formatted string with masked host details
*/","* Get the host details as a string
   * @param destHost destinatioon host (nullable)
   * @param destPort destination port
   * @param localHost local host (nullable)
   * @return a string describing the destination host:port and the local host",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getIPs,"org.apache.hadoop.net.NetUtils:getIPs(java.lang.String,boolean)",1041,1068,"/**
 * Retrieves IP addresses within a specified subnet.
 * @param subnet CIDR notation of the subnet
 * @param returnSubinterfaces whether to include sub-interfaces' IPs
 * @return list of matching InetAddress objects
 */","* Return an InetAddress for each interface that matches the
   * given subnet specified using CIDR notation.
   *
   * @param subnet subnet specified using CIDR notation
   * @param returnSubinterfaces
   *            whether to return IPs associated with subinterfaces
   * @throws IllegalArgumentException if subnet is invalid
   * @return ips.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getFreeSocketPorts,org.apache.hadoop.net.NetUtils:getFreeSocketPorts(int),1098,1113,"/**
* Acquires a set of unique free ports.
* @param numOfPorts number of ports to acquire, must be 1-25
* @return set of acquired port numbers
*/","* Return free ports. There is no guarantee they will remain free, so
   * ports should be used immediately. The number of free ports returned by
   * this method should match argument {@code numOfPorts}. Num of ports
   * provided in the argument should not exceed 25.
   *
   * @param numOfPorts Number of free ports to acquire.
   * @return Free ports for binding a local socket.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,getConf,org.apache.hadoop.net.TableMapping:getConf(),71,74,"/**
 * Retrieves configuration using nested method calls.
 * @return Configuration object from nested methods
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,setConf,org.apache.hadoop.net.TableMapping:setConf(org.apache.hadoop.conf.Configuration),76,80,"/**
* Overrides and extends base method by calling additional methods.
* @param conf configuration settings to apply
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:<init>(),172,172,"/**
 * Default constructor for RawScriptBasedMapping.
 */","* Constructor. The mapping is not ready to use until
     * {@link #setConf(Configuration)} has been called",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/CachedDNSToSwitchMapping.java,<init>,org.apache.hadoop.net.CachedDNSToSwitchMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping),50,52,"/**
 * Initializes a cached DNS-to-switch mapping.
 * @param rawMapping the initial DNS-to-switch mapping to cache
 */","* cache a raw DNS mapping
   * @param rawMapping the raw mapping to cache",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,org.apache.hadoop.net.NodeBase:<init>(java.lang.String),53,61,"/**
* Initializes a NodeBase with a normalized path.
* @param path the file or directory path
*/","Construct a node from its path
   * @param path 
   *   a concatenation of this node's location, the path separator, and its name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,"org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String)",67,69,"/**
 * Initializes a new NodeBase with a name and normalized location.
 * @param name the name of the node
 * @param location the location to be normalized
 */","Construct a node from its name and its location
   * @param name this node's name (can be null, must not contain {@link #PATH_SEPARATOR})
   * @param location this node's location",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,"org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.Node,int)",77,81,"/**
* Constructs a new NodeBase.
* @param name node's name
* @param location node's location
* @param parent parent node reference
* @param level hierarchical level of the node
*/","Construct a node from its name and its location
   * @param name this node's name (can be null, must not contain {@link #PATH_SEPARATOR})
   * @param location this node's location 
   * @param parent this node's parent node
   * @param level this node's level in the tree",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,getConf,org.apache.hadoop.net.ScriptBasedMapping:getConf(),116,119,"/**
 * Retrieves configuration using nested method calls.
 * @return Configuration object from nested methods
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,toString,org.apache.hadoop.net.ScriptBasedMapping:toString(),121,124,"/**
 * Returns a script-based mapping string.
 * @return concatenated string from m1's m2 method
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:read(java.nio.ByteBuffer),602,628,"/**
* Reads data from a domain socket into a ByteBuffer.
* @param dst destination ByteBuffer for reading data
* @return number of bytes read, or -1 if no bytes were read
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,write,org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(int),563,575,"/**
 * Sends a single byte value over a domain socket.
 * @param val the byte value to send
 * @throws IOException if an I/O error occurs during transmission
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,write,"org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(byte[],int,int)",577,587,"/**
* Masks data in buffer.
* @param b byte array containing data
* @param off offset to start masking
* @param len length of data to mask
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read(),507,519,"/**
* Reads a byte from the domain socket.
* @return read byte value or -1 if error occurs
* @throws IOException if I/O error happens
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,"org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read(byte[],int,int)",521,532,"/**
* Reads data from a domain socket into a byte array.
* @param b destination buffer
* @param off offset in the buffer
* @param len number of bytes to read
* @return number of bytes read or -1 if end of stream
* @throws IOException on I/O error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,available,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:available(),534,545,"/**
* Returns the number of available bytes in the domain socket.
* @return number of available bytes
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,<init>,"org.apache.hadoop.net.unix.DomainSocket:<init>(java.lang.String,int)",168,172,"/**
* Initializes a DomainSocket with a path and file descriptor.
* @param path socket path
* @param fd file descriptor
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,sendCallback,"org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)",386,424,"/**
 * Handles sending callback for a file descriptor.
 * @param caller method name of the caller
 * @param entries map of file descriptors to entries
 * @param fdSet set of tracked file descriptors
 * @param fd file descriptor number
 * @return true if fd is closed, false otherwise
 */","* Send callback and return whether or not the domain socket was closed as a
   * result of processing.
   *
   * @param caller reason for call
   * @param entries mapping of file descriptor to entry
   * @param fdSet set of file descriptors
   * @param fd file descriptor
   * @return true if the domain socket was closed as a result of processing",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,isOpen,org.apache.hadoop.net.unix.DomainSocket:isOpen(),268,270,"/**
* Delegates to refCount's m1 method.
* @return result of refCount.m1()
*/","* Return true if the file descriptor is currently open.
   *
   * @return                 True if the file descriptor is currently open.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket:close(),344,388,"/**
 * Handles shutdown sequence by decrementing reference count and managing interruptions.
 * @throws IOException if an I/O error occurs during operations
 */",* Close the Socket.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,addNotificationSocket,"org.apache.hadoop.net.unix.DomainSocketWatcher:addNotificationSocket(java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet)",546,561,"/**
* Adds a notification socket to the entries and fdSet.
* @param entries TreeMap containing integer keys and Entry values
* @param fdSet set of file descriptors
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,trimIdleSelectors,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:trimIdleSelectors(long),426,444,"/**
* Masks selectors older than a specified timeout.
* @param now current timestamp in milliseconds
*/","* Closes selectors that are idle for IDLE_TIMEOUT (10 sec). It does not
     * traverse the whole list, just over the one that have crossed 
     * the timeout.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,isLeafParent,org.apache.hadoop.net.InnerNodeImpl:isLeafParent(),302,304,"/**
 * Determines if mask function is active.
 * @return true if mask is enabled, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,getNextAncestorName,org.apache.hadoop.net.InnerNodeImpl:getNextAncestorName(org.apache.hadoop.net.Node),113,127,"/**
* Masks the node's name based on ancestor relationship.
* @param n target Node
* @return masked name of the node
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int)",70,76,"/**
* Creates and connects a socket to the specified address and port.
* @param addr IP address to connect to
* @param port port number to connect to
* @return connected Socket object
* @throws IOException if an I/O error occurs during connection
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",78,86,"/**
* Establishes a socket connection with specified remote and local addresses.
* @param addr remote address to connect to
* @param port remote port number
* @param localHostAddr local address to bind to
* @param localPort local port number
* @return connected Socket object
* @throws IOException if an I/O error occurs during socket creation or binding
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int)",88,95,"/**
* Creates and connects to a socket at the specified host and port.
* @param host IP address or hostname of the server
* @param port port number on the server
* @return connected Socket object
* @throws IOException if an I/O error occurs
* @throws UnknownHostException if the host is unknown
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",97,106,"/**
 * Establishes a connection to a remote host.
 * @param host remote server address
 * @param port remote server port
 * @param localHostAddr local address to bind the socket
 * @param localPort local port to bind the socket
 * @return connected Socket object
 * @throws IOException if an I/O error occurs
 * @throws UnknownHostException if the IP address of a host could not be determined
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int)",65,71,"/**
* Establishes a socket connection.
* @param addr IP address to connect to
* @param port port number for the connection
* @return connected Socket object
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",73,81,"/**
* Establishes a socket connection.
* @param addr remote address to connect
* @param port remote port number
* @param localHostAddr local address to bind
* @param localPort local port number
* @return connected Socket object
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int)",83,90,"/**
* Establishes a socket connection to a specified host and port.
* @param host the target server's hostname or IP address
* @param port the target server's port number
* @return connected Socket object
* @throws IOException if an I/O error occurs during connection
* @throws UnknownHostException if the host is unknown
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",92,101,"/**
 * Establishes a socket connection to a remote host and binds it to a local address.
 * @param host remote host name
 * @param port remote port number
 * @param localHostAddr local host address for binding
 * @param localPort local port number for binding
 * @return connected Socket object
 * @throws IOException if an I/O error occurs
 * @throws UnknownHostException if the IP address of a host could not be determined
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,toString,org.apache.hadoop.net.ScriptBasedMappingWithDependency:toString(),71,74,"/**
* Returns script-based mapping string.
* @return concatenated string from script-based mapping and result of m1().m2()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,getDependency,org.apache.hadoop.net.ScriptBasedMappingWithDependency:getDependency(java.lang.String),96,115,"/**
* Retrieves dependencies for a given name.
* @param name the input name to fetch dependencies for
* @return list of dependencies or empty list if none found
*/","* Get dependencies in the topology for a given host
   * @param name - host name for which we are getting dependency
   * @return a list of hosts dependent on the provided host name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,setTimeout,org.apache.hadoop.net.SocketInputWrapper:setTimeout(long),69,75,"/**
* Sets read timeout on input stream.
* @param timeoutMs timeout duration in milliseconds
* @throws SocketException if an I/O error occurs
*/","* Set the timeout for reads from this stream.
   * 
   * Note: the behavior here can differ subtly depending on whether the
   * underlying socket has an associated Channel. In particular, if there is no
   * channel, then this call will affect the socket timeout for <em>all</em>
   * readers of this socket. If there is a channel, then this call will affect
   * the timeout only for <em>this</em> stream. As such, it is recommended to
   * only create one {@link SocketInputWrapper} instance per socket.
   * 
   * @param timeoutMs
   *          the new timeout, 0 for no timeout
   * @throws SocketException
   *           if the timeout cannot be set",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPs,"org.apache.hadoop.net.DNS:getIPs(java.lang.String,boolean)",175,209,"/**
* Retrieves IP addresses of a network interface.
* @param strInterface name of the network interface
* @param returnSubinterfaces if true, includes subinterface IPs
* @return array of IP addresses or cached host address if interface is default
*/","* Returns all the IPs associated with the provided interface, if any, in
   * textual form.
   * 
   * @param strInterface
   *            The name of the network interface or sub-interface to query
   *            (eg eth0 or eth0:0) or the string ""default""
   * @param returnSubinterfaces
   *            Whether to return IPs associated with subinterfaces of
   *            the given interface
   * @return A string vector of all the IPs associated with the provided
   *         interface. The local host IP is returned if the interface
   *         name ""default"" is specified or there is an I/O error looking
   *         for the given interface.
   * @throws UnknownHostException
   *             If the given interface is invalid
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPsAsInetAddressList,"org.apache.hadoop.net.DNS:getIPsAsInetAddressList(java.lang.String,boolean)",428,457,"/**
* Retrieves network interface addresses.
* @param strInterface name of the network interface
* @param returnSubinterfaces whether to include subinterface addresses
* @return list of InetAddress objects for the specified interface
*/","* Returns all the IPs associated with the provided interface, if any, as
   * a list of InetAddress objects.
   *
   * @param strInterface
   *            The name of the network interface or sub-interface to query
   *            (eg eth0 or eth0:0) or the string ""default""
   * @param returnSubinterfaces
   *            Whether to return IPs associated with subinterfaces of
   *            the given interface
   * @return A list of all the IPs associated with the provided
   *         interface. The local host IP is returned if the interface
   *         name ""default"" is specified or there is an I/O error looking
   *         for the given interface.
   * @throws UnknownHostException
   *             If the given interface is invalid
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,read,"org.apache.hadoop.net.SocketInputStream:read(byte[],int,int)",129,132,"/**
* Wraps byte array in ByteBuffer and delegates to m2.
* @param b byte array containing data
* @param off offset within the byte array
* @param len length of data to process
* @return result from m2 method call
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getRack,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getRack(java.lang.String),57,80,"/**
* Masks location string based on node properties.
* @param loc original location string
* @return masked location or original if no masking needed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getNodeGroup,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeGroup(java.lang.String),90,118,"/**
* Processes location string and returns modified or root value.
* @param loc input location string
* @return processed location or NodeBase.ROOT if conditions met
*/","* Given a string representation of a node group for a specific network
   * location
   * 
   * @param loc
   *            a path-like string representation of a network location
   * @return a node group string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,dumpTopology,org.apache.hadoop.net.AbstractDNSToSwitchMapping:dumpTopology(),112,133,"/**
* Generates a masked string representation of network topology.
* @return String containing mapped nodes and switches, or error message if no data
*/","* Generate a string listing the switch mapping implementation,
   * the mapping for every known node and the number of nodes and
   * unique switches known about -each entry to a separate line.
   * @return a string that can be presented to the ops team or used in
   * debug messages.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,isMappingSingleSwitch,org.apache.hadoop.net.AbstractDNSToSwitchMapping:isMappingSingleSwitch(org.apache.hadoop.net.DNSToSwitchMapping),150,153,"/**
* Checks if DNSToSwitchMapping is valid and calls m1.
* @param mapping the DNSToSwitchMapping to check
* @return true if mapping is valid and m1 returns true, false otherwise
*/","* Query for a {@link DNSToSwitchMapping} instance being on a single
   * switch.
   * <p>
   * This predicate simply assumes that all mappings not derived from
   * this class are multi-switch.
   * @param mapping the mapping to query
   * @return true if the base class says it is single switch, or the mapping
   * is not derived from this class.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getWeight,"org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getWeight(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",256,271,"/**
* Determines the mask value for a given node based on reader conditions.
* @param reader Node used to evaluate conditions
* @param node Target node to apply mask
* @return Integer representing the mask weight (0, 1, 2, or 3)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,writeAttribute,"org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)",330,388,"/**
 * Generates JSON for a MBean attribute.
 * @param jg JsonGenerator object to write JSON data
 * @param oname ObjectName of the MBean
 * @param attr MBeanAttributeInfo describing the attribute
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,writeObject,"org.apache.hadoop.jmx.JMXJsonServlet:writeObject(com.fasterxml.jackson.core.JsonGenerator,java.lang.Object,java.lang.String)",395,436,"/**
 * Writes a value to JSON generator.
 * @param jg JsonGenerator instance
 * @param value Object to write
 * @param attName attribute name (unused)
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,getCurrentStats,"org.apache.hadoop.log.LogThrottlingHelper:getCurrentStats(java.lang.String,int)",290,297,"/**
* Retrieves statistics for a specific index from the current log.
* @param recorderName name of the recorder
* @param idx index of the statistics to retrieve
* @return SummaryStatistics object or null if not found
*/","* Return the summary information for given index.
   *
   * @param recorderName The name of the recorder.
   * @param idx The index value.
   * @return The summary information.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseProtocolArgs,"org.apache.hadoop.log.LogLevel$CLI:parseProtocolArgs(java.lang.String[],int)",209,228,"/**
 * Parses and validates the -protocol command.
 * @param args array of command line arguments
 * @param index current index in args array
 * @return updated index after processing
 * @throws HadoopIllegalArgumentException if invalid usage or protocol
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,printUsage,org.apache.hadoop.log.LogLevel:printUsage(),87,90,"/**
 * Prints usage information and parses command-line options.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,printGenericCommandUsage,org.apache.hadoop.util.ToolRunner:printGenericCommandUsage(java.io.PrintStream),105,107,"/**
 * Delegates to GenericOptionsParser.m1 with provided PrintStream.
 * @param out the PrintStream to be used by GenericOptionsParser.m1
 */","* Prints generic command-line argurments and usage information.
   * 
   *  @param out stream to write usage information to.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericsUtil.java,isLog4jLogger,org.apache.hadoop.util.GenericsUtil:isLog4jLogger(java.lang.Class),95,100,"/**
* Checks if class or its parent has annotation.
* @param clazz Class to check
* @return true if annotation found, false otherwise
*/","* Determine whether the log of <code>clazz</code> is Log4j implementation.
   * @param clazz a class to be determined
   * @return true if the log of <code>clazz</code> is Log4j implementation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,<init>,"org.apache.hadoop.log.LogThrottlingHelper:<init>(long,java.lang.String)",159,161,"/**
* Initializes log throttling with specified period and recorder.
* @param minLogPeriodMs minimum time between logs in milliseconds
* @param primaryRecorderName name of the primary log recorder
*/","* Create a log helper with a specified primary recorder name; this can be
   * used in conjunction with {@link #record(String, long, double...)} to set up
   * primary and dependent recorders. See
   * {@link #record(String, long, double...)} for more details.
   *
   * @param minLogPeriodMs The minimum period with which to log; do not log
   *                       more frequently than this.
   * @param primaryRecorderName The name of the primary recorder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,record,"org.apache.hadoop.log.LogThrottlingHelper:record(java.lang.String,long,double[])",247,281,"/**
* Logs action with masked values.
* @param recorderName name of the recorder
* @param currentTimeMs current timestamp in milliseconds
* @param values variable number of double values to log
* @return LoggingAction object or DO_NOT_LOG if not logged
*/","* Record some set of values at the specified time into this helper. This can
   * be useful to avoid fetching the current time twice if the caller has
   * already done so for other purposes. This additionally allows the caller to
   * specify a name for this recorder. When multiple names are used, one is
   * denoted as the primary recorder. Only recorders named as the primary
   * will trigger logging; other names not matching the primary can <i>only</i>
   * be triggered by following the primary. This is used to coordinate multiple
   * logging points. A primary can be set via the
   * {@link #LogThrottlingHelper(long, String)} constructor. If no primary
   * is set in the constructor, then the first recorder name used becomes the
   * primary.
   *
   * If multiple names are used, they maintain entirely different sets of values
   * and summary information. For example:
   * <pre>{@code
   *   // Initialize ""pre"" as the primary recorder name
   *   LogThrottlingHelper helper = new LogThrottlingHelper(1000, ""pre"");
   *   LogAction preLog = helper.record(""pre"", Time.monotonicNow());
   *   if (preLog.shouldLog()) {
   *     // ...
   *   }
   *   double eventsProcessed = ... // perform some action
   *   LogAction postLog =
   *       helper.record(""post"", Time.monotonicNow(), eventsProcessed);
   *   if (postLog.shouldLog()) {
   *     // ...
   *     // Can use postLog.getStats(0) to access eventsProcessed information
   *   }
   * }</pre>
   * Since ""pre"" is the primary recorder name, logging to ""pre"" will trigger a
   * log action if enough time has elapsed. This will indicate that ""post""
   * should log as well. This ensures that ""post"" is always logged in the same
   * iteration as ""pre"", yet each one is able to maintain its own summary
   * information.
   *
   * <p>Other behavior is the same as {@link #record(double...)}.
   *
   * @param recorderName The name of the recorder. This is used to check if the
   *                     current recorder is the primary. Other names are
   *                     arbitrary and are only used to differentiate between
   *                     distinct recorders.
   * @param currentTimeMs The current time.
   * @param values The values to log.
   * @return The LogAction for the specified recorder.
   *
   * @see #record(double...)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,fromInternalName,org.apache.hadoop.http.ProfileServlet$Event:fromInternalName(java.lang.String),148,156,"/**
* Retrieves an event by name.
* @param name event identifier
* @return Event object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfilerDisabledServlet.java,doGet,"org.apache.hadoop.http.ProfilerDisabledServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",34,48,"/**
* Handles requests when profiler servlet is disabled.
* Sets response status and sends error message.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,<init>,org.apache.hadoop.http.ProfileServlet:<init>(),177,181,"/**
* Initializes servlet with async profiler home and process ID.
* Sets up logging for servlet initialization details.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,needsQuoting,org.apache.hadoop.http.HtmlQuoting:needsQuoting(java.lang.String),68,74,"/**
* Checks if string is valid UTF-8.
* @param str input string to check
* @return true if valid, false otherwise
*/","* Does the given string need to be quoted?
   * @param str the string to check
   * @return does the string contain any of the active html characters?",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,quoteHtmlChars,org.apache.hadoop.http.HtmlQuoting:quoteHtmlChars(java.lang.String),114,131,"/**
 * Processes a string by encoding and decoding it if condition is met.
 * @param item input string to process
 * @return processed string or original if condition fails, null on error
 */","* Quote the given item to make it html-safe.
   * @param item the string to quote
   * @return the quoted string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addJerseyResourcePackage,"org.apache.hadoop.http.HttpServer2:addJerseyResourcePackage(java.lang.String,java.lang.String)",1018,1022,"/**
 * Calls overloaded m2 with default configuration.
 * @param packageName package name to process
 * @param pathSpec path specification for processing
 */","* Add a Jersey resource package.
   * @param packageName The Java package name containing the Jersey resource.
   * @param pathSpec The path spec for the servlet",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addServlet,"org.apache.hadoop.http.HttpServer2:addServlet(java.lang.String,java.lang.String,java.lang.Class)",1050,1053,"/**
 * Registers a servlet with optional masking.
 * @param name servlet name
 * @param pathSpec URL path specification
 * @param clazz servlet class
 */","* Add a servlet in the server.
   * @param name The name of the servlet (can be passed as null)
   * @param pathSpec The path spec for the servlet
   * @param clazz The servlet class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addInternalServlet,"org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class)",1065,1068,"/**
* Registers servlet with optional async support.
* @param name servlet name
* @param pathSpec URL pattern for servlet
* @param clazz servlet class type
*/","* Add an internal servlet in the server.
   * Note: This method is to be used for adding servlets that facilitate
   * internal communication and not for user facing functionality. For
   * servlets added using this method, filters are not enabled.
   *
   * @param name The name of the servlet (can be passed as null)
   * @param pathSpec The path spec for the servlet
   * @param clazz The servlet class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addFilter,"org.apache.hadoop.http.HttpServer2:addFilter(java.lang.String,java.lang.String,java.util.Map)",1170,1193,"/**
 * Adds a filter to specified contexts.
 * @param name filter name
 * @param classname filter class name
 * @param parameters filter configuration parameters
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addGlobalFilter,"org.apache.hadoop.http.HttpServer2:addGlobalFilter(java.lang.String,java.lang.String,java.util.Map)",1195,1206,"/**
 * Adds a global filter to the web application.
 * @param name unique filter name
 * @param classname fully qualified class name of the filter
 * @param parameters configuration parameters for the filter
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,defineFilter,"org.apache.hadoop.http.HttpServer2:defineFilter(org.eclipse.jetty.servlet.ServletContextHandler,java.lang.String,java.lang.String,java.util.Map,java.lang.String[])",1217,1222,"/**
* Configures a servlet context with a filter.
* @param ctx ServletContextHandler instance
* @param name filter name
* @param classname fully qualified class name of the filter
* @param parameters map of initialization parameters
* @param urls array of URL patterns to apply the filter
*/","* Define a filter for a context and set up default url mappings.
   *
   * @param ctx ctx.
   * @param name name.
   * @param classname classname.
   * @param parameters parameters.
   * @param urls urls.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,bindForSinglePort,"org.apache.hadoop.http.HttpServer2:bindForSinglePort(org.eclipse.jetty.server.ServerConnector,int)",1478,1493,"/**
 * Establishes a connection using the provided listener and port.
 * Retries on IOException if findPort is true.
 * @param listener ServerConnector instance for handling connections
 * @param port initial port number to attempt connection
 * @throws Exception if connection fails after retries
 */","* Bind using single configured port. If findPort is true, we will try to bind
   * after incrementing port till a free port is found.
   * @param listener jetty listener.
   * @param port port which is set in the listener.
   * @throws Exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,toString,org.apache.hadoop.http.HttpServer2:toString(),1631,1642,"/**
* Returns server status and listener details.
* @return formatted string with server state and listening addresses
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getEnum,org.apache.hadoop.http.HttpServer2$XFrameOption:getEnum(java.lang.String),1946,1954,"/**
* Converts string to XFrameOption.
* @param value string representation of XFrameOption
* @return corresponding XFrameOption or throws exception if invalid
*/","* We cannot use valueOf since the AllowFrom enum differs from its value
     * Allow-From. This is a helper method that does exactly what valueof does,
     * but allows us to handle the AllowFrom issue gracefully.
     *
     * @param value - String must be DENY, SAMEORIGIN or ALLOW-FROM.
     * @return XFrameOption or throws IllegalException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,init,org.apache.hadoop.http.HttpServer2$QuotingInputFilter:init(javax.servlet.FilterConfig),1860,1864,"/**
 * Initializes filter with configuration.
 * @param config Filter configuration object
 * @throws ServletException on initialization failure
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,doFilter,"org.apache.hadoop.http.HttpServer2$QuotingInputFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",1870,1893,"/**
* Filters and sets response headers based on request MIME type.
* @param request client request
* @param response server response
* @param chain filter chain for processing the request
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,init,org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter:init(javax.servlet.FilterConfig),114,118,"/**
* Initializes user from configuration.
* @param conf Filter configuration object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileMonitoringTimerTask.java,<init>,"org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.util.List,java.util.function.Consumer,java.util.function.Consumer)",75,89,"/**
 * Initializes a file monitoring task.
 * @param filePaths list of paths to monitor
 * @param onFileChange action to perform on file change
 * @param onChangeFailure action to handle change failure
 */","* Create file monitoring task to be scheduled using a standard
   * Java {@link java.util.Timer} instance.
   *
   * @param filePaths The path to the file to monitor.
   * @param onFileChange The function to call when the file has changed.
   * @param onChangeFailure The function to call when an exception is
   *                       thrown during the file change processing.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getNonNegative,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNonNegative(java.lang.String,int)",408,417,"/**
* Retrieves and validates a non-negative integer property.
* @param key the property key to fetch
* @param defaultValue default value if property is not found
* @return the non-negative property value
* @throws MetricsException if the property value is negative
*/","* Return the property value if it's non-negative and throw an exception if
   * it's not.
   *
   * @param key the property key
   * @param defaultValue the default value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkIfPropertyExists,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkIfPropertyExists(java.lang.String),424,429,"/**
* Validates metrics configuration property.
* @param key property name to validate
* @throws MetricsException if property is missing
*/","* Throw a {@link MetricsException} if the given property is not set.
   *
   * @param key the key to validate",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkForErrors,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkForErrors(java.lang.String),905,910,"/**
* Throws MetricsException if conditions are met.
* @param message error message to include
*/","* If the sink isn't set to ignore errors, throw a {@link MetricsException}
   * if the stream encountered an exception.  The message parameter will be used
   * as the new exception's message with the current file name
   * ({@link #currentFilePath}) appended to it.
   *
   * @param message the exception message. The message will have a colon and
   * the current file name ({@link #currentFilePath}) appended to it.
   * @throws MetricsException thrown if there was an error and the sink isn't
   * ignoring errors",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,throwMetricsException,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String),940,944,"/**
* Throws MetricsException with error message and file path.
* @param message custom error message
*/","* If the sink isn't set to ignore errors, throw a new
   * {@link MetricsException}.  The message parameter will be used  as the
   * new exception's message with the current file name
   * ({@link #currentFilePath}) appended to it.
   *
   * @param message the exception message. The message will have a colon and
   * the current file name ({@link #currentFilePath}) appended to it.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String),29,31,"/**
 * Constructs a new MetricsConfigException with the specified message.
 * @param message detailed error message
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,checkMetricName,org.apache.hadoop.metrics2.lib.MetricsRegistry:checkMetricName(java.lang.String),434,452,"/**
* Validates metric name for whitespace and uniqueness.
* @param name metric name to validate
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,checkTagName,org.apache.hadoop.metrics2.lib.MetricsRegistry:checkTagName(java.lang.String),454,458,"/**
* Checks and throws an exception if a tag already exists.
* @param name the tag name to check
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsFactory.java,getInstance,org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getInstance(java.lang.Class),37,46,"/**
* Retrieves or creates a metrics factory instance.
* @param <T> generic type of the metrics factory
* @param cls class type of the metrics factory
* @return instance of the specified metrics factory
* @throws MetricsException if unknown factory type is requested
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,build,org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:build(),76,92,"/**
 * Returns a MetricsSource instance.
 * Throws MetricsException if conditions are not met.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newTag,org.apache.hadoop.metrics2.lib.MethodMetric:newTag(java.lang.Class),125,139,"/**
 * Creates a MutableMetric for String type.
 * @param resType the result type class
 * @return MutableMetric instance for String, throws exception if unsupported
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getRollInterval,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getRollInterval(),341,399,"/**
* Parses and converts a roll interval string to milliseconds.
* @return Milliseconds equivalent of the roll interval
*/","* Extract the roll interval from the configuration and return it in
   * milliseconds.
   *
   * @return the roll interval in millis",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,throwMetricsException,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String,java.lang.Throwable)",924,929,"/**
* Throws a MetricsException with error details.
* @param message custom error message
* @param t Throwable object containing exception details
*/","* If the sink isn't set to ignore errors, wrap the Throwable in a
   * {@link MetricsException} and throw it.  The message parameter will be used
   * as the new exception's message with the current file name
   * ({@link #currentFilePath}) and the Throwable's string representation
   * appended to it.
   *
   * @param message the exception message. The message will have a colon, the
   * current file name ({@link #currentFilePath}), and the Throwable's string
   * representation (wrapped in square brackets) appended to it.
   * @param t the Throwable to wrap",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/FileSink.java,init,org.apache.hadoop.metrics2.sink.FileSink:init(org.apache.commons.configuration2.SubsetConfiguration),45,55,"/**
* Initializes writer for logging.
* @param conf configuration object containing file settings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String,java.lang.Throwable)",33,35,"/**
 * Constructs a new MetricsConfigException with a specified message and cause.
 * @param message descriptive error message
 * @param cause underlying exception causing this one
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.Throwable),37,39,"/**
 * Constructs a new MetricsConfigException with the specified cause.
 * @param cause the underlying cause of this exception
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,equals,org.apache.hadoop.metrics2.MetricsTag:equals(java.lang.Object),71,78,"/**
* Compares this object with another for equality.
* @param obj the object to compare
* @return true if objects are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,toString,org.apache.hadoop.metrics2.MetricsTag:toString(),84,89,"/**
* Constructs a formatted string with class information and values.
* @return formatted string containing class details and member values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,appendPrefix,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:appendPrefix(org.apache.hadoop.metrics2.MetricsRecord,java.lang.StringBuilder)",86,106,"/**
 * Appends selected tags from a metrics record to a StringBuilder.
 * @param record the MetricsRecord containing context and tags
 * @param sb StringBuilder to append formatted tag data
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,accepts,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(org.apache.hadoop.metrics2.MetricsTag),102,119,"/**
* Checks tag against include and exclude patterns.
* @param tag the MetricsTag to evaluate
* @return true if tag matches include patterns or doesn't match exclude patterns, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,accepts,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.Iterable),121,142,"/**
* Determines if tags match include/exclude patterns.
* @param tags iterable collection of MetricsTag objects
* @return true if tags match include patterns and not in exclude, else false
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordImpl.java,context,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:context(),72,80,"/**
* Retrieves context value from tags.
* @return context string or default if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,"org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsInfo,java.lang.Object)",61,63,"/**
 * Masks and builds metric string.
 * @param info MetricsInfo object containing metadata
 * @param value Object with data to mask
 * @return MetricStringBuilder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsTag),86,89,"/**
 * Applies mask to metrics record using provided tags.
 * @param tag MetricsTag object containing mask details
 * @return Modified MetricsRecordBuilder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,setContext,org.apache.hadoop.metrics2.MetricStringBuilder:setContext(java.lang.String),97,100,"/**
 * Sets context in metrics record.
 * @param value context string to be set
 * @return updated MetricsRecordBuilder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/AbstractMetric.java,equals,org.apache.hadoop.metrics2.AbstractMetric:equals(java.lang.Object),75,82,"/**
* Compares current metric with another for equality.
* @param obj object to compare
* @return true if metrics are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/FileSink.java,putMetrics,org.apache.hadoop.metrics2.sink.FileSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),57,80,"/**
 * Writes metrics record to output.
 * @param record MetricsRecord object containing data to write
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,update,"org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord,boolean)",154,177,"/**
* Masks metrics record, caching and updating as needed.
* @param mr source MetricsRecord to process
* @param includingTags flag to include tags in the record
* @return processed Record object
*/","* Update the cache and return the current cached record
   * @param mr the update record
   * @param includingTags cache tag values (for later lookup by name) if true
   * @return the updated cache record",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,loadGangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:loadGangliaConf(org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaConfType),185,218,"/**
* Processes configuration properties for a given Ganglia type.
* @param gtype the Ganglia configuration type to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,xdr_string,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:xdr_string(java.lang.String),245,252,"/**
* Masks a string by converting it to bytes and processing with system methods.
* @param s the input string to mask
*/","* Puts a string into the buffer by first writing the size of the string as an
   * int, followed by the bytes of the string, padded if necessary to a multiple
   * of 4.
   * @param s the string to be written to buffer at offset location",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.java,emitMetric,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",47,104,"/**
 * Emits a metric with specified parameters.
 * @param groupName name of the group
 * @param name metric name
 * @param type metric type
 * @param value metric value
 * @param gConf Ganglia configuration
 * @param gSlope Ganglia slope
 * @throws IOException if an I/O error occurs
 */","* The method sends metrics to Ganglia servers. The method has been taken from
   * org.apache.hadoop.metrics.ganglia.GangliaContext31 with minimal changes in
   * order to keep it in sync.
   * @param groupName The group name of the metric
   * @param name The metric name
   * @param type The type of the metric
   * @param value The value of the metric
   * @param gConf The GangliaConf for this metric
   * @param gSlope The slope for this metric
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,emitMetric,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",221,252,"/**
* Emits a metric with specified details.
* @param groupName group name of the metric
* @param name unique name of the metric
* @param type data type of the metric value
* @param value actual value of the metric
* @param gConf configuration settings for Ganglia
* @param gSlope slope information for the metric
* @throws IOException if an I/O error occurs during emission
*/","* The method sends metrics to Ganglia servers. The method has been taken from
   * org.apache.hadoop.metrics.ganglia.GangliaContext30 with minimal changes in
   * order to keep it in sync.
   * @param groupName The group name of the metric
   * @param name The metric name
   * @param type The type of the metric
   * @param value The value of the metric
   * @param gConf The GangliaConf for this metric
   * @param gSlope The slope for this metric
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,calculateSlope,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:calculateSlope(org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",196,207,"/**
* Determines GangliaSlope based on configuration and metric.
* @param gConf Ganglia configuration object
* @param slopeFromMetric Slope derived from metric data
* @return GangliaSlope value, preferring config over metric, default if none specified
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,flush,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:flush(),176,180,"/**
 * Calls writer's m2 method if m1 returns true.
 * @throws IOException if an I/O error occurs during m2 execution
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,connect,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:connect(),143,165,"/**
* Establishes a connection to the Graphite server.
* Throws an exception if already connected or on connection failure.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,close,org.apache.hadoop.metrics2.sink.GraphiteSink:close(),124,127,"/**
 * Delegates to graphite's m1 method.
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,init,org.apache.hadoop.metrics2.sink.StatsDSink:init(org.apache.commons.configuration2.SubsetConfiguration),78,95,"/**
* Configures the system with settings from SubsetConfiguration.
* @param conf configuration object containing server and service details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,close,org.apache.hadoop.metrics2.sink.StatsDSink:close(),163,166,"/**
 * Delegates call to statsd's m1 method.
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,putMetrics,org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),68,82,"/**
* Processes metrics record, updating Prometheus metrics for counters and gauges.
* @param metricsRecord the MetricsRecord to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,getMetricKey,"org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:getMetricKey(java.lang.String,org.apache.hadoop.metrics2.AbstractMetric,java.util.List)",165,174,"/**
 * Masks and modifies the metric key based on pattern matching.
 * @param promMetricKey original metric key string
 * @param metric AbstractMetric object containing metric data
 * @param extendTags list to store extended tags
 * @return masked metric key or original if no match
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,init,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:init(org.apache.commons.configuration2.SubsetConfiguration),54,84,"/**
* Processes configuration for filtering and tagging.
* @param conf configuration settings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsBufferBuilder.java,add,"org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:add(java.lang.String,java.lang.Iterable)",29,31,"/**
 * Checks metrics entries.
 * @param name entry name
 * @param records iterable of metric records
 * @return boolean result of the check
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsBufferBuilder.java,get,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:get(),33,35,"/**
 * Creates a new MetricsBuffer instance.
 * @return MetricsBuffer object initialized with current context
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:<init>(org.apache.hadoop.metrics2.impl.MetricsBuffer),240,242,"/**
 * Initializes a new WaitableMetricsBuffer with the given MetricsBuffer.
 * @param metricsBuffer the underlying buffer to manage metrics
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,dequeue,org.apache.hadoop.metrics2.impl.SinkQueue:dequeue(),101,108,"/**
* Synchronizes and processes data.
* Blocks until data is available.
* @return processed result of type T
*/","* Dequeue one element from head of the queue, will block if queue is empty
   * @return  the first element
   * @throws InterruptedException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,clear,org.apache.hadoop.metrics2.impl.SinkQueue:clear(),154,161,"/**
* Clears all elements and resets size.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,waitForData,org.apache.hadoop.metrics2.impl.SinkQueue:waitForData(),110,118,"/**
 * Executes operations with synchronization.
 * @throws InterruptedException if thread is interrupted during wait
 * @return result of m4()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(org.apache.hadoop.metrics2.MetricsSystem$Callback),308,311,"/**
 * Masks and processes a callback.
 * @param callback the callback to be processed
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)",313,315,"/**
 * Masks and processes a callback by name.
 * @param name unique identifier for the callback
 * @param callback the callback to be processed
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,incr,org.apache.hadoop.metrics2.lib.MutableCounterLong:incr(),42,45,"/**
 * Calls overloaded method m1 with default value 1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrSentBytes,org.apache.hadoop.ipc.metrics.RpcMetrics:incrSentBytes(int),256,258,"/**
 * Masks data by sending a count to sentBytes.
 * @param count number of bytes to mask
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrReceivedBytes,org.apache.hadoop.ipc.metrics.RpcMetrics:incrReceivedBytes(int),265,267,"/**
 * Masks data by setting bits in received bytes.
 * @param count number of bits to mask
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordImpl.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsRecordImpl:<init>(org.apache.hadoop.metrics2.MetricsInfo,long,java.util.List,java.lang.Iterable)",47,54,"/**
* Initializes a MetricsRecordImpl with specified info, timestamp, tags, and metrics.
* @param info MetricsInfo object containing record information
* @param timestamp record creation time in milliseconds
* @param tags list of MetricsTag objects associated with the record
* @param metrics iterable collection of AbstractMetric objects representing metric data
*/","* Construct a metrics record
   * @param info  {@link MetricsInfo} of the record
   * @param timestamp of the record
   * @param tags  of the record
   * @param metrics of the record",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,toString,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:toString(),46,54,"/**
* Constructs a formatted string representation.
* @return formatted string with various attributes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,hashCode,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:hashCode(),42,44,"/**
 * Calls Objects.m4 with results of m1, m2, and m3.
 * @return Result of Objects.m4 invocation
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,equals,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:equals(java.lang.Object),29,39,"/**
* Compares this record with another for equality.
* @param obj the object to compare
* @return true if records are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,newAttrInfo,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(java.lang.String,java.lang.String,java.lang.String)",53,56,"/**
 * Creates an MBean attribute info with read-only access.
 * @param name attribute name
 * @param desc attribute description
 * @param type attribute data type
 * @return MBeanAttributeInfo object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,setAttrCacheTag,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheTag(org.apache.hadoop.metrics2.MetricsTag,int)",279,282,"/**
* Updates attribute cache with new metric data.
* @param tag MetricsTag object containing metric information
* @param recNo Record number for the metric
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,setAttrCacheMetric,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheMetric(org.apache.hadoop.metrics2.AbstractMetric,int)",296,299,"/**
* Updates attribute cache with metric data.
* @param metric the AbstractMetric object containing metric information
* @param recNo the record number associated with the metric
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,refreshQueueSizeGauge,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:refreshQueueSizeGauge(),168,170,"/**
* Updates queue size using current queue metrics.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/UniqueNames.java,uniqueName,org.apache.hadoop.metrics2.lib.UniqueNames:uniqueName(java.lang.String),47,65,"/**
* Generates a unique name by appending a counter to the input.
* @param name base name for which to generate a unique version
* @return unique name with appended counter if necessary
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,incr,org.apache.hadoop.metrics2.lib.MutableCounterInt:incr(),41,44,"/**
 * Calls overloaded method m1 with default value 1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableCounterInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",59,65,"/**
* Adds metrics to the record builder.
* @param builder MetricsRecordBuilder instance to add data to
* @param all flag indicating whether to include all metrics
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,<init>,org.apache.hadoop.metrics2.lib.MutableQuantiles$RolloverSample:<init>(org.apache.hadoop.metrics2.lib.MutableQuantiles),236,238,"/**
 * Constructs a new RolloverSample with the specified parent.
 * @param parent MutableQuantiles instance to associate with this sample
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableQuantiles:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",129,146,"/**
 * Updates metrics record with quantile information.
 * @param builder MetricsRecordBuilder to update
 * @param all true to include all metrics, false for filtered updates
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addGetGroups,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:addGetGroups(long),155,162,"/**
 * Processes latency by updating group and quantile metrics.
 * @param latency the latency value to process
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcEnQueueTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcEnQueueTime(long),277,284,"/**
* Records enqueue time and updates quantiles if enabled.
* @param enQTime the timestamp of the enqueue event
*/","* Sometimes, the request time observed by the client is much longer than
   * the queue + process time on the RPC server.Perhaps the RPC request
   * 'waiting enQueue' took too long on the RPC server, so we should add
   * enQueue time to RpcMetrics. See HADOOP-18840 for details.
   * Add an RPC enqueue time sample
   * @param enQTime the queue time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcQueueTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcQueueTime(long),290,297,"/**
* Records queue time and updates quantiles if enabled.
* @param qTime the queue time to record
*/","* Add an RPC queue time sample
   * @param qTime the queue time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcLockWaitTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcLockWaitTime(long),299,306,"/**
* Updates RPC lock wait time metrics.
* @param waitTime duration of the wait time in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcProcessingTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcProcessingTime(long),312,319,"/**
* Records processing time and updates quantiles if enabled.
* @param processingTime the time taken to process a request in milliseconds
*/","* Add an RPC processing time sample
   * @param processingTime the processing time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcResponseTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcResponseTime(long),321,328,"/**
* Records RPC response time.
* @param responseTime duration of the RPC call in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addDeferredRpcProcessingTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addDeferredRpcProcessingTime(long),330,337,"/**
 * Records processing time and updates quantiles if enabled.
 * @param processingTime the time taken for processing in milliseconds
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,addWriteFileLatency,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addWriteFileLatency(long),110,116,"/**
* Records write latency in quantile measurements.
* @param writeLatency time taken for a write operation
*/","* Add the file write latency to {@link MutableQuantiles} metrics.
   *
   * @param writeLatency file write latency in microseconds",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,addReadFileLatency,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addReadFileLatency(long),123,129,"/**
* Records read latency in quantile metrics.
* @param readLatency time taken to read data
*/","* Add the file read latency to {@link MutableQuantiles} metrics.
   *
   * @param readLatency file read latency in microseconds",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,<init>,org.apache.hadoop.metrics2.lib.MutableInverseQuantiles$InversePercentile:<init>(double),41,43,"/**
 * Constructs an object with normalized percentile values.
 * @param inversePercentile the inverse percentile value to be normalized
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,initialize,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:initialize(java.lang.String),57,59,"/**
 * Retrieves a MetricsSystem instance with a specified prefix.
 * @param prefix string prefix for metric identification
 * @return MetricsSystem object associated with the prefix
 */","* Convenience method to initialize the metrics system
   * @param prefix  for the metrics system configuration
   * @return the metrics system instance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,instance,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:instance(),68,70,"/**
 * Retrieves metrics system instance.
 * @return MetricsSystem object from singleton instance
 */",* @return the metrics system object,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,shutdown,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:shutdown(),75,77,"/**
 * Calls method m1 on the INSTANCE.
 */",* Shutdown the metrics system,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,setInstance,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:setInstance(org.apache.hadoop.metrics2.MetricsSystem),87,90,"/**
 * Applies mask to metrics system.
 * @param ms target metrics system
 * @return masked metrics system instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,removeMBeanName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeMBeanName(javax.management.ObjectName),113,116,"/**
 * Masks an ObjectName by invoking m2 on its m1 result.
 * @param name the ObjectName to mask
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,removeSourceName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeSourceName(java.lang.String),118,121,"/**
 * Masks a function by name.
 * @param name function identifier to be masked
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTag,org.apache.hadoop.ipc.metrics.RpcMetrics:getTag(java.lang.String),449,452,"/**
 * Retrieves a MetricsTag by name.
 * @param tagName the name of the tag to retrieve
 * @return MetricsTag object or null if not found
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,snapshot,"org.apache.hadoop.metrics2.lib.MetricsRegistry:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",465,472,"/**
 * Adds metrics tags and records to the builder.
 * @param builder MetricsRecordBuilder instance to populate
 * @param all flag indicating whether to include all metrics
 */","* Sample all the mutable metrics and put the snapshot in the builder
   * @param builder to contain the metrics snapshot
   * @param all get all the metrics even if the values are not changed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,toString,org.apache.hadoop.metrics2.lib.MetricsRegistry:toString(),474,481,"/**
* Constructs a formatted string with metric information.
* @return A formatted string containing info, tags, and metrics
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,getStats,org.apache.hadoop.metrics2.lib.MutableRollingAverages:getStats(long),292,314,"/**
 * Calculates average values for entries with sufficient sample size.
 * @param minSamples minimum number of samples required
 * @return map of entry names to their average values
 */","* Retrieve a map of metric name {@literal ->} (aggregate).
   * Filter out entries that don't have at least minSamples.
   *
   * @param minSamples input minSamples.
   * @return a map of peer DataNode Id to the average latency to that
   *         node seen over the measurement period.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingSampleCount,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingSampleCount(),396,398,"/**
 * Retrieves processing time for RPC.
 * @return Processing time in milliseconds
 */","* Returns the number of samples that we have seen so far.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingSampleCount,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingSampleCount(),437,439,"/**
 * Returns processing time for deferred RPCs.
 * @return Processing time in milliseconds
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,rollOverAvgs,org.apache.hadoop.metrics2.lib.MutableRollingAverages:rollOverAvgs(),247,274,"/**
* Processes current snapshot, updating averages.
* Uses synchronized block for thread safety.
*/","* Iterates over snapshot to capture all Avg metrics into rolling structure
   * {@link MutableRollingAverages#averages}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",83,89,"/**
 * Masks metrics in the builder.
 * @param builder MetricsRecordBuilder to update
 * @param all whether to mask all metrics
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeLong:incr(),46,49,"/**
 * Calls overloaded method m1 with default value 1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeLong:decr(),60,63,"/**
 * Calls overloaded method m1 with default value 1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,info,"org.apache.hadoop.metrics2.lib.Interns:info(java.lang.String,java.lang.String)",117,119,"/**
 * Creates and caches metrics info.
 * @param name unique metric name
 * @param description metric description
 * @return MetricsInfo object
 */","* Get a metric info object.
   * @param name Name of metric info object
   * @param description Description of metric info object
   * @return an interned metric info object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,tag,"org.apache.hadoop.metrics2.lib.Interns:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",151,153,"/**
 * Creates a MetricsTag from MetricsInfo and a string value.
 * @param info metrics information object
 * @param value tag value as a string
 * @return MetricsTag instance
 */","* Get a metrics tag.
   * @param info  of the tag
   * @param value of the tag
   * @return an interned metrics tag",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeFloat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",52,58,"/**
* Builds metrics record with optional filtering.
* @param builder MetricsRecordBuilder instance to populate
* @param all whether to include all metrics regardless of condition
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr(float),70,79,"/**
* Adjusts value by incrementing until condition met.
* @param delta amount to increment value by
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,add,"org.apache.hadoop.metrics2.lib.MutableStat:add(long,long)",123,126,"/**
 * Updates statistics and triggers secondary processing.
 * @param numSamples number of samples collected
 * @param sum total sum of the samples
 */","* Add a number of samples and their sum to the running stat
   *
   * Note that although use of this method will preserve accurate mean values,
   * large values for numSamples may result in inaccurate variance values due
   * to the use of a single step of the Welford method for variance calculation.
   * @param numSamples  number of samples
   * @param sum of the samples",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,add,org.apache.hadoop.metrics2.util.SampleStat:add(double),68,71,"/**
 * Updates statistics with a new value and returns a sample statistic.
 * @param x the value to process
 * @return SampleStat object containing updated statistics
 */","* Add a sample the running stat.
   * @param x the sample number
   * @return  self",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingMean,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingMean(),404,406,"/**
 * Retrieves processing time from RPC.
 * @return Processing time as a double value
 */","* Returns mean of RPC Processing Times.
   * @return double",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingMean,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingMean(),441,443,"/**
 * Retrieves processed time from deferred RPC.
 * @return Processed time as a double value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,min,org.apache.hadoop.metrics2.util.SampleStat:min(),134,136,"/**
 * Calls and returns the result of minmax's m1 method.
 * @return Result from minmax.m1()
 */",* @return  the minimum value of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,max,org.apache.hadoop.metrics2.util.SampleStat:max(),141,143,"/**
 * Delegates to minmax's m1 method.
 * @return result of minmax.m1()
 */",* @return  the maximum value of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,org.apache.hadoop.metrics2.util.SampleStat$MinMax:reset(org.apache.hadoop.metrics2.util.SampleStat$MinMax),188,191,"/**
 * Updates min and max values from another MinMax object.
 * @param other source MinMax object to copy values from
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,resetMinMax,org.apache.hadoop.metrics2.lib.MutableStat:resetMinMax(),177,179,"/**
 * Calls m1 method from minMax object.
 */",* Reset the all time min max of the metric,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,org.apache.hadoop.metrics2.util.SampleStat:reset(),40,45,"/**
* Resets statistics and updates minmax.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,snapshot,"org.apache.hadoop.metrics2.lib.MethodMetric$1:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",141,143,"/**
 * Delegates to implementation for building metrics record.
 * @param builder used to construct metrics record
 * @param all flag indicating whether to include all metrics
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newCounter,org.apache.hadoop.metrics2.lib.MethodMetric:newCounter(java.lang.Class),72,87,"/**
* Creates a MutableMetric for the given type.
* @param type class type of the metric
* @return MutableMetric instance or throws exception if unsupported
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,snapshot,"org.apache.hadoop.metrics2.lib.MethodMetric$2:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",141,143,"/**
 * Delegates MetricsRecordBuilder processing to implementation.
 * @param builder object to build metrics record
 * @param all flag indicating whether to include all metrics
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newGauge,org.apache.hadoop.metrics2.lib.MethodMetric:newGauge(java.lang.Class),106,123,"/**
* Creates a MutableMetric based on class type.
* @param t the target class type
* @return MutableMetric instance or throws exception if unsupported
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",83,89,"/**
 * Records metrics to a builder.
 * @param builder MetricsRecordBuilder instance to record metrics
 * @param all flag indicating whether to include all metrics
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getNextTgtRenewalTime,"org.apache.hadoop.security.UserGroupInformation:getNextTgtRenewalTime(long,long,org.apache.hadoop.io.retry.RetryPolicy)",1110,1117,"/**
* Calculates the next retry time based on policy.
* @param tgtEndTime target end time for ticket
* @param now current time in milliseconds
* @param rp retry policy to apply
* @return calculated retry time in milliseconds
*/","* Get time for next login retry. This will allow the thread to retry with
   * exponential back-off, until tgt endtime.
   * Last retry is {@link #kerberosMinSecondsBeforeRelogin} before endtime.
   *
   * @param tgtEndTime EndTime of the tgt.
   * @param now Current time.
   * @param rp The retry policy.
   * @return Time for next login retry.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeInt:incr(),46,49,"/**
 * Calls overloaded method m1 with default argument 1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeInt:decr(),60,63,"/**
 * Calls overloaded method m1 with default parameter value 1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,reattach,"org.apache.hadoop.metrics2.source.JvmMetrics:reattach(org.apache.hadoop.metrics2.MetricsSystem,org.apache.hadoop.metrics2.source.JvmMetrics)",125,127,"/**
 * Updates metrics system with JVM data.
 * @param ms MetricsSystem instance to update
 * @param jvmMetrics JVM metrics to include
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getMemoryUsage,org.apache.hadoop.metrics2.source.JvmMetrics:getMemoryUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder),157,168,"/**
* Records memory usage metrics.
* @param rb MetricsRecordBuilder to store the metrics
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Metrics2Util.java,equals,org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:equals(java.lang.Object),56,62,"/**
* Checks equality with another NameValuePair.
* @param other object to compare
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,stddev,org.apache.hadoop.metrics2.util.SampleStat:stddev(),127,129,"/**
 * Applies mask function to input value.
 * @return masked result as double
 */",* @return  the standard deviation of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,compress,org.apache.hadoop.metrics2.util.SampleQuantiles:compress(),176,198,"/**
* Processes sample items, merging them based on conditions.
*/","* Try to remove extraneous items from the set of sampled items. This checks
   * if an item is unnecessary based on the desired error bounds, and merges it
   * with the adjacent item if it is.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,query,org.apache.hadoop.metrics2.util.SampleQuantiles:query(double),206,228,"/**
 * Computes the mask value for a given quantile.
 * @param quantile the desired quantile (0.0 to 1.0)
 * @return the computed mask value
 */","* Get the estimated value at the specified quantile.
   * 
   * @param quantile Queried quantile, e.g. 0.50 or 0.99.
   * @return Estimated value at that quantile.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,insertBatch,org.apache.hadoop.metrics2.util.SampleQuantiles:insertBatch(),129,169,"/**
* Processes buffer data and updates samples.
* Masks buffer values into sample items.
*/","* Merges items from buffer into the samples array in one pass.
   * This is more efficient than doing an insert on every item.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Metrics2Util.java,offer,org.apache.hadoop.metrics2.util.Metrics2Util$TopN:offer(org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair),84,95,"/**
* Processes a name-value pair, updating internal state.
* @param entry the NameValuePair to process
* @return true if processing is successful, otherwise false
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,<init>,org.apache.hadoop.metrics2.util.MetricsCache:<init>(),136,138,"/**
* Constructs a MetricsCache with default max records per name.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,tag,"org.apache.hadoop.metrics2.MetricsJsonBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",66,69,"/**
 * Sets a metric with the given info and value.
 * @param info metric information
 * @param value metric value as a string
 * @return updated MetricsRecordBuilder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,add,org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.MetricsTag),71,74,"/**
 * Applies mask to metrics record.
 * @param tag metrics tag containing mask values
 * @return modified MetricsRecordBuilder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,add,org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.AbstractMetric),76,79,"/**
 * Applies mask to metric and builds metrics record.
 * @param metric input metric object
 * @return MetricsRecordBuilder with masked data
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,setContext,org.apache.hadoop.metrics2.MetricsJsonBuilder:setContext(java.lang.String),81,84,"/**
 * Sets context metric with given value.
 * @param value the context value to set
 * @return updated MetricsRecordBuilder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",86,89,"/**
 * Sets metric value with mask.
 * @param info metric information
 * @param value metric value
 * @return updated MetricsRecordBuilder
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",91,94,"/**
 * Sets a metric value with given info.
 * @param info metric information
 * @param value metric value to set
 * @return updated MetricsRecordBuilder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",96,99,"/**
 * Masks and records metrics.
 * @param info metric information
 * @param value metric value to record
 * @return updated MetricsRecordBuilder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",101,104,"/**
 * Applies mask to metrics record.
 * @param info metrics information
 * @param value metric value
 * @return modified MetricsRecordBuilder
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",106,109,"/**
 * Records a metric with masking.
 * @param info metric information
 * @param value metric value
 * @return MetricsRecordBuilder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",111,114,"/**
 * Masks and records a metric.
 * @param info metric information
 * @param value metric value
 * @return updated MetricsRecordBuilder
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolServerSideTranslatorPB.java,getProtocolVersion,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)",68,72,"/**
 * Executes RPC call for ZKFC protocol.
 * @param protocol protocol name as String
 * @param clientVersion version of the client
 * @return result of the RPC call as long
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getProtocolVersion,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)",180,184,"/**
* Invokes RPC method with specified protocol and version.
* @param protocol service protocol class
* @param clientVersion version of the client
* @return result of the RPC call
* @throws IOException if communication fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/NetgroupCache.java,getNetgroupNames,org.apache.hadoop.security.NetgroupCache:getNetgroupNames(),63,65,"/**
 * Returns a list of masked strings.
 * @return List containing masked string values
 */","* Get the list of cached netgroups
   *
   * @return list of cached groups",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/NetgroupCache.java,isCached,org.apache.hadoop.security.NetgroupCache:isCached(java.lang.String),81,83,"/**
 * Checks if a group is masked.
 * @param group name of the group to check
 * @return true if the group is masked, false otherwise
 */","* Returns true if a given netgroup is cached
   *
   * @param group check if this group is cached
   * @return true if group is cached, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getDefaults,org.apache.hadoop.security.UserGroupInformation$LoginParams:getDefaults(),2097,2103,"/**
* Creates login parameters for Kerberos authentication.
* @return LoginParams object configured with Kerberos principal, keytab, and cache
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,toLowerCase,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:toLowerCase(javax.servlet.http.HttpServletRequest),132,196,"/**
 * Wraps and modifies request parameters.
 * @param request original HttpServletRequest object
 * @return modified HttpServletRequestWrapper object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getServerProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getServerProperties(java.net.InetAddress,int)",103,106,"/**
 * Calls m1 with client address and port.
 * @param clientAddress IP address of the client
 * @param ingressPort port number of incoming connection
 * @return map of strings from m1 method
 */","* Identify the Sasl Properties to be used for a connection with a  client.
   * @param clientAddress  client's address
   * @param ingressPort the port that the client is connecting
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getClientProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getClientProperties(java.net.InetAddress,int)",123,126,"/**
 * Calls m1 with server address and default port.
 * @param serverAddress IP address of the server
 * @param ingressPort port number (unused in this method)
 * @return Map containing server information
 */","* Identify the Sasl Properties to be used for a connection with a server.
   * @param serverAddress server's address
   * @param ingressPort the port that is used to connect to server
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,getPassword,org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:getPassword(org.apache.hadoop.security.token.TokenIdentifier),289,292,"/**
 * Masks a token identifier.
 * @param tokenid the token to be masked
 * @return masked character array of the token
 * @throws InvalidToken if the token is invalid
 * @throws StandbyException if in standby mode
 * @throws RetriableException if operation is retryable
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:getGroupsSet(java.lang.String),65,68,"/**
 * Retrieves a set of strings associated with a user.
 * @param user the username or identifier
 * @return a set of strings related to the user
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:getGroupsSet(java.lang.String),64,67,"/**
 * Retrieves a set of strings associated with a user.
 * @param user unique user identifier
 * @return Set of strings related to the user
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.CompositeGroupsMapping:getGroupsSet(java.lang.String),110,131,"/**
 * Retrieves groups for a user from multiple providers.
 * @param user the username to fetch groups for
 * @return set of groups associated with the user
 * @throws IOException if an I/O error occurs during retrieval
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,getEnabledConfigKey,org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getEnabledConfigKey(),71,73,"/**
 * Concatenates result of m1() with ENABLED_SUFFIX.
 * @return concatenated string
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCredentialsInternal,org.apache.hadoop.security.UserGroupInformation:getCredentialsInternal(),1759,1770,"/**
* Retrieves or creates credentials for the subject.
* @return Credentials object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/User.java,<init>,"org.apache.hadoop.security.User:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,javax.security.auth.login.LoginContext)",46,57,"/**
 * Constructs a User with the given name, authentication method, and login context.
 * @param name full user principal name
 * @param authMethod method of authentication
 * @param login login context details
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getHostFromPrincipal,org.apache.hadoop.security.SecurityUtil:getHostFromPrincipal(java.lang.String),353,355,"/**
 * Masks the given principal name using Hadoop Kerberos logic.
 * @param principalName the original principal name to be masked
 * @return the masked principal name as a String
 */","* Get the host name from the principal name of format {@literal <}service
   * {@literal >}/host@realm.
   * @param principalName principal name of format as described above
   * @return host name if the the string conforms to the above format, else null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroupInternal,org.apache.hadoop.security.Groups:getGroupInternal(java.lang.String),242,264,"/**
* Retrieves group memberships for a user.
* @param user the username
* @return set of group names or throws IOException if user not found
*/","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * @param user User's name
   * @return the group memberships of the user as Set
   * @throws IOException if user does not exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,refresh,org.apache.hadoop.security.Groups:refresh(),430,441,"/**
* Clears userToGroupsMap cache and refreshes group data.
* Handles IOExceptions by logging errors.
*/",* Refresh all user-to-groups mappings.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMapping.java,getGroups,org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroups(java.lang.String),79,82,"/**
 * Masks sensitive information in user data.
 * @param user input string containing user data
 * @return list of masked strings
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroupsSet(java.lang.String),84,90,"/**
* Masks user data by fetching and processing groups.
* @param user the user identifier
* @return a set of masked group identifiers
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,close,org.apache.hadoop.security.KDiag:close(),189,195,"/**
* Calls m1 and delegates to out.m2 if out is not null.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,println,"org.apache.hadoop.security.KDiag:println(java.lang.String,java.lang.Object[])",854,863,"/**
* Processes a formatted message and outputs it.
* @param format message format string
* @param args arguments for the format string
*/","* Print a line of output. This goes to any output file, or
   * is logged at info. The output is flushed before and after, to
   * try and stay in sync with JRE logging.
   *
   * @param format format string
   * @param args any arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,usage,org.apache.hadoop.security.KDiag:usage(),246,263,"/**
 * Generates a formatted string with Kerberos diagnostic options.
 * @return String containing Kerberos diagnostic configuration details.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapInternal,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapInternal(org.apache.hadoop.thirdparty.com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)",224,281,"/**
* Updates a map with data from a command output.
* @param map the BiMap to update
* @param mapName name of the map for logging
* @param command shell command to execute
* @param regex pattern to parse command output lines
* @param staticMapping mapping of IDs
* @return true if the map was updated, false otherwise
*/","* Get the list of users or groups returned by the specified command,
   * and save them in the corresponding map.
   *
   * @param map map.
   * @param mapName mapName.
   * @param command command.
   * @param staticMapping staticMapping.
   * @param regex regex.
   * @throws IOException raised on errors performing I/O.
   * @return updateMapInternal.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getRunScriptCommand,org.apache.hadoop.util.Shell:getRunScriptCommand(java.io.File),443,448,"/**
* Constructs command array for running a script.
* @param script File object representing the script
* @return Array of strings forming the command to execute the script
*/","* Returns a command to run the given script.  The script interpreter is
   * inferred by platform: cmd on Windows or bash otherwise.
   *
   * @param script File script to run
   * @return String[] command to run the script",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,valueOf,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:valueOf(org.apache.hadoop.security.SaslRpcServer$AuthMethod),1504,1512,"/**
* Retrieves the corresponding AuthenticationMethod.
* @param authMethod the method to look up
* @return matching AuthenticationMethod or throws exception if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,switchBindUser,org.apache.hadoop.security.LdapGroupsMapping:switchBindUser(javax.naming.AuthenticationException),644,651,"/**
* Handles AuthenticationException by switching bind users.
* @param e the AuthenticationException that triggered the switch
*/","* Switch to the next available user to bind to.
   * @param e AuthenticationException encountered when contacting LDAP",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,"org.apache.hadoop.security.SaslOutputStream:write(byte[],int,int)",166,193,"/**
 * Writes data to output stream with SASL wrapping if enabled.
 * @param inBuf input buffer containing data
 * @param off offset within the buffer to start reading from
 * @param len length of data to read from the buffer
 * @throws IOException on I/O errors or SASL exceptions
 */","* Writes <code>len</code> bytes from the specified byte array starting at
   * offset <code>off</code> to this output stream.
   * 
   * @param inBuf
   *          the data.
   * @param off
   *          the start offset in the data.
   * @param len
   *          the number of bytes to write.
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,close,org.apache.hadoop.security.SaslOutputStream:close(),213,217,"/**
 * Calls m1 and then invokes outStream's m2.
 * @throws IOException if an I/O error occurs
 */","* Closes this output stream and releases any system resources associated with
   * this stream.
   * 
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,init,org.apache.hadoop.security.SaslRpcServer:init(org.apache.hadoop.conf.Configuration),175,182,"/**
* Initializes SASL factory with security provider.
* @param conf Configuration object (not used in method)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,createSaslServer,"org.apache.hadoop.security.SaslPlainServer$SaslPlainServerFactory:createSaslServer(java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)",48,53,"/**
* Creates a SASL server for the PLAIN mechanism.
* @param mechanism the requested SASL mechanism
* @param protocol the security protocol name
* @param serverName the unqualified host name of the server
* @param props additional properties for the SASL server
* @param cbh the CallbackHandler to use for authentication callbacks
* @return a SaslServer instance or null if PLAIN is not requested
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,getAuthorizationID,org.apache.hadoop.security.SaslPlainServer:getAuthorizationID(),127,131,"/**
 * Masks function and returns authorization status.
 * @return authorization status as a string
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,getNegotiatedProperty,org.apache.hadoop.security.SaslPlainServer:getNegotiatedProperty(java.lang.String),133,137,"/**
* Masks property name using SASL QOP.
* @param propName property name to mask
* @return ""auth"" if masking successful, otherwise null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,wrap,"org.apache.hadoop.security.SaslPlainServer:wrap(byte[],int,int)",139,145,"/**
* Throws exception as PLAIN does not support integrity or privacy.
* @param outgoing data to be masked
* @param offset starting index in the data array
* @param len length of the data to be processed
* @throws SaslException always thrown due to unsupported feature
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,unwrap,"org.apache.hadoop.security.SaslPlainServer:unwrap(byte[],int,int)",147,153,"/**
 * Throws an exception as PLAIN mechanism does not support integrity or privacy.
 * @param incoming input data array
 * @param offset starting index in the array
 * @param len length of the data to process
 * @throws SaslException always thrown due to unsupported features
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createKeyStore,"org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyStore(java.lang.String,java.lang.String)",1102,1109,"/**
* Loads a KeyStore from the specified location.
* @param location file path to the KeyStore
* @param password password for accessing the KeyStore
* @return loaded KeyStore instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,init,org.apache.hadoop.security.http.RestCsrfPreventionFilter:init(javax.servlet.FilterConfig),71,93,"/**
* Initializes CSRF filter with configuration parameters.
* @param filterConfig contains filter initialization data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,handleHttpInteraction,org.apache.hadoop.security.http.RestCsrfPreventionFilter:handleHttpInteraction(org.apache.hadoop.security.http.RestCsrfPreventionFilter$HttpInteraction),193,203,"/**
* Checks and processes HTTP request for CSRF protection.
* @param httpInteraction encapsulates HTTP interaction details
* @throws IOException if I/O error occurs
* @throws ServletException if servlet processing fails
*/","* Handles an {@link HttpInteraction} by applying the filtering logic.
   *
   * @param httpInteraction caller's HTTP interaction
   * @throws IOException if there is an I/O error
   * @throws ServletException if the implementation relies on the servlet API
   *     and a servlet API call has failed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,initializeAllowedMethods,org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedMethods(javax.servlet.FilterConfig),165,174,"/**
 * Configures allowed HTTP methods from filter configuration.
 * @param filterConfig configuration for the filter
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,doCrossFilter,"org.apache.hadoop.security.http.CrossOriginFilter:doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",107,153,"/**
* Handles cross-origin request checks and sets response headers.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,initializeAllowedHeaders,org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedHeaders(javax.servlet.FilterConfig),176,185,"/**
 * Configures allowed headers for the filter.
 * @param filterConfig configuration object containing header settings
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,parsePartialGroupNames,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:parsePartialGroupNames(java.lang.String,java.lang.String)",242,269,"/**
 * Masks group names based on IDs.
 * @param groupNames comma-separated group names
 * @param groupIDs comma-separated group IDs
 * @return set of masked group names
 * @throws PartialGroupNameException if name and ID counts mismatch
 */","* Attempt to parse group names given that some names are not resolvable.
   * Use the group id list to identify those that are not resolved.
   *
   * @param groupNames a string representing a list of group names
   * @param groupIDs a string representing a list of group ids
   * @return a linked list of group names
   * @throws PartialGroupNameException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,newLoginContext,"org.apache.hadoop.security.UserGroupInformation:newLoginContext(java.lang.String,javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration)",503,518,"/**
* Creates a HadoopLoginContext for the given app and configuration.
* @param appName application name
* @param subject user subject
* @param loginConf Hadoop configuration
* @return HadoopLoginContext instance
* @throws LoginException if context creation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,login,org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:login(),2142,2155,"/**
 * Handles login process with exception handling.
 * @throws LoginException if login fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logout,org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:logout(),2157,2164,"/**
* Checks login status and calls superclass method.
* @throws LoginException if login fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,createSecretKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createSecretKey(byte[]),692,694,"/**
 * Generates a secret key from the provided byte array.
 * @param key byte array representing the secret key
 * @return SecretKey object
 */","* Convert the byte[] to a secret key
   * @param key the byte[] to create the secret key from
   * @return the secret key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,formatTokenId,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:formatTokenId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),83,90,"/**
 * Formats a token identifier.
 * @param id TokenIdent object to be formatted
 * @return String representation of the token or sequence number if exception occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeStoredToken,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),204,211,"/**
 * Masks a token using the SQL secret manager.
 * @param ident Token identifier object
 * @throws IOException if an I/O error occurs
 */","* Removes the existing TokenInformation from the SQL database to
   * invalidate it.
   * @param ident TokenInformation to remove from the SQL database.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Daemon.java,newThread,org.apache.hadoop.util.Daemon$DaemonFactory:newThread(java.lang.Runnable),42,45,"/**
 * Creates and returns a daemon thread.
 * @param runnable task to be executed by the thread
 * @return Daemon thread instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceStart,org.apache.hadoop.util.JvmPauseMonitor:serviceStart(),81,86,"/**
* Initializes and starts monitoring thread.
* @throws Exception if any error occurs during initialization
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,reset,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:reset(),182,187,"/**
* Executes masked operations on shared resources.
*/",* Reset all data structures and mutable state.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),350,352,"/**
 * Masks delegation key.
 * @param key DelegationKey to be masked
 * @throws IOException if an I/O error occurs
 */","* For subclasses externalizing the storage, for example Zookeeper
   * based implementations.
   *
   * @param key DelegationKey.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeStoredMasterKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey),370,377,"/**
* Masks a delegation key.
* @param key DelegationKey object to be masked
*/","* Removes the existing DelegationKey from the SQL database to
   * invalidate it.
   * @param key DelegationKey to remove from the SQL database.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addKey(org.apache.hadoop.security.token.delegation.DelegationKey),213,220,"/**
* Adds a delegation key if not running.
* @param key DelegationKey to add
* @throws IOException if SecretManager is running or invalid key
*/","* Add a previously used master key to cache (when NN restarts), 
   * should be called before activate().
   *
   * @param key delegation key.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),338,341,"/**
* Masks delegation key by updating internal structures.
* @param key DelegationKey to be masked
*/","* For subclasses externalizing the storage, for example Zookeeper
   * based implementations.
   *
   * @param key DelegationKey.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(long,byte[])",707,709,"/**
 * Constructs a DelegationTokenInformation with a specified renew date and password.
 * @param renewDate expiration timestamp for the token
 * @param password associated with the delegation token
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredKeys,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredKeys(),478,491,"/**
* Removes expired delegation keys.
* Iterates through all keys, removes those expired and updates current key if needed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTokenTrackingId,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenTrackingId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),561,567,"/**
* Masks token information.
* @param identifier unique token identifier
* @return masked token string or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredStoredToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),795,797,"/**
 * Masks a token identifier.
 * @param ident TokenIdent to be masked
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,setExternalDelegationTokenSecretManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager),136,141,"/**
* Updates the secret manager and marks it as unmanaged.
* @param secretManager new secret manager to use
*/","* Sets an external <code>DelegationTokenSecretManager</code> instance to
   * manage creation and verification of Delegation Tokens.
   * <p>
   * This is useful for use cases where secrets must be shared across multiple
   * services.
   *
   * @param secretManager a <code>DelegationTokenSecretManager</code> instance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,destroy,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:destroy(),154,158,"/**
 * Masks secrets using SecretManager.
 * @param managedSecretManager flag indicating if SecretManager is enabled
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,stopThreads,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:stopThreads(),437,475,"/**
* Stops various services and logs exceptions.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationTokenLoadingCache.java,isEmpty,org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:isEmpty(),57,60,"/**
 * Checks if mask function is disabled.
 * @return true if mask is off, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,<init>,"org.apache.hadoop.security.token.delegation.DelegationKey:<init>(int,long,javax.crypto.SecretKey)",51,53,"/**
* Constructs a DelegationKey with specified ID, expiry date, and encoded secret.
* @param keyId unique identifier for the delegation key
* @param expiryDate timestamp when the key expires
* @param key SecretKey object or null if no key is provided
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,getConfiguration,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getConfiguration(java.lang.String,javax.servlet.FilterConfig)",114,120,"/**
* Overrides base method to load and modify properties.
* @param configPrefix prefix for configuration keys
* @param filterConfig filter configuration object
* @return modified Properties object
* @throws ServletException if an error occurs during processing
*/","* It delegates to
   * {@link AuthenticationFilter#getConfiguration(String, FilterConfig)} and
   * then overrides the {@link AuthenticationHandler} to use if authentication
   * type is set to <code>simple</code> or <code>kerberos</code> in order to use
   * the corresponding implementation with delegation token support.
   *
   * @param configPrefix parameter not used.
   * @param filterConfig parameter not used.
   * @return hadoop-auth de-prefixed configuration for the filter and handler.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,initializeAuthHandler,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:initializeAuthHandler(java.lang.String,javax.servlet.FilterConfig)",204,215,"/**
* Initializes the secret manager with a Curator client and resets it afterward.
* @param authHandlerClassName class name of the authentication handler
* @param filterConfig configuration for the filter
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/HttpUserGroupInformation.java,get,org.apache.hadoop.security.token.delegation.web.HttpUserGroupInformation:get(),36,39,"/**
 * Retrieves UserGroupInformation using delegation token authentication.
 * @return UserGroupInformation object
 */","* Returns the remote {@link UserGroupInformation} in context for the current
   * HTTP request, taking into account proxy user requests.
   *
   * @return the remote {@link UserGroupInformation}, <code>NULL</code> if none.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:<init>(),89,92,"/**
* Initializes a new instance of MultiSchemeDelegationTokenAuthenticationHandler.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:<init>(),49,52,"/**
 * Constructs a Kerberos delegation token authentication handler.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:<init>(),50,53,"/**
* Constructs a new PseudoDelegationTokenAuthenticationHandler.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,obtainDelegationTokenAuthenticator,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:obtainDelegationTokenAuthenticator(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",128,140,"/**
* Initializes or configures a DelegationTokenAuthenticator.
* @param dta existing authenticator, may be null
* @param connConfigurator connection configuration settings
* @return configured DelegationTokenAuthenticator
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticator.java,<init>,org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:<init>(),41,52,"/**
* Constructs a PseudoDelegationTokenAuthenticator.
* Initializes with a custom PseudoAuthenticator that retrieves the current user's short name.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticator.java,<init>,org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator:<init>(),38,45,"/**
* Initializes KerberosDelegationTokenAuthenticator with a fallback to PseudoDelegationTokenAuthenticator.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,isManagementOperation,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:isManagementOperation(javax.servlet.http.HttpServletRequest),211,218,"/**
* Checks if operation is valid for delegation token.
* @param request HTTP request object
* @return true if operation is valid and not an OPTIONS request, false otherwise
*/","* This method checks if the given HTTP request corresponds to a management
   * operation.
   *
   * @param request The HTTP request
   * @return true if the given HTTP request corresponds to a management
   *         operation false otherwise
   * @throws IOException In case of I/O error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,getDelegationToken,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getDelegationToken(javax.servlet.http.HttpServletRequest),412,421,"/**
* Retrieves delegation token from request.
* @param request HTTP servlet request
* @return delegation token as String or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,hasDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:hasDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",115,132,"/**
* Checks if a delegation token is present in the URL or token.
* @param url the URL to check
* @param token the authentication token
* @return true if a delegation token is found, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,org.apache.hadoop.crypto.key.kms.KMSClientProvider$TokenSelector:<init>(),167,169,"/**
 * Constructs a new TokenSelector with the specified token kind.
 * @param TOKEN_KIND type of token to select
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,incrementDelegationTokenSeqNum,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementDelegationTokenSeqNum(),504,531,"/**
* Fetches and returns the next sequence number.
* Handles locking, fetching new range if needed, and logging.
* @return Next sequence number
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,incrementCurrentKeyId,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementCurrentKeyId(),547,559,"/**
* Increments and returns the shared key ID counter.
* @return current value of the incremented key ID counter
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredMasterKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey),727,754,"/**
* Removes a delegation key from ZooKeeper.
* @param key DelegationKey object to be removed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,equals,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:equals(java.lang.Object),166,182,"/**
* Compares two token identifiers for equality.
* @param obj the object to compare with
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,isManaged,org.apache.hadoop.security.token.Token:isManaged(),487,489,"/**
 * Calls a method on the result of m1().
 * @return boolean result from m2()
 * @throws IOException if an I/O error occurs
 */","* Is this token managed so that it can be renewed or cancelled?
   * @return true, if it can be renewed and cancelled.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,renew,org.apache.hadoop.security.token.Token:renew(org.apache.hadoop.conf.Configuration),498,501,"/**
 * Delegates to another method with current instance and configuration.
 * @param conf Configuration object
 * @return Result from delegated method call
 * @throws IOException if an I/O error occurs
 * @throws InterruptedException if the operation is interrupted
 */","* Renew this delegation token.
   * @param conf configuration.
   * @return the new expiration time
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,cancel,org.apache.hadoop.security.token.Token:cancel(org.apache.hadoop.conf.Configuration),510,513,"/**
* Calls method m2 on result of m1 with current instance and configuration.
* @param conf Configuration object
* @throws IOException if an I/O error occurs
* @throws InterruptedException if the thread is interrupted
*/","* Cancel this delegation token.
   *
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,validate,org.apache.hadoop.security.token.DtUtilShell$Get:validate(),225,239,"/**
 * Validates service and URL configuration.
 * @return true if valid, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,getCommandUsage,org.apache.hadoop.security.token.DtUtilShell:getCommandUsage(),179,187,"/**
* Generates formatted string with various operation descriptions.
* @return Formatted string containing descriptions of operations
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,"org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[],int,int)",580,593,"/**
 * Reads data from buffer into byte array.
 * @param buf destination byte array
 * @param off offset in the array to start writing
 * @param len number of bytes to read
 * @return number of bytes actually read
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,isValidAuthType,org.apache.hadoop.security.SaslRpcClient:isValidAuthType(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),189,199,"/**
* Validates authentication type.
* @param authType SaslAuth object containing authentication details
* @return true if valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getInputStream,org.apache.hadoop.security.SaslRpcClient:getInputStream(java.io.InputStream),533,538,"/**
* Wraps input stream if condition is met.
* @param in original input stream
* @return wrapped or original input stream
* @throws IOException if an I/O error occurs
*/","* Get SASL wrapped InputStream if SASL QoP requires unwrapping,
   * otherwise return original stream.  Can be called only after
   * saslConnect() has been called.
   * 
   * @param in - InputStream used to make the connection
   * @return InputStream that may be using SASL unwrap
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getOutputStream,org.apache.hadoop.security.SaslRpcClient:getOutputStream(java.io.OutputStream),549,558,"/**
* Wraps OutputStream with buffering if condition met.
* @param out original output stream
* @return wrapped or original output stream
*/","* Get SASL wrapped OutputStream if SASL QoP requires wrapping,
   * otherwise return original stream.  Can be called only after
   * saslConnect() has been called.
   * 
   * @param out - OutputStream used to make the connection
   * @return OutputStream that may be using wrapping
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,disposeSasl,org.apache.hadoop.ipc.Client$Connection:disposeSasl(),546,554,"/**
* Calls m1 on SASL RPC client and resets it to null.
* @throws IOException if an I/O error occurs during m1 call
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,noPasswordWarning,"org.apache.hadoop.security.ProviderUtils:noPasswordWarning(java.lang.String,java.lang.String)",244,247,"/**
* Masks environment and file keys with warning messages.
* @param envKey environment key to be masked
* @param fileKey file key to be masked
* @return masked string with warnings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,noPasswordError,"org.apache.hadoop.security.ProviderUtils:noPasswordError(java.lang.String,java.lang.String)",249,251,"/**
 * Masks environment and file keys with error message.
 * @param envKey environment key string
 * @param fileKey file key string
 * @return masked string with error prefix
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,readMoreData,org.apache.hadoop.security.SaslInputStream:readMoreData(),95,125,"/**
* Reads and processes SASL token from input stream.
* @throws IOException on I/O error
* @return length of processed token or -1 if EOF reached
*/","* Read more data and get them processed <br>
   * Entry condition: ostart = ofinish <br>
   * Exit condition: ostart <= ofinish <br>
   * 
   * return (ofinish-ostart) (we have this many bytes for you), 0 (no data now,
   * but could have more later), or -1 (absolutely no more data)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,close,org.apache.hadoop.security.SaslInputStream:close(),341,348,"/**
* Resets stream and calls m1.
* @throws IOException if an I/O error occurs
*/","* Closes this input stream and releases any system resources associated with
   * the stream.
   * <p>
   * The <code>close</code> method of <code>SASLInputStream</code> calls the
   * <code>close</code> method of its underlying input stream.
   * 
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(),37,39,"/**
 * Constructs an AuthorizationException with no detail message.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.Throwable),54,56,"/**
 * Constructs an AuthorizationException with the specified cause.
 * @param cause the underlying reason for this exception
 */","* Constructs a new exception with the specified cause and a detail
   * message of <tt>(cause==null ? null : cause.toString())</tt> (which
   * typically contains the class and detail message of <tt>cause</tt>).
   * @param  cause the cause (which is saved for later retrieval by the
   *         {@link #getCause()} method).  (A <tt>null</tt> value is
   *         permitted, and indicates that the cause is nonexistent or
   *         unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,<init>,org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:<init>(org.apache.hadoop.security.token.Token),664,667,"/**
* Initializes with a token to set user credentials.
* @param token containing user identifier and password
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reset,org.apache.hadoop.security.UserGroupInformation:reset(),368,379,"/**
* Resets Kerberos authentication configuration.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getLogin,org.apache.hadoop.security.UserGroupInformation:getLogin(),522,526,"/**
* Retrieves and casts login context to HadoopLoginContext.
* @return HadoopLoginContext object or null if not an instance of it
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginSuccess,org.apache.hadoop.security.UserGroupInformation:isLoginSuccess(),537,542,"/**
* Checks if user has Hadoop login context.
* @return true if HadoopLoginContext, else true
*/","This method checks for a successful Kerberos login
    * and returns true by default if it is not using Kerberos.
    *
    * @return true on successful login",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setLogin,org.apache.hadoop.security.UserGroupInformation:setLogin(javax.security.auth.login.LoginContext),528,530,"/**
 * Delegates login processing to user.
 * @param login Login context containing authentication details
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setLastLogin,org.apache.hadoop.security.UserGroupInformation:setLastLogin(long),548,550,"/**
 * Updates user's last login time.
 * @param loginTime timestamp of the login event
 */","* Set the last login time for logged in user
   * @param loginTime the number of milliseconds since the beginning of time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,<init>,org.apache.hadoop.security.UserGroupInformation:<init>(javax.security.auth.Subject),559,568,"/**
* Initializes UserGroupInformation with a Subject.
* @param subject security context containing user principals
*/","* Create a UserGroupInformation for the given subject.
   * This does not change the subject or acquire new credentials.
   *
   * The creator of subject is responsible for renewing credentials.
   * @param subject the user's subject",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUserName,org.apache.hadoop.security.UserGroupInformation:getUserName(),1667,1671,"/**
 * Returns masked string from user.
 * @return Masked string representation of user data
 */","* Get the user's full principal name.
   * @return the user's full principal name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hasKerberosCredentials,org.apache.hadoop.security.UserGroupInformation:hasKerberosCredentials(),574,576,"/**
 * Checks if the current user's authentication method is KERBEROS.
 * @return true if the user's auth method is KERBEROS, false otherwise
 */","* checks if logged in using kerberos
   * @return true if the subject logged via keytab or has a Kerberos TGT",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getAuthenticationMethod(),1853,1855,"/**
 * Retrieves authentication method for the user.
 * @return AuthenticationMethod object
 */","* Get the authentication method from the subject
   * 
   * @return AuthenticationMethod in the subject, null if not present.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,fixKerberosTicketOrder,org.apache.hadoop.security.UserGroupInformation:fixKerberosTicketOrder(),1204,1233,"/**
* Processes Kerberos tickets for renewal or removal.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hasSufficientTimeElapsed,org.apache.hadoop.security.UserGroupInformation:hasSufficientTimeElapsed(long),1401,1410,"/**
* Determines if a re-login attempt should be made.
* @param now current timestamp in milliseconds
* @return true if re-login is needed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealUser,org.apache.hadoop.security.UserGroupInformation:getRealUser(),1543,1550,"/**
* Retrieves a RealUser from the subject.
* @return RealUser object or null if none found
*/","* get RealUser (vs. EffectiveUser)
   * @return realUser running over proxy user",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getShortUserName,org.apache.hadoop.security.UserGroupInformation:getShortUserName(),1651,1653,"/**
 * Returns masked string from user.
 * @return Masked string representation of user data
 */","* Get the user's login name.
   * @return the user's name up to the first '/' or '@'.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod),1834,1837,"/**
 * Delegates authentication to the user.
 * @param authMethod authentication method to use
 */","* Sets the authentication method in the subject
   * 
   * @param authMethod authMethod.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.security.cert.X509Certificate)",349,360,"/**
* Validates SSL certificate against host.
* @param host array of host names
* @param cert X509Certificate to validate
* @throws SSLException if validation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.lang.String[],java.lang.String[],boolean,boolean)",362,456,"/**
 * Validates SSL hostname against certificate CNs and subjectAlts.
 * @param hosts array of hostnames to validate
 * @param cns array of certificate common names
 * @param subjectAlts array of certificate subject alternative names
 * @param ie6 flag for Internet Explorer 6 compatibility
 * @param strictWithSubDomains flag for strict subdomain matching
 * @throws SSLException if hostname does not match any CN or SAN
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509TrustManager.java,<init>,"org.apache.hadoop.security.ssl.ReloadingX509TrustManager:<init>(java.lang.String,java.lang.String,java.lang.String)",72,78,"/**
 * Initializes a ReloadingX509TrustManager.
 * @param type the trust store type (e.g., ""JKS"")
 * @param location the path to the trust store file
 * @param password the password for the trust store
 * @throws IOException if an I/O error occurs
 * @throws GeneralSecurityException if a security exception occurs
 */","* Creates a reloadable trustmanager. The trustmanager reloads itself
   * if the underlying trustore file has changed.
   *
   * @param type type of truststore file, typically 'jks'.
   * @param location local path to the truststore file.
   * @param password password of the truststore file.
   * changed, in milliseconds.
   * @throws IOException thrown if the truststore could not be initialized due
   * to an IO error.
   * @throws GeneralSecurityException thrown if the truststore could not be
   * initialized due to a security error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509TrustManager.java,loadFrom,org.apache.hadoop.security.ssl.ReloadingX509TrustManager:loadFrom(java.nio.file.Path),115,123,"/**
* Reloads trust manager with certificates from a file.
* @param path Path to the certificate file
* @return This ReloadingX509TrustManager instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509KeystoreManager.java,<init>,"org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",69,77,"/**
 * Initializes a keystore manager with specified parameters.
 * @param type keystore type (e.g., JKS, PKCS12)
 * @param location path to the keystore file
 * @param storePassword password for the keystore
 * @param keyPassword password for the private keys
 * @throws IOException if an I/O error occurs
 * @throws GeneralSecurityException if a security exception occurs
 */","* Construct a <code>Reloading509KeystoreManager</code>
   *
   * @param type type of keystore file, typically 'jks'.
   * @param location local path to the keystore file.
   * @param storePassword password of the keystore file.
   * @param keyPassword The password of the key.
   * @throws IOException raised on errors performing I/O.
   * @throws GeneralSecurityException thrown if create encryptor error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509KeystoreManager.java,loadFrom,org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:loadFrom(java.nio.file.Path),123,131,"/**
* Initializes keystore from file.
* @param path to the keystore file
* @return ReloadingX509KeystoreManager instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,getResource,org.apache.hadoop.util.FindClass:getResource(java.lang.String),163,165,"/**
 * Retrieves a URL based on the provided name.
 * @param name identifier used to find the URL
 * @return URL object or null if not found
 */","* Get the resource
   * @param name resource name
   * @return URL or null for not found",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getConfResourceAsInputStream,org.apache.hadoop.conf.Configuration:getConfResourceAsInputStream(java.lang.String),2893,2908,"/**
* Retrieves an InputStream for a named resource.
* @param name the name of the resource
* @return InputStream or null if not found
*/","* Get an input stream attached to the configuration resource with the
   * given <code>name</code>.
   * 
   * @param name configuration resource name.
   * @return an input stream attached to the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getConfResourceAsReader,org.apache.hadoop.conf.Configuration:getConfResourceAsReader(java.lang.String),2917,2932,"/**
* Fetches and returns a Reader for the specified resource.
* @param name resource name to fetch
* @return Reader object or null if resource not found
*/","* Get a {@link Reader} attached to the configuration resource with the
   * given <code>name</code>.
   * 
   * @param name configuration resource name.
   * @return a reader attached to the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,createSSLEngine,org.apache.hadoop.security.ssl.SSLFactory:createSSLEngine(),256,268,"/**
* Initializes and configures an SSLEngine.
* @return configured SSLEngine instance
* @throws GeneralSecurityException if security setup fails
* @throws IOException if I/O error occurs during setup
*/","* Returns a configured SSLEngine.
   *
   * @return the configured SSLEngine.
   * @throws GeneralSecurityException thrown if the SSL engine could not
   * be initialized.
   * @throws IOException thrown if and IO error occurred while loading
   * the server keystore.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,configure,org.apache.hadoop.security.ssl.SSLFactory:configure(java.net.HttpURLConnection),358,372,"/**
* Configures SSL connection settings.
* @param conn HttpURLConnection object to configure
* @return configured HttpURLConnection object
*/","* If the given {@link HttpURLConnection} is an {@link HttpsURLConnection}
   * configures the connection with the {@link SSLSocketFactory} and
   * {@link HostnameVerifier} of this SSLFactory, otherwise does nothing.
   *
   * @param conn the {@link HttpURLConnection} instance to configure.
   * @return the configured {@link HttpURLConnection} instance.
   *
   * @throws IOException if an IO error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,configureConnection,org.apache.hadoop.crypto.key.kms.KMSClientProvider:configureConnection(java.net.HttpURLConnection),488,500,"/**
* Configures SSL settings on the given HttpURLConnection.
* @param conn the connection to configure
* @return the configured HttpURLConnection
* @throws IOException if there's an error setting up SSL
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,initializeSSLContext,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),153,185,"/**
* Initializes SSL context based on the specified channel mode.
* @param preferredChannelMode desired SSL channel mode
* @throws NoSuchAlgorithmException if algorithm is not available
* @throws KeyManagementException if key management fails
* @throws IOException for I/O errors or unknown channel mode
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(),236,239,"/**
 * Creates and returns an SSL socket.
 * @throws IOException if an I/O error occurs during socket creation
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.Socket,java.lang.String,int,boolean)",241,248,"/**
* Establishes an SSL socket connection.
* @param s existing Socket object
* @param host target host address
* @param port target port number
* @param autoClose whether to close the socket on error
* @return SSLSocket connected to the specified host and port
* @throws IOException if connection fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",250,257,"/**
* Creates and returns an SSL socket connection.
* @param address remote server's IP address
* @param port remote server's port number
* @param localAddress local interface to bind the socket
* @param localPort local port to bind the socket
* @return connected SSL socket
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",259,266,"/**
* Establishes an SSL socket connection.
* @param host remote server hostname
* @param port remote server port number
* @param localHost local address to bind
* @param localPort local port to bind
* @return connected Socket object
* @throws IOException if connection fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int)",268,273,"/**
* Establishes an SSL socket connection.
* @param host server address
* @param port server port
* @return connected Socket object
* @throws IOException if connection fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int)",275,280,"/**
* Establishes an SSL socket connection.
* @param host server hostname
* @param port server port number
* @return connected Socket object
* @throws IOException if connection fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configured.java,<init>,org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration),39,41,"/**
* Constructs a Configured instance with the given Configuration.
* @param conf Configuration object to be used
*/","Construct a Configured.
   * @param conf the Configuration object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,handleExecutorTimeout,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:handleExecutorTimeout(org.apache.hadoop.util.Shell$ShellCommandExecutor,java.lang.String)",174,192,"/**
* Checks if shell command execution timed out.
* @param executor ShellCommandExecutor instance
* @param user username for logging
* @return true if command timed out, false otherwise
*/","* Check if the executor had a timeout and logs the event.
   * @param executor to check
   * @param user user to log
   * @return true if timeout has occurred",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,toString,org.apache.hadoop.util.Shell$ShellCommandExecutor:toString(),1312,1325,"/**
* Constructs a formatted string from array elements.
* @return Formatted string with quoted elements containing spaces
*/","* Returns the commands of this instance.
     * Arguments with spaces in are presented with quotes round; other
     * arguments are presented raw
     *
     * @return a string representation of the object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,read,org.apache.hadoop.security.SaslRpcServer$AuthMethod:read(java.io.DataInput),262,264,"/**
 * Parses input to determine authentication method.
 * @param in DataInput source containing method data
 * @return AuthMethod parsed from input
 * @throws IOException if reading fails
 */","* Read from in.
     *
     * @param in DataInput.
     * @throws IOException raised on errors performing I/O.
     * @return AuthMethod.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByExactName,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByExactName(java.lang.String),687,704,"/**
* Resolves host to InetAddress.
* @param host hostname to resolve
* @return resolved InetAddress or null if unresolved
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKerberosEntry,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getKerberosEntry(),2232,2285,"/**
* Creates Kerberos login configuration entry.
* @return AppConfigurationEntry for Kerberos authentication
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,"org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:<init>(java.util.Map,java.util.Map)",572,576,"/**
* Initializes mapping for user and group IDs.
* @param uidMapping map of user ID mappings
* @param gidMapping map of group ID mappings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:<init>(),545,547,"/**
 * Constructs a new PassThroughMap with an empty map.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,addUser,org.apache.hadoop.security.authorize.AccessControlList:addUser(java.lang.String),152,159,"/**
* Adds a user if they are valid and conditions allow.
* @param user the username to add
*/","* Add user to the names of users allowed for this service.
   * 
   * @param user
   *          The user name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,addGroup,org.apache.hadoop.security.authorize.AccessControlList:addGroup(java.lang.String),167,177,"/**
* Adds a group to the system.
* @param group name of the group to add
*/","* Add group to the names of groups allowed for this service.
   * 
   * @param group
   *          The group name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,removeUser,org.apache.hadoop.security.authorize.AccessControlList:removeUser(java.lang.String),185,192,"/**
* Masks a user by removing them if conditions allow.
* @param user username to be masked
*/","* Remove user from the names of users allowed for this service.
   * 
   * @param user
   *          The user name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,removeGroup,org.apache.hadoop.security.authorize.AccessControlList:removeGroup(java.lang.String),200,208,"/**
* Masks a group by name, throwing an exception if removal is restricted.
* @param group the name of the group to mask
*/","* Remove group from the names of groups allowed for this service.
   * 
   * @param group
   *          The group name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getUsersString,org.apache.hadoop.security.authorize.AccessControlList:getUsersString(),337,339,"/**
 * Masks user data using a specific function.
 * @return masked user data as a string
 */","* Returns comma-separated concatenated single String of the set 'users'
   *
   * @return comma separated list of users",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getGroupsString,org.apache.hadoop.security.authorize.AccessControlList:getGroupsString(),346,348,"/**
 * Masks groups using function m1.
 * @return masked string representation of groups
 */","* Returns comma-separated concatenated single String of the set 'groups'
   *
   * @return comma separated list of groups",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,printStackTrace,org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace(),65,68,"/**
* Calls m1 with System.err as the PrintStream.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getProxyGroups,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyGroups(),175,182,"/**
* Builds a map of proxy groups with their associated permissions.
* @return Map where key is group name and value is collection of permissions
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getProxyHosts,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyHosts(),184,193,"/**
* Masks proxy hosts.
* @return Map of masked proxy hosts and their collections
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,innerSetCredential,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:innerSetCredential(java.lang.String,char[])",266,281,"/**
* Stores a credential with the given alias and material.
* @param alias unique identifier for the credential
* @param material secret data to be stored
* @return CredentialEntry object representing the stored credential
* @throws IOException if there's an issue storing the credential
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$CheckCommand:validate(),321,346,"/**
 * Checks and processes alias, handling help request and I/O exceptions.
 * @return true if processing is successful, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$CreateCommand:validate(),411,436,"/**
* Checks and processes alias, handles help request, and validates provider.
* @return true if processing is successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,execute,org.apache.hadoop.security.alias.CredentialShell$CheckCommand:execute(),348,386,"/**
* Checks alias password for CredentialProvider.
* @throws IOException if console is unavailable or I/O error occurs
* @throws NoSuchAlgorithmException if algorithm not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,promptForCredential,org.apache.hadoop.security.alias.CredentialShell:promptForCredential(),473,499,"/**
* Prompts user to enter and confirm an alias password.
* @return char array of the entered password
* @throws IOException if no console is available for input
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,warnIfTransientProvider,org.apache.hadoop.security.alias.CredentialShell$Command:warnIfTransientProvider(),172,176,"/**
* Checks and logs warning if provider is transient.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,createPermissions,org.apache.hadoop.security.alias.LocalKeyStoreProvider:createPermissions(java.lang.String),80,90,"/**
* Sets file permissions based on a string input.
* @param perms permission string in octal format
* @throws IOException if invalid permissions are provided
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,isOriginalTGT,org.apache.hadoop.security.SecurityUtil:isOriginalTGT(javax.security.auth.kerberos.KerberosTicket),168,170,"/**
* Checks if Kerberos ticket is masked.
* @param ticket KerberosTicket object to check
* @return true if ticket is masked, false otherwise
*/","* Check whether the server principal is the TGS's principal
   * @param ticket the original TGT (the ticket that is obtained when a 
   * kinit is done)
   * @return true or false",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setSslConfiguration,"org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore,org.apache.zookeeper.common.ClientX509Util)",825,850,"/**
* Configures SSL/TLS for ZooKeeper client.
* @param zkClientConfig configuration for ZooKeeper client
* @param truststoreKeystore truststore and keystore details
* @param x509Util utility for X509 configurations
* @throws ConfigurationException if configuration fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KerberosAuthException.java,<init>,"org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.String,java.lang.Throwable)",51,54,"/**
 * Constructs a Kerberos authentication exception with an initial message and cause.
 * @param initialMsg the initial error message
 * @param cause the underlying cause of the exception
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,getServerProperties,org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.net.InetAddress),116,122,"/**
* Masks properties based on client address.
* @param clientAddress IP address of the client
* @return Map of masked properties or default SASL properties if condition not met
*/","* Identify the Sasl Properties to be used for a connection with a client.
   * @param clientAddress client's address
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,getIdentifier,"org.apache.hadoop.security.SaslRpcServer:getIdentifier(java.lang.String,org.apache.hadoop.security.token.SecretManager)",192,204,"/**
* Deserializes a token identifier from a given ID.
* @param id unique token identifier string
* @param secretManager SecretManager instance for token operations
* @return deserialized TokenIdentifier object
* @throws InvalidToken if deserialization fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,checkCodec,org.apache.hadoop.crypto.CryptoStreamUtils:checkCodec(org.apache.hadoop.crypto.CryptoCodec),75,81,"/**
* Validates the cipher suite of the provided codec.
* @param codec cryptographic codec to validate
* @throws UnsupportedCodecException if the codec does not use AES/CTR/NoPadding or SM4/CTR/NoPadding
*/","* AES/CTR/NoPadding or SM4/CTR/NoPadding is required.
   *
   * @param codec crypto codec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,<init>,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:<init>(),35,40,"/**
* Constructor checks OpenSSL cipher availability, throws exception if unavailable.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getPos,org.apache.hadoop.crypto.CryptoInputStream:getPos(),580,585,"/**
 * Calculates masked function value.
 * @return long result of masking operation
 * @throws IOException if I/O error occurs
 */",Get underlying stream position.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,available,org.apache.hadoop.crypto.CryptoInputStream:available(),672,677,"/**
 * Calls m1 and returns sum of in.m2() and outBuffer.m3().
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFromUnderlyingStream,org.apache.hadoop.crypto.CryptoInputStream:readFromUnderlyingStream(java.nio.ByteBuffer),223,231,"/**
 * Reads data from ByteBuffer into a temporary array.
 * @param inBuffer source of data
 * @return number of bytes read, or -1 if no data available
 */",Read data from underlying stream.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:<init>(int,java.lang.String,org.apache.hadoop.crypto.CipherSuite,java.lang.String)",115,126,"/**
* Initializes a JceCtrCipher with specified parameters.
* @param mode encryption/decryption mode
* @param provider security provider, can be null
* @param suite cryptographic cipher suite
* @param name cipher name
* @throws GeneralSecurityException if initialization fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherSuite.java,convert,org.apache.hadoop.crypto.CipherSuite:convert(java.lang.String),84,92,"/**
* Retrieves CipherSuite by name.
* @param name the name of the cipher suite
* @return matching CipherSuite or throws exception if not found
*/","* Convert to CipherSuite from name, {@link #algoBlockSize} is fixed for
   * certain cipher suite, just need to compare the name.
   * @param name cipher suite name
   * @return CipherSuite cipher suite",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,encrypt,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",140,143,"/**
 * Masks input buffer and writes to output buffer.
 * @param inBuffer source data buffer
 * @param outBuffer destination data buffer
 * @throws IOException if I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,decrypt,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",145,148,"/**
 * Masks input buffer and writes to output buffer.
 * @param inBuffer source ByteBuffer containing data to mask
 * @param outBuffer destination ByteBuffer for masked data
 * @throws IOException if an I/O error occurs during processing
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoProtocolVersion.java,supports,org.apache.hadoop.crypto.CryptoProtocolVersion:supports(org.apache.hadoop.crypto.CryptoProtocolVersion),54,64,"/**
* Checks if a given protocol version is masked.
* @param version the CryptoProtocolVersion to check
* @return true if version is masked, false otherwise
*/","* Returns if a given protocol version is supported.
   *
   * @param version version number
   * @return true if the version is supported, else false",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherOption.java,<init>,org.apache.hadoop.crypto.CipherOption:<init>(org.apache.hadoop.crypto.CipherSuite),34,36,"/**
 * Constructs a CipherOption with a specified CipherSuite.
 * @param suite the CipherSuite to use
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,tokenizeTransformation,org.apache.hadoop.crypto.OpensslCipher:tokenizeTransformation(java.lang.String),155,179,"/**
 * Parses a transformation string and creates a Transform object.
 * @param transformation input transformation string in the form of ""part1/part2/part3""
 * @return Transform object initialized with parsed parts
 * @throws NoSuchAlgorithmException if the input is null or format is invalid
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,finalize,org.apache.hadoop.crypto.OpensslCipher:finalize(),293,296,"/**
* Calls method m1.
* @throws Throwable if m1 throws an exception
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OpensslSecureRandom.java,next,org.apache.hadoop.crypto.random.OpensslSecureRandom:next(int),105,118,"/**
* Generates a mask with the specified number of bits.
* @param numBits number of bits in the mask (0 to 32)
* @return integer representing the bit mask
*/","* Generates an integer containing the user-specified number of
   * random bits (right justified, with leading zeros).
   *
   * @param numBits number of random bits to be generated, where
   * 0 {@literal <=} <code>numBits</code> {@literal <=} 32.
   *
   * @return int an <code>int</code> containing the user-specified number
   * of random bits (right justified, with leading zeros).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersion(java.lang.String),327,350,"/**
* Retrieves a key version by name.
* @param versionName unique key version identifier
* @return KeyVersion object or null if not found
* @throws IOException on key retrieval failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,innerSetKeyVersion,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:innerSetKeyVersion(java.lang.String,java.lang.String,byte[],java.lang.String)",495,506,"/**
 * Stores a key with the given name and version.
 * @param name unique key identifier
 * @param versionName specific version of the key
 * @param material raw key material
 * @param cipher encryption algorithm used
 * @return new KeyVersion object
 * @throws IOException if key storage fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSKeyVersion:<init>(java.lang.String,java.lang.String,byte[])",611,613,"/**
* Constructs a new KMSKeyVersion.
* @param keyName name of the encryption key
* @param versionName version identifier for the key
* @param material cryptographic material associated with the key version
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,createKeyProviderCryptoExtension,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider),605,621,"/**
* Wraps a KeyProvider with CryptoExtension.
* @param keyProvider the original KeyProvider
* @return KeyProviderCryptoExtension instance
*/","* Creates a <code>KeyProviderCryptoExtension</code> using a given
   * {@link KeyProvider}.
   * <p>
   * If the given <code>KeyProvider</code> implements the
   * {@link CryptoExtension} interface the <code>KeyProvider</code> itself
   * will provide the extension functionality.
   * If the given <code>KeyProvider</code> implements the
   * {@link KeyProviderExtension} interface and the KeyProvider being
   * extended by the <code>KeyProvider</code> implements the
   * {@link CryptoExtension} interface, the KeyProvider being extended will
   * provide the extension functionality. Otherwise, a default extension
   * implementation will be used.
   *
   * @param keyProvider <code>KeyProvider</code> to use to create the
   * <code>KeyProviderCryptoExtension</code> extension.
   * @return a <code>KeyProviderCryptoExtension</code> instance using the
   * given <code>KeyProvider</code>.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,close,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:close(),623,629,"/**
* Recursively calls m2 on nested KeyProviders.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,readObject,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:readObject(java.io.ObjectInputStream),700,705,"/**
* Reads and initializes metadata from input stream.
* @param in ObjectInputStream to read data from
* @throws IOException if an I/O error occurs
* @throws ClassNotFoundException if class of serialized object cannot be found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSMetadata:<init>(java.lang.String,int,java.lang.String,java.util.Map,java.util.Date,int)",647,650,"/**
* Constructs a KMSMetadata object with specified parameters.
* @param cipher encryption algorithm used
* @param bitLength key length in bits
* @param description metadata description
* @param attributes additional metadata attributes
* @param created creation date of the metadata
* @param versions number of version iterations
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,writeObject,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:writeObject(java.io.ObjectOutputStream),694,698,"/**
* Writes serialized metadata to ObjectOutputStream.
* @param out stream to write metadata to
* @throws IOException if I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,printException,org.apache.hadoop.crypto.key.KeyShell:printException(java.lang.Exception),533,537,"/**
* Logs an error message for a failed command execution.
* @param e the exception that caused the failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$ListCommand:execute(),252,271,"/**
* Lists keys from a KeyProvider.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,getKeysMetadata,org.apache.hadoop.crypto.key.KeyProviderExtension:getKeysMetadata(java.lang.String[]),61,64,"/**
 * Retrieves metadata for given names.
 * @param names variable length array of names
 * @return Metadata array or empty if none found
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,cleanupNewAndOld,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:cleanupNewAndOld(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",600,605,"/**
* Masks old file by moving it and updating metadata.
* @param newPath destination path for masking
* @param oldPath original file path to be masked
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,backupToOld,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:backupToOld(org.apache.hadoop.fs.Path),622,630,"/**
 * Masks an old path by moving it to a new location.
 * @param oldPath the original file path to mask
 * @return true if masking is successful, false if file not found
 * @throws IOException if an I/O error occurs during the process
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,revertFromOld,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:revertFromOld(org.apache.hadoop.fs.Path,boolean)",632,637,"/**
 * Masks a file by renaming it.
 * @param oldPath original file path
 * @param fileExisted flag indicating if the file exists
 * @throws IOException if an I/O error occurs during masking
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,deleteKey,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:deleteKey(java.lang.String),462,493,"/**
* Masks a key by removing all its versions and the key itself.
* @param name key to be masked
* @throws IOException if an I/O error occurs or key does not exist
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getAlgorithm,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:getAlgorithm(),679,682,"/**
 * Retrieves masked data from metadata.
 * @return Masked string representation of data
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.KeyProvider:getCurrentKey(java.lang.String),496,502,"/**
 * Retrieves a key version by name.
 * @param name the name of the key
 * @return KeyVersion object or null if not found
 * @throws IOException if an I/O error occurs
 */","* Get the current version of the key, which should be used for encrypting new
   * data.
   * @param name the base name of the key
   * @return the version name of the current version of the key or null if the
   *    key version doesn't exist
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,generateKey,"org.apache.hadoop.crypto.key.KeyProvider:generateKey(int,java.lang.String)",545,552,"/**
 * Generates a cryptographic key.
 * @param size key size in bits
 * @param algorithm encryption algorithm name
 * @return generated key as byte array
 * @throws NoSuchAlgorithmException if the algorithm is not available
 */","* Generates a key material.
   *
   * @param size length of the key.
   * @param algorithm algorithm to use for generating the key.
   * @return the generated key.
   * @throws NoSuchAlgorithmException no such algorithm exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,<init>,"org.apache.hadoop.crypto.key.CachingKeyProvider:<init>(org.apache.hadoop.crypto.key.KeyProvider,long,long)",91,95,"/**
* Initializes a CachingKeyProvider with given parameters.
* @param keyProvider base KeyProvider for key management
* @param keyTimeoutMillis default timeout for keys in milliseconds
* @param currKeyTimeoutMillis current specific key timeout in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,invalidateCache,org.apache.hadoop.crypto.key.CachingKeyProvider:invalidateCache(java.lang.String),156,164,"/**
* Processes a key-related operation.
* @param name the key identifier
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,invalidateCache,org.apache.hadoop.crypto.key.KeyProviderExtension:invalidateCache(java.lang.String),120,123,"/**
 * Delegates call to key provider with given name.
 * @param name identifier for the key
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:execute(),511,525,"/**
* Invalidates cache for a specific key in the KeyProvider.
* @throws NoSuchAlgorithmException if algorithm is not found
* @throws IOException if I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,toJSON,org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProvider$KeyVersion),95,108,"/**
* Converts KeyVersion to JSON map.
* @param keyVersion KeyProvider.KeyVersion object
* @return Map containing key details or empty if null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,generateEncryptedKey,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(org.apache.hadoop.crypto.Encryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,byte[],byte[])",307,325,"/**
* Encrypts a key using the provided encryptor and parameters.
* @param encryptor encryption utility
* @param encryptionKey versioned encryption key
* @param key data to be encrypted
* @param iv initialization vector
* @return EncryptedKeyVersion containing encrypted key details
* @throws IOException if I/O error occurs during encryption
* @throws GeneralSecurityException if encryption fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,createForDecryption,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:createForDecryption(java.lang.String,java.lang.String,byte[],byte[])",103,110,"/**
* Creates an EncryptedKeyVersion from given parameters.
* @param keyName name of the key
* @param encryptionKeyVersionName version name of the encryption key
* @param encryptedKeyIv initialization vector for the encrypted key
* @param encryptedKeyMaterial encrypted key material
* @return new EncryptedKeyVersion object
*/","* Factory method to create a new EncryptedKeyVersion that can then be
     * passed into {@link #decryptEncryptedKey}. Note that the fields of the
     * returned EncryptedKeyVersion will only partially be populated; it is not
     * necessarily suitable for operations besides decryption.
     *
     * @param keyName Key name of the encryption key use to encrypt the
     *                encrypted key.
     * @param encryptionKeyVersionName Version name of the encryption key used
     *                                 to encrypt the encrypted key.
     * @param encryptedKeyIv           Initialization vector of the encrypted
     *                                 key. The IV of the encryption key used to
     *                                 encrypt the encrypted key is derived from
     *                                 this IV.
     * @param encryptedKeyMaterial     Key material of the encrypted key.
     * @return EncryptedKeyVersion suitable for decryption.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,decryptEncryptedKey,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.Decryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",410,431,"/**
* Decrypts an encrypted key version.
* @param decryptor used for decryption
* @param encryptionKey key for decryption
* @param encryptedKeyVersion encrypted key to decrypt
* @return decrypted KeyVersion object
* @throws IOException if I/O error occurs
* @throws GeneralSecurityException if security error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension.java,createKeyProviderDelegationTokenExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:createKeyProviderDelegationTokenExtension(org.apache.hadoop.crypto.key.KeyProvider),138,148,"/**
* Creates a KeyProviderDelegationTokenExtension.
* @param keyProvider the original KeyProvider
* @return KeyProviderDelegationTokenExtension instance
*/","* Creates a <code>KeyProviderDelegationTokenExtension</code> using a given 
   * {@link KeyProvider}.
   * <p>
   * If the given <code>KeyProvider</code> implements the 
   * {@link DelegationTokenExtension} interface the <code>KeyProvider</code> 
   * itself will provide the extension functionality, otherwise a default 
   * extension implementation will be used.
   * 
   * @param keyProvider <code>KeyProvider</code> to use to create the 
   * <code>KeyProviderDelegationTokenExtension</code> extension.
   * @return a <code>KeyProviderDelegationTokenExtension</code> instance 
   * using the given <code>KeyProvider</code>.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$CreateCommand:validate(),431,454,"/**
 * Masks data based on provider configuration.
 * @return true if masking is successful, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,writeJson,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:writeJson(java.lang.Object,java.io.OutputStream)",253,257,"/**
 * Writes an object to an output stream in JSON format.
 * @param obj the object to serialize
 * @param os the output stream to write to
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,checkNotEmpty,"org.apache.hadoop.util.KMSUtil:checkNotEmpty(java.lang.String,java.lang.String)",133,141,"/**
 * Masks a string with given name.
 * @param s the string to mask
 * @param name the name of the parameter for error message
 * @return the masked string
 * @throws IllegalArgumentException if the string is empty
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,warmUpEncryptedKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:warmUpEncryptedKeys(java.lang.String[]),951,959,"/**
 * Masks encryption keys by names.
 * @param keyNames variable number of key names to mask
 * @throws IOException if execution fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,close,org.apache.hadoop.crypto.key.kms.KMSClientProvider:close(),1195,1207,"/**
* Executes mask operation and handles exceptions.
* @throws IOException if an error occurs during execution
*/",* Shutdown valueQueue executor threads,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,submitRefillTask,"org.apache.hadoop.crypto.key.kms.ValueQueue:submitRefillTask(java.lang.String,java.util.Queue)",401,443,"/**
* Masks a key and processes the queue.
* @param keyName name of the key to mask
* @param keyQueue queue containing keys to process
* @throws InterruptedException if thread is interrupted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,deleteByName,org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:deleteByName(java.lang.String),187,194,"/**
* Retrieves and processes a named runnable.
* @param name unique identifier for the runnable
* @return NamedRunnable object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getLock,org.apache.hadoop.crypto.key.kms.ValueQueue:getLock(java.lang.String),136,138,"/**
 * Retrieves a read-write lock for a given key.
 * @param keyName identifier for the lock
 * @return ReadWriteLock associated with the key
 */","* Get the stripped lock given a key name.
   *
   * @param keyName The key name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,flush,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:flush(),557,567,"/**
* Flushes all KMSClientProviders.
* Logs errors for individual providers on failure.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,isTransient,org.apache.hadoop.crypto.key.KeyProviderExtension:isTransient(),56,59,"/**
 * Checks key availability.
 * @return true if key is available, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,warnIfTransientProvider,org.apache.hadoop.crypto.key.KeyShell$Command:warnIfTransientProvider(),219,223,"/**
* Checks and logs warning if modifying a transient provider.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,createKeyProviderFromUri,"org.apache.hadoop.util.KMSUtil:createKeyProviderFromUri(org.apache.hadoop.conf.Configuration,java.net.URI)",81,93,"/**
* Creates a KeyProvider instance from configuration and URI.
* @param conf configuration settings
* @param providerUri URI for the key provider
* @return KeyProvider object
* @throws IOException if instantiation fails or provider is transient
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,append,org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String),207,215,"/**
* Appends a field to the builder if valid.
* @param field the field to append
* @return the Builder instance
*/","* Append new field to the context.
     * @param field one of fields to append.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,append,"org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String,java.lang.String)",223,231,"/**
* Adds key-value pair to the builder.
* @param key unique identifier
* @param value associated data
* @return Builder instance for chaining
*/","* Append new field which contains key and value to the context.
     * @param key the key of field.
     * @param value the value of field.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,appendIfAbsent,"org.apache.hadoop.ipc.CallerContext$Builder:appendIfAbsent(java.lang.String,java.lang.String)",240,251,"/**
* Appends key-value pair to the builder.
* @param key unique identifier for the value
* @param value associated with the key
* @return Builder instance for method chaining
*/","* Append new field which contains key and value to the context
     * if the key(""key:"") is absent.
     * @param key the key of field.
     * @param value the value of field.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,"org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,java.lang.String)",148,154,"/**
* Initializes a Builder with given context and separator.
* @param context initial string context
* @param separator character to separate fields
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RefreshResponse.java,successResponse,org.apache.hadoop.ipc.RefreshResponse:successResponse(),37,39,"/**
 * Returns a successful refresh response.
 * @return RefreshResponse with status code 0 and message ""Success""
 */","* Convenience method to create a response for successful refreshes.
   * @return void response",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,unpack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto),82,104,"/**
* Constructs a RefreshResponse from a GenericRefreshResponseProto.
* @param proto protocol buffer containing refresh data
* @return RefreshResponse object with extracted data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolServerSideTranslatorPB.java,pack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:pack(java.util.Collection),66,83,"/**
 * Converts a collection of RefreshResponse to GenericRefreshResponseCollectionProto.
 * @param responses list of RefreshResponse objects
 * @return GenericRefreshResponseCollectionProto object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ObserverRetryOnActiveException.java,<init>,org.apache.hadoop.ipc.ObserverRetryOnActiveException:<init>(java.lang.String),32,34,"/**
 * Constructs an ObserverRetryOnActiveException with a message.
 * @param msg detailed error message
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientId.java,toString,org.apache.hadoop.ipc.ClientId:toString(byte[]),52,62,"/**
 * Generates a UUID from client ID.
 * @param clientId byte array representing the client ID
 * @return UUID string or empty if input is invalid
 */","* @return Convert a clientId byte[] to string.
   * @param clientId input clientId.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long)",72,82,"/**
* Constructs a CacheEntry with client ID, call ID, and expiration time.
* @param clientId unique client identifier as byte array (must be 16 bytes)
* @param callId unique call identifier
* @param expirationTime entry expiration timestamp in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,isServerFailOverEnabledByQueue,org.apache.hadoop.ipc.CallQueueManager:isServerFailOverEnabledByQueue(),242,249,"/**
* Checks if the queue is fair.
* @return true if the queue is an instance of FairCallQueue and m2() returns true, otherwise false
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.ipc.Schedulable),722,725,"/**
 * Delegates task to call queue.
 * @param e schedulable task to be queued
 * @return result of queuing operation
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isClientBackoffEnabled,org.apache.hadoop.ipc.Server:isClientBackoffEnabled(),3872,3874,"/**
 * Delegates to callQueue's m1 method.
 * @return result of callQueue.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,addInternal,"org.apache.hadoop.ipc.CallQueueManager:addInternal(org.apache.hadoop.ipc.Schedulable,boolean)",306,321,"/**
* Performs a masked operation with optional backoff.
* @param e element to process
* @param checkBackoff flag to enable backoff logic
* @return result of the operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,offer,org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object),335,338,"/**
 * Checks if an element is present.
 * @param e the element to check
 * @return true if element is present, false otherwise
 */","* Insert e into the backing queue.
   * Return true if e is queued.
   * Return false if the queue is full.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,offer,"org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)",340,344,"/**
 * Delegates to m2 of putRef's m1 with provided parameters.
 * @param e element to process
 * @param timeout maximum time to wait
 * @param unit time unit for timeout
 * @return true if successful, false otherwise
 * @throws InterruptedException if interrupted while waiting
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getCallQueueLen,org.apache.hadoop.ipc.Server:getCallQueueLen(),3868,3870,"/**
 * Returns result of calling m1 on callQueue.
 * @return integer value from m1 method
 */","* The number of rpc calls in the queue.
   * @return The number of rpc calls in the queue.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolInterfaces,org.apache.hadoop.ipc.RPC:getProtocolInterfaces(java.lang.Class),144,147,"/**
 * Retrieves and processes interfaces from a given protocol class.
 * @param protocol the protocol class to process
 * @return an array of processed interface classes
 */","* Get all interfaces that the given protocol implements or extends
   * which are assignable from VersionedProtocol.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getServerAddress,org.apache.hadoop.ipc.RPC:getServerAddress(java.lang.Object),740,742,"/**
 * Retrieves socket address from proxy.
 * @param proxy proxy object to extract address from
 * @return InetSocketAddress of the proxy or null if not applicable
 */","* @return Returns the server address for a given proxy.
   * @param proxy input proxy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,org.apache.hadoop.ipc.CallerContext:<init>(org.apache.hadoop.ipc.CallerContext$Builder),71,74,"/**
 * Constructs a CallerContext using a Builder.
 * @param builder the Builder object containing context and signature
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,toString,org.apache.hadoop.ipc.CallerContext:toString(),112,123,"/**
* Generates masked function string.
* @return masked string based on context and signature
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,sendPing,org.apache.hadoop.ipc.Client$Connection:sendPing(),1071,1080,"/**
* Sends a ping request if the interval since last activity exceeds the ping interval.
* @throws IOException if an I/O error occurs while sending the ping
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,registerProtocolEngine,"org.apache.hadoop.ipc.Server:registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)",288,300,"/**
* Registers RPC kind with its handler.
* @param rpcKind type of RPC
* @param rpcRequestWrapperClass class for request wrapping
* @param rpcInvoker invoker for handling RPCs
* Throws IllegalArgumentException if already registered
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ExternalCall.java,get,org.apache.hadoop.ipc.ExternalCall:get(),47,53,"/**
* Executes task and returns result.
* @throws InterruptedException if thread is interrupted
* @throws ExecutionException if error occurs during execution
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,monotonicNowNanos,org.apache.hadoop.util.Timer:monotonicNowNanos(),58,60,"/**
 * Returns the current time in milliseconds.
 * @return Current time in milliseconds
 */","* Same as {@link #monotonicNow()} but returns its result in nanoseconds.
   * Note that this is subject to the same resolution constraints as
   * {@link System#nanoTime()}.
   * @return a monotonic clock that counts in nanoseconds.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getUserGroupInformation,org.apache.hadoop.ipc.Server$Call:getUserGroupInformation(),1112,1115,"/**
 * Returns masked UserGroupInformation.
 * @return Masked UserGroupInformation object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteUser,org.apache.hadoop.ipc.Server:getRemoteUser(),445,448,"/**
* Retrieves user group info from current call.
* @return UserGroupInformation object or null if no call present
*/","Returns the RPC remote user when invoked inside an RPC.  Note this
   *  may be different than the current user if called within another doAs
   *  @return connection's UGI or null if not an RPC",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doResponse,org.apache.hadoop.ipc.Server$Call:doResponse(java.lang.Throwable),1105,1107,"/**
 * Calls m1 with Throwable and FATAL status.
 * @param t the Throwable to process
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler$DecayTask:<init>(org.apache.hadoop.ipc.DecayRpcScheduler,java.util.Timer)",210,213,"/**
* Initializes a DecayTask with a scheduler and timer.
* @param scheduler the DecayRpcScheduler to be referenced weakly
* @param timer the Timer used for scheduling tasks
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,putVersionSignatureMap,"org.apache.hadoop.ipc.RpcClientUtil:putVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String,java.util.Map)",86,89,"/**
 * Caches RPC protocol signatures.
 * @param addr InetSocketAddress of the server
 * @param protocol name of the protocol
 * @param rpcKind type of RPC (e.g., simple or fat)
 * @param map mapping of method IDs to ProtocolSignature objects
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,getVersionSignatureMap,"org.apache.hadoop.ipc.RpcClientUtil:getVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String)",91,94,"/**
 * Retrieves protocol signatures by address and RPC kind.
 * @param addr InetSocketAddress for the connection
 * @param protocol Protocol name
 * @param rpcKind RPC kind identifier
 * @return Map of Long to ProtocolSignature
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getFingerprints,org.apache.hadoop.ipc.ProtocolSignature:getFingerprints(java.lang.reflect.Method[]),121,130,"/**
* Generates an array of hash codes for given methods.
* @param methods array of Method objects
* @return int array of hash codes or null if input is null
*/","* Convert an array of Method into an array of hash codes
   * 
   * @param methods
   * @return array of hash codes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,convertProtocolSignatureProtos,org.apache.hadoop.ipc.RpcClientUtil:convertProtocolSignatureProtos(java.util.List),149,161,"/**
* Converts a list of protocol signatures to a map.
* @param protoList list of ProtocolSignatureProto objects
* @return map mapping protocol IDs to ProtocolSignature objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,methodExists,"org.apache.hadoop.ipc.RpcClientUtil:methodExists(int,long,java.util.Map)",163,174,"/**
* Checks if method hash is present in the protocol signature for a given version.
* @param methodHash unique identifier of the method
* @param version specific version to check
* @param versionMap mapping of versions to their respective signatures
* @return true if method hash exists in the signature, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RefreshRegistry.java,dispatch,"org.apache.hadoop.ipc.RefreshRegistry:dispatch(java.lang.String,java.lang.String[])",94,131,"/**
* Processes refresh requests for a given identifier.
* @param identifier unique request identifier
* @param args additional arguments for processing
* @return collection of RefreshResponse objects
*/","* Lookup the responsible handler and return its result.
   * This should be called by the RPC server when it gets a refresh request.
   * @param identifier the resource to refresh
   * @param args the arguments to pass on, not including the program name
   * @throws IllegalArgumentException on invalid identifier
   * @return the response from the appropriate handler",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,<init>,"org.apache.hadoop.ipc.RemoteException:<init>(java.lang.String,java.lang.String)",40,42,"/**
 * Constructs a RemoteException with a class name and message.
 * @param className name of the class where the exception occurred
 * @param msg detailed error message
 */","* @param className wrapped exception, may be null
   * @param msg may be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,unwrapRemoteException,org.apache.hadoop.ipc.RemoteException:unwrapRemoteException(java.lang.Class[]),81,96,"/**
* Masks IOException for specified lookup types.
* @param lookupTypes array of Class objects to check
* @return this object if no matching type found or exception occurs
*/","* If this remote exception wraps up one of the lookupTypes
   * then return this exception.
   * <p>
   * Unwraps any IOException.
   * 
   * @param lookupTypes the desired exception class. may be null.
   * @return IOException, which is either the lookupClass exception or this.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,unwrapRemoteException,org.apache.hadoop.ipc.RemoteException:unwrapRemoteException(),107,115,"/**
* Masks an exception by returning an IOException.
* @return IOException instance or current object if an error occurs
*/","* Instantiate and return the exception wrapped up by this remote exception.
   * 
   * <p> This unwraps any <code>Throwable</code> that has a constructor taking
   * a <code>String</code> as a parameter.
   * Otherwise it returns this.
   * 
   * @return <code>Throwable</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getNumInProcessHandler,org.apache.hadoop.ipc.metrics.RpcMetrics:getNumInProcessHandler(),157,160,"/**
 * Retrieves the number of in-process handlers.
 * @return Count of active handlers
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTotalRequests,org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequests(),175,178,"/**
 * Returns the total number of requests.
 * @Metric(""Number of total requests"")
 * @return Total request count as a long
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTotalRequestsPerSecond,org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequestsPerSecond(),180,183,"/**
 * Returns the number of total requests processed per second.
 * @return Requests count as a long value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[],org.apache.hadoop.tracing.Span,org.apache.hadoop.ipc.CallerContext)",1000,1012,"/**
 * Initializes a new RPC call.
 * @param id unique call identifier
 * @param retryCount number of retries allowed
 * @param kind type of RPC call
 * @param clientId client identifier bytes
 * @param span tracing span
 * @param callerContext context for the caller
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,get,"org.apache.hadoop.ipc.ProcessingDetails:get(org.apache.hadoop.ipc.ProcessingDetails$Timing,java.util.concurrent.TimeUnit)",73,75,"/**
 * Converts timing value to specified unit.
 * @param type Timing enum representing the type of timing
 * @param timeUnit Target time unit for conversion
 * @return Converted timing value in target unit
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,toString,org.apache.hadoop.ipc.ProcessingDetails:toString(),97,108,"/**
* Generates a formatted string of timing information.
* @return Formatted string with timing details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedTimeCostProvider.java,getCost,org.apache.hadoop.ipc.WeightedTimeCostProvider:getCost(org.apache.hadoop.ipc.ProcessingDetails),100,109,"/**
* Calculates the total cost based on processing details.
* @param details contains necessary data for calculation
* @return computed cost as a long value
*/","* Calculates a weighted sum of the times stored on the provided processing
   * details to be used as the cost in {@link DecayRpcScheduler}.
   *
   * @param details Processing details
   * @return The weighted sum of the times. The returned unit is the same
   *         as the default unit used by the provided processing details.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,set,"org.apache.hadoop.ipc.ProcessingDetails:set(org.apache.hadoop.ipc.ProcessingDetails$Timing,long,java.util.concurrent.TimeUnit)",81,83,"/**
* Calls m2 with Timing and converted time value.
* @param type timing type
* @param value time duration
* @param timeUnit unit of time
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getSchedulingDecisionSummary,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getSchedulingDecisionSummary(),904,912,"/**
* Retrieves status from the active scheduler.
* @return Status string or ""No Active Scheduler"" if none is found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getUniqueIdentityCount,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getUniqueIdentityCount(),924,932,"/**
* Retrieves and executes an RPC scheduler, returning its result.
* @return result of the scheduler execution or -1 if scheduler is null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getTotalCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getTotalCallVolume(),934,942,"/**
* Retrieves a timestamp from the scheduler.
* @return current timestamp or -1 if scheduler is unavailable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getAverageResponseTime,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getAverageResponseTime(),944,952,"/**
* Retrieves response times, using default if scheduler is null.
* @return array of response times or default values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getResponseTimeCountInLastWindow,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getResponseTimeCountInLastWindow(),954,961,"/**
* Retrieves call counts using a scheduler or default value.
* @return long array of call counts
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumDroppedConnections,org.apache.hadoop.ipc.Server:getNumDroppedConnections(),3859,3862,"/**
 * Returns mask value from connection manager.
 * @return long value representing the mask
 */","* The number of RPC connections dropped due to
   * too many connections.
   * @return the number of dropped rpc connections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isFull,org.apache.hadoop.ipc.Server$ConnectionManager:isFull(),4088,4091,"/**
 * Checks if the number of active connections exceeds the maximum allowed.
 * @return true if active connections exceed maxConnections, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumOpenConnections,org.apache.hadoop.ipc.Server:getNumOpenConnections(),3837,3839,"/**
 * Retrieves mask value from connection manager.
 * @return integer mask value
 */","* The number of open RPC conections
   * @return the number of open rpc connections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getConnections,org.apache.hadoop.ipc.Server:getConnections(),756,759,"/**
 * Returns an array of database connections.
 * For testing purposes only.
 * @return Array of Connection objects
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,startIdleScan,org.apache.hadoop.ipc.Server$ConnectionManager:startIdleScan(),4159,4161,"/**
 * Calls function m1 to perform an operation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,putQueue,"org.apache.hadoop.ipc.FairCallQueue:putQueue(int,org.apache.hadoop.ipc.Schedulable)",229,233,"/**
* Masks an event with a given priority.
* @param priority level of importance for the event
* @param e event to be masked
* @throws InterruptedException if operation is interrupted
*/","* Put the element in a queue of a specific priority.
   * @param priority - queue priority
   * @param e - element to add",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offerQueue,"org.apache.hadoop.ipc.FairCallQueue:offerQueue(int,org.apache.hadoop.ipc.Schedulable)",241,248,"/**
* Masks an element in a queue based on priority.
* @param priority the priority level for queuing
* @param e the element to be masked
* @return true if masking is successful, false otherwise
*/","* Offer the element to queue of a specific priority.
   * @param priority - queue priority
   * @param e - element to add
   * @return boolean if added to the given queue",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,"org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable,long,java.util.concurrent.TimeUnit)",269,279,"/**
* Adds an element to the appropriate queue with a timeout.
* @param e element to add
* @param timeout maximum time to wait
* @param unit time unit for timeout
* @return true if added, false otherwise
* @throws InterruptedException if interrupted while waiting
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable),281,290,"/**
* Adds element to prioritized queue and triggers action.
* @param e element to add
* @return true if added, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,drainTo,org.apache.hadoop.ipc.FairCallQueue:drainTo(java.util.Collection),366,369,"/**
 * Calls m1 with default max size.
 * @param c collection to operate on
 * @return result of m1 with max size Integer.MAX_VALUE
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addTerseExceptions,org.apache.hadoop.ipc.Server:addTerseExceptions(java.lang.Class[]),174,176,"/**
 * Masks specified exceptions.
 * @param exceptionClass variable number of exception classes to mask
 */","* Add exception classes for which server won't log stack traces.
   *
   * @param exceptionClass exception classes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addSuppressedLoggingExceptions,org.apache.hadoop.ipc.Server:addSuppressedLoggingExceptions(java.lang.Class[]),183,185,"/**
 * Handles exceptions by delegating to another handler.
 * @param exceptionClass variable number of exception classes to handle
 */","* Add exception classes which server won't log at all.
   *
   * @param exceptionClass exception classes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,logException,"org.apache.hadoop.ipc.Server:logException(org.slf4j.Logger,java.lang.Throwable,org.apache.hadoop.ipc.Server$Call)",3244,3262,"/**
* Handles exceptions by logging them.
* @param logger Logger instance for logging messages
* @param e Throwable exception to handle
* @param call Call object associated with the exception
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getSupportedProtocolVersions,"org.apache.hadoop.ipc.RPC$Server:getSupportedProtocolVersions(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",1146,1164,"/**
* Filters and returns protocol implementations by name.
* @param rpcKind type of RPC
* @param protocolName name of the protocol to filter
* @return array of VerProtocolImpl or null if none found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getHighestSupportedProtocol,"org.apache.hadoop.ipc.RPC$Server:getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",1166,1187,"/**
 * Finds the highest version of a protocol for a given RPC kind.
 * @param rpcKind type of RPC
 * @param protocolName name of the protocol to search
 * @return VerProtocolImpl with the highest version or null if not found
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UnexpectedServerException.java,<init>,org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String),32,34,"/**
 * Constructs an unexpected server exception with a specified message.
 * @param message detailed error message
 */","* Constructs exception with the specified detail message.
   * 
   * @param messages detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcServerException.java,<init>,org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String),33,35,"/**
 * Constructs an RpcServerException with a specified error message.
 * @param message detailed error description
 */","* Constructs exception with the specified detail message.
   * @param message detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientException.java,<init>,org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String),31,33,"/**
 * Constructs an RpcClientException with the specified detail message.
 * @param message the detail message of the exception
 */","* Constructs exception with the specified detail message.
   * 
   * @param messages detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UnexpectedServerException.java,<init>,"org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String,java.lang.Throwable)",45,47,"/**
 * Constructs an UnexpectedServerException with a message and a cause.
 * @param message detail message of the exception
 * @param cause underlying cause of the exception
 */","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause that cause this exception
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcServerException.java,<init>,"org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String,java.lang.Throwable)",45,47,"/**
 * Constructs an RpcServerException with a specified message and cause.
 * @param message descriptive error message
 * @param cause underlying exception that caused this exception
 */","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientException.java,<init>,"org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String,java.lang.Throwable)",44,46,"/**
 * Constructs an RpcClientException with a message and a cause.
 * @param message exception message
 * @param cause underlying throwable causing this exception
 */","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause that cause this exception
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,setCapacity,org.apache.hadoop.ipc.ResponseBuffer:setCapacity(int),60,62,"/**
 * Sets buffer capacity.
 * @param capacity new buffer size
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,reset,org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:reset(),98,102,"/**
 * Resets count to framing bytes and initializes mask.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,getFramedBuffer,org.apache.hadoop.ipc.ResponseBuffer:getFramedBuffer(),42,46,"/**
* Masks and returns the output buffer.
* @return FramedBuffer with applied mask
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addCost,"org.apache.hadoop.ipc.DecayRpcScheduler:addCost(java.lang.Object,long)",568,600,"/**
 * Updates call costs for a given identity.
 * @param identity unique identifier for the call
 * @param costDelta change in cost to be applied
 */","* Adjust the stored cost for a given identity.
   *
   * @param identity the identity of the user whose cost should be adjusted
   * @param costDelta the cost to add for the given identity",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,computePriorityLevel,"org.apache.hadoop.ipc.DecayRpcScheduler:computePriorityLevel(long,java.lang.Object)",609,634,"/**
* Determines priority based on cost and identity.
* @param cost the call cost
* @param identity unique identifier for the entity
* @return priority level or 0 if not applicable
*/","* Given the cost for an identity, compute a scheduling decision.
   *
   * @param cost the cost for an identity
   * @param identity the identity of the user
   * @return scheduling decision from 0 to numLevels - 1",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,setPriorityLevel,"org.apache.hadoop.ipc.DecayRpcScheduler:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",693,699,"/**
* Sets priority for a user based on UserGroupInformation.
* @param ugi user group information object
* @param priority initial priority level
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getCallVolumeSummary,org.apache.hadoop.ipc.DecayRpcScheduler:getCallVolumeSummary(),1127,1133,"/**
* Masks data using writer and error handling.
* @return masked string or error message
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,wrap,org.apache.hadoop.ipc.RpcWritable$Buffer:wrap(java.nio.ByteBuffer),145,147,"/**
 * Converts ByteBuffer to Buffer.
 * @param bb input ByteBuffer
 * @return Buffer object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>(),514,515,"/**
 * Constructs a new instance of RpcProtobufRequest.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto,com.google.protobuf.Message)",517,520,"/**
* Constructs an RPC request with a header and payload.
* @param header RequestHeaderProto object containing metadata
* @param payload Message object representing the request data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>(),652,653,"/**
* Constructs an empty RpcProtobufRequest instance.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto,org.apache.hadoop.thirdparty.protobuf.Message)",655,658,"/**
* Constructs an RPC request with a header and payload.
* @param header RequestHeaderProto object containing metadata
* @param payload Message object representing the request data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getRemoteException,org.apache.hadoop.ipc.ProtobufHelper:getRemoteException(org.apache.hadoop.thirdparty.protobuf.ServiceException),56,58,"/**
 * Converts ServiceException to IOException.
 * @param se the ServiceException to convert
 * @return an IOException based on the given ServiceException
 */","* Return the IOException thrown by the remote server wrapped in
   * ServiceException as cause.
   * @param se ServiceException that wraps IO exception thrown by the server
   * @return Exception wrapped in ServiceException or
   *         a new IOException that wraps the unexpected ServiceException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,ipc,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:ipc(org.apache.hadoop.ipc.internal.ShadedProtobufHelper$IpcCall),158,164,"/**
* Executes an IPC call and handles exceptions.
* @param call IPC call to execute
* @return result of the IPC call
* @throws IOException if a service exception occurs
*/","* Evaluate a protobuf call, converting any ServiceException to an IOException.
   * @param call invocation to make
   * @return the result of the call
   * @param <T> type of the result
   * @throws IOException any translated protobuf exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(java.lang.String),94,96,"/**
 * Converts a string key to a ByteString.
 * @param key input string key
 * @return ByteString representation of the key
 */","* Get the ByteString for frequently used fixed and small set strings.
   * @param key string
   * @return ByteString for frequently used fixed and small set strings.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getByteString,org.apache.hadoop.ipc.ProtobufHelper:getByteString(byte[]),104,107,"/**
 * Wraps byte array in a ByteString.
 * @param bytes input byte array
 * @return ByteString representation of the input bytes
 */","* Get the byte string of a non-null byte array.
   * If the array is 0 bytes long, return a singleton to reduce object allocation.
   * @param bytes bytes to convert.
   * @return a value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,skipRetryCache,"org.apache.hadoop.ipc.RetryCache:skipRetryCache(byte[],int)",206,211,"/**
* Checks if client ID is masked or invalid.
* @param clientId byte array representing the client ID
* @param callId unique call identifier
* @return true if masking condition met, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,setState,"org.apache.hadoop.ipc.RetryCache:setState(org.apache.hadoop.ipc.RetryCache$CacheEntry,boolean)",382,387,"/**
* Updates cache entry status.
* @param e CacheEntry to update
* @param success indicates operation success
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,toString,org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:toString(),538,547,"/**
* Generates a masked string from request header.
* @return masked string combining header fields m2 and m3
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setCallIdAndRetryCount,"org.apache.hadoop.ipc.Client:setCallIdAndRetryCount(int,int,java.lang.Object)",122,128,"/**
* Masks function with given parameters.
* @param cid call identifier
* @param rc retry count
* @param externalHandler handler for external operations
*/","* Set call id and retry count for the next call.
   * @param cid input cid.
   * @param rc input rc.
   * @param externalHandler input externalHandler.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client:close(),1881,1885,"/**
* Masks functionality with an unstable override.
* Throws Exception on failure.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,checkAsyncCall,org.apache.hadoop.ipc.Client:checkAsyncCall(),1430,1442,"/**
* Checks and enforces the maximum limit of asynchronous calls.
* Throws AsyncCallLimitExceededException if exceeded.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getListenerAddress,org.apache.hadoop.ipc.Server:getListenerAddress(),3751,3753,"/**
 * Returns the address of the listener.
 * @return InetSocketAddress of the listener
 */","* Return the socket (ip+port) on which the RPC server is listening to.
   * @return the socket (ip+port) on which the RPC server is listening to.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuxiliaryListenerAddresses,org.apache.hadoop.ipc.Server:getAuxiliaryListenerAddresses(),3762,3770,"/**
* Retrieves all auxiliary listener addresses.
* @return Set of InetSocketAddress objects representing listener addresses
*/","* Return the set of all the configured auxiliary socket addresses NameNode
   * RPC is listening on. If there are none, or it is not configured at all, an
   * empty set is returned.
   * @return the set of all the auxiliary addresses on which the
   *         RPC server is listening on.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doStop,org.apache.hadoop.ipc.Server$Listener:doStop(),1673,1688,"/**
* Performs cleanup tasks: closes channels, logs exceptions, and resets readers.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,stopClient,org.apache.hadoop.ipc.ClientCache:stopClient(org.apache.hadoop.ipc.Client),99,120,"/**
 * Stops and removes a client from cache.
 * @param client the client to be stopped and removed
 */","* Stop a RPC client connection 
   * A RPC client is closed only when its reference count becomes zero.
   *
   * @param client input client.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,clearClientCache,org.apache.hadoop.ipc.ProtobufRpcEngine2:clearClientCache(),392,395,"/**
 * Calls m1 on CLIENTS instance.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getHostInetAddress,org.apache.hadoop.ipc.Server$RpcCall:getHostInetAddress(),1224,1227,"/**
 * Retrieves the remote address of the connection.
 * @return InetAddress representing the remote address
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemotePort,org.apache.hadoop.ipc.Server$RpcCall:getRemotePort(),1229,1232,"/**
 * Delegates call to connection's m1 method.
 * @return result from connection's m1
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setDeferredResponse,org.apache.hadoop.ipc.Server$RpcCall:setDeferredResponse(org.apache.hadoop.io.Writable),1347,1365,"/**
* Sets up a deferred success response if the connection is running.
* @param response writable object to send the response
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,toString,org.apache.hadoop.ipc.Server$RpcCall:toString(),1404,1407,"/**
 * Extends parent method by appending RPC request and connection details.
 * @return Concatenated string with additional information
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,waitForWork,org.apache.hadoop.ipc.Client$Connection:waitForWork(),1036,1062,"/**
* Handles connection logic based on call status and timeouts.
* @return true if processing should continue, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleConnectionTimeout,"org.apache.hadoop.ipc.Client$Connection:handleConnectionTimeout(int,int,java.io.IOException)",921,932,"/**
* Handles retry logic for IO exceptions.
* @param curRetries current number of retries
* @param maxRetries maximum allowed retries
* @param ioe the IOException to handle
* @throws IOException if max retries reached or another exception occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleConnectionFailure,"org.apache.hadoop.ipc.Client$Connection:handleConnectionFailure(int,java.io.IOException)",934,968,"/**
 * Handles retries for failed connections.
 * @param curRetries current number of retries
 * @param ioe the IOException that caused the failure
 * @throws IOException if the operation fails after retries
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,equals,org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:equals(java.lang.Object),168,171,"/**
 * Calls superclass method m1.
 * @param obj object to process
 * @return result of superclass method call
 */",Override equals to avoid findbugs warnings,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getQueueSizes,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getQueueSizes(),438,446,"/**
* Calls m1 and returns result of m2 on the returned object.
* @return array of integers or empty array if m1 returns null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getOverflowedCalls,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getOverflowedCalls(),448,456,"/**
 * Retrieves an array of long values from a FairCallQueue.
 * @return Array of long values or empty array if queue is null
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufWrapperLegacy.java,<init>,org.apache.hadoop.ipc.ProtobufWrapperLegacy:<init>(java.lang.Object),51,56,"/**
 * Constructs a ProtobufWrapperLegacy instance.
 * @param message the unshaded protobuf message to wrap
 */","* Construct.
   * The type of the parameter is Object so as to keep the casting internal
   * to this class.
   * @param message message to wrap.
   * @throws IllegalArgumentException if the class is not a protobuf message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,switchToSimple,org.apache.hadoop.ipc.Server$Connection:switchToSimple(),2401,2405,"/**
* Resets authentication protocol and calls m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,close,org.apache.hadoop.ipc.Server$Connection:close(),3082,3094,"/**
* Masks data and handles socket operations.
* Resets data, checks channel status, shuts down socket, and logs I/O.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processSaslToken,org.apache.hadoop.ipc.Server$Connection:processSaslToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2387,2399,"/**
* Processes SASL message and evaluates server response.
* @param saslMessage incoming SASL message from client
* @return processed RpcSaslProto with server state and token
* @throws SaslException if client does not send a token or evaluation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,checkDataLength,org.apache.hadoop.ipc.Server$Connection:checkDataLength(int),2446,2459,"/**
* Validates data length and throws exception if invalid.
* @param dataLength the length of the data to be processed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseOldVersionFatal,"org.apache.hadoop.ipc.Server:setupResponseOldVersionFatal(java.io.ByteArrayOutputStream,org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)",3627,3639,"/**
* Handles RPC response with error.
* @param response output stream for the response
* @param call RPC call object
* @param rv return value (unused)
* @param errorClass class name of the error
* @param error error message
*/","* Setup response for the IPC Call on Fatal Error from a 
   * client that is using old version of Hadoop.
   * The response is serialized using the previous protocol's response
   * layout.
   * 
   * @param response buffer to serialize the response into
   * @param call {@link Call} to which we are setting up the response
   * @param rv return value for the IPC Call, if the call was successful
   * @param errorClass error class, if the the call failed
   * @param error error message, if the call failed
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRpcRequestWrapper,org.apache.hadoop.ipc.Server:getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto),302,308,"/**
* Retrieves the Writable class for a given RPC kind.
* @param rpcKind type of RPC request
* @return Writable class or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,toString,org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:toString(),676,685,"/**
 * Generates a masked string from request header.
 * @return Concatenated string of header fields or throws exception on error
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,hashCode,org.apache.hadoop.ipc.RetryCache$CacheEntry:hashCode(),94,97,"/**
* Computes hash based on client ID and call ID.
* @return computed hash value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,advanceIndex,org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:advanceIndex(),121,132,"/**
* Checks and processes remaining API requests.
* Decrements request count; triggers action if none left.
*/","* Advances the index, which will change the current index
   * if called enough times.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:<init>(),398,403,"/**
* Initializes a new ProtobufRpcEngineCallbackImpl with current call details.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:<init>(),430,435,"/**
* Initializes the callback with server, call, and method details.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,error,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable),412,418,"/**
* Records error and processing metrics.
* @param t Throwable object representing the error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,error,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable),444,450,"/**
 * Logs error and records processing metrics.
 * @param t the Throwable to handle
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,capacity,org.apache.hadoop.ipc.ResponseBuffer:capacity(),56,58,"/**
 * Calls method m1 on the FramedBuffer instance.
 * @return result of calling m1 on FramedBuffer
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,ensureCapacity,org.apache.hadoop.ipc.ResponseBuffer:ensureCapacity(int),64,68,"/**
* Adjusts buffer size to accommodate specified capacity.
* @param capacity desired buffer capacity
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setException,org.apache.hadoop.ipc.Client$Call:setException(java.io.IOException),341,344,"/**
 * Handles IO exception by setting it and calling another method.
 * @param error IOException to be handled
 */","Set the exception when there is an error.
     * Notify the caller the call is done.
     * 
     * @param error exception thrown by the call; either local or remote",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setRpcResponse,org.apache.hadoop.ipc.Client$Call:setRpcResponse(org.apache.hadoop.io.Writable),351,354,"/**
 * Sets RPC response and calls helper method.
 * @param rpcResponse the Writable object containing the response
 */","Set the return value when there is no error. 
     * Notify the caller the call is done.
     * 
     * @param rpcResponse return value of the rpc call.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,read,org.apache.hadoop.ipc.Client$Connection$PingInputStream:read(),513,524,"/**
* Overrides method to handle SocketTimeoutException with retries.
* @return result of super.m2()
* @throws IOException if an I/O error occurs
*/","Read a byte from the stream.
       * Send a ping if timeout on read. Retries if no failure is detected
       * until a byte is read.
       * @throws IOException for any IO problem other than socket timeout",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,read,"org.apache.hadoop.ipc.Client$Connection$PingInputStream:read(byte[],int,int)",532,543,"/**
* Overrides method to handle socket timeouts with retries.
* @param buf buffer to write data into
* @param off offset in the buffer
* @param len number of bytes to read/write
* @return result of the operation
* @throws IOException if an I/O error occurs
*/","Read bytes into a buffer starting from offset <code>off</code>
       * Send a ping if timeout on read. Retries if no failure is detected
       * until a byte is read.
       * 
       * @return the total number of bytes read; -1 if the connection is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getHostAddress,org.apache.hadoop.ipc.Server$Call:getHostAddress(),1063,1066,"/**
* Retrieves hostname from network address.
* @return Hostname as String or null if no address is available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteIp,org.apache.hadoop.ipc.Server:getRemoteIp(),385,388,"/**
* Retrieves InetAddress from current call if available.
* @return InetAddress or null if no current call exists
*/","* @return Returns the remote side ip address when invoked inside an RPC
   *  Returns null in case of an error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getServerRpcInvoker,org.apache.hadoop.ipc.Server:getServerRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind),310,312,"/**
 * Returns an RPC invoker based on the specified kind.
 * @param rpcKind type of RPC to be used
 * @return RpcInvoker instance for the given RPC kind
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemotePort,org.apache.hadoop.ipc.Server:getRemotePort(),394,397,"/**
* Calls a method on an object returned by another method.
* @return integer result from m2 of Call or 0 if Call is null
*/","* @return Returns the remote side port when invoked inside an RPC
   * Returns 0 in case of an error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuxiliaryPortEstablishedQOP,org.apache.hadoop.ipc.Server:getAuxiliaryPortEstablishedQOP(),411,423,"/**
* Masks function for RPC calls.
* @return masked string or null if conditions not met
*/","* Returns the SASL qop for the current call, if the current call is
   * set, and the SASL negotiation is done. Otherwise return null
   * Note this only returns established QOP for auxiliary port, and
   * returns null for primary (non-auxiliary) port.
   *
   * Also note that CurCall is thread local object. So in fact, different
   * handler threads will process different CurCall object.
   *
   * Also, only return for RPC calls, not supported for other protocols.
   * @return the QOP of the current connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getProtocol,org.apache.hadoop.ipc.Server:getProtocol(),450,453,"/**
* Retrieves data from current call.
* @return result of call's m2 method or null if call is null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(),465,468,"/**
* Calls method m1 to get a Call object and then invokes m2 on it.
* @return result of call.m2() or 0 if call is null
*/","* @return Return the priority level assigned by call queue to an RPC
   * Returns 0 in case no priority is assigned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setLogSlowRPCThresholdTime,org.apache.hadoop.ipc.Server:setLogSlowRPCThresholdTime(long),556,560,"/**
* Sets the threshold for logging slow RPCs.
* @param logSlowRPCThresholdMs threshold in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setClientBackoffEnabled,org.apache.hadoop.ipc.Server:setClientBackoffEnabled(boolean),3876,3878,"/**
 * Delegates boolean value to callQueue.
 * @param value boolean flag to pass
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addAuxiliaryListener,org.apache.hadoop.ipc.Server:addAuxiliaryListener(int),3427,3443,"/**
* Binds a listener to an auxiliary port.
* @param auxiliaryPort the port number to bind the listener to
* @throws IOException if a listener is already bound to the port or an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseForProtobuf,"org.apache.hadoop.ipc.Server:setupResponseForProtobuf(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3585,3607,"/**
 * Encodes RpcResponseHeader and payload into a byte array.
 * @param header response header proto
 * @param rv writable object containing payload
 * @return byte array of encoded data
 * @throws IOException if encoding fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumOpenConnectionsPerUser,org.apache.hadoop.ipc.Server:getNumOpenConnectionsPerUser(),3844,3852,"/**
* Masks data using connection manager.
* @return masked JSON string or null if error occurs
*/",* @return Get the NumOpenConnections/User.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isServerFailOverEnabled,org.apache.hadoop.ipc.Server:isServerFailOverEnabled(),3880,3883,"/**
* Checks if the queue is empty.
* @return true if the queue is empty, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processResponse,"org.apache.hadoop.ipc.Server$Responder:processResponse(java.util.LinkedList,boolean)",1844,1922,"/**
* Processes RPC responses in the queue.
* @param responseQueue list of pending RPC calls
* @param inHandler flag indicating if called from handler
* @return true if processing is complete, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,equals,org.apache.hadoop.ipc.Client$ConnectionId:equals(java.lang.Object),1823,1841,"/**
* Compares ConnectionId for equality.
* @param obj object to compare
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/Tracer.java,build,org.apache.hadoop.tracing.Tracer$Builder:build(),91,96,"/**
* Returns the global tracer instance.
* Initializes it with the given name if not already created.
* @return Tracer object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/Tracer.java,newSpan,"org.apache.hadoop.tracing.Tracer:newSpan(java.lang.String,org.apache.hadoop.tracing.SpanContext)",55,57,"/**
 * Creates a new Span with the given description and context.
 * @param description textual description of the span
 * @param spanCtx context from which to derive the span
 * @return new Span object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/NullTraceScope.java,<init>,org.apache.hadoop.tracing.NullTraceScope:<init>(),23,25,"/**
 * Constructs a NullTraceScope with no parent scope.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/TraceScope.java,close,org.apache.hadoop.tracing.TraceScope:close(),53,57,"/**
 * Delegates method call to span if it's not null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,"org.apache.hadoop.util.MachineList:<init>(java.util.Collection,org.apache.hadoop.util.MachineList$InetAddressFactory)",95,136,"/**
* Initializes MachineList with host entries.
* @param hostEntries collection of host addresses or CIDR ranges
* @param addressFactory factory for creating InetAddress objects
*/","* Accepts a collection of ip/cidr/host addresses
   * 
   * @param hostEntries hostEntries.
   * @param addressFactory addressFactory to convert host to InetAddress",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,includes,org.apache.hadoop.util.MachineList:includes(java.lang.String),145,160,"/**
* Checks if the IP address is allowed.
* @param ipAddress the IP address to check
* @return true if allowed, false otherwise
*/","* Accepts an ip address and return true if ipAddress is in the list.
   * {@link #includes(InetAddress)} should be preferred
   * to avoid possibly re-resolving the ip address.
   *
   * @param ipAddress ipAddress.
   * @return true if ipAddress is part of the list",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,checkConf,org.apache.hadoop.util.ConfTest:checkConf(java.io.InputStream),136,216,"/**
 * Parses XML input to validate properties configuration.
 * @param in XML input stream
 * @return List of error messages or empty list if valid
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,listFiles,org.apache.hadoop.util.ConfTest:listFiles(java.io.File),218,225,"/**
* Filters XML files in the specified directory.
* @param dir directory to search for XML files
* @return array of File objects representing XML files
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,<init>,"org.apache.hadoop.util.SysInfoLinux:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,long)",195,210,"/**
* Initializes system info with specified file paths and jiffy length.
* @param procfsMemFile path to memory info file
* @param procfsCpuFile path to CPU info file
* @param procfsStatFile path to stat info file
* @param procfsNetFile path to network info file
* @param procfsDisksFile path to disks info file
* @param jiffyLengthInMillis length of a jiffy in milliseconds
*/","* Constructor which allows assigning the /proc/ directories. This will be
   * used only in unit tests.
   * @param procfsMemFile fake file for /proc/meminfo
   * @param procfsCpuFile fake file for /proc/cpuinfo
   * @param procfsStatFile fake file for /proc/stat
   * @param procfsNetFile fake file for /proc/net/dev
   * @param procfsDisksFile fake file for /proc/diskstats
   * @param jiffyLengthInMillis fake jiffy length value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcMemInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(boolean),238,305,"/**
* Reads memory info from a file and updates system memory metrics.
* @param readAgain flag to force re-reading the file
*/","* Read /proc/meminfo, parse and compute memory information.
   * @param readAgain if false, read only on the first time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumProcessors,org.apache.hadoop.util.SysInfoLinux:getNumProcessors(),625,629,"/**
 * Returns the number of processors.
 * Calls m1() before returning.
 * @return number of processors
 */",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumCores,org.apache.hadoop.util.SysInfoLinux:getNumCores(),632,636,"/**
 * Returns the number of CPU cores.
 * Calls m1() before returning.
 * @return Number of CPU cores
 */",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCpuFrequency,org.apache.hadoop.util.SysInfoLinux:getCpuFrequency(),639,643,"/**
 * Returns CPU frequency after executing m1().
 * @return CPU frequency in Hz
 */",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcStatFile,org.apache.hadoop.util.SysInfoLinux:readProcStatFile(),376,421,"/**
* Reads CPU time from a procfs file and updates the tracker.
*/","* Read /proc/stat file, parse and calculate cumulative CPU.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNetworkBytesRead,org.apache.hadoop.util.SysInfoLinux:getNetworkBytesRead(),675,679,"/**
 * Calls m1 and returns the number of bytes read.
 * @return Number of network bytes read
 */",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNetworkBytesWritten,org.apache.hadoop.util.SysInfoLinux:getNetworkBytesWritten(),682,686,"/**
 * Overrides to mask function.
 * Calls m1() and returns number of bytes written.
 * @return Number of network bytes written
 */",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcDisksInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcDisksInfoFile(),483,543,"/**
 * Reads disk I/O statistics from a file and updates byte counters.
 * Handles exceptions during file reading and ensures resources are closed.
 */","* Read /proc/diskstats file, parse and calculate amount
   * of bytes read and written from/to disks.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,realloc,org.apache.hadoop.util.IdentityHashStore:realloc(int),74,90,"/**
* Resizes the buffer to a new capacity.
* @param newCapacity the new capacity for the buffer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,get,org.apache.hadoop.util.IdentityHashStore:get(java.lang.Object),152,158,"/**
* Retrieves value by key using a hash function.
* @param k the key to look up
* @return the associated value or null if not found
*/","* Retrieve a value associated with a given key.
   *
   * @param k Generics Type k.
   * @return Generics Type V.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,remove,org.apache.hadoop.util.IdentityHashStore:remove(java.lang.Object),167,177,"/**
* Removes and returns value by key.
* @param k key to remove
* @return associated value or null if not found
*/","* Retrieve a value associated with a given key, and delete the
   * relevant entry.
   *
   * @param k Generics Type k.
   * @return Generics Type V.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,ensureNext,org.apache.hadoop.util.LightWeightGSet$SetIterator:ensureNext(),312,327,"/**
* Checks for concurrent modifications and advances iterator.
* Throws ConcurrentModificationException if modification mismatch detected.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,"org.apache.hadoop.util.LightWeightGSet:remove(int,java.lang.Object)",188,219,"/**
 * Removes an element by key from the specified index.
 * @param index position in entries array
 * @param key unique identifier for the element to remove
 * @return removed element or null if not found
 */","* Remove the element corresponding to the key,
   * given key.hashCode() == index.
   *
   * @param key key.
   * @param index index.
   * @return If such element exists, return it.
   *         Otherwise, return null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MergeSort.java,mergeSort,"org.apache.hadoop.util.MergeSort:mergeSort(int[],int[],int,int)",42,83,"/**
* Sorts elements between low and high indices using merge sort.
* @param src source array to sort
* @param dest destination array for merging
* @param low starting index of the range to sort
* @param high ending index (exclusive) of the range to sort
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,setOptionalSecureTransformerAttributes,org.apache.hadoop.util.XMLUtils:setOptionalSecureTransformerAttributes(javax.xml.transform.TransformerFactory),180,186,"/**
* Configures TransformerFactory to restrict external access.
* @param transformerFactory the factory to configure
*/","* These attributes are recommended for maximum security but some JAXP transformers do
   * not support them. If at any stage, we fail to set these attributes, then we won't try again
   * for subsequent transformers.
   *
   * @param transformerFactory to update",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,string2long,org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:string2long(java.lang.String),906,927,"/**
* Converts a string with optional size suffix to a long value.
* @param s input string representing the size
* @return long value of the parsed size
*/","* Convert a string to long.
     * The input string is first be trimmed
     * and then it is parsed with traditional binary prefix.
     *
     * For example,
     * ""-1230k"" will be converted to -1230 * 1024 = -1259520;
     * ""891g"" will be converted to 891 * 1024^3 = 956703965184;
     *
     * @param s input string
     * @return a long value represented by the input string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,long2String,"org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:long2String(long,java.lang.String,int)",937,977,"/**
 * Formats a number with a unit and optional decimal places.
 * @param n the number to format
 * @param unit the unit to append (can be null)
 * @param decimalPlaces number of decimal places for formatting
 * @return formatted string representation of the number
 */","* Convert a long integer to a string with traditional binary prefix.
     * 
     * @param n the value to be converted
     * @param unit The unit, e.g. ""B"" for bytes.
     * @param decimalPlaces The number of decimal places.
     * @return a string with traditional binary prefix.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,formatPercent,"org.apache.hadoop.util.StringUtils:formatPercent(double,int)",153,155,"/**
* Formats a fraction as a percentage with specified decimal places.
* @param fraction the value to format (e.g., 0.75 for 75%)
* @param decimalPlaces number of decimal places in the output
* @return formatted percentage string
*/","* Format a percentage for presentation to the user.
   * @param fraction the percentage as a fraction, e.g. 0.1 = 10%
   * @param decimalPlaces the number of decimal places
   * @return a string representation of the percentage",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,"org.apache.hadoop.util.StringUtils:byteToHexString(byte[],int,int)",183,192,"/**
* Masks byte array to hex string.
* @param bytes input byte array
* @param start starting index
* @param end ending index
* @return masked hex string representation of byte range
*/","* Given an array of bytes it will convert the bytes to a hex string
   * representation of the bytes
   * @param bytes bytes.
   * @param start start index, inclusively
   * @param end end index, exclusively
   * @return hex string representation of the byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,limitDecimalTo2,org.apache.hadoop.util.StringUtils:limitDecimalTo2(double),1033,1036,"/**
 * Formats a double to a string with two decimal places.
 * @param d the number to format
 * @return formatted string representation of the number
 */","* limitDecimalTo2.
   *
   * @param d double param.
   * @return string value (""%.2f"").
   * @deprecated use StringUtils.format(""%.2f"", d).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HeapSort.java,sort,"org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)",56,74,"/**
* Sorts elements in the given IndexedSortable.
* @param s the IndexedSortable to sort
* @param p starting index (inclusive)
* @param r ending index (exclusive)
* @param rep optional Progressable for reporting progress
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,subtract,org.apache.hadoop.util.JvmPauseMonitor$GcTimes:subtract(org.apache.hadoop.util.JvmPauseMonitor$GcTimes),166,169,"/**
* Subtracts another GcTimes instance from this one.
* @param other GcTimes object to subtract
* @return New GcTimes with the difference in gcCount and gcTimeMillis
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,org.apache.hadoop.util.ExitUtil:terminate(org.apache.hadoop.util.ExitUtil$ExitException),233,272,"/**
* Handles exit exception by logging and propagating errors.
* @param ee ExitException to process
* @throws ExitException if not handled
*/","* Exits the JVM if exit is enabled, rethrow provided exception or any raised error otherwise.
   * Inner termination: either exit with the exception's exit code,
   * or, if system exits are disabled, rethrow the exception.
   * @param ee exit exception
   * @throws ExitException if {@link System#exit(int)} is disabled and not suppressed by an Error
   * @throws Error if {@link System#exit(int)} is disabled and one Error arise, suppressing
   * anything else, even <code>ee</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,org.apache.hadoop.util.ExitUtil:halt(org.apache.hadoop.util.ExitUtil$HaltException),286,326,"/**
* Handles HaltException by logging and possibly rethrowing.
* @param he HaltException to process
* @throws HaltException if not disabled
*/","* Halts the JVM if halt is enabled, rethrow provided exception or any raised error otherwise.
   * If halt is disabled, this method throws either the exception argument if no
   * error arise, the first error if at least one arise, suppressing <code>he</code>.
   * If halt is enabled, all throwables are caught, even errors.
   *
   * @param he the exception containing the status code, message and any stack
   * trace.
   * @throws HaltException if {@link Runtime#halt(int)} is disabled and not suppressed by an Error
   * @throws Error if {@link Runtime#halt(int)} is disabled and one Error arise, suppressing
   * anyuthing else, even <code>he</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,<init>,org.apache.hadoop.util.SysInfoWindows:<init>(),56,59,"/**
 * Initializes system information on Windows.
 * Sets the last refresh time to 0 and resets system info.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,addShutdownHook,"org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int,long,java.util.concurrent.TimeUnit)",319,331,"/**
 * Adds a shutdown hook with specified priority and timeout.
 * @param shutdownHook the Runnable to execute on shutdown
 * @param priority execution priority of the shutdown hook
 * @param timeout maximum time to wait for the hook to complete
 * @param unit time unit for the timeout
 */","*
   * Adds a shutdownHook with a priority and timeout the higher the priority
   * the earlier will run. ShutdownHooks with same priority run
   * in a non-deterministic order. The shutdown hook will be terminated if it
   * has not been finished in the specified period of time.
   *
   * @param shutdownHook shutdownHook <code>Runnable</code>
   * @param priority priority of the shutdownHook
   * @param timeout timeout of the shutdownHook
   * @param unit unit of the timeout <code>TimeUnit</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,removeShutdownHook,org.apache.hadoop.util.ShutdownHookManager:removeShutdownHook(java.lang.Runnable),340,350,"/**
* Registers a shutdown hook.
* @param shutdownHook the hook to add
* @return true if successful, false otherwise
*/","* Removes a shutdownHook.
   *
   * @param shutdownHook shutdownHook to remove.
   * @return TRUE if the shutdownHook was registered and removed,
   * FALSE otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,hasShutdownHook,org.apache.hadoop.util.ShutdownHookManager:hasShutdownHook(java.lang.Runnable),358,363,"/**
* Adds a shutdown hook with minimum timeout.
* @param shutdownHook the Runnable to execute on shutdown
* @return true if added successfully, false otherwise
*/","* Indicates if a shutdownHook is registered or not.
   *
   * @param shutdownHook shutdownHook to check if registered.
   * @return TRUE/FALSE depending if the shutdownHook is is registered.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,compareTo,org.apache.hadoop.util.ComparableVersion:compareTo(org.apache.hadoop.util.ComparableVersion),460,463,"/**
 * Compares this version to another.
 * @param o the other ComparableVersion object
 * @return comparison result as an integer
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ThreadUtil.java,getResourceAsStream,org.apache.hadoop.util.ThreadUtil:getResourceAsStream(java.lang.String),91,99,"/**
* Loads a resource as an InputStream.
* @param resourceName name of the resource to load
* @return InputStream for the resource or throws IOException if not found
*/","* Convenience method that returns a resource as inputstream from the
   * classpath.
   * <p>
   * Uses the Thread's context classloader to load resource.
   *
   * @param resourceName resource to retrieve.
   *
   * @throws IOException thrown if resource cannot be loaded
   * @return inputstream with the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,logWarning,"org.apache.hadoop.util.InstrumentedLock:logWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)",151,161,"/**
* Logs a warning when a lock's held time exceeds the threshold.
* @param lockHeldTime duration for which the lock was held
* @param stats snapshot of suppressed statistics
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,logWaitWarning,"org.apache.hadoop.util.InstrumentedLock:logWaitWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)",163,172,"/**
* Logs warning for long lock acquisition time.
* @param lockWaitTime time taken to acquire the lock in milliseconds
* @param stats snapshot of suppressed statistics
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sortInternal,"org.apache.hadoop.util.QuickSort:sortInternal(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable,int)",69,136,"/**
* Sorts indexed sortable elements using a hybrid quicksort with insertion sort for small subarrays.
* @param s the IndexedSortable to be sorted
* @param p starting index (inclusive)
* @param r ending index (exclusive)
* @param rep progress reporter, may be null
* @param depth recursion depth limit
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,org.apache.hadoop.util.LineReader:<init>(java.io.InputStream),69,71,"/**
 * Constructs a LineReader with the specified InputStream.
 * Uses default buffer size.
 * @param in InputStream to read from
 */","* Create a line reader that reads from the given stream using the
   * default buffer-size (64k).
   * @param in The input stream",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell$1:run(),951,960,"/**
* Executes function with rate limiting and platform-specific configuration.
* @throws IOException if an I/O error occurs during execution
*/","* Check to see if a command needs to be executed and execute if needed.
   *
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,newDaemonThreadFactory,org.apache.hadoop.util.BlockingThreadPoolExecutorService:newDaemonThreadFactory(java.lang.String),86,102,"/**
* Creates a named thread factory with normal priority.
* @param prefix name prefix for threads
* @return ThreadFactory that creates threads with specified name and priority
*/","* Get a named {@link ThreadFactory} that just builds daemon threads.
   *
   * @param prefix name prefix for all threads created from the factory
   * @return a thread factory that creates named, daemon threads with
   * the supplied exception handler and normal priority",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,"org.apache.hadoop.util.LightWeightResizableGSet:<init>(int,float)",66,81,"/**
* Constructs a LightWeightResizableGSet with specified initial capacity and load factor.
* @param initCapacity initial capacity of the set
* @param loadFactor load factor for resizing the set
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,size,org.apache.hadoop.util.LightWeightResizableGSet:size(),108,111,"/**
* Overrides and returns result of superclass's m1 method.
* @return integer value from superclass's m1
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,getIterator,org.apache.hadoop.util.LightWeightResizableGSet:getIterator(java.util.function.Consumer),113,115,"/**
 * Applies a consumer to an iterator of elements.
 * @param consumer function to process the iterator
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,expandIfNecessary,org.apache.hadoop.util.LightWeightResizableGSet:expandIfNecessary(),148,152,"/**
* Increases array capacity if size exceeds threshold and max length isn't reached.
*/","* Checks if we need to expand, and expands if necessary.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.util.Iterator),109,113,"/**
 * Creates an ArrayList from Iterator elements.
 * @param elements Iterator of elements to add to the list
 * @return ArrayList containing the elements from the iterator
 */","* Creates a <i>mutable</i> {@code ArrayList} instance containing the
   * given elements; a very thin shortcut for creating an empty list
   * and then calling Iterators#addAll.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,addAll,"org.apache.hadoop.util.Lists:addAll(java.util.Collection,java.lang.Iterable)",248,258,"/**
 * Adds all elements from the source iterable to the target collection.
 * @param addTo collection where elements will be added
 * @param elementsToAdd iterable of elements to add
 * @return true if any element was added, false otherwise
 */","* Adds all elements in {@code iterable} to {@code collection}.
   *
   * @return {@code true} if {@code collection} was modified as a result of
   *     this operation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayListWithCapacity,org.apache.hadoop.util.Lists:newArrayListWithCapacity(int),128,132,"/**
* Creates an ArrayList with a specified initial capacity.
* @param initialArraySize the initial size of the array list
* @return an ArrayList instance with the given initial capacity
*/","* Creates an {@code ArrayList} instance backed by an array with the
   * specified initial size;
   * simply delegates to {@link ArrayList#ArrayList(int)}.
   *
   * @param <E> Generics Type E.
   * @param initialArraySize the exact size of the initial backing array for
   *     the returned array list
   *     ({@code ArrayList} documentation calls this value the ""capacity"").
   * @return a new, empty {@code ArrayList} which is guaranteed not to
   *     resize itself unless its size reaches {@code initialArraySize + 1}.
   * @throws IllegalArgumentException if {@code initialArraySize} is negative.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,computeArrayListCapacity,org.apache.hadoop.util.Lists:computeArrayListCapacity(int),192,195,"/**
* Computes a mask value based on array size.
* @param arraySize the size of the input array
* @return computed mask value as an integer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,getResource,org.apache.hadoop.util.ApplicationClassLoader:getResource(java.lang.String),128,153,"/**
* Retrieves a resource URL by name, handling leading slashes.
* @param name resource name
* @return URL of the resource or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,loadClass,"org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String,boolean)",160,204,"/**
* Loads a class by name.
* @param name class name to load
* @param resolve whether to resolve the class
* @return loaded Class object
* @throws ClassNotFoundException if class not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,writeJsonAsBytes,"org.apache.hadoop.util.JsonSerialization:writeJsonAsBytes(java.lang.Object,java.io.OutputStream)",305,312,"/**
* Masks and writes an object to an output stream.
* @param instance the object to be masked
* @param dataOutputStream the output stream to write to
*/","* Write the JSON as bytes, then close the stream.
   * @param instance instance to write
   * @param dataOutputStream an output stream that will always be closed
   * @throws IOException on any failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,<init>,org.apache.hadoop.util.OperationDuration:<init>(),48,51,"/**
 * Initializes operation duration with current start and finish times.
 */","* Instantiate.
   * The start time and finished time are both set
   * to the current clock time.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,finished,org.apache.hadoop.util.OperationDuration:finished(),64,66,"/**
* Sets the 'finished' flag based on m1's result.
* @param none
* @return void
*/",* Update the finished time with the current system time.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,asDuration,org.apache.hadoop.util.OperationDuration:asDuration(),114,116,"/**
 * Converts duration using mask functions.
 * @return Duration object after applying mask conversion
 */","* Get the duration of an operation as a java Duration
   * instance.
   * @return a duration.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,getDurationString,org.apache.hadoop.util.OperationDuration:getDurationString(),72,74,"/**
 * Masks data using nested transformation functions.
 * @return masked string result of transformations
 */","* Return the duration as {@link #humanTime(long)}.
   * @return a printable duration.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,iterator,org.apache.hadoop.util.LightWeightCache:iterator(),234,256,"/**
* Returns an iterator over elements, with remove operation unsupported.
* @return Iterator over elements of type E
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,iterator,org.apache.hadoop.util.LightWeightGSet$Values:iterator(),240,243,"/**
 * Returns an iterator over the elements in this set.
 * @return Iterator<E> object for traversing the set
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,getMonomial,"org.apache.hadoop.util.CrcUtil:getMonomial(long,int)",52,77,"/**
* Computes a masked value using modular exponentiation.
* @param lengthBytes the number of bytes in the input data
* @param mod the modulus for the operation
* @return the computed masked value as an integer
*/","* Compute x^({@code lengthBytes} * 8) mod {@code mod}, where {@code mod} is
   * in ""reversed"" (little-endian) format such that {@code mod & 1} represents
   * x^31 and has an implicit term x^32.
   *
   * @param lengthBytes lengthBytes.
   * @param mod mod.
   * @return monomial.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,composeWithMonomial,"org.apache.hadoop.util.CrcUtil:composeWithMonomial(int,int,int,int)",88,91,"/**
* Computes masked CRC value.
* @param crcA first CRC value
* @param crcB second CRC value
* @param monomial polynomial coefficient
* @param mod modulus for operation
* @return XOR result of modified crcA and crcB
*/","* composeWithMonomial.
   *
   * @param crcA crcA.
   * @param crcB crcB.
   * @param monomial Precomputed x^(lengthBInBytes * 8) mod {@code mod}
   * @param mod mod.
   * @return compose with monomial.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,intToBytes,org.apache.hadoop.util.CrcUtil:intToBytes(int),113,125,"/**
 * Masks an integer value into a byte array.
 * @param value integer to be masked
 * @return byte array containing the masked value
 */","* @return 4-byte array holding the big-endian representation of
   *     {@code value}.
   *
   * @param value value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,toSingleCrcString,org.apache.hadoop.util.CrcUtil:toSingleCrcString(byte[]),182,190,"/**
 * Masks a 4-byte array as a hexadecimal string.
 * @param bytes byte array of length 4
 * @return masked hexadecimal string representation
 * @throws IOException if input array is not of length 4
 */","* For use with debug statements; verifies bytes.length on creation,
   * expecting it to represent exactly one CRC, and returns a hex
   * formatted value.
   *
   * @param bytes bytes.
   * @throws IOException raised on errors performing I/O.
   * @return a list of hex formatted values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,toMultiCrcString,org.apache.hadoop.util.CrcUtil:toMultiCrcString(byte[]),201,218,"/**
* Masks byte array into hexadecimal string.
* @param bytes input byte array
* @return masked hexadecimal representation of the byte array
* @throws IOException if byte array length is not divisible by 4
*/","* For use with debug statements; verifies bytes.length on creation,
   * expecting it to be divisible by CRC byte size, and returns a list of
   * hex formatted values.
   *
   * @param bytes bytes.
   * @throws IOException raised on errors performing I/O.
   * @return a list of hex formatted values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.InputStream,java.io.File,java.util.regex.Pattern)",119,152,"/**
 * Unpacks a JAR file to a directory based on regex pattern.
 * @param inputStream input stream of the JAR file
 * @param toDir target directory for unpacking
 * @param unpackRegex regex pattern for filtering entries
 * @throws IOException if an I/O error occurs
 */","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped.
   *
   * @param inputStream the jar stream to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File,java.util.regex.Pattern)",191,222,"/**
 * Unpacks files from a JAR archive to a directory based on regex pattern.
 * @param jarFile the source JAR file
 * @param toDir the target directory for unpacked files
 * @param unpackRegex regex pattern to filter entries
 * @throws IOException if an I/O error occurs
 */","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped.
   *
   * @param jarFile the .jar file to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newCachedThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newCachedThreadPool(java.util.concurrent.ThreadFactory),36,42,"/**
* Creates an unbounded thread pool with core threads set to 0.
* @param threadFactory ThreadFactory for creating new threads
* @return ExecutorService using HadoopThreadPoolExecutor
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newFixedThreadPool,"org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int,java.util.concurrent.ThreadFactory)",44,50,"/**
* Creates a fixed-size thread pool.
* @param nThreads number of threads in the pool
* @param threadFactory factory to create new threads
* @return ExecutorService with specified parameters
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newFixedThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int),52,56,"/**
* Creates a fixed thread pool.
* @param nThreads number of threads in the pool
* @return ExecutorService with specified number of threads
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newScheduledThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int),71,74,"/**
 * Creates a HadoopScheduledThreadPoolExecutor with specified core pool size.
 * @param corePoolSize number of threads to keep in the pool
 * @return ScheduledExecutorService instance for scheduling tasks
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newScheduledThreadPool,"org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int,java.util.concurrent.ThreadFactory)",76,79,"/**
* Creates a scheduled executor service with specified parameters.
* @param corePoolSize number of threads to keep in the pool
* @param threadFactory factory for creating new threads
* @return ScheduledExecutorService instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopScheduledThreadPoolExecutor.java,afterExecute,"org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)",66,70,"/**
* Overrides and extends error handling by executing additional logic.
* @param r Runnable task to be executed
* @param t Throwable that occurred during execution
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopThreadPoolExecutor.java,afterExecute,"org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)",87,91,"/**
* Overrides parent method to handle runnable and throwable.
* @param r task to be executed
* @param t error that occurred
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,get,org.apache.hadoop.util.concurrent.AsyncGetFuture:get(),56,60,"/**
* Calls m1 with -1 and MILLISECONDS, then returns result of super.m2().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,get,"org.apache.hadoop.util.concurrent.AsyncGetFuture:get(long,java.util.concurrent.TimeUnit)",62,67,"/**
* Executes task with specified timeout.
* @param timeout time duration to wait for task completion
* @param unit time unit of the timeout
* @return result of the task execution
* @throws InterruptedException if the current thread is interrupted
* @throws TimeoutException if the task times out
* @throws ExecutionException if an error occurs during task execution
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,isDone,org.apache.hadoop.util.concurrent.AsyncGetFuture:isDone(),69,73,"/**
 * Calls m1 with 0 milliseconds and invokes superclass's m2.
 * @return result of superclass's m2
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,<init>,org.apache.hadoop.util.StopWatch:<init>(),33,35,"/**
 * Initializes a new StopWatch with a default Timer.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/UTF8ByteArrayUtils.java,findNthByte,"org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],int,int,byte,int)",78,89,"/**
 * Finds position of byte 'b' in UTF array within range.
 * @param utf byte array containing UTF data
 * @param start starting index for search
 * @param length number of bytes to consider
 * @param b byte to find
 * @param n number of occurrences to find
 * @return last found position or -1 if not found
 */","* Find the nth occurrence of the given byte b in a UTF-8 encoded string
   * @param utf a byte array containing a UTF-8 encoded string
   * @param start starting offset
   * @param length the length of byte array
   * @param b the byte to find
   * @param n the desired occurrence of the given byte
   * @return position that nth occurrence of the given byte if exists; otherwise -1",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,<init>,"org.apache.hadoop.util.CacheableIPList:<init>(org.apache.hadoop.util.FileBasedIPList,long)",33,37,"/**
* Initializes a CacheableIPList with an IP list and cache timeout.
* @param ipList the FileBasedIPList to be cached
* @param cacheTimeout the duration (in milliseconds) for which the cache is valid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,toString,org.apache.hadoop.util.WeakReferenceMap:toString(),108,115,"/**
* Returns a string representation of the WeakReferenceMap.
* @return formatted string containing size, lost references, and created entries count
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,put,"org.apache.hadoop.util.WeakReferenceMap:put(java.lang.Object,java.lang.Object)",246,248,"/**
 * Stores a key-value pair with weak reference.
 * @param key unique identifier for the value
 * @param value object to be stored
 * @return the stored value wrapped in a WeakReference
 */","* Put a value under the key.
   * A null value can be put, though on a get() call
   * a new entry is generated
   *
   * @param key key
   * @param value value
   * @return any old non-null reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,remove,org.apache.hadoop.util.WeakReferenceMap:remove(java.lang.Object),255,257,"/**
 * Retrieves value by key using nested map operations.
 * @param key unique identifier for the value
 * @return value associated with the key or null if not found
 */","* Remove any value under the key.
   * @param key key
   * @return any old non-null reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,containsKey,org.apache.hadoop.util.WeakReferenceMap:containsKey(java.lang.Object),266,269,"/**
 * Checks if a value exists for the given key.
 * @param key the key to check
 * @return true if a value exists, false otherwise
 */","* Does the map have a valid reference for this object?
   * no-side effects: there's no attempt to notify or cleanup
   * if the reference is null.
   * @param key key to look up
   * @return true if there is a valid reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,create,org.apache.hadoop.util.WeakReferenceMap:create(java.lang.Object),197,235,"/**
* Creates or retrieves a value associated with a key.
* @param key unique identifier for the value
* @return the created or retrieved value
*/","* Create a new instance under a key.
   * <p>
   * The instance is created, added to the map and then the
   * map value retrieved.
   * This ensures that the reference returned is that in the map,
   * even if there is more than one entry being created at the same time.
   * If that race does occur, it will be logged the first time it happens
   * for this specific map instance.
   * <p>
   * HADOOP-18456 highlighted the risk of a concurrent GC resulting a null
   * value being retrieved and so returned.
   * To prevent this:
   * <ol>
   *   <li>A strong reference is retained to the newly created instance
   *       in a local variable.</li>
   *   <li>That variable is used after the resolution process, to ensure
   *       the JVM doesn't consider it ""unreachable"" and so eligible for GC.</li>
   *   <li>A check is made for the resolved reference being null, and if so,
   *       the put() is repeated</li>
   * </ol>
   * @param key key
   * @return the created value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,prune,org.apache.hadoop.util.WeakReferenceMap:prune(),288,300,"/**
* Cleans up expired entries in the map.
* @return number of removed entries
*/","* Prune all null weak references, calling the referenceLost
   * callback for each one.
   *
   * non-atomic and non-blocking.
   * @return the number of entries pruned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,snapshot,org.apache.hadoop.util.InstrumentedLock$SuppressedStats:snapshot(),262,268,"/**
* Resets and returns a snapshot of current suppression stats.
* @return SuppressedSnapshot containing count and max wait time
*/","* Captures the current value of the counts into a SuppressedSnapshot object
     * and resets the values to zero.
     *
     * @return SuppressedSnapshot containing the current value of the counters",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,formatTimeDiff,"org.apache.hadoop.util.StringUtils:formatTimeDiff(long,long)",294,297,"/**
 * Calculates and masks the time difference between two timestamps.
 * @param finishTime end timestamp in milliseconds
 * @param startTime start timestamp in milliseconds
 * @return masked time difference as a string
 */","* 
   * Given a finish and start time in long milliseconds, returns a 
   * String in the format Xhrs, Ymins, Z sec, for the time difference between two times. 
   * If finish time comes before start time then negative valeus of X, Y and Z wil return. 
   * 
   * @param finishTime finish time
   * @param startTime start time
   * @return a String in the format Xhrs, Ymins, Z sec,
   *         for the time difference between two times.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getTrimmedStringCollectionSplitByEquals,org.apache.hadoop.util.StringUtils:getTrimmedStringCollectionSplitByEquals(java.lang.String),505,525,"/**
 * Masks input string into a key-value map.
 * @param str input string to be masked
 * @return Map of key-value pairs after masking
 */","* Splits an ""="" separated value <code>String</code>, trimming leading and
   * trailing whitespace on each value after splitting by comma and new line separator.
   *
   * @param str a comma separated <code>String</code> with values, may be null
   * @return a <code>Map</code> of <code>String</code> keys and values, empty
   * Collection if null String input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,split,"org.apache.hadoop.util.StringUtils:split(java.lang.String,char,char)",581,601,"/**
 * Splits a string by a separator, handling escape characters.
 * @param str input string to split
 * @param escapeChar character used to escape the separator
 * @param separator delimiter for splitting the string
 * @return array of split strings or null if input is null
 */","* Split a string using the given separator
   * @param str a string that may have escaped separator
   * @param escapeChar a char that be used to escape the separator
   * @param separator a separator char
   * @return an array of strings",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,"org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char[])",701,716,"/**
* Masks characters in a string using an escape character.
* @param str input string to process
* @param escapeChar character used for escaping
* @param charsToEscape array of characters to be escaped
* @return masked string with escape characters added
*/","* escapeString.
   *
   * @param str str.
   * @param escapeChar escapeChar.
   * @param charsToEscape array of characters to be escaped
   * @return escapeString.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,"org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char[])",748,782,"/**
* Masks characters in a string based on escape rules.
* @param str input string to process
* @param escapeChar character used for escaping
* @param charsToEscape array of characters that need to be escaped
* @return masked string or throws IllegalArgumentException if invalid
*/","* unEscapeString.
   * @param str str.
   * @param escapeChar escapeChar.
   * @param charsToEscape array of characters to unescape
   * @return escape string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getVersion,org.apache.hadoop.util.VersionInfo:getVersion(),105,107,"/**
 * Returns version information string.
 * @return Version info as a string
 */","* Get the Hadoop version.
   * @return the Hadoop version string, eg. ""0.6.3-dev""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getRevision,org.apache.hadoop.util.VersionInfo:getRevision(),113,115,"/**
 * Returns version information.
 * @return Version string from common info
 */","* Get the Git commit hash of the repository when compiled.
   * @return the commit hash, eg. ""18f64065d5db6208daf50b02c1b5ed4ee3ce547a""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getBranch,org.apache.hadoop.util.VersionInfo:getBranch(),121,123,"/**
 * Returns version information.
 * @return version string from common info
 */","* Get the branch on which this originated.
   * @return The branch name, e.g. ""trunk"" or ""branches/branch-0.20""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getDate,org.apache.hadoop.util.VersionInfo:getDate(),129,131,"/**
 * Returns version information.
 * @return Version string from common info
 */","* The date that Hadoop was compiled.
   * @return the compilation date in unix date format",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getUser,org.apache.hadoop.util.VersionInfo:getUser(),137,139,"/**
 * Retrieves version information.
 * @return Version string from common info
 */","* The user that compiled Hadoop.
   * @return the username of the user",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getUrl,org.apache.hadoop.util.VersionInfo:getUrl(),145,147,"/**
 * Returns version information string.
 * @return Version info from COMMON_VERSION_INFO
 */","* Get the URL for the Hadoop repository.
   * @return the URL of the Hadoop repository",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,_getBuildVersion,org.apache.hadoop.util.VersionInfo:_getBuildVersion(),85,90,"/**
 * Generates a masked string combining values from multiple methods.
 * @return Concatenated string of masked information
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getSrcChecksum,org.apache.hadoop.util.VersionInfo:getSrcChecksum(),153,155,"/**
 * Returns version information from a common source.
 * @return Version string from COMMON_VERSION_INFO
 */","* Get the checksum of the source files from which Hadoop was built.
   * @return the checksum of the source files",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getProtocVersion,org.apache.hadoop.util.VersionInfo:getProtocVersion(),170,172,"/**
 * Returns version information.
 * @return Version string from common info
 */","* Returns the protoc version used for the build.
   * @return the protoc version",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getCompilePlatform,org.apache.hadoop.util.VersionInfo:getCompilePlatform(),178,180,"/**
 * Returns version information.
 * @return Version string from common info
 */","* Returns the OS platform used for the build.
   * @return the OS platform",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",604,610,"/**
* Initializes a filtering iterator.
* @param source remote iterator to wrap
* @param filter function to apply for filtering elements
*/","* An iterator which combines filtering with transformation.
     * All source elements for which filter = true are returned,
     * transformed via the mapper.
     * @param source source iterator.
     * @param filter filter predicate.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",521,526,"/**
* Constructs an iterator with a mapping function.
* @param source the original iterator to map over
* @param mapper function to transform elements from S to T
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,java.io.Closeable)",677,682,"/**
* Initializes and wraps a remote iterator with a resource to close.
* @param source the remote iterator to wrap
* @param toClose the resource that implements Closeable to be closed later
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.CallableRaisingIOE)",773,778,"/**
* Initializes a new HaltableRemoteIterator.
* @param source the underlying iterator to wrap
* @param continueWork logic to determine if iteration should continue
*/","* Wrap an iterator with one which adds a continuation probe.
     * The probe will be called in the {@link #hasNext()} method, before
     * the source iterator is itself checked and in {@link #next()}
     * before retrieval.
     * That is: it may be called multiple times per iteration.
     * @param source source iterator.
     * @param continueWork predicate which will trigger a fast halt if it returns false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator),553,556,"/**
 * Constructs a TypeCastingRemoteIterator.
 * @param source the original remote iterator to wrap
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,hasNext,org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:hasNext(),634,640,"/**
 * Checks if there is a next element.
 * @return true if next exists, false otherwise
 * @throws IOException if an I/O error occurs
 */","* Trigger a fetch if an entry is needed.
     * @return true if there was already an entry return,
     * or there was not but one could then be retrieved.set
     * @throws IOException failure in fetch operation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:<init>(java.lang.Object),723,725,"/**
 * Initializes with an object and sets close flag to true.
 * @param o the object to be managed
 */","* Construct.
     * @param o object to close.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:close(),415,419,"/**
 * Delegates call to sourceToClose's m1 method.
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:close(),454,457,"/**
 * Calls m1 on sourceToClose and propagates IOException.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:next(),829,837,"/**
 * Retrieves and increments the current value.
 * Throws NoSuchElementException if not available.
 * @return Current value before increment
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,submit,"org.apache.hadoop.util.functional.CommonCallableSupplier:submit(java.util.concurrent.Executor,java.util.concurrent.Callable)",82,87,"/**
* Executes a callable asynchronously.
* @param executor execution context
* @param call task to be executed
* @return CompletableFuture representing the asynchronous computation
*/","* Submit a callable into a completable future.
   * RTEs are rethrown.
   * Non RTEs are caught and wrapped; IOExceptions to
   * {@code RuntimeIOException} instances.
   * @param executor executor.
   * @param call     call to invoke
   * @param <T>      type
   * @return the future to wait for",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,<init>,org.apache.hadoop.util.functional.LazyAutoCloseableReference:<init>(org.apache.hadoop.util.functional.CallableRaisingIOE),43,45,"/**
 * Initializes a reference with a callable that constructs an object.
 * @param constructor callable to create the referenced object
 */","* Constructor for this instance.
   * @param constructor method to invoke to actually construct the inner object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,lazyAtomicReferenceFromSupplier,org.apache.hadoop.util.functional.LazyAtomicReference:lazyAtomicReferenceFromSupplier(java.util.function.Supplier),148,151,"/**
* Creates a LazyAtomicReference with a supplier.
* @param supplier provides the value lazily
* @return LazyAtomicReference instance initialized with the supplier
*/","* Create from a supplier.
   * This is not a constructor to avoid ambiguity when a lambda-expression is
   * passed in.
   * @param supplier supplier implementation.
   * @return a lazy reference.
   * @param <T> type of reference",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,eval,org.apache.hadoop.util.functional.LazyAutoCloseableReference:eval(),51,55,"/**
 * Checks and calls m2 if not closed, then returns result of super.m3().
 * @throws IOException if an I/O error occurs
 */","* {@inheritDoc}
   * @throws IllegalStateException if the reference is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,apply,org.apache.hadoop.util.functional.LazyAtomicReference:apply(),104,107,"/**
 * Calls m1 and returns its result.
 * @throws IOException if an I/O error occurs during execution
 */","* Implementation of {@code CallableRaisingIOE.apply()}.
   * Invoke {@link #eval()}.
   * @return the value
   * @throws IOException on any evaluation failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,uncheckIOExceptions,org.apache.hadoop.util.functional.FunctionalIO:uncheckIOExceptions(org.apache.hadoop.util.functional.CallableRaisingIOE),45,47,"/**
 * Executes a callable that may raise an I/O exception.
 * @param call Callable object to execute
 * @return Result of the callable execution
 */","* Invoke any operation, wrapping IOExceptions with
   * {@code UncheckedIOException}.
   * @param call callable
   * @param <T> type of result
   * @return result
   * @throws UncheckedIOException if an IOE was raised.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,toUncheckedIOExceptionSupplier,org.apache.hadoop.util.functional.FunctionalIO:toUncheckedIOExceptionSupplier(org.apache.hadoop.util.functional.CallableRaisingIOE),55,57,"/**
 * Converts CallableRaisingIOE to Supplier by masking IOExceptions.
 * @param call the CallableRaisingIOE instance
 * @return a Supplier that executes the callable without throwing IOEs
 */","* Wrap a {@link CallableRaisingIOE} as a {@link Supplier}.
   * @param call call to wrap
   * @param <T> type of result
   * @return a supplier which invokes the call.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:next(),340,349,"/**
 * Returns the singleton instance if available, otherwise throws an exception.
 * @return singleton instance of type T
 * @throws IOException if an I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(org.apache.hadoop.fs.RemoteIterator),591,593,"/**
 * Creates a builder for masking remote iterator items.
 * @param items RemoteIterator containing items to be masked
 * @return Builder instance initialized with the given items
 */","* Create a task builder for the remote iterator.
   * @param items item source.
   * @param <I> type of result.
   * @return builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,throwOne,org.apache.hadoop.util.functional.TaskPool:throwOne(java.util.Collection),607,622,"/**
 * Masks exceptions by merging them into a single exception.
 * @param exceptions collection of exceptions to be processed
 * @throws E the merged exception
 */","* Throw one exception, adding the others as suppressed
   * exceptions attached to the one thrown.
   * This method never completes normally.
   * @param exceptions collection of exceptions
   * @param <E> class of exceptions
   * @throws E an extracted exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,<init>,org.apache.hadoop.util.functional.TaskPool$Builder:<init>(java.lang.Iterable),161,163,"/**
 * Constructs a builder with items from an iterable.
 * @param items collection of items to be processed
 */","* Create the builder.
     * @param items items to process",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,suppressExceptions,org.apache.hadoop.util.functional.TaskPool$Builder:suppressExceptions(),197,199,"/**
 * Returns a builder instance with default enabled flag.
 * @return Builder instance configured with defaults
 */","* Suppress exceptions from tasks.
     * RemoteIterator exceptions are not suppressable.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,raiseInnerCause,org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.ExecutionException),254,257,"/**
 * Masks and rethrows an ExecutionException as an IOException.
 * @param e the ExecutionException to mask
 * @throws IOException the masked exception
 */","* From the inner cause of an execution exception, extract the inner cause
   * if it is an IOE or RTE.
   * This will always raise an exception, either the inner IOException,
   * an inner RuntimeException, or a new IOException wrapping the raised
   * exception.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,raiseInnerCause,org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.CompletionException),269,272,"/**
 * Throws an IOException from a CompletionException.
 * @param e the CompletionException to process
 * @throws IOException if processing fails
 */","* Extract the cause of a completion failure and rethrow it if an IOE
   * or RTE.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,setJobConf,"org.apache.hadoop.util.ReflectionUtils:setJobConf(java.lang.Object,org.apache.hadoop.conf.Configuration)",89,115,"/**
* Configures an object using Hadoop JobConf if applicable.
* @param theObject object to be configured
* @param conf configuration object
*/","* This code is to support backward compatibility and break the compile  
   * time dependency of core on mapred.
   * This should be made deprecated along with the mapred package HADOOP-1230. 
   * Should be removed when mapred package is removed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClassByName,org.apache.hadoop.conf.Configuration:getClassByName(java.lang.String),2638,2644,"/**
 * Retrieves a class by name.
 * @param name the fully qualified class name
 * @return the Class object for the given name
 * @throws ClassNotFoundException if the class is not found
 */","* Load a class by name.
   * 
   * @param name the class name.
   * @return the class object.
   * @throws ClassNotFoundException if the class is not found.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,printThreadInfo,"org.apache.hadoop.util.ReflectionUtils:printThreadInfo(java.io.PrintStream,java.lang.String)",183,221,"/**
* Generates a thread dump with detailed information.
* @param stream output stream for the dump
* @param title title of the thread dump
*/","* Print all of the thread's information and stack traces.
   * 
   * @param stream the stream to
   * @param title a string title for the stack trace",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(boolean),830,836,"/**
* Initializes configuration with default loading option.
* @param loadDefaults flag to determine if defaults should be loaded
*/","A new configuration where the behavior of reading from the default 
   * resources can be turned off.
   * 
   * If the parameter {@code loadDefaults} is false, the new instance
   * will not load resources from the default files. 
   * @param loadDefaults specifies whether to load from the default files",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,createServletExceptionResponse,"org.apache.hadoop.util.HttpExceptionUtils:createServletExceptionResponse(javax.servlet.http.HttpServletResponse,int,java.lang.Throwable)",72,86,"/**
 * Sends error response to client.
 * @param response HttpServletResponse object
 * @param status HTTP status code
 * @param ex Throwable exception
 * @throws IOException if an I/O error occurs
 */","* Creates a HTTP servlet response serializing the exception in it as JSON.
   *
   * @param response the servlet response
   * @param status the error code to set in the response
   * @param ex the exception to serialize in the response
   * @throws IOException thrown if there was an error while creating the
   * response",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,createJerseyExceptionResponse,"org.apache.hadoop.util.HttpExceptionUtils:createJerseyExceptionResponse(javax.ws.rs.core.Response$Status,java.lang.Throwable)",95,104,"/**
* Creates a JSON response for an error.
* @param status HTTP status code
* @param ex exception object causing the error
* @return Response object with error details in JSON format
*/","* Creates a HTTP JAX-RPC response serializing the exception in it as JSON.
   *
   * @param status the error code to set in the response
   * @param ex the exception to serialize in the response
   * @return the JAX-RPC response with the set error and JSON encoded exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,throwEx,org.apache.hadoop.util.HttpExceptionUtils:throwEx(java.lang.Throwable),119,121,"/**
 * Masks exceptions by wrapping them in an unchecked exception.
 * @param ex original exception to be masked
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PureJavaCrc32C.java,<init>,org.apache.hadoop.util.PureJavaCrc32C:<init>(),41,43,"/**
* Initializes a new PureJavaCrc32C instance and resets its state.
*/",Create a new PureJavaCrc32 object.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,remove,org.apache.hadoop.util.IntrusiveCollection:remove(java.lang.Object),326,338,"/**
* Checks and processes an element.
* @param o object to check and process
* @return true if processed successfully, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,toArray,org.apache.hadoop.util.IntrusiveCollection:toArray(),256,264,"/**
* Creates an array of objects from iterator.
* @return Array containing elements from iterator
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,retainAll,org.apache.hadoop.util.IntrusiveCollection:retainAll(java.util.Collection),372,384,"/**
* Removes elements not present in the given collection.
* @param collection source collection to compare against
* @return true if any elements were removed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,clear,org.apache.hadoop.util.IntrusiveCollection:clear(),389,395,"/**
* Iterates over elements and performs operations.
*/",* Remove all elements.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,containsAll,org.apache.hadoop.util.IntrusiveCollection:containsAll(java.util.Collection),340,348,"/**
* Checks if all elements in the collection satisfy a condition.
* @param collection the collection of objects to check
* @return true if all elements satisfy the condition, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CleanerUtil.java,unmapHackImpl,org.apache.hadoop.util.CleanerUtil:unmapHackImpl(),89,160,"/**
 * Creates a method to unmap direct ByteBuffers.
 * @return MethodHandle for unmapping or error message if unsupported
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,setIncludesFile,org.apache.hadoop.util.HostsFileReader:setIncludesFile(java.lang.String),313,319,"/**
* Updates the includes file for host details.
* @param includesFile path to the new includes file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,setExcludesFile,org.apache.hadoop.util.HostsFileReader:setExcludesFile(java.lang.String),321,328,"/**
* Updates host details with a new excludes file.
* @param excludesFile path to the new excludes file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,updateFileNames,"org.apache.hadoop.util.HostsFileReader:updateFileNames(java.lang.String,java.lang.String)",330,337,"/**
* Updates includes and excludes files for host details.
* @param includesFile path to the includes file
* @param excludesFile path to the excludes file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getExcludedHosts,org.apache.hadoop.util.HostsFileReader:getExcludedHosts(),266,269,"/**
 * Retrieves a set of strings from host details.
 * @return Set of strings representing some host information
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGroupsForUserCommand,org.apache.hadoop.util.Shell:getGroupsForUserCommand(java.lang.String),229,239,"/**
* Generates command to get groups for a user.
* @param user username
* @return array of strings representing the command
*/","* A command to get a given user's groups list.
   * If the OS is not WINDOWS, the command will get the user's primary group
   * first and finally get the groups list which includes the primary group.
   * i.e. the user's primary group will be included twice.
   *
   * @param user user.
   * @return groups for user command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGroupsIDForUserCommand,org.apache.hadoop.util.Shell:getGroupsIDForUserCommand(java.lang.String),251,261,"/**
 * Generates command to fetch groups for a user.
 * @param user username to query
 * @return array of strings representing the command
 */","* A command to get a given user's group id list.
   * The command will get the user's primary group
   * first and finally get the groups list which includes the primary group.
   * i.e. the user's primary group will be included twice.
   * This command does not support Windows and will only return group names.
   *
   * @param user user.
   * @return groups id for user command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGetPermissionCommand,org.apache.hadoop.util.Shell:getGetPermissionCommand(),279,282,"/**
* Returns command array for file listing.
* @return Array of strings representing the command and its options
*/","* Return a command to get permission information.
   *
   * @return permission command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetPermissionCommand,"org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean)",291,301,"/**
* Generates command for setting file permissions.
* @param perm permission string
* @param recursive true if applied recursively
* @return array of command strings
*/","* Return a command to set permission.
   *
   * @param perm permission.
   * @param recursive recursive.
   * @return set permission command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetOwnerCommand,org.apache.hadoop.util.Shell:getSetOwnerCommand(java.lang.String),326,330,"/**
* Generates chown command array.
* @param owner user or group to own the file
* @return command array for Windows or Unix
*/","* Return a command to set owner.
   *
   * @param owner owner.
   * @return set owner command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSymlinkCommand,"org.apache.hadoop.util.Shell:getSymlinkCommand(java.lang.String,java.lang.String)",339,343,"/**
* Creates a symbolic link command.
* @param target path to the target file or directory
* @param link path where the symlink should be created
* @return array of command strings for symlink creation
*/","* Return a command to create symbolic links.
   *
   * @param target target.
   * @param link link.
   * @return symlink command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getReadlinkCommand,org.apache.hadoop.util.Shell:getReadlinkCommand(java.lang.String),351,355,"/**
* Generates command array for reading a symbolic link.
* @param link path to the symbolic link
* @return array of strings representing the command
*/","* Return a command to read the target of the a symbolic link.
   *
   * @param link link.
   * @return read link command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSignalKillCommand,"org.apache.hadoop.util.Shell:getSignalKillCommand(int,java.lang.String)",373,394,"/**
* Generates a command array for killing a process.
* @param code kill signal code
* @param pid process identifier
* @return string array representing the shell command
*/","* Return a command to send a signal to a given pid.
   *
   * @param code code.
   * @param pid pid.
   * @return signal kill command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,appendScriptExtension,"org.apache.hadoop.util.Shell:appendScriptExtension(java.io.File,java.lang.String)",419,421,"/**
 * Creates a new File with the specified parent and basename.
 * @param parent parent directory as File
 * @param basename name of the file
 * @return new File instance
 */","* Returns a File referencing a script with the given basename, inside the
   * given parent directory.  The file extension is inferred by platform:
   * <code>"".cmd""</code> on Windows, or <code>"".sh""</code> otherwise.
   *
   * @param parent File parent directory
   * @param basename String script file basename
   * @return File referencing the script in the directory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkHadoopHome,org.apache.hadoop.util.Shell:checkHadoopHome(),483,493,"/**
* Retrieves Hadoop home directory file.
* @return File object representing Hadoop home directory
* @throws FileNotFoundException if directory not found
*/","*  Centralized logic to discover and validate the sanity of the Hadoop
   *  home directory.
   *
   *  This does a lot of work so it should only be called
   *  privately for initialization once per process.
   *
   * @return A directory that exists and via was specified on the command line
   * via <code>-Dhadoop.home.dir</code> or the <code>HADOOP_HOME</code>
   * environment variable.
   * @throws FileNotFoundException if the properties are absent or the specified
   * path is not a reference to a valid directory.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getHadoopHomeDir,org.apache.hadoop.util.Shell:getHadoopHomeDir(),620,627,"/**
* Retrieves the Hadoop home directory file.
* Throws FileNotFoundException if Hadoop home setup failed.
* @return File object representing the Hadoop home directory
*/","* Get the Hadoop home directory. If it is invalid,
   * throw an exception.
   * @return a path referring to hadoop home.
   * @throws FileNotFoundException if the directory doesn't exist.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBinInner,"org.apache.hadoop.util.Shell:getQualifiedBinInner(java.io.File,java.lang.String)",656,685,"/**
 * Locates an executable file within the Hadoop bin directory.
 * @param hadoopHomeDir Hadoop installation directory
 * @param executable name of the executable to find
 * @return File object representing the executable
 * @throws FileNotFoundException if executable is not found or access issues occur
 */","* Inner logic of {@link #getQualifiedBin(String)}, accessible
   * for tests.
   * @param hadoopHomeDir home directory (assumed to be valid)
   * @param executable executable
   * @return path to the binary
   * @throws FileNotFoundException if the executable was not found/valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getWinUtilsFile,org.apache.hadoop.util.Shell:getWinUtilsFile(),800,808,"/**
* Returns the WinUtils file or throws an exception.
* @throws FileNotFoundException if WinUtils is not available
*/","* Get a file reference to winutils.
   * Always raises an exception if there isn't one
   * @return the file instance referring to the winutils bin.
   * @throws FileNotFoundException on any failure to locate that file.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,destroyAllShellProcesses,org.apache.hadoop.util.Shell:destroyAllShellProcesses(),1428,1437,"/**
* Masks shells by invoking specific methods.
* Synchronizes access to child shells and processes them.
*/","* Static method to destroy all running <code>Shell</code> processes.
   * Iterates through a map of all currently running <code>Shell</code>
   * processes and destroys them one by one. This method is thread safe",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:run(),1406,1420,"/**
* Executes a process with error handling.
* @throws Exception if an error occurs during process execution
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownThreadsHelper.java,shutdownThread,org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread),43,45,"/**
 * Attempts to shut down a thread with a specified wait time.
 * @param thread the thread to be shut down
 * @return true if shutdown is successful, false otherwise
 */","* @param thread {@link Thread to be shutdown}
   * @return <tt>true</tt> if the thread is successfully interrupted,
   * <tt>false</tt> otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownThreadsHelper.java,shutdownExecutorService,org.apache.hadoop.util.ShutdownThreadsHelper:shutdownExecutorService(java.util.concurrent.ExecutorService),78,81,"/**
 * Shuts down an executor service with a specified wait time.
 * @param service ExecutorService to shut down
 * @return true if shutdown was successful, false otherwise
 */","* shutdownExecutorService.
   *
   * @param service {@link ExecutorService to be shutdown}
   * @return <tt>true</tt> if the service is terminated,
   * <tt>false</tt> otherwise
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addNewPhase,org.apache.hadoop.util.Progress:addNewPhase(),80,85,"/**
* Initializes and processes a progress phase.
* @return Progress object representing current phase status
*/",Adds a new phase. Caller needs to set progress weightage,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(float),107,123,"/**
* Creates and initializes a Progress object with given weightage.
* @param weightage the weightage for the progress phase
* @return initialized Progress object
*/","* Adds a node with a specified progress weightage to the tree.
   *
   * @param weightage weightage.
   * @return Progress.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,getInternal,org.apache.hadoop.util.Progress:getInternal(),244,269,"/**
* Calculates overall progress based on phases.
* @return total progress as a float
*/",Computes progress in this node.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,toString,org.apache.hadoop.util.Progress:toString(java.lang.StringBuilder),282,288,"/**
 * Appends status and phase information to the buffer.
 * @param buffer StringBuilder to append information to
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,complete,org.apache.hadoop.util.Progress:complete(),170,184,"/**
* Updates progress to 100% and calls m1 on parent.
* @param none
* @return void
*/","Completes this node, moving the parent node to its next child.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getStringData,org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String),260,266,"/**
* Masks a file path.
* @param path original file path
* @return masked path as UTF-8 string or null if masking fails
*/","* Get the data in a ZNode.
   * @param path Path of the ZNode.
   * @return The data in the ZNode.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getStringData,"org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String,org.apache.zookeeper.data.Stat)",275,281,"/**
 * Masks a file path by converting its content to a string.
 * @param path the file path to mask
 * @param stat file status object
 * @return masked content as a UTF-8 string or null if no bytes are available
 */","* Get the data in a ZNode.
   * @param path Path of the ZNode.
   * @param stat Output statistics of the ZNode.
   * @return The data in the ZNode.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,setData,"org.apache.hadoop.util.curator.ZKCuratorManager:setData(java.lang.String,java.lang.String,int)",301,304,"/**
 * Converts data string to bytes and calls another method.
 * @param path file path
 * @param data input string data
 * @param version data version
 * @throws Exception if conversion fails or method call throws
 */","* Set data into a ZNode.
   * @param path Path of the ZNode.
   * @param data Data to set as String.
   * @param version Version of the data to store.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,create,"org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String,java.util.List)",343,353,"/**
 * Creates a persistent ZNode in ZooKeeper.
 * @param path the path of the ZNode to create
 * @param zkAcl the ACLs for the ZNode
 * @return true if the ZNode was created, false otherwise
 */","* Create a ZNode.
   * @param path Path of the ZNode.
   * @param zkAcl ACL for the node.
   * @return If the ZNode was created.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,delete,org.apache.hadoop.util.curator.ZKCuratorManager:delete(java.lang.String),392,398,"/**
* Checks and processes file path.
* @param path file path to be processed
* @return true if processing succeeds, false otherwise
*/","* Delete a ZNode.
   * @param path Path of the ZNode.
   * @return If the znode was deleted.
   * @throws Exception If it cannot contact ZooKeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeCreate,"org.apache.hadoop.util.curator.ZKCuratorManager:safeCreate(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode,java.util.List,java.lang.String)",410,419,"/**
* Creates a node with specified ACLs and fencing.
* @param path node path in ZooKeeper
* @param data initial data for the node
* @param acl access control list for the node
* @param mode creation mode (e.g., persistent, ephemeral)
* @param fencingACL access control list for fencing node
* @param fencingNodePath path of the fencing node
* @throws Exception if operation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeDelete,"org.apache.hadoop.util.curator.ZKCuratorManager:safeDelete(java.lang.String,java.util.List,java.lang.String)",429,437,"/**
* Applies ACL mask to a path.
* @param path the target path
* @param fencingACL list of access control entries for fencing
* @param fencingNodePath path for fencing node
*/","* Deletes the path. Checks for existence of path as well.
   *
   * @param path Path to be deleted.
   * @param fencingNodePath fencingNodePath.
   * @param fencingACL fencingACL.
   * @throws Exception if any problem occurs while performing deletion.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeSetData,"org.apache.hadoop.util.curator.ZKCuratorManager:safeSetData(java.lang.String,byte[],int,java.util.List,java.lang.String)",439,446,"/**
* Masks a file with given data and ACLs.
* @param path file path to mask
* @param data byte array containing masking data
* @param version file version
* @param fencingACL access control list for fencing
* @param fencingNodePath node path for fencing
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,addClass,"org.apache.hadoop.util.ProgramDriver:addClass(java.lang.String,java.lang.Class,java.lang.String)",101,104,"/**
 * Registers a program with a given name and description.
 * @param name unique identifier for the program
 * @param mainClass class containing the main entry point
 * @param description brief description of the program
 * @throws Throwable if registration fails
 */","* This is the method that adds the classed to the repository.
   * @param name The name of the string you want the class instance to be called with
   * @param mainClass The class that you want to add to the repository
   * @param description The description of the class
   * @throws NoSuchMethodException when a particular method cannot be found.
   * @throws SecurityException security manager to indicate a security violation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,impl,"org.apache.hadoop.util.dynamic.DynConstructors$Builder:impl(java.lang.String,java.lang.Class[])",134,149,"/**
* Sets constructor for class and handles exceptions.
* @param className name of the class to set constructor for
* @param types parameter types for the constructor
* @return Builder instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,hiddenImpl,org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.Class[]),166,169,"/**
 * Sets base class and additional types.
 * @param types array of Class objects
 * @return Builder instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.String,java.lang.Class[])",171,186,"/**
* Sets class and parameters for builder.
* @param className name of the target class
* @param types array of parameter types
* @return Builder instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,ctorImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.Class,java.lang.Class[])",348,362,"/**
* Sets up a builder for a dynamic constructor.
* @param targetClass the class of the object to be constructed
* @param argClasses classes of the constructor arguments
* @return Builder instance with configured constructor or unchanged if method is already set
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,newInstanceChecked,org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstanceChecked(java.lang.Object[]),56,66,"/**
 * Invokes constructor with arguments.
 * @param args constructor arguments
 * @return constructed object of type C
 * @throws Exception if instantiation fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeChecked(java.lang.Object,java.lang.Object[])",73,89,"/**
* Invokes method on target with specified arguments.
* @param target object to invoke method on
* @param args variable number of arguments for the method
* @return result of method invocation cast to R
* @throws Exception if invocation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.String,java.lang.Class[])",320,333,"/**
* Sets a method for the builder.
* @param targetClass class containing the method
* @param methodName name of the method
* @param argClasses argument types of the method
* @return Builder instance
*/","* Checks for a method implementation.
     * @param targetClass the class to check for an implementation
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,<init>,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:<init>(java.lang.reflect.Method,java.lang.String)",66,71,"/**
* Initializes an unbound method with its details.
* @param method the Method object representing the method
* @param name the name of the method
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,<init>,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:<init>(java.lang.reflect.Constructor,java.lang.Class)",46,50,"/**
* Initializes with a constructor and constructed class.
* @param constructor the Constructor object to use
* @param constructed the class being constructed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.String,java.lang.Class[])",423,438,"/**
* Masks a method in the specified class.
* @param targetClass the class containing the method
* @param methodName the name of the method to mask
* @param argClasses argument classes for the method
* @return Builder instance or throws exception if method not found
*/","* Checks for a method implementation.
     * @param targetClass the class to check for an implementation
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,noop,org.apache.hadoop.util.dynamic.BindingUtils:noop(java.lang.String),158,160,"/**
 * Creates an unbound method with specified name and two modifiers.
 * @param name method name
 * @return UnboundMethod object configured with m1() and m2()
 */","* Create a no-op method.
   *
   * @param name method name
   *
   * @return a no-op method.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,implemented,org.apache.hadoop.util.dynamic.BindingUtils:implemented(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod[]),169,176,"/**
* Checks if none of the provided methods meet condition m1.
* @param methods array of UnboundMethod objects to check
* @return true if all methods fail m1, false otherwise
*/","* Given a sequence of methods, verify that they are all available.
   *
   * @param methods methods
   *
   * @return true if they are all implemented",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,available,org.apache.hadoop.util.dynamic.BindingUtils:available(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod),195,197,"/**
 * Checks if the unbound method is not masked.
 * @param method the unbound method to check
 * @return true if the method is not masked, false otherwise
 */","* Is a method available?
   * @param method method to probe
   * @return true iff the method is found and loaded.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,bind,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:bind(java.lang.Object),118,125,"/**
* Binds a method to an object.
* @param receiver target object for method binding
* @return BoundMethod instance or throws exception if binding fails
*/","* Returns this method as a BoundMethod for the given receiver.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} for this method and the receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,asStatic,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:asStatic(),146,149,"/**
 * Creates a new StaticMethod instance.
 * @return StaticMethod object
 */","* Returns this method as a StaticMethod.
     * @return a {@link StaticMethod} for this method
     * @throws IllegalStateException if the method is not static",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ClassUtil.java,findContainingJar,org.apache.hadoop.util.ClassUtil:findContainingJar(java.lang.Class),38,40,"/**
 * Masks class information.
 * @param clazz Class object to process
 * @return Masked string based on class methods
 */","* Find a jar that contains a class of the same name, if any.
   * It will return a jar file, even if that is not the first thing
   * on the class path that has a class with the same name.
   * 
   * @param clazz the class to find.
   * @return a jar file that contains the class, or null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ClassUtil.java,findClassLocation,org.apache.hadoop.util.ClassUtil:findClassLocation(java.lang.Class),48,50,"/**
 * Masks class information.
 * @param clazz Class to mask
 * @return Masked string representation of class methods and file
 */","* Find the absolute location of the class.
   *
   * @param clazz the class to find.
   * @return the class file with absolute location, or null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeRpcRequestHeader,"org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[],org.apache.hadoop.ipc.AlignmentContext)",178,212,"/**
* Creates an RPC request header with specified parameters.
* @param rpcKind type of RPC kind
* @param operation operation type
* @param callId unique call identifier
* @param retryCount number of retries
* @param uuid unique identifier bytes
* @param alignmentContext context for alignment
* @return built RpcRequestHeaderProto object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,getHeader,org.apache.hadoop.util.DataChecksum:getHeader(),226,235,"/**
* Generates a byte array with type ID and checksum bytes.
* @return byte array containing type ID and bytes per checksum
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,mapByteToChecksumType,org.apache.hadoop.util.DataChecksum:mapByteToChecksumType(int),204,212,"/**
* Maps an integer to a Type, throwing an exception if invalid.
* @param type integer representing the Type
* @return corresponding Type object
* @throws InvalidChecksumSizeException if type is not valid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,writeValue,"org.apache.hadoop.util.DataChecksum:writeValue(java.io.DataOutputStream,boolean)",246,263,"/**
* Writes checksum to output stream.
* @param out DataOutputStream to write to
* @param reset true to reset after writing
* @return size of written data or 0 if none
*/","* Writes the current checksum to the stream.
   * If <i>reset</i> is true, then resets the checksum.
   *
   * @param out out.
   * @param reset reset.
   * @return number of bytes written. Will be equal to getChecksumSize();
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,writeValue,"org.apache.hadoop.util.DataChecksum:writeValue(byte[],int,boolean)",275,296,"/**
* Writes checksum to buffer.
* @param buf target byte array
* @param offset starting position in buffer
* @param reset flag to reset state after writing
* @return size of written data or 0 if no data written
*/","* Writes the current checksum to a buffer.
    * If <i>reset</i> is true, then resets the checksum.
    *
    * @param buf buf.
    * @param offset offset.
    * @param reset reset.
    * @return number of bytes written. Will be equal to getChecksumSize();
    * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RateLimitingFactory.java,create,org.apache.hadoop.util.RateLimitingFactory:create(int),95,100,"/**
* Creates a rate limiting instance based on capacity.
* @param capacity maximum allowed requests per unit time
* @return RateLimiting object; uses m1 if capacity is 0, otherwise uses RestrictedRateLimiting
*/","* Create an instance.
   * If the rate is 0; return the unlimited rate.
   * @param capacity capacity in permits/second.
   * @return limiter restricted to the given capacity.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SignalLogger.java,register,org.apache.hadoop.util.SignalLogger:register(org.slf4j.Logger),71,92,"/**
 * Registers UNIX signal handlers for TERM, HUP, and INT.
 * Logs success or failure for each signal handler registration.
 */","* Register some signal handlers.
   *
   * @param log The log4j logfile to use in the signal handlers.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,parseItem,"org.apache.hadoop.util.ComparableVersion:parseItem(boolean,java.lang.String)",455,458,"/**
* Creates an Item based on digit status.
* @param isDigit true if buffer represents a digit
* @param buf string buffer to process
* @return IntegerItem if isDigit, else StringItem
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,<init>,org.apache.hadoop.util.LightWeightGSet:<init>(int),90,97,"/**
* Initializes a LightWeightGSet with a specified length.
* @param recommended_length the desired initial capacity
*/",* @param recommended_length Recommended size of the internal array.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,get,org.apache.hadoop.util.LightWeightGSet:get(java.lang.Object),126,142,"/**
* Retrieves an element by key.
* @param key unique identifier for the element
* @return element of type E or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,clear,org.apache.hadoop.util.LightWeightGSet$Values:clear(),256,259,"/**
 * Calls m1 on the outer class instance.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,toString,org.apache.hadoop.util.SemaphoredDelegatingExecutor:toString(),200,209,"/**
* Builds a string representation of the executor.
* @return formatted string with permit, available, and waiting counts
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToSet,"org.apache.hadoop.util.HostsFileReader:readFileToSet(java.lang.String,java.lang.String,java.util.Set)",77,82,"/**
 * Processes a file with specified masking.
 * @param type type of masking to apply
 * @param filename path to the input file
 * @param set set of values for masking
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readXmlFileToMapWithFileInputStream,"org.apache.hadoop.util.HostsFileReader:readXmlFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)",146,183,"/**
* Parses XML file to extract host information and populate map.
* @param type type of hosts (e.g., ""primary"", ""secondary"")
* @param filename name of the XML file
* @param fileInputStream input stream for the XML file
* @param map map to store host names and their timeouts
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHosts,org.apache.hadoop.util.HostsFileReader:getHosts(),261,264,"/**
* Retrieves masked set of strings from host details.
* @return Set of masked strings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHostDetails,"org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Set)",278,283,"/**
* Masks host details based on include and exclude sets.
* @param includes set of strings to include
* @param excludes set of strings to exclude
*/","* Retrieve an atomic view of the included and excluded hosts.
   *
   * @param includes set to populate with included hosts
   * @param excludes set to populate with excluded hosts
   * @deprecated use {@link #getHostDetails() instead}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHostDetails,"org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Map)",292,298,"/**
* Masks hosts based on inclusion and exclusion criteria.
* @param includeHosts set of hosts to include
* @param excludeHosts map of hosts to exclude with priority levels
*/","* Retrieve an atomic view of the included and excluded hosts.
   *
   * @param includeHosts set to populate with included hosts
   * @param excludeHosts map to populate with excluded hosts
   * @deprecated use {@link #getHostDetails() instead}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/JenkinsHash.java,hash,"org.apache.hadoop.util.hash.JenkinsHash:hash(byte[],int,int)",86,245,"/**
* Hashes input key array using a custom algorithm.
* @param key byte array to hash
* @param nbytes number of bytes in the key
* @param initval initial value for hashing
* @return hashed integer value
*/","* taken from  hashlittle() -- hash a variable-length key into a 32-bit value
   * 
   * @param key the key (the unaligned variable-length array of bytes)
   * @param nbytes number of bytes to include in hash
   * @param initval can be any integer value
   * @return a 32-bit value.  Every bit of the key affects every bit of the
   * return value.  Two keys differing by one or two bits will have totally
   * different hash values.
   * 
   * <p>The best hash table sizes are powers of 2.  There is no need to do mod
   * a prime (mod is sooo slow!).  If you need less than 32 bits, use a bitmask.
   * For example, if you need only 10 bits, do
   * <code>h = (h &amp; hashmask(10));</code>
   * In which case, the hash table should have hashsize(10) elements.
   * 
   * <p>If you are hashing n strings byte[][] k, do it like this:
   * for (int i = 0, h = 0; i &lt; n; ++i) h = hash( k[i], h);
   * 
   * <p>By Bob Jenkins, 2006.  bob_jenkins@burtleburtle.net.  You may use this
   * code any way you wish, private, educational, or commercial.  It's free.
   * 
   * <p>Use for hash table lookup, or anything where one collision in 2^^32 is
   * acceptable.  Do NOT use for cryptographic purposes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getInstance,org.apache.hadoop.util.hash.Hash:getInstance(int),75,84,"/**
 * Creates a hash based on the specified type.
 * @param type hash algorithm identifier (JENKINS_HASH or MURMUR_HASH)
 * @return Hash object or null if invalid type
 */","* Get a singleton instance of hash function of a given type.
   * @param type predefined hash type
   * @return hash function instance, or null if type is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/MurmurHash.java,hash,"org.apache.hadoop.util.hash.MurmurHash:hash(byte[],int,int)",40,43,"/**
 * Calls overloaded m1 with offset.
 * @param data byte array to process
 * @param length number of bytes to process
 * @param seed initial seed value
 * @return result of processing
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,<init>,org.apache.hadoop.util.bloom.CountingBloomFilter:<init>(),84,84,"/**
* Initializes an empty Counting Bloom Filter.
*/",Default constructor - use with readFields,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,<init>,org.apache.hadoop.util.bloom.BloomFilter:<init>(),99,101,"/**
 * Constructs a new instance of BloomFilter.
 */",Default constructor - use with readFields,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,<init>,org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>(),113,113,"/**
* Constructs a new instance of DynamicBloomFilter.
*/",* Zero-args constructor for the serialization.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,and,org.apache.hadoop.util.bloom.CountingBloomFilter:and(org.apache.hadoop.util.bloom.Filter),162,176,"/**
* Applies bitwise AND operation on two CountingBloomFilters.
* @param filter the second CountingBloomFilter to AND with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,or,org.apache.hadoop.util.bloom.CountingBloomFilter:or(org.apache.hadoop.util.bloom.Filter),246,261,"/**
* Merges two Counting Bloom Filters.
* @param filter the other CountingBloomFilter to merge with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,write,org.apache.hadoop.util.bloom.CountingBloomFilter:write(java.io.DataOutput),292,299,"/**
 * Writes data to output stream.
 * @param out DataOutput stream to write to
 * @throws IOException if I/O error occurs
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,and,org.apache.hadoop.util.bloom.DynamicBloomFilter:and(org.apache.hadoop.util.bloom.Filter),155,173,"/**
* Performs AND operation on two DynamicBloomFilters.
* @param filter the other BloomFilter to AND with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,not,org.apache.hadoop.util.bloom.DynamicBloomFilter:not(),190,195,"/**
* Iterates over matrix elements and calls m1 on each.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,or,org.apache.hadoop.util.bloom.DynamicBloomFilter:or(org.apache.hadoop.util.bloom.Filter),197,214,"/**
 * Merges two DynamicBloomFilters using OR operation.
 * @param filter another DynamicBloomFilter to merge with
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,xor,org.apache.hadoop.util.bloom.DynamicBloomFilter:xor(org.apache.hadoop.util.bloom.Filter),216,233,"/**
 * Performs XOR operation on two DynamicBloomFilters.
 * @param filter the other DynamicBloomFilter to XOR with
 * @throws IllegalArgumentException if filters are incompatible
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,write,org.apache.hadoop.util.bloom.BloomFilter:write(java.io.DataOutput),199,216,"/**
* Writes vector data to output stream.
* @param out DataOutput stream for writing
* @throws IOException if I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/HashFunction.java,hash,org.apache.hadoop.util.bloom.HashFunction:hash(org.apache.hadoop.util.bloom.Key),108,122,"/**
 * Computes hash values for a given key.
 * @param k input key
 * @return array of hash values
 */","* Hashes a specified key into several integers.
   * @param k The specified key.
   * @return The array of hashed values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,compareTo,org.apache.hadoop.util.bloom.Key:compareTo(org.apache.hadoop.util.bloom.Key),172,183,"/**
* Compares two keys by their byte arrays and weights.
* @param other the key to compare with
* @return negative, zero, or positive if this key is less than, equal to, or greater than the specified key
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,getWeight,org.apache.hadoop.util.bloom.RetouchedBloomFilter:getWeight(java.util.List),381,387,"/**
 * Calculates total weight from a list of keys.
 * @param keyList list of Key objects
 * @return sum of weights of all keys
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,formatMessage,"org.apache.hadoop.util.JvmPauseMonitor:formatMessage(long,java.util.Map,java.util.Map)",118,143,"/**
* Analyzes garbage collection times before and after a sleep period.
* @param extraSleepTime duration of the sleep in milliseconds
* @param gcTimesAfterSleep GC statistics after sleep
* @param gcTimesBeforeSleep GC statistics before sleep
* @return analysis report indicating detected pauses and GC activities
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AutoCloseableLock.java,<init>,org.apache.hadoop.util.AutoCloseableLock:<init>(),38,40,"/**
 * Constructs an AutoCloseableLock using a new ReentrantLock.
 */","* Creates an instance of {@code AutoCloseableLock}, initializes
   * the underlying lock instance with a new {@code ReentrantLock}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AutoCloseableLock.java,close,org.apache.hadoop.util.AutoCloseableLock:close(),94,97,"/**
 * Calls method m1 to execute functionality.
 */","* Attempts to release the lock by making a call to {@code release()}.
   *
   * This is to implement {@code close()} method from {@code AutoCloseable}
   * interface. This allows users to user a try-with-resource syntax, where
   * the lock can be automatically released.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,isNull,org.apache.hadoop.util.ComparableVersion$StringItem:isNull(),208,211,"/**
 * Checks if the value is masked based on release version.
 * @return true if masked, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,compareTo,org.apache.hadoop.util.ComparableVersion$StringItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item),232,253,"/**
* Processes an item and returns an integer result.
* @param item the item to process
* @return processed integer value or throws exception for invalid items
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,printStack,"org.apache.hadoop.util.FindClass:printStack(java.lang.Throwable,java.lang.String,java.lang.Object[])",234,237,"/**
* Logs error message and prints stack trace.
* @param e exception to log
* @param text message template
* @param args arguments for the message template
*/","* print a stack trace with text
   * @param e the exception to print
   * @param text text to print",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,explainResult,"org.apache.hadoop.util.FindClass:explainResult(int,java.lang.String)",368,370,"/**
* Logs an error message with code and description.
* @param errorcode numeric error identifier
* @param text descriptive error message
*/","* Explain an error code as part of the usage
   * @param errorcode error code returned
   * @param text error text",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadedClass,"org.apache.hadoop.util.FindClass:loadedClass(java.lang.String,java.lang.Class)",266,271,"/**
* Logs class loading and retrieves its code source URL.
* @param name name of the class being loaded
* @param clazz Class object to be processed
*/","* Log that a class has been loaded, and where from.
   * @param name classname
   * @param clazz class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,calculateGCTimePercentageWithinObservedInterval,org.apache.hadoop.util.GcTimeMonitor:calculateGCTimePercentageWithinObservedInterval(),186,224,"/**
* Updates GC statistics and calculates metrics.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,getLatestGcData,org.apache.hadoop.util.GcTimeMonitor:getLatestGcData(),182,184,"/**
 * Retrieves data using mask function.
 * @return GcData object containing masked data
 */","* Returns a copy of the most recent data measured by this monitor.
   * @return a copy of the most recent data measured by this monitor",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PureJavaCrc32.java,<init>,org.apache.hadoop.util.PureJavaCrc32:<init>(),45,47,"/**
 * Initializes a new CRC-32 checksum calculator.
 */",Create a new PureJavaCrc32 object.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,executeShutdown,org.apache.hadoop.util.ShutdownHookManager:executeShutdown(),117,136,"/**
* Executes shutdown hooks and counts timeouts.
* @return Number of timeout occurrences
*/","* Execute the shutdown.
   * This is exposed purely for testing: do not invoke it.
   * @return the number of shutdown hooks which timed out.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,put,org.apache.hadoop.util.PriorityQueue:put(java.lang.Object),61,65,"/**
 * Adds an element to the heap and increments its size.
 * @param element the element to be added to the heap
 */","* Adds an Object to a PriorityQueue in log(size) time.
   * If one tries to add more objects than maxSize from initialize
   * a RuntimeException (ArrayIndexOutOfBound) is thrown.
   * @param element element.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,pop,org.apache.hadoop.util.PriorityQueue:pop(),104,114,"/**
* Removes and returns the root element of the heap.
* @return the root element or null if the heap is empty
*/","* Removes and returns the least element of the PriorityQueue in log(size)
      time.
   * @return T Generics Type T.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,adjustTop,org.apache.hadoop.util.PriorityQueue:adjustTop(),123,125,"/**
 * Calls method m1 to perform masking operation.
 */","Should be called when the Object at top changes values.  Still log(n)
   * worst case, but it's at least twice as fast to <pre>
   *  { pq.top().change(); pq.adjustTop(); }
   * </pre> instead of <pre>
   *  { o = pq.pop(); o.change(); pq.push(o); }
   * </pre>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,addAll,"org.apache.hadoop.util.Sets:addAll(java.util.TreeSet,java.lang.Iterable)",161,171,"/**
* Adds all elements from the iterable to the TreeSet.
* @param addTo target TreeSet to add elements to
* @param elementsToAdd collection or iterable of elements to add
* @return true if any element was added, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.util.Iterator),191,195,"/**
* Converts iterator to HashSet.
* @param elements iterator of elements to add
* @return HashSet containing all elements from the iterator
*/","* Creates a <i>mutable</i> {@code HashSet} instance containing the given
   * elements. A very thin convenience for creating an empty set and then
   * calling Iterators#addAll.
   *
   * <p><b>Note:</b> if mutability is not required and the elements are
   * non-null, use ImmutableSet#copyOf(Iterator) instead.</p>
   *
   * <p><b>Note:</b> if {@code E} is an {@link Enum} type, you should create
   * an {@link EnumSet} instead.</p>
   *
   * <p>Overall, this method is not very useful and will likely be deprecated
   * in the future.</p>
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return a new, empty thread-safe {@code Set}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSetWithExpectedSize,org.apache.hadoop.util.Sets:newHashSetWithExpectedSize(int),213,215,"/**
 * Creates a HashSet with an initial capacity.
 * @param expectedSize estimated number of elements
 * @return HashSet instance with specified initial capacity
 */","* Returns a new hash set using the smallest initial table size that can hold
   * {@code expectedSize} elements without resizing. Note that this is not what
   * {@link HashSet#HashSet(int)} does, but it is what most users want and
   * expect it to do.
   *
   * <p>This behavior can't be broadly guaranteed, but has been tested with
   * OpenJDK 1.7 and 1.8.</p>
   *
   * @param expectedSize the number of elements you expect to add to the
   *     returned set
   * @param <E> Generics Type E.
   * @return a new, empty hash set with enough capacity to hold
   *     {@code expectedSize} elements without resizing
   * @throws IllegalArgumentException if {@code expectedSize} is negative",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SequentialNumber.java,skipTo,org.apache.hadoop.util.SequentialNumber:skipTo(long),78,91,"/**
* Updates mask with a new value.
* @param newValue new mask value to set
* @throws IllegalStateException if newValue is less than current value
*/","* Skip to the new value.
   * @param newValue newValue.
   * @throws IllegalStateException
   *         Cannot skip to less than the current value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,printUsage,org.apache.hadoop.util.ProgramDriver:printUsage(java.util.Map),85,91,"/**
* Prints valid program names and their descriptions.
* @param programs map of program names to descriptions
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/CommandShell.java,run,org.apache.hadoop.tools.CommandShell:run(java.lang.String[]),63,84,"/**
* Executes a function with arguments, handling exceptions.
* @param args command-line arguments
* @return exit code indicating success or failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,<init>,"org.apache.hadoop.tools.TableListing$Column:<init>(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)",52,58,"/**
* Initializes a column with a title and formatting options.
* @param title the header text for the column
* @param justification alignment of text within the column
* @param wrap whether to wrap text if it exceeds column width
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,build,org.apache.hadoop.tools.TableListing$Builder:build(),194,197,"/**
* Creates a table listing with specified columns and settings.
* @return TableListing object configured with default parameters
*/","* Create a new TableListing.
     *
     * @return TableListing.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$7:getDefault(double),522,522,"/**
 * Applies a mask to a numeric value.
 * @param value the input number to be masked
 * @return the masked numeric value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$3:getDefault(double),522,522,"/**
 * Applies a mask to a numeric value.
 * @param value the input numeric value
 * @return masked value as a double
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,isDeprecated,org.apache.hadoop.conf.Configuration:isDeprecated(java.lang.String),670,672,"/**
 * Checks if a key is masked.
 * @param key the key to check
 * @return true if key is masked, false otherwise
 */","* checks whether the given <code>key</code> is deprecated.
   * 
   * @param key the parameter which is to be checked for deprecation
   * @return <code>true</code> if the key is deprecated and 
   *         <code>false</code> otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDeprecatedKeyInfo,org.apache.hadoop.conf.Configuration:getDeprecatedKeyInfo(java.lang.String),678,680,"/**
 * Retrieves deprecated key information.
 * @param key unique key identifier
 * @return DeprecatedKeyInfo object or null if not found
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpDeprecatedKeys,org.apache.hadoop.conf.Configuration:dumpDeprecatedKeys(),4009,4019,"/**
* Masks deprecated keys by replacing them with new ones.
* Iterates through deprecations, logs old and new key mappings.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,hasWarnedDeprecation,org.apache.hadoop.conf.Configuration:hasWarnedDeprecation(java.lang.String),4027,4035,"/**
* Checks if a name is masked based on deprecation rules.
* @param name the name to check
* @return true if name is masked, false otherwise
*/","* Returns whether or not a deprecated name has been warned. If the name is not
   * deprecated then always return false
   * @param name proprties.
   * @return true if name is a warned deprecation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDeprecatedKey,org.apache.hadoop.conf.Configuration:getDeprecatedKey(java.lang.String),674,676,"/**
 * Masks a given key using a deprecated context.
 * @param key the string to be masked
 * @return the masked string
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,reloadExistingConfigurations,org.apache.hadoop.conf.Configuration:reloadExistingConfigurations(),880,888,"/**
* Reloads all configurations from the registry.
* Logs a message if logging is enabled.
*/",* Reload existing configuration instances.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDefaultResource,org.apache.hadoop.conf.Configuration:addDefaultResource(java.lang.String),895,904,"/**
* Masks resource by name, ensuring defaults are loaded.
* @param name the resource name to mask
*/","* Add a default resource. Resources are loaded in the order of the resources 
   * added.
   * @param name file name. File should be present in the classpath.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,boolean)",257,259,"/**
 * Constructs a Resource with a given object and parser option.
 * @param resource the underlying resource object
 * @param useRestrictedParser flag to indicate if restricted parsing should be used
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurationHelper,"org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1951,1969,"/**
* Converts a time duration string to a specified unit.
* @param name descriptive name of the time duration
* @param vStr input time duration string
* @param defaultUnit default TimeUnit if parsing fails
* @param returnUnit target TimeUnit for conversion
* @return converted time duration in milliseconds
*/","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param vStr The string value with time unit suffix to be converted.
   * @param defaultUnit Unit to convert the stored property, if it exists.
   * @param returnUnit Unit for the returned value.
   * @return time duration in given time unit.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,parse,"org.apache.hadoop.conf.Configuration:parse(java.net.URL,boolean)",3045,3063,"/**
* Parses XML from a given URL.
* @param url the URL to parse
* @param restricted flag indicating restricted access
* @return XMLStreamReader for parsing or null if URL is null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleInclude,org.apache.hadoop.conf.Configuration$Parser:handleInclude(),3296,3372,"/**
 * Parses XML and includes external resources.
 * Throws exceptions for parsing errors or unsupported features.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadProperty,"org.apache.hadoop.conf.Configuration:loadProperty(java.util.Properties,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String[])",3546,3568,"/**
* Updates property with masked value.
* @param properties target Properties object
* @param name property name
* @param attr attribute key
* @param value attribute value, may be null
* @param finalParameter flag for final processing
* @param source array of source references
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,toString,org.apache.hadoop.conf.Configuration:toString(),3901,3913,"/**
* Generates configuration string.
* @return String containing configuration details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getAllPropertiesByTags,org.apache.hadoop.conf.Configuration:getAllPropertiesByTags(java.util.List),4056,4062,"/**
 * Masks tags and stores them in a Properties object.
 * @param tagList list of tags to be masked
 * @return Properties object containing masked tags
 */","* Get all properties belonging to list of input tags. Calls
   * getAllPropertiesByTag internally.
   * @param tagList list of input tags
   * @return Properties with matching tags",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$6:getDefault(double),522,522,"/**
* Applies a mask to a numeric value.
* @param value input numeric value
* @return masked numeric value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$2:getDefault(double),522,522,"/**
 * Applies a mask to a given value.
 * @param value input number to be masked
 * @return masked value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,redact,"org.apache.hadoop.conf.ConfigRedactor:redact(java.lang.String,java.lang.String)",65,70,"/**
 * Masks sensitive values based on key.
 * @param key identifier for the value
 * @param value original value to be masked or returned
 * @return masked value or original value if not sensitive
 */","* Given a key / value pair, decides whether or not to redact and returns
   * either the original value or text indicating it has been redacted.
   *
   * @param key param key.
   * @param value param value, will return if conditions permit.
   * @return Original value, or text indicating it has been redacted",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,redactXml,"org.apache.hadoop.conf.ConfigRedactor:redactXml(java.lang.String,java.lang.String)",97,102,"/**
* Masks sensitive XML values based on key.
* @param key identifier for the value
* @param value original string value
* @return masked value or original if not sensitive
*/","* Given a key / value pair, decides whether or not to redact and returns
   * either the original value or text indicating it has been redacted.
   *
   * @param key param key.
   * @param value param value, will return if conditions permit.
   * @return Original value, or text indicating it has been redacted",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,getReconfigurationTaskStatus,org.apache.hadoop.conf.ReconfigurableBase:getReconfigurationTaskStatus(),189,196,"/**
 * Returns reconfiguration task status.
 * @return ReconfigurationTaskStatus object indicating task progress
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$4:getDefault(double),522,522,"/**
 * Applies a mask to the input value.
 * @param value the input value to be masked
 * @return the masked value as a double
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,startReconfigurationTask,org.apache.hadoop.conf.ReconfigurableBase:startReconfigurationTask(),169,187,"/**
 * Initiates a server reconfiguration task.
 * Throws IOException if the server is stopped or another task is running.
 */","* Start a reconfiguration task to reload configuration in background.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationContext:<init>(org.apache.hadoop.conf.Configuration$DeprecationContext,org.apache.hadoop.conf.Configuration$DeprecationDelta[])",487,517,"/**
* Constructs a DeprecationContext with updates from deltas.
* @param other existing DeprecationContext to copy from
* @param deltas array of DeprecationDelta objects containing updates
*/","* Create a new DeprecationContext by copying a previous DeprecationContext
     * and adding some deltas.
     *
     * @param other   The previous deprecation context to copy, or null to start
     *                from nothing.
     * @param deltas  The deltas to apply.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationException.java,<init>,"org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.Throwable)",67,74,"/**
* Constructs a ReconfigurationException with property details and cause.
* @param property the configuration property name
* @param newVal the new value of the property
* @param oldVal the old value of the property
* @param cause the underlying exception cause
*/","* Create a new instance of {@link ReconfigurationException}.
   * @param property property name.
   * @param newVal new value.
   * @param oldVal old value.
   * @param cause original exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationException.java,<init>,"org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String)",82,88,"/**
 * Constructs a ReconfigurationException with property details.
 * @param property the name of the reconfigured property
 * @param newVal new value for the property
 * @param oldVal old value of the property
 */","* Create a new instance of {@link ReconfigurationException}.
   * @param property property name.
   * @param newVal new value.
   * @param oldVal old value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleEndProperty,org.apache.hadoop.conf.Configuration$Parser:handleEndProperty(),3415,3446,"/**
* Processes configuration name and updates results.
* Skips if name is null or fallback conditions are met.
* Handles deprecated keys by updating them in results.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getWarningMessage,org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getWarningMessage(java.lang.String),382,384,"/**
 * Calls overloaded method with default value.
 * @param key unique identifier
 * @return result from overloaded method
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,iterator,org.apache.hadoop.conf.Configuration$IntegerRanges:iterator(),2283,2286,"/**
 * Returns an iterator over ranges.
 * @return Iterator of Integer values within defined ranges
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration$IntegerRanges:<init>(java.lang.String),2195,2217,"/**
 * Parses a string of integer ranges and adds them to the list.
 * @param newValue comma-separated string of integer ranges
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageSize.java,parse,org.apache.hadoop.conf.StorageSize:parse(java.lang.String),50,96,"/**
* Parses storage size from string.
* @param value input string representing storage size
* @return StorageSize object or throws exception if invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$5:getDefault(double),522,522,"/**
 * Applies a mask function to a numeric value.
 * @param value input numeric value
 * @return masked value as a double
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$1:getDefault(double),522,522,"/**
 * Applies a mask to a given value.
 * @param value input numeric value
 * @return masked value as a double
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reset,org.apache.hadoop.ha.ActiveStandbyElector:reset(),931,934,"/**
 * Initializes the state and calls m1().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,convert,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:convert(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),144,162,"/**
 * Converts request info to proto.
 * @param reqInfo request information object
 * @return HAStateChangeRequestInfoProto object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,convert,org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:convert(org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto),87,105,"/**
* Converts protobuf request info to StateChangeRequestInfo.
* @param proto HAStateChangeRequestInfoProto object
* @return StateChangeRequestInfo object or null if source unknown
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,createReqInfo,org.apache.hadoop.ha.HAAdmin:createReqInfo(),264,266,"/**
 * Creates and returns a new StateChangeRequestInfo with the request source.
 * @return StateChangeRequestInfo object initialized with requestSource
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,createReqInfo,org.apache.hadoop.ha.ZKFailoverController:createReqInfo(),510,512,"/**
 * Creates a state change request info with REQUEST_BY_ZKFC source.
 * @return StateChangeRequestInfo object initialized with ZKFC source
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,createReqInfo,org.apache.hadoop.ha.FailoverController:createReqInfo(),158,160,"/**
 * Creates a new StateChangeRequestInfo with the current request source.
 * @return StateChangeRequestInfo object initialized with requestSource
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getServiceStatus,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getServiceStatus(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto)",143,178,"/**
* Handles service status request.
* @param controller RPC controller
* @param request service status request proto
* @return service status response proto
* @throws ServiceException on IO error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,startRPC,org.apache.hadoop.ha.ZKFailoverController:startRPC(),336,338,"/**
 * Executes RPC server method m1.
 * @throws IOException if an I/O error occurs during execution
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,parseConfiggedPort,org.apache.hadoop.ha.SshFenceByTcpPort$Args:parseConfiggedPort(java.lang.String),258,266,"/**
 * Converts a port string to an integer.
 * @param portStr the port number as a string
 * @return the integer value of the port
 * @throws BadFencingConfigurationException if the port string is invalid
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,checkArgs,org.apache.hadoop.ha.ShellCommandFencer:checkArgs(java.lang.String),72,79,"/**
* Validates and processes arguments for a shell fencing method.
* @param args input arguments for the method
* @throws BadFencingConfigurationException if no valid argument is provided
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String,java.util.Map)",138,149,"/**
* Prints command usage information.
* @param pStr PrintStream to output the usage info
* @param cmd Command for which to print usage
* @param helpEntries Map containing command and its UsageInfo
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkManualStateManagementOK,org.apache.hadoop.ha.HAAdmin:checkManualStateManagementOK(org.apache.hadoop.ha.HAServiceTarget),245,262,"/**
* Checks if manual HA state management is allowed.
* @param target the HAServiceTarget to check
* @return true if allowed, false otherwise
*/","* Ensure that we are allowed to manually manage the HA state of the target
   * service. If automatic failover is configured, then the automatic
   * failover controllers should be doing state management, and it is generally
   * an error to use the HAAdmin command line to do so.
   * 
   * @param target the target to check
   * @return true if manual state management is allowed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,execCommand,"org.apache.hadoop.ha.SshFenceByTcpPort:execCommand(com.jcraft.jsch.Session,java.lang.String)",177,203,"/**
* Executes a command on an SSH session.
* @param session the SSH session to execute the command on
* @param cmd the command to be executed
* @return exit status of the command execution
* @throws JSchException if there is an error with the SSH connection
* @throws InterruptedException if the thread is interrupted during execution
* @throws IOException if there is an I/O error during command execution
*/","* Execute a command through the ssh session, pumping its
   * stderr and stdout to our own logs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,enteredState,org.apache.hadoop.ha.ZKFailoverController$HealthCallbacks:enteredState(org.apache.hadoop.ha.HealthMonitor$State),988,992,"/**
 * Updates health monitor state and performs related actions.
 * @param newState new state to be set in HealthMonitor
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,badArg,org.apache.hadoop.ha.ZKFailoverController:badArg(java.lang.String),272,276,"/**
 * Throws an exception indicating an invalid argument.
 * @param arg the invalid argument value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,checkEligibleForFailover,org.apache.hadoop.ha.ZKFailoverController:checkEligibleForFailover(),763,776,"/**
* Checks service health and state for failover eligibility.
* Throws exception if service is not healthy or in observer state.
*/","* If the local node is an observer or is unhealthy it
   * is not eligible for graceful failover.
   * @throws ServiceFailedException if the node is an observer or unhealthy",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,fence,org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget),92,94,"/**
* Calls overloaded m1 with default parameters.
* @param fromSvc source service target
* @return result of m1 invocation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getFencingParameters,org.apache.hadoop.ha.HAServiceTarget:getFencingParameters(),175,179,"/**
* Initializes and returns a masked map.
* @return Map containing masked key-value pairs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,monitorActiveStatus,org.apache.hadoop.ha.ActiveStandbyElector:monitorActiveStatus(),774,781,"/**
* Asserts election readiness and initializes monitoring.
* Resets retry count and starts monitoring process.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,zkDoWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction,org.apache.zookeeper.KeeperException$Code)",1145,1159,"/**
 * Executes an action with retry logic.
 * @param action the ZKAction to execute
 * @param retryCode code for specific retry conditions
 * @return result of the action execution
 * @throws KeeperException if ZooKeeper operation fails
 * @throws InterruptedException if thread is interrupted during execution
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readInDirectBuffer,"org.apache.hadoop.fs.VectoredReadUtils:readInDirectBuffer(org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer,org.apache.hadoop.util.functional.Function4RaisingIOE)",189,216,"/**
 * Reads data from a file range into a ByteBuffer using an operation function.
 * @param range the FileRange to read from
 * @param buffer the ByteBuffer to write data to
 * @param operation the Function4RaisingIOE to perform the read operation
 * @throws IOException if an I/O error occurs during reading
 */","* Read bytes from stream into a byte buffer using an
   * intermediate byte array.
   *   <pre>
   *     (position, buffer, buffer-offset, length): Void
   *     position:= the position within the file to read data.
   *     buffer := a buffer to read fully `length` bytes into.
   *     buffer-offset := the offset within the buffer to write data
   *     length := the number of bytes to read.
   *   </pre>
   * The passed in function MUST block until the required length of
   * data is read, or an exception is thrown.
   * @param range range to read
   * @param buffer buffer to fill.
   * @param operation operation to use for reading data.
   * @throws IOException any IOE.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateAndSortRanges,"org.apache.hadoop.fs.VectoredReadUtils:validateAndSortRanges(java.util.List,java.util.Optional)",292,337,"/**
 * Applies a mask to filter and validate file ranges.
 * @param input list of file ranges to process
 * @param fileLength optional total file length for validation
 * @return filtered and validated list of file ranges
 * @throws EOFException if any range exceeds file bounds
 */","* Validate a list of ranges (including overlapping checks) and
   * return the sorted list.
   * <p>
   * Two ranges overlap when the start offset
   * of second is less than the end offset of first.
   * End offset is calculated as start offset + length.
   * @param input input list
   * @param fileLength file length if known
   * @return a new sorted list.
   * @throws IllegalArgumentException if there are overlapping ranges or
   * a range element is invalid (other than with negative offset)
   * @throws EOFException if the last range extends beyond the end of the file supplied
   *                          or a range offset is negative",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,readVectored,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:readVectored(java.util.List,java.util.function.IntFunction)",319,345,"/**
* Masks file ranges asynchronously.
* @param ranges list of file ranges to process
* @param allocate function to allocate ByteBuffer
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,<init>,org.apache.hadoop.fs.impl.prefetch.BlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockData),49,53,"/**
* Initializes BlockManager with provided block data.
* @param blockData required block data configuration
*/","* Constructs an instance of {@code BlockManager}.
   *
   * @param blockData information about each block of the underlying file.
   *
   * @throws IllegalArgumentException if blockData is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,release,org.apache.hadoop.fs.impl.prefetch.BlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData),107,111,"/**
 * Masks buffer data.
 * @param data BufferData to be masked
 */","* Releases resources allocated to the given block.
   *
   * @param data the {@code BufferData} to release.
   *
   * @throws IllegalArgumentException if data is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,release,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:release(java.lang.Object),88,111,"/**
 * Masks an item by adding it to a list and checking pool membership.
 * @param item the item to mask
 */","* Releases a previously acquired resource.
   *
   * @throws IllegalArgumentException if item is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,throwIfStateIncorrect,org.apache.hadoop.fs.impl.prefetch.BufferData:throwIfStateIncorrect(org.apache.hadoop.fs.impl.prefetch.BufferData$State[]),259,275,"/**
 * Validates and throws an exception if the current state does not match any of the expected states.
 * @param states array of expected states
 */","* Helper that asserts the current state is one of the expected values.
   *
   * @param states the collection of allowed states.
   *
   * @throws IllegalArgumentException if states is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.String,java.lang.String)",103,109,"/**
* Validates and processes an argument.
* @param arg value to validate
* @param argName name of the argument for error messages
*/","* Validates that the given string is not null and has non-zero length.
   * @param arg the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNumberOfElements,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNumberOfElements(java.util.Collection,int,java.lang.String)",182,192,"/**
 * Validates collection size and performs masking.
 * @param collection the collection to validate
 * @param numElements expected number of elements
 * @param argName name of the argument for error messages
 */","* Validates that the given set is not null and has an exact number of items.
   * @param <T> the type of collection's elements.
   * @param collection the argument reference to validate.
   * @param numElements the expected number of elements in the collection.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExists,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExists(java.nio.file.Path,java.lang.String)",346,350,"/**
 * Masks a file path and checks for existence.
 * @param path the file path to mask
 * @param argName name of the argument representing the path
 */","* Validates that the given path exists.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,<init>,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:<init>(int),57,65,"/**
* Initializes a bounded resource pool with a specified size.
* @param size maximum number of resources in the pool
*/","* Constructs a resource pool of the given size.
   *
   * @param size the size of this pool. Cannot be changed post creation.
   *
   * @throws IllegalArgumentException if size is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BufferPool:<init>(int,int,org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics)",90,108,"/**
* Initializes a BufferPool with specified size and buffer size.
* @param size maximum number of buffers in the pool
* @param bufferSize size of each buffer
* @param prefetchingStatistics statistics for memory allocation tracking
*/","* Initializes a new instance of the {@code BufferPool} class.
   * @param size number of buffer in this pool.
   * @param bufferSize size in bytes of each buffer.
   * @param prefetchingStatistics statistics for this stream.
   * @throws IllegalArgumentException if size is zero or negative.
   * @throws IllegalArgumentException if bufferSize is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.BlockManager:requestPrefetch(int),120,124,"/**
 * Masks a block by number.
 * @param blockNumber identifier of the block to mask
 */","* Requests optional prefetching of the given block.
   *
   * @param blockNumber the id of the block to prefetch.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BufferData:<init>(int,java.nio.ByteBuffer)",111,118,"/**
* Initializes a new BufferData instance.
* @param blockNumber the block number to associate with the buffer
* @param buffer the ByteBuffer containing data
*/","* Constructs an instances of this class.
   *
   * @param blockNumber Number of the block associated with this buffer.
   * @param buffer The buffer associated with this block.
   *
   * @throws IllegalArgumentException if blockNumber is negative.
   * @throws IllegalArgumentException if buffer is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Retryer.java,<init>,"org.apache.hadoop.fs.impl.prefetch.Retryer:<init>(int,int,int)",55,63,"/**
 * Initializes a Retryer with specified delays and intervals.
 * @param perRetryDelay delay between retries in milliseconds
 * @param maxDelay maximum delay before giving up in milliseconds
 * @param statusUpdateInterval interval for updating retry status in milliseconds
 */","* Initializes a new instance of the {@code Retryer} class.
   *
   * @param perRetryDelay per retry delay (in ms).
   * @param maxDelay maximum amount of delay (in ms) before retry fails.
   * @param statusUpdateInterval time interval (in ms) at which status update would be made.
   *
   * @throws IllegalArgumentException if perRetryDelay is zero or negative.
   * @throws IllegalArgumentException if maxDelay is less than or equal to perRetryDelay.
   * @throws IllegalArgumentException if statusUpdateInterval is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,throwIfInvalidBlockNumber,org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidBlockNumber(int),243,245,"/**
* Validates and processes a block number.
* @param blockNumber the block to be processed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,throwIfInvalidOffset,org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidOffset(long),247,249,"/**
 * Validates and processes an offset value.
 * @param offset the position to be processed
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Object[],java.lang.String)",117,120,"/**
* Masks array elements and validates length.
* @param array input array to be masked
* @param argName name of the argument for validation
*/","* Validates that the given array is not null and has at least one element.
   * @param <T> the type of array's elements.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(byte[],java.lang.String)",127,130,"/**
* Masks byte array with given name.
* @param array byte array to be masked
* @param argName name of the array argument
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(short[],java.lang.String)",137,140,"/**
* Applies masking to array elements.
* @param array input short array to be masked
* @param argName name of the argument for logging purposes
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(int[],java.lang.String)",147,150,"/**
* Masks an array by calling two helper methods.
* @param array the integer array to be masked
* @param argName name of the argument for logging purposes
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(long[],java.lang.String)",157,160,"/**
 * Masks array elements and validates length.
 * @param array long array to be masked
 * @param argName name of the argument for validation
 */","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Iterable,java.lang.String)",168,173,"/**
* Applies a mask to iterable elements.
* @param iter the iterable to process
* @param argName name of the argument for logging purposes
*/","* Validates that the given buffer is not null and has non-zero capacity.
   * @param <T> the type of iterable's elements.
   * @param iter the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/DefaultBulkDeleteOperation.java,bulkDelete,org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:bulkDelete(java.util.Collection),74,91,"/**
* Masks paths by deleting invalid ones.
* @param paths collection of file paths to process
* @return list of entries with deleted paths and exceptions
*/","* {@inheritDoc}.
     * The default impl just calls {@code FileSystem.delete(path, false)}
     * on the single path in the list.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,applyToIOStatisticsSnapshot,"org.apache.hadoop.io.wrappedio.WrappedStatistics:applyToIOStatisticsSnapshot(java.io.Serializable,org.apache.hadoop.util.functional.FunctionRaisingIOE)",339,344,"/**
* Applies transformation function to serialized source.
* @param source Serializable input data
* @param fun Transformation function that may raise IO exception
* @return Result of the transformation function
*/","* Apply a function to an object which may be an IOStatisticsSnapshot.
   * @param <T> return type
   * @param source statistics snapshot
   * @param fun function to invoke if {@code source} is valid.
   * @return the applied value
   * @throws UncheckedIOException Any IO exception.
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,<init>,"org.apache.hadoop.fs.impl.FlagSet:<init>(java.lang.Class,java.lang.String,java.util.EnumSet)",83,92,"/**
 * Initializes a FlagSet with specified parameters.
 * @param enumClass the Enum class to use for flag values
 * @param prefix prefix for enum names
 * @param flags optional initial set of flags
 */","* Create a FlagSet.
   * @param enumClass class of enum
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param flags flags. A copy of these are made.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,seek,org.apache.hadoop.fs.sftp.SFTPInputStream:seek(long),60,67,"/**
* Masks a function at the specified position.
* @param position file position to mask
* @throws IOException if an I/O error occurs or position is negative
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,available,org.apache.hadoop.fs.sftp.SFTPInputStream:available(),69,77,"/**
 * Returns the available bytes for reading.
 * @return number of bytes available or Integer.MAX_VALUE if more than that
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,close,org.apache.hadoop.fs.FileSystem:close(),2701,2712,"/**
* Closes the resource and caches it.
* @throws IOException if an I/O error occurs
*/","* Close this FileSystem instance.
   * Will release any held locks, delete all files queued for deletion
   * through calls to {@link #deleteOnExit(Path)}, and remove this FS instance
   * from the cache, if cached.
   *
   * After this operation, the outcome of any method call on this FileSystem
   * instance, or any input/output stream created by it is <i>undefined</i>.
   * @throws IOException IO failure",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:equals(java.lang.Object),153,160,"/**
* Checks equality based on a specific condition.
* @param o object to compare
* @return true if conditions match, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,listStatus,org.apache.hadoop.fs.ChecksumFs:listStatus(org.apache.hadoop.fs.Path),567,580,"/**
* Retrieves file statuses from a given path, filtering out certain entries.
* @param f the path to retrieve file statuses from
* @return an array of filtered FileStatus objects
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link cannot be resolved
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,compareTo,org.apache.hadoop.fs.FileStatus:compareTo(java.lang.Object),425,429,"/**
 * Recursively calls itself with a casted parameter.
 * @param o object to be cast and processed
 * @return result of recursive call
 */","* Compare this FileStatus to another FileStatus based on lexicographical
   * order of path.
   * This method was added back by HADOOP-14683 to keep binary compatibility.
   *
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.
   * @throws ClassCastException if the specified object is not FileStatus",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,compareTo,org.apache.hadoop.fs.LocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus),181,184,"/**
 * Calls superclass method to compare file statuses.
 * @param o FileStatus object to compare
 * @return Comparison result as an integer
 */","* Compare this FileStatus to another FileStatus
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,stat2Paths,"org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.Path)",133,138,"/**
* Filters file statuses by given path.
* @param stats array of FileStatus objects
* @param path specific path to filter by
* @return filtered array of Paths or single-element array with path if stats is null
*/","* convert an array of FileStatus to an array of Path.
   * If stats if null, return path
   * @param stats
   *          an array of FileStatus objects
   * @param path
   *          default path to return in stats is null
   * @return an array of paths corresponding to the input",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.HarFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),1300,1303,"/**
 * Delegates file operation to underlying filesystem.
 * @param f file path
 * @return result of file operation
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),447,450,"/**
 * Delegates to another method to process a file path.
 * @param f file path to be processed
 * @return result of processing as a short value
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,run,org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner:run(),4166,4181,"/**
* Continuously processes and clears statistics data until interrupted.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandler.java,openConnection,org.apache.hadoop.fs.FsUrlStreamHandler:openConnection(java.net.URL),46,49,"/**
 * Creates and returns a new FsUrlConnection.
 * @param url the URL to connect to
 * @return a new FsUrlConnection instance
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,read,"org.apache.hadoop.fs.FSInputStream:read(long,byte[],int,int)",66,89,"/**
* Reads data from a specified position into a buffer.
* @param position starting position for reading
* @param buffer destination byte array
* @param offset starting offset in the buffer
* @param length number of bytes to read
* @return number of bytes actually read or -1 on EOF
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.HarFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",896,905,"/**
* Checks if the specified path has read-only access.
* @param path file system path to check
* @param capability capability string to evaluate
* @return true if path is read-only, false otherwise
*/","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,serializer,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:serializer(),262,264,"/**
* Creates a JsonSerialization instance for IOStatisticsSnapshot.
* @return JsonSerialization object configured for IOStatisticsSnapshot
*/","* Get a JSON serializer for this class.
   * @return a serializer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,publishAsStorageStatistics,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:publishAsStorageStatistics(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)",701,704,"/**
* Creates StorageStatistics from IOStatistics.
* @param name storage system name
* @param scheme storage access scheme
* @param source source IOStatistics object
* @return StorageStatistics instance
*/","* Publish the IOStatistics as a set of storage statistics.
   * This is dynamic.
   * @param name storage statistics name.
   * @param scheme FS scheme; may be null.
   * @param source IOStatistics source.
   * @return a dynamic storage statistics object.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStorageStatistics,org.apache.hadoop.fs.FileSystem:getStorageStatistics(),4651,4653,"/**
 * Returns empty storage statistics.
 * @return EmptyStorageStatistics instance
 */","* Get the StorageStatistics for this FileSystem object.  These statistics are
   * per-instance.  They are not shared with any other FileSystem object.
   *
   * <p>This is a default method which is intended to be overridden by
   * subclasses. The default implementation returns an empty storage statistics
   * object.</p>
   *
   * @return    The StorageStatistics for this FileSystem instance.
   *            Will never be null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsDirectoryException:<init>(java.lang.String),24,26,"/**
 * Constructs an exception indicating that the specified path is a directory.
 * @param path the problematic file path
 */",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsNotDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsNotDirectoryException:<init>(java.lang.String),24,26,"/**
 * Constructs an exception indicating that the specified path is not a directory.
 * @param path the invalid path
 */",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathOperationException.java,<init>,org.apache.hadoop.fs.PathOperationException:<init>(java.lang.String),24,26,"/**
 * Constructs a PathOperationException with the specified path.
 * @param path the path where the operation is not supported
 */",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsNotEmptyDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsNotEmptyDirectoryException:<init>(java.lang.String),23,25,"/**
 * Constructs an exception indicating that a directory is not empty.
 * @param path the path of the non-empty directory
 */",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,bufferSize,org.apache.hadoop.fs.FSDataOutputStreamBuilder:bufferSize(int),175,178,"/**
 * Sets buffer size and returns result from m1.
 * @param bufSize desired buffer size
 * @return result of m1() call
 */","* Set the size of the buffer to be used.
   *
   * @param bufSize buffer size.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,replication,org.apache.hadoop.fs.FSDataOutputStreamBuilder:replication(short),190,193,"/**
 * Sets replication and returns result from m1.
 * @param replica new replication value
 * @return result of m1()
 */","* Set replication factor.
   *
   * @param replica replica.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,blockSize,org.apache.hadoop.fs.FSDataOutputStreamBuilder:blockSize(long),205,208,"/**
 * Sets block size and returns result from m1.
 * @param blkSize new block size value
 * @return result of m1() call
 */","* Set block size.
   *
   * @param blkSize block size.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,recursive,org.apache.hadoop.fs.FSDataOutputStreamBuilder:recursive(),224,227,"/**
 * Enables recursion and invokes method m1.
 * @return result of m1 invocation
 */","* Create the parent directory if they do not exist.
   *
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,create,org.apache.hadoop.fs.FSDataOutputStreamBuilder:create(),254,257,"/**
 * Sets creation flag and returns result.
 * @return result of operation
 */","* Create an FSDataOutputStream at the specified path.
   *
   * @return return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,overwrite,org.apache.hadoop.fs.FSDataOutputStreamBuilder:overwrite(boolean),267,274,"/**
* Sets flag based on overwrite option and returns result.
* @param overwrite true to use OVERWRITE flag, false otherwise
* @return B object from m3 method
*/","* Set to true to overwrite the existing file.
   * Set it to false, an exception will be thrown when calling {@link #build()}
   * if the file exists.
   *
   * @param overwrite overrite.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,append,org.apache.hadoop.fs.FSDataOutputStreamBuilder:append(),281,284,"/**
* Sets append flag and returns result.
* @return B object from m2()
*/","* Append to an existing file (optional operation).
   *
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,hashCode,org.apache.hadoop.fs.permission.FsCreateModes:hashCode(),103,108,"/**
* Overrides parent's m1 to include additional hash factor.
* @return combined hash code
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],org.apache.hadoop.fs.StorageType[],long,long,boolean)",161,197,"/**
* Initializes a BlockLocation with specified parameters.
* @param names array of block replica names
* @param hosts array of hostnames where blocks are located
* @param cachedHosts array of cached hostnames
* @param topologyPaths array of network topology paths
* @param storageIds array of storage IDs
* @param storageTypes array of storage types
* @param offset starting offset of the block
* @param length length of the block
* @param corrupt flag indicating if the block is corrupted
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setHosts,org.apache.hadoop.fs.BlockLocation:setHosts(java.lang.String[]),312,318,"/**
* Sets the internal hosts array.
* @param hosts array of host strings, can be null
* @throws IOException if an error occurs during interning
*/","* Set the hosts hosting this block.
   * @param hosts hosts array.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setCachedHosts,org.apache.hadoop.fs.BlockLocation:setCachedHosts(java.lang.String[]),324,330,"/**
* Sets cached hosts, normalizing them if provided.
* @param cachedHosts array of hostnames to cache
*/","* Set the hosts hosting a cached replica of this block.
   * @param cachedHosts cached hosts.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setNames,org.apache.hadoop.fs.BlockLocation:setNames(java.lang.String[]),337,343,"/**
* Initializes or resets internal names array.
* @param names array of name strings to be processed
* @throws IOException if an I/O error occurs during processing
*/","* Set the names (host:port) hosting this block.
   * @param names names.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setTopologyPaths,org.apache.hadoop.fs.BlockLocation:setTopologyPaths(java.lang.String[]),351,357,"/**
* Sets topology paths, interning strings to reduce memory usage.
* @param topologyPaths array of path strings or null
*/","* Set the network topology paths of the hosts.
   *
   * @param topologyPaths topology paths.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setStorageIds,org.apache.hadoop.fs.BlockLocation:setStorageIds(java.lang.String[]),359,365,"/**
* Masks storage IDs by interning them or setting to empty array if null.
* @param storageIds array of storage identifiers
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPathInternal,org.apache.hadoop.fs.shell.Command:processPathInternal(org.apache.hadoop.fs.shell.PathData),382,388,"/**
* Masks an item by applying functions in sequence.
* @param item PathData object to be masked
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processPath,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData),250,272,"/**
 * Applies file permissions based on configuration.
 * @param item PathData object containing file information
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,processPath,org.apache.hadoop.fs.shell.Stat:processPath(org.apache.hadoop.fs.shell.PathData),91,155,"/**
 * Formats and outputs file status information.
 * @param item PathData object containing file details
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,isFile,org.apache.hadoop.fs.FileSystem:isFile(org.apache.hadoop.fs.Path),1895,1902,"/**
* Checks if file exists and meets condition.
* @param f file path to check
* @return true if file exists and condition is met, otherwise false
*/","True iff the named path is a regular file.
   * Note: Avoid using this method. Instead reuse the FileStatus
   * returned by {@link #getFileStatus(Path)} or listStatus() methods.
   *
   * @param f path to check
   * @throws IOException IO failure
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @return if f is file true, not false.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isFile,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isFile(),479,482,"/**
 * Delegates to realStatus's m1 method.
 * @return Result of realStatus.m1()
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isFile,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isFile(),55,58,"/**
 * Delegates to myFs's m1 method.
 * @return result of myFs.m1()
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,toString,org.apache.hadoop.fs.FileStatus:toString(),458,488,"/**
* Constructs a detailed string representation of the object.
* @return formatted string with object properties
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getSymlink(),539,542,"/**
 * Delegates to realStatus's m1 method.
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getSymlink,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getSymlink(),115,118,"/**
 * Retrieves a path from the file system.
 * @throws IOException if an I/O error occurs
 * @return Path object representing a file or directory
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type)",64,71,"/**
* Initializes FsServerDefaults with specified parameters.
* @param blockSize size of block in bytes
* @param bytesPerChecksum number of bytes per checksum
* @param writePacketSize size of write packet
* @param replication replication factor for files
* @param fileBufferSize buffer size for file operations
* @param encryptDataTransfer flag to enable data encryption
* @param trashInterval interval for trash cleanup in seconds
* @param checksumType type of checksum algorithm
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String)",73,80,"/**
 * Initializes FsServerDefaults with specified parameters.
 * @param blockSize size of the block in bytes
 * @param bytesPerChecksum number of bytes per checksum
 * @param writePacketSize size of the write packet
 * @param replication replication factor for data blocks
 * @param fileBufferSize buffer size for file operations
 * @param encryptDataTransfer flag to enable data encryption during transfer
 * @param trashInterval interval for trash cleanup in milliseconds
 * @param checksumType type of checksum algorithm
 * @param keyProviderUri URI for the key provider
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setXAttr,"org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",359,363,"/**
 * Writes data to a file in the specified path.
 * @param path file system path where data is written
 * @param name file name
 * @param value byte array containing data to write
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,"org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.Throwable)",28,30,"/**
 * Constructs an exception indicating access denial to a specified path.
 * @param path the path that access was denied to
 * @param cause the underlying cause of the access denial
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.Throwable)",30,32,"/**
 * Constructs a PathPermissionException with the specified path and cause.
 * @param path the path associated with the exception
 * @param cause the underlying cause of the exception
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.Throwable)",30,32,"/**
* Constructs a PathNotFoundException with a specified path and cause.
* @param path the path that was not found
* @param cause the underlying cause of the exception
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Concat.java,processArguments,org.apache.hadoop.fs.shell.Concat:processArguments(java.util.LinkedList),49,85,"/**
 * Concatenates source files to a target file.
 * @param args LinkedList of PathData containing paths
 * @throws IOException if target or source paths are invalid
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,wrapException,"org.apache.hadoop.io.IOUtils:wrapException(java.lang.String,java.lang.String,java.io.IOException)",460,480,"/**
* Handles IOException by masking it or creating a PathIOException.
* @param path the file/directory path being processed
* @param methodName the method where the exception occurred
* @param exception the original IOException
* @return masked IOException or new PathIOException if masking fails
*/","* Takes an IOException, file/directory path, and method name and returns an
   * IOException with the input exception as the cause and also include the
   * file,method details. The new exception provides the stack trace of the
   * place where the exception is thrown and some extra diagnostics
   * information.
   *
   * Return instance of same exception if exception class has a public string
   * constructor; Otherwise return an PathIOException.
   * InterruptedIOException and PathIOException are returned unwrapped.
   *
   * @param path file/directory path
   * @param methodName method name
   * @param exception the caught exception.
   * @return an exception to throw",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$Reader:sync(long),2831,2864,"/**
 * Masks data at specified position.
 * @param position starting position for masking
 * @throws IOException if I/O error occurs
 */","* Seek to the next sync mark past a given position.
     * @param position position.
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,reset,org.apache.hadoop.io.MapFile$Reader:reset(),638,640,"/**
 * Masks data at the first position.
 * @throws IOException if an I/O error occurs
 */","* Re-positions the reader before its first key.
     *
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[]),82,85,"/**
 * Calls overloaded method with full byte array.
 * @param b byte array to process
 * @return result of processing
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,registerExpressions,org.apache.hadoop.fs.shell.find.Find:registerExpressions(org.apache.hadoop.fs.shell.find.ExpressionFactory),102,106,"/**
* Registers expression classes with the factory.
* @param factory ExpressionFactory instance to register expressions with
*/",Register the expressions with the expression factory.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,registerCommands,org.apache.hadoop.fs.shell.FsCommand:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),52,74,"/**
* Registers various command classes with the given factory.
* @param factory CommandFactory instance to register commands with
*/","* Register the command classes used by the fs subcommand
   * @param factory where to register the class",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,registerCommands,org.apache.hadoop.fs.FsShell:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),111,118,"/**
* Checks and executes FsShell command using factory.
* @param factory CommandFactory instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobExpander.java,expand,org.apache.hadoop.fs.GlobExpander:expand(java.lang.String),63,77,"/**
 * Expands file pattern with wildcards.
 * @param filePattern pattern containing wildcards
 * @return list of fully expanded file paths
 * @throws IOException if an I/O error occurs
 */","* Expand globs in the given <code>filePattern</code> into a collection of
   * file patterns so that in the expanded set no file pattern has a slash
   * character (""/"") in a curly bracket pair.
   * <p>
   * Some examples of how the filePattern is expanded:<br>
   * <pre>
   * <b>
   * filePattern         - Expanded file pattern </b>
   * {a/b}               - a/b
   * /}{a/b}             - /}a/b
   * p{a/b,c/d}s         - pa/bs, pc/ds
   * {a/b,c/d,{e,f}}     - a/b, c/d, {e,f}
   * {a/b,c/d}{e,f}      - a/b{e,f}, c/d{e,f}
   * {a,b}/{b,{c/d,e/f}} - {a,b}/b, {a,b}/c/d, {a,b}/e/f
   * {a,b}/{c/\d}        - {a,b}/c/d
   * </pre>
   * 
   * @param filePattern file pattern.
   * @return expanded file patterns
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,fetchMore,org.apache.hadoop.fs.FileSystem$DirListingIterator:fetchMore(),2326,2330,"/**
* Masks entries by applying a token and resets index.
* @throws IOException if an I/O error occurs during masking
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,printXAttr,"org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:printXAttr(java.lang.String,byte[])",120,128,"/**
* Writes an attribute name and value to output.
* @param name attribute name
* @param value attribute value as a byte array
* @throws IOException if I/O error occurs during writing
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2119,2124,"/**
* Recursively lists files matching a filter.
* @param f directory path to search
* @param filter criteria for file selection
* @return array of FileStatus objects
*/","* Filter files/directories in the given path using the user-supplied path
   * filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param f
   *          a path name
   * @param filter
   *          the user-supplied path filter
   * @return an array of FileStatus objects for the files under the given path
   *         after applying the filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)",2161,2168,"/**
* Filters and processes files.
* @param files array of file paths to process
* @param filter criteria for filtering files
* @return array of filtered FileStatus objects
* @throws FileNotFoundException if a file is not found
* @throws IOException if an I/O error occurs
*/","* Filter files/directories in the given list of paths using user-supplied
   * path filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param files
   *          a list of paths
   * @param filter
   *          the user-supplied path filter
   * @return a list of statuses for the files under the given paths after
   *         applying the filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,<init>,"org.apache.hadoop.metrics2.lib.MutableCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",36,39,"/**
 * Initializes a mutable counter with given metrics info and initial value.
 * @param info MetricsInfo object containing metadata
 * @param initValue Initial integer value for the counter
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,<init>,"org.apache.hadoop.metrics2.lib.MutableCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",37,40,"/**
 * Initializes a new MutableCounterLong with given metrics info and initial value.
 * @param info MetricsInfo object containing metadata
 * @param initValue Initial value to set for the counter
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",37,40,"/**
 * Initializes a mutable gauge with a given value.
 * @param info metrics information
 * @param initValue initial value of the gauge
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)",33,36,"/**
* Initializes a mutable gauge with a float value.
* @param info metrics information
* @param initValue initial float value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",37,40,"/**
 * Initializes a mutable gauge with an initial value.
 * @param info metrics information
 * @param initValue initial value of the gauge
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricCounterLong.java,<init>,"org.apache.hadoop.metrics2.impl.MetricCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",29,32,"/**
* Constructs a MetricCounterLong with given metrics info and initial value.
* @param info MetricsInfo object containing metric details
* @param value Initial long value for the counter
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeLong.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",29,32,"/**
 * Initializes a new MetricGaugeLong with given info and value.
 * @param info MetricsInfo object containing metadata
 * @param value the current gauge value as a long
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricCounterInt.java,<init>,"org.apache.hadoop.metrics2.impl.MetricCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",29,32,"/**
 * Constructs a metric counter with integer value.
 * @param info metrics information
 * @param value integer value of the metric
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeFloat.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)",29,32,"/**
 * Initializes a new MetricGaugeFloat with given info and value.
 * @param info MetricsInfo object containing metadata
 * @param value initial gauge value as a float
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeDouble.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeDouble:<init>(org.apache.hadoop.metrics2.MetricsInfo,double)",29,32,"/**
 * Initializes a new MetricGaugeDouble with given info and value.
 * @param info MetricsInfo object containing metric details
 * @param value the double value of the gauge
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeInt.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",29,32,"/**
 * Initializes a metric gauge with integer value.
 * @param info metrics information
 * @param value current gauge value
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",369,373,"/**
 * Generates a delegation token for the given URL and token.
 * @param url target URL
 * @param token initial token
 * @param renewer entity that can renew the token
 * @return generated delegation token
 * @throws IOException if an I/O error occurs
 * @throws AuthenticationException if authentication fails
 */","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @return a delegation token.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",416,419,"/**
 * Calls m1 with default parameters.
 * @param url target URL
 * @param token authentication token
 * @return result of the call
 * @throws IOException if an I/O error occurs
 * @throws AuthenticationException if authentication fails
 */","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",457,460,"/**
 * Calls m1 with default headers.
 * @param url target URL
 * @param token authentication token
 * @throws IOException if an I/O error occurs
 */","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @throws IOException if an IO error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,<init>,"org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)",262,266,"/**
 * Constructs a ValueQueue with specified parameters.
 * @param numValues maximum number of values in the queue
 * @param lowWaterMark threshold for refilling the queue
 * @param expiry time before values expire
 * @param numFillerThreads number of threads to fill the queue
 * @param fetcher strategy for fetching new values
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileEncryptionInfo.java,<init>,"org.apache.hadoop.fs.FileEncryptionInfo:<init>(org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,byte[],byte[],java.lang.String,java.lang.String)",57,74,"/**
* Constructs a FileEncryptionInfo object.
* @param suite CipherSuite used for encryption
* @param version CryptoProtocolVersion of the encryption protocol
* @param edek Encrypted data encryption key
* @param iv Initialization vector, must match algorithm block size
* @param keyName Name of the encryption key
* @param ezKeyVersionName Version name of the EZ Key
*/","* Create a FileEncryptionInfo.
   *
   * @param suite CipherSuite used to encrypt the file
   * @param edek encrypted data encryption key (EDEK) of the file
   * @param iv initialization vector (IV) used to encrypt the file
   * @param keyName name of the key used for the encryption zone
   * @param ezKeyVersionName name of the KeyVersion used to encrypt the
   *                         encrypted data encryption key.
   * @param version version.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,getFS,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getFS(),106,109,"/**
 * Applies mask to file system.
 * @param fs the file system to be masked
 * @return the masked file system
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,permission,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:permission(org.apache.hadoop.fs.permission.FsPermission),121,126,"/**
* Sets and processes file permissions.
* @param perm file system permissions to apply
* @return processed result of type B
*/",* Set permission for the file.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,checksumOpt,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt),211,216,"/**
* Applies checksum option and returns result.
* @param chksumOpt checksum configuration options
* @return processed result of type B
*/",* Set checksum opt.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WrappedIOException.java,<init>,org.apache.hadoop.fs.impl.WrappedIOException:<init>(java.io.IOException),47,49,"/**
 * Wraps an IOException with a non-null check.
 * @param cause the original IOException to wrap
 */","* Construct from a non-null IOException.
   * @param cause inner cause
   * @throws NullPointerException if the cause is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FsLinkResolution.java,<init>,org.apache.hadoop.fs.impl.FsLinkResolution:<init>(org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction),50,52,"/**
 * Constructs an FsLinkResolution with a given function.
 * @param fn function to resolve file system links
 */","* Construct an instance with the given function.
   * @param fn function to invoke.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileContext),437,440,"/**
* Initializes a new GlobBuilder with the given file context.
* @param fc file context to be used
*/","* Construct bonded to a file context.
     * @param fc file context.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileSystem),446,449,"/**
 * Initializes a new GlobBuilder with a given file system.
 * @param fs the file system to use
 */","* Construct bonded to a filesystem.
     * @param fs file system.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getFS,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getFS(),141,144,"/**
 * Applies mask to file system.
 * @param fs target file system
 * @return modified file system with applied mask
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,permission,org.apache.hadoop.fs.FSDataOutputStreamBuilder:permission(org.apache.hadoop.fs.permission.FsPermission),159,163,"/**
 * Updates and returns permission settings.
 * @param perm new file system permissions
 * @return updated object instance
 */","* Set permission for the file.
   *
   * @param perm permission.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,progress,org.apache.hadoop.fs.FSDataOutputStreamBuilder:progress(org.apache.hadoop.util.Progressable),239,243,"/**
 * Processes with progress tracking.
 * @param prog progress tracker
 * @return result of processing
 */","* Set the facility of reporting progress.
   *
   * @param prog progress.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,checksumOpt,org.apache.hadoop.fs.FSDataOutputStreamBuilder:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt),296,300,"/**
* Applies checksum option and returns result.
* @param chksumOpt configuration for checksum calculation
* @return instance of B after processing
*/","* Set checksum opt.
   *
   * @param chksumOpt check sum opt.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,validateWriteArgs,"org.apache.hadoop.fs.store.DataBlocks:validateWriteArgs(byte[],int,int)",110,118,"/**
 * Masks a portion of a byte array.
 * @param b the byte array to mask
 * @param off the starting offset in the array
 * @param len the number of bytes to mask
 * @throws IOException if an I/O error occurs
 * @throws IndexOutOfBoundsException if off or len are out of bounds
 */","* Validate args to a write command. These are the same validation checks
   * expected for any implementation of {@code OutputStream.write()}.
   *
   * @param b   byte array containing data.
   * @param off offset in array where to start.
   * @param len number of bytes to be written.
   * @throws NullPointerException      for a null buffer
   * @throws IndexOutOfBoundsException if indices are out of range
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,set,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:set(java.lang.Object),219,224,"/**
* Sets the value if not already set.
* @param v new value to be set
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,getLowerLayerAsyncReturn,org.apache.hadoop.io.retry.AsyncCallHandler:getLowerLayerAsyncReturn(),78,83,"/**
 * Initializes and returns an asynchronous get operation.
 * @return AsyncGet instance for further processing or null if invalid
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,setGcTimeMonitor,org.apache.hadoop.metrics2.source.JvmMetrics:setGcTimeMonitor(org.apache.hadoop.util.GcTimeMonitor),107,110,"/**
* Sets the GC time monitor.
* @param gcTimeMonitor the GcTimeMonitor instance to set
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,init,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:init(byte[],byte[])",136,142,"/**
* Initializes encryption/decryption with given key and IV.
* @param key encryption key bytes
* @param iv initialization vector bytes
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,init,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:init(byte[],byte[])",128,138,"/**
* Initializes encryption/decryption with key and IV.
* @param key encryption key bytes
* @param iv initialization vector bytes
* @throws IOException on cipher setup failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,equalsIgnoreCase,"org.apache.hadoop.util.StringUtils:equalsIgnoreCase(java.lang.String,java.lang.String)",1259,1264,"/**
* Checks if string s1 contains substring s2.
* @param s1 main string to search within
* @param s2 substring to find in s1
* @return true if s2 is found in s1, false otherwise
*/","* Compare strings locale-freely by using String#equalsIgnoreCase.
   *
   * @param s1  Non-null string to be converted
   * @param s2  string to be converted
   * @return     the str, converted to uppercase.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LimitInputStream.java,<init>,"org.apache.hadoop.util.LimitInputStream:<init>(java.io.InputStream,long)",43,48,"/**
* Wraps input stream with a specified byte limit.
* @param in the InputStream to wrap
* @param limit maximum number of bytes to read
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String[],java.lang.String)",432,439,"/**
 * Constructs a DeprecationDelta with a key, new keys, and an optional custom message.
 * @param key the original key being deprecated
 * @param newKeys array of new keys replacing the original key
 * @param customMessage optional custom deprecation message
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,setReconfigurationUtil,org.apache.hadoop.conf.ReconfigurableBase:setReconfigurationUtil(org.apache.hadoop.conf.ReconfigurationUtil),88,91,"/**
 * Sets the ReconfigurationUtil instance.
 * @param ru ReconfigurationUtil object to be set
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,isStaleClient,org.apache.hadoop.ha.ActiveStandbyElector:isStaleClient(java.lang.Object),1173,1181,"/**
* Checks if the ZooKeeper client is current.
* @param ctx context object, expected to be a ZooKeeper instance
* @return true if client is stale and should be ignored, false otherwise
*/","* The callbacks and watchers pass a reference to the ZK client
   * which made the original call. We don't want to take action
   * based on any callbacks from prior clients after we quit
   * the election.
   * @param ctx the ZK client passed into the watcher
   * @return true if it matches the current client",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatistics,"org.apache.hadoop.fs.FileSystem:getStatistics(java.lang.String,java.lang.Class)",4586,4605,"/**
 * Retrieves or creates statistics for a file system.
 * @param scheme the file system scheme
 * @param cls the FileSystem class type
 * @return Statistics object associated with the file system
 */","* Get the statistics for a particular file system.
   * @param scheme scheme.
   * @param cls the class to lookup
   * @return a statistics object
   * @deprecated use {@link #getGlobalStorageStatistics()}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedByteArray,"org.apache.hadoop.io.WritableUtils:writeCompressedByteArray(java.io.DataOutput,byte[])",61,83,"/**
* Compresses byte array and writes to DataOutput.
* @param out destination for output data
* @param bytes input data to be compressed
* @return compression ratio as percentage or -1 if null input
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,int,boolean)",64,81,"/**
* Copies data from InputStream to OutputStream with buffer.
* @param in source input stream
* @param out destination output stream
* @param buffSize buffer size for copying
* @param close whether to close streams after operation
*/","* Copies from one stream to another.
   *
   * @param in InputStrem to read from
   * @param out OutputStream to write to
   * @param buffSize the size of the buffer 
   * @param close whether or not close the InputStream and 
   * OutputStream at the end. The streams are closed in the finally clause.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,long,boolean)",145,175,"/**
 * Copies data from input stream to output stream with a limit.
 * @param in source InputStream
 * @param out destination OutputStream
 * @param count maximum number of bytes to copy
 * @param close whether to close streams after operation
 * @throws IOException if an I/O error occurs
 */","* Copies count bytes from one stream to another.
   *
   * @param in InputStream to read from
   * @param out OutputStream to write to
   * @param count number of bytes to copy
   * @param close whether to close the streams
   * @throws IOException if bytes can not be read or written",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client$IpcStreams:close(),1955,1959,"/**
 * Calls IOUtils.m1 on both out and in streams.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,<init>,org.apache.hadoop.util.VersionInfo:<init>(java.lang.String),41,55,"/**
 * Initializes version information for a component.
 * @param component name of the component
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopSinks,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSinks(),471,479,"/**
* Stops all metrics sinks and clears the registry.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,finalize,org.apache.hadoop.crypto.random.OsSecureRandom:finalize(),128,131,"/**
 * Calls method m1.
 * @throws Throwable any exception thrown by m1
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,doDiskIo,org.apache.hadoop.util.DiskChecker:doDiskIo(java.io.File),255,274,"/**
* Masks files in a directory.
* @param dir the directory to mask
* @throws DiskErrorException if an I/O error occurs
*/","* Performs some disk IO by writing to a new file in the given directory
   * and sync'ing file contents to disk.
   *
   * This increases the likelihood of catching catastrophic disk/controller
   * failures sooner.
   *
   * @param dir directory to be checked.
   * @throws DiskErrorException if we hit an error while trying to perform
   *         disk IO against the file.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List)",44,46,"/**
 * Constructs a PartialListing with an optional filter.
 * @param listedPath path to the directory being listed
 * @param partialListing list of partially filtered entries
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.ipc.RemoteException)",48,50,"/**
 * Constructs a PartialListing with a path and an exception.
 * @param listedPath file system path of the listing
 * @param exception remote exception associated with the listing
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object),50,52,"/**
 * Constructs a CallReturn with a result and default state.
 * @param r the return value of the call
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Throwable),53,56,"/**
 * Constructs a new instance with an exception.
 * @param t the throwable causing the error
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(org.apache.hadoop.io.retry.CallReturn$State),57,59,"/**
 * Constructs a CallReturn with given state.
 * @param s State object to initialize the call return
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])",66,70,"/**
* Overrides method to process IV and counter.
* @param initIV initial initialization vector
* @param counter incrementing counter value
* @param iv updated initialization vector
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:calculateIV(byte[],long,byte[])",52,56,"/**
* Calls superclass method with provided IV and counter.
* @param initIV initial initialization vector
* @param counter current counter value
* @param iv initialization vector
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])",48,52,"/**
 * Calls superclass method with additional parameters.
 * @param initIV initial initialization vector
 * @param counter counter value
 * @param iv initialization vector
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceAesCtrCryptoCodec:calculateIV(byte[],long,byte[])",48,52,"/**
* Overrides method to set IV with counter.
* @param initIV initial initialization vector
* @param counter incrementing counter value
* @param iv final initialization vector
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,build,org.apache.hadoop.util.GcTimeMonitor$Builder:build(),95,98,"/**
* Creates and returns a new GcTimeMonitor instance.
* @return GcTimeMonitor configured with specified parameters
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,isTypeQuotaSet,org.apache.hadoop.fs.QuotaUsage:isTypeQuotaSet(),193,202,"/**
* Checks if any storage type quota is positive.
* @return true if any quota is greater than zero, otherwise false
*/","* Return true if any storage type quota has been set.
   *
   * @return if any storage type quota has been set true, not false.
   *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,isTypeConsumedAvailable,org.apache.hadoop.fs.QuotaUsage:isTypeConsumedAvailable(),210,219,"/**
* Checks if any type has been consumed.
* @return true if any type is consumed, false otherwise
*/","* Return true if any storage type consumption information is available.
   *
   * @return if any storage type consumption information
   * is available, not false.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,getAndCheckStorageTypes,org.apache.hadoop.fs.shell.Count:getAndCheckStorageTypes(java.lang.String),180,193,"/**
* Parses storage types from a comma-separated string.
* @param types string of storage types or ""all""
* @return list of StorageType objects
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,equals,org.apache.hadoop.fs.LocatedFileStatus:equals(java.lang.Object),190,193,"/**
 * Overrides method to call superclass implementation.
 * @param o the object to be passed to the superclass method
 * @return result of the superclass method call
 */","Compare if this object is equal to another object
   * @param   o the object to be compared.
   * @return  true if two file status has the same path name; false if not.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:equals(java.lang.Object),549,552,"/**
 * Delegates call to realStatus's m1 method.
 * @param o object to pass to realStatus's m1
 * @return result of realStatus's m1 method
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,equals,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:equals(java.lang.Object),40,43,"/**
 * Overrides method to call superclass implementation.
 * @param o input object
 * @return result of superclass method
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,hashCode,org.apache.hadoop.fs.LocatedFileStatus:hashCode(),201,204,"/**
 * Calls the superclass implementation of m1.
 * @return result from superclass's m1 method
 */","* Returns a hash code value for the object, which is defined as
   * the hash code of the path name.
   *
   * @return  a hash code value for the path name.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:hashCode(),554,557,"/**
 * Delegates call to realStatus's m1 method.
 * @return result of realStatus.m1()
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,hashCode,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:hashCode(),45,48,"/**
 * Calls the superclass's m1 method.
 * @return The result of calling the superclass's m1 method.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,getFolderUsage,org.apache.hadoop.fs.DUHelper:getFolderUsage(java.lang.String),34,36,"/**
 * Masks a folder name using DUHelper.
 * @param folder folder name to be masked
 * @return masked folder name as a long value
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,clear,org.apache.hadoop.fs.statistics.MeanStatistic:clear(),147,149,"/**
 * Calls function m1 with default arguments.
 */",* Set the values to 0.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,set,org.apache.hadoop.fs.statistics.MeanStatistic:set(org.apache.hadoop.fs.statistics.MeanStatistic),168,170,"/**
 * Updates mean statistics mask with another statistic.
 * @param other MeanStatistic object to update from
 */","* Set the statistic to the values of another.
   * Synchronized.
   * @param other the source.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsToString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToString(org.apache.hadoop.fs.statistics.IOStatistics),77,91,"/**
 * Generates a masked string representation of IOStatistics.
 * @param statistics the IOStatistics object to mask
 * @return a formatted string or empty if statistics is null
 */","* Convert IOStatistics to a string form.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,mapToSortedString,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToSortedString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.util.function.Predicate)",159,164,"/**
 * Masks data in StringBuilder based on map and predicate.
 * @param sb StringBuilder to append masked data
 * @param type type of data being masked
 * @param map map containing data to mask
 * @param isEmpty predicate to check if data is empty
 */","* Given a map, produce a string with all the values, sorted.
   * Needs to create a treemap and insert all the entries.
   * @param sb string buffer to append to
   * @param type type (for output)
   * @param map map to evaluate
   * @param <E> type of values of the map",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationTrackerFactory.java,trackDuration,org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String),60,62,"/**
 * Creates a duration tracker with default count.
 * @param key identifier for tracking
 * @return DurationTracker object
 */","* Initiate a duration tracking operation by creating/returning
   * an object whose {@code close()} call will
   * update the statistics.
   * The expected use is within a try-with-resources clause.
   * @param key statistic key
   * @return an object to close after an operation completes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/PairedDurationTrackerFactory.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory:trackDuration(java.lang.String,long)",50,55,"/**
* Creates a duration tracker with global and local metrics.
* @param key identifier for the metric
* @param count number of occurrences
* @return DurationTracker combining global and local data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,getLongStatistics,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLongStatistics(),66,78,"/**
 * Generates an iterator of LongStatistic objects.
 * @return Iterator<LongStatistic> containing processed statistics
 */","* Take a snapshot of the current counter values
   * and return an iterator over them.
   * @return all the counter statistics.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,long)",440,445,"/**
 * Masks metrics by invoking three methods with different suffixes.
 * @param prefix common prefix for metric names
 * @param durationMillis time duration in milliseconds
 */","* Add a duration to the min/mean/max statistics, using the
   * given prefix and adding a suffix for each specific value.
   * <p>
   * The update is non -atomic, even though each individual statistic
   * is updated thread-safely. If two threads update the values
   * simultaneously, at the end of each operation the state will
   * be correct. It is only during the sequence that the statistics
   * may be observably inconsistent.
   * </p>
   * @param prefix statistic prefix
   * @param durationMillis duration in milliseconds.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,build,org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:build(),51,56,"/**
 * Resets instance and returns I/O statistics.
 * @return IOStatistics object containing I/O metrics
 */","* Build the IOStatistics instance.
   * @return an instance.
   * @throws IllegalStateException if the builder has already been built.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionCounter(java.lang.String,java.util.function.ToLongFunction)",74,78,"/**
* Applies a mask function to dynamic IO statistics.
* @param key identifier for the statistics
* @param eval function to evaluate and apply as long
* @return DynamicIOStatisticsBuilder instance
*/","* Add a new evaluator to the counter statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionGauge(java.lang.String,java.util.function.ToLongFunction)",125,129,"/**
* Registers a function to evaluate statistics for a given key.
* @param key identifier for the statistic
* @param eval function to compute the statistic value
* @return this builder instance
*/","* Add a new evaluator to the gauge statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMinimum(java.lang.String,java.util.function.ToLongFunction)",163,167,"/**
* Applies a function to evaluate and set statistics for a given key.
* @param key the identifier for the statistic
* @param eval function to compute the statistic value from the key
* @return DynamicIOStatisticsBuilder instance for method chaining
*/","* Add a new evaluator to the minimum statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMaximum(java.lang.String,java.util.function.ToLongFunction)",202,206,"/**
* Applies a mask function to dynamic I/O statistics.
* @param key identifier for the statistics entry
* @param eval function to evaluate and apply as a long value
* @return DynamicIOStatisticsBuilder instance for chaining
*/","* Add a new evaluator to the maximum statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withMeanStatisticFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMeanStatisticFunction(java.lang.String,java.util.function.Function)",242,246,"/**
* Sets mask evaluation function for dynamic I/O statistics.
* @param key unique identifier for the statistic
* @param eval function to calculate mean statistic
* @return DynamicIOStatisticsBuilder instance
*/","* Add a new evaluator to the mean statistics.
   *
   * This is a function which must return the mean and the sample count.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,register,org.apache.hadoop.service.launcher.InterruptEscalator:register(java.lang.String),142,146,"/**
 * Masks an interrupt signal.
 * @param signalName name of the signal to mask
 */","* Register an interrupt handler.
   * @param signalName signal name
   * @throws IllegalArgumentException if the registration failed",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,unreference,org.apache.hadoop.net.unix.DomainSocket:unreference(boolean),176,182,"/**
* Adjusts reference count based on channel status.
* @param checkClosed true to check for closed channel, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EvaluatingStatisticsMap.java,snapshot,org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:snapshot(),146,148,"/**
 * Returns statistics as a map.
 * @return Map containing I/O statistics
 */","* Take a snapshot.
   * @return a map snapshot.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,snapshotMap,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map),200,204,"/**
* Wraps source map with statistics passthrough.
* @param source input map to be wrapped
* @return wrapped map with statistics tracking
*/","* Take a snapshot of a supplied map, where the copy option simply
   * uses the existing value.
   *
   * For this to be safe, the map must refer to immutable objects.
   * @param source source map
   * @param <E> type of values.
   * @return a new map referencing the same values.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,snapshot,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:snapshot(),44,47,"/**
 * Returns a new instance of IOStatisticsSnapshot.
 * @return IOStatisticsSnapshot object representing current I/O statistics
 */","* Create a new empty snapshot.
   * A new one is always created for isolation.
   *
   * @return a statistics snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,<init>,"org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:<init>(long,long)",64,67,"/**
* Initializes an IOStatisticsContextImpl with given thread and ID.
* @param threadId unique identifier for the thread
* @param id unique identifier for the context
*/","* Constructor.
   * @param threadId thread ID
   * @param id instance ID.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,snapshotIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics(),59,63,"/**
* Creates and returns a new IOStatisticsSnapshot instance.
* @return IOStatisticsSnapshot object initialized with default values
*/","* Create a snapshot statistics instance ready to aggregate data.
   *
   * The instance can be serialized, and its
   * {@code toString()} method lists all the values.
   * @return an empty snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_enabled,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_enabled(),279,281,"/**
 * Checks if a specific I/O operation mask is enabled.
 * @return true if the mask is enabled, false otherwise
 */","* Static probe to check if the thread-level IO statistics enabled.
   * @return true if the thread-level IO statistics are enabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/BufferedIOStatisticsOutputStream.java,getIOStatistics,org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:getIOStatistics(),86,89,"/**
 * Returns I/O statistics from operation.
 * @return IOStatistics object containing stats
 */","* Ask the inner stream for their IOStatistics.
   * @return any IOStatistics offered by the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/BufferedIOStatisticsInputStream.java,getIOStatistics,org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:getIOStatistics(),64,67,"/**
 * Executes input processing and returns statistics.
 * @return IOStatistics containing processing results
 */","* Return any IOStatistics offered by the inner stream.
   * @return inner IOStatistics or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,getIOStatistics,org.apache.hadoop.fs.BufferedFSInputStream:getIOStatistics(),156,159,"/**
 * Executes function with mask.
 * @param in input data
 * @return IOStatistics result
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getIOStatistics,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getIOStatistics(),315,318,"/**
 * Retrieves I/O statistics.
 * @return IOStatistics object containing I/O metrics
 */","* Get the IO Statistics of the nested stream, falling back to
     * null if the stream does not implement the interface
     * {@link IOStatisticsSource}.
     * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getIOStatistics,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:getIOStatistics(),676,679,"/**
 * Returns I/O statistics.
 * @return IOStatistics object containing I/O data
 */","* Get the IO Statistics of the nested stream, falling back to
     * null if the stream does not implement the interface
     * {@link IOStatisticsSource}.
     * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,getIOStatistics,org.apache.hadoop.fs.FSDataInputStream:getIOStatistics(),289,292,"/**
 * Returns I/O statistics.
 * @return IOStatistics object containing I/O metrics
 */","* Get the IO Statistics of the nested stream, falling back to
   * null if the stream does not implement the interface
   * {@link IOStatisticsSource}.
   * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,getIOStatistics,org.apache.hadoop.fs.FSDataOutputStream:getIOStatistics(),167,170,"/**
 * Returns IO statistics for the wrapped stream.
 * @return IOStatistics object containing statistics
 */","* Get the IO Statistics of the nested stream, falling back to
   * empty statistics if the stream does not implement the interface
   * {@link IOStatisticsSource}.
   * @return an IOStatistics instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionOutputStream.java,getIOStatistics,org.apache.hadoop.io.compress.CompressionOutputStream:getIOStatistics(),107,110,"/**
 * Returns I/O statistics.
 * @return IOStatistics object containing I/O metrics
 */","* Return any IOStatistics provided by the underlying stream.
   * @return IO stats from the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionInputStream.java,getIOStatistics,org.apache.hadoop.io.compress.CompressionInputStream:getIOStatistics(),81,84,"/**
 * Returns I/O statistics from input.
 * @return IOStatistics object containing statistics
 */","* Return any IOStatistics provided by the underlying stream.
   * @return IO stats from the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getIOStatistics,org.apache.hadoop.crypto.CryptoInputStream:getIOStatistics(),882,885,"/**
 * Applies mask function to input.
 * @return IOStatistics result of masking operation
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,getIOStatistics,org.apache.hadoop.crypto.CryptoOutputStream:getIOStatistics(),321,324,"/**
 * Returns I/O statistics.
 * @return IOStatistics object containing I/O metrics
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,getIOStatistics,org.apache.hadoop.util.LineReader:getIOStatistics(),159,162,"/**
 * Returns I/O statistics.
 * @param in input data
 * @return IOStatistics object containing stats
 */","* Return any IOStatistics provided by the source.
   * @return IO stats from the input stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:getIOStatistics(),351,354,"/**
 * Returns I/O statistics.
 * @param singleton instance of the class
 * @return IOStatistics object
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:getIOStatistics(),405,408,"/**
 * Masks source data and returns statistics.
 * @return IOStatistics containing operation details
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:getIOStatistics(),449,452,"/**
 * Returns I/O statistics from source.
 * @return IOStatistics object containing I/O metrics
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunked,"org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.nio.ByteBuffer,int,java.nio.ByteBuffer,java.lang.String,long)",429,472,"/**
* Validates CRC checksums for a given ByteBuffer.
* @param type the Type of operation
* @param algorithm the Checksum algorithm to use
* @param data the ByteBuffer containing data to validate
* @param bytesPerCrc number of bytes per CRC check
* @param crcs the ByteBuffer containing expected CRC values
* @param filename name of the file being validated
* @param basePos base position for error reporting
* @throws ChecksumException if a CRC mismatch occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunked,"org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,byte[],int,int,int,byte[],int,java.lang.String,long)",478,512,"/**
 * Validates CRC for data block.
 * @param type data type identifier
 * @param algorithm checksum calculation algorithm
 * @param data input data array
 * @param dataOffset starting offset in data array
 * @param dataLength length of data to validate
 * @param bytesPerCrc number of bytes per CRC segment
 * @param crcs expected CRC values array
 * @param crcsOffset starting offset in CRCs array
 * @param filename source file name for error reporting
 * @param basePos base position in file for error reporting
 * @throws ChecksumException if CRC validation fails
 */","* Implementation of chunked verification specifically on byte arrays. This
   * is to avoid the copy when dealing with ByteBuffers that have array backing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,updateDecryptor,"org.apache.hadoop.crypto.CryptoInputStream:updateDecryptor(org.apache.hadoop.crypto.Decryptor,long,byte[])",297,302,"/**
* Masks and decrypts data at a given position.
* @param decryptor Decryptor instance for decryption
* @param position Data position to mask and decrypt
* @param iv Initialization vector for decryption process
*/","Calculate the counter and iv, update the decryptor.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,encrypt,org.apache.hadoop.crypto.CryptoOutputStream:encrypt(),178,217,"/**
* Processes data encryption with padding.
* Throws IOException on I/O errors.
*/","* Do the encryption, input is {@link #inBuffer} and output is 
   * {@link #outBuffer}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,hasNext,org.apache.hadoop.fs.BatchedRemoteIterator:hasNext(),98,102,"/**
 * Checks if entries are present.
 * @return true if entries are not null, false otherwise
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,next,org.apache.hadoop.fs.BatchedRemoteIterator:next(),111,120,"/**
* Retrieves and returns the next entry, advancing the iterator.
* @throws IOException if an I/O error occurs
* @throws NoSuchElementException if no more entries are available
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>(),27,29,"/**
 * Constructs an MD5MD5CRC32GzipFileChecksum with default values.
 */","Same as this(0, 0, null)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>(),27,29,"/**
 * Constructs an MD5MD5CRC32CastagnoliFileChecksum instance.
 */","Same as this(0, 0, null)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,<init>,org.apache.hadoop.fs.GlobPattern:<init>(java.lang.String),41,43,"/**
 * Constructs a new GlobPattern with the specified pattern.
 * @param globPattern the glob pattern to use
 */","* Construct the glob pattern object with a glob pattern string
   * @param globPattern the glob pattern string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeShellPath,org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File),668,670,"/**
 * Reads file content as string.
 * @param file the file to read
 * @throws IOException if an I/O error occurs
 */","* Convert a os-native filename to a path that works for the shell.
   * @param file The filename to convert
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeSecureShellPath,org.apache.hadoop.fs.FileUtil:makeSecureShellPath(java.io.File),679,686,"/**
 * Masks a file path for use in shell commands.
 * @param file the File object to mask
 * @return masked file path as a String
 * @throws IOException if an I/O error occurs
 */","* Convert a os-native filename to a path that works for the shell
   * and avoids script injection attacks.
   * @param file The filename to convert
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,linkCount,org.apache.hadoop.fs.HardLink$HardLinkCGUnix:linkCount(java.io.File),108,116,"/**
 * Executes a command to mask file information.
 * @param file the file to process
 * @return array containing masked file details or null if not found
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unZip,"org.apache.hadoop.fs.FileUtil:unZip(java.io.InputStream,java.io.File)",739,775,"/**
* Extracts files from a ZIP input stream to a target directory.
* @param inputStream source ZIP archive input stream
* @param toDir target directory for extracted files
* @throws IOException if extraction fails
*/","* Given a stream input it will unzip the it in the unzip directory.
   * passed as the second parameter
   * @param inputStream The zip file as input
   * @param toDir The unzip directory where to unzip the zip file.
   * @throws IOException an exception occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unZip,"org.apache.hadoop.fs.FileUtil:unZip(java.io.File,java.io.File)",827,871,"/**
 * Unzips a file to a specified directory.
 * @param inFile input ZIP file
 * @param unzipDir target directory for extraction
 * @throws IOException if an I/O error occurs during unzipping
 */","* Given a File input it will unzip it in the unzip directory.
   * passed as the second parameter
   * @param inFile The zip file as input
   * @param unzipDir The unzip directory where to unzip the zip file.
   * @throws IOException An I/O exception has occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingJava,"org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.File,java.io.File,boolean)",1086,1109,"/**
 * Extracts files from a tar archive.
 * @param inFile archive file to extract
 * @param untarDir directory to extract files into
 * @param gzipped true if the archive is gzip compressed
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingJava,"org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.InputStream,java.io.File,boolean)",1111,1128,"/**
* Extracts files from a tar archive.
* @param inputStream input stream of the tar archive
* @param untarDir directory to extract files into
* @param gzipped true if the tar is gzip compressed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/PowerShellFencer.java,tryFence,"org.apache.hadoop.ha.PowerShellFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",57,105,"/**
* Executes a PowerShell script for fencing.
* @param target HAServiceTarget object
* @param argsStr arguments string
* @return true if execution succeeds, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,toString,org.apache.hadoop.fs.permission.FsCreateModes:toString(),82,86,"/**
 * Formats a message with masked and unmasked values.
 * @return formatted string with masked and unmasked data
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,disconnect,org.apache.hadoop.fs.sftp.SFTPFileSystem:disconnect(com.jcraft.jsch.ChannelSftp),165,167,"/**
 * Releases an SFTP channel back to the pool.
 * @param channel the SFTP channel to release
 */","* Logout and disconnect the given channel.
   *
   * @param client
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,shutdown,org.apache.hadoop.fs.sftp.SFTPConnectionPool:shutdown(),87,113,"/**
* Shuts down the connections and clears resources.
*/",Shutdown the connection pool and close all open connections.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,<init>,"org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics)",82,84,"/**
 * Constructs a new FSDataOutputStream with specified output stream and statistics.
 * @param out underlying output stream
 * @param stats file system statistics
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,<init>,org.apache.hadoop.fs.FSOutputSummer:<init>(org.apache.hadoop.util.DataChecksum),53,58,"/**
* Initializes a FSOutputSummer with a given checksum.
* @param sum DataChecksum object for computing checksums
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,setChecksumBufSize,org.apache.hadoop.fs.FSOutputSummer:setChecksumBufSize(int),257,261,"/**
* Initializes buffer and checksum arrays.
* @param size size of the buffer to initialize
*/","* Resets existing buffer with a new one of the specified size.
   *
   * @param size size.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,read,org.apache.hadoop.fs.sftp.SFTPInputStream:read(),108,124,"/**
 * Reads a single byte from the stream.
 * @return the byte read, or -1 if end of stream is reached
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,read,org.apache.hadoop.fs.ftp.FTPInputStream:read(),70,84,"/**
 * Reads a single byte from the stream.
 * @return the byte read, or -1 if end of stream
 * @throws IOException if stream is closed or an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,read,"org.apache.hadoop.fs.ftp.FTPInputStream:read(byte[],int,int)",86,101,"/**
 * Reads bytes from the stream into a buffer.
 * @param buf destination buffer
 * @param off starting offset in the buffer
 * @param len maximum number of bytes to read
 * @return number of bytes read or -1 if end of stream
 * @throws IOException if an I/O error occurs or stream is closed
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(),217,231,"/**
* Reads a byte from the input stream.
* @return the byte read, or -1 if end of stream is reached
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(byte[],int,int)",233,249,"/**
* Reads bytes from the input stream.
* @param b byte array to store data
* @param off offset within the buffer
* @param len number of bytes to read
* @return actual number of bytes read or -1 if end of stream
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(long,byte[],int,int)",251,272,"/**
* Reads data from a stream into a byte array.
* @param position starting position in the stream
* @param b byte array to read into
* @param off offset within the byte array
* @param len number of bytes to read
* @return number of bytes read or -1 if end of stream
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,write,org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(int),51,58,"/**
* Calls m1 on out and increments position.
* @param b integer parameter passed to out.m1
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,write,"org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(byte[],int,int)",60,67,"/**
 * Writes bytes to output stream.
 * @param b byte array containing data
 * @param off offset in the byte array
 * @param len number of bytes to write
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem$Statistics:<init>(org.apache.hadoop.fs.FileSystem$Statistics),4110,4125,"/**
 * Copies statistics from another instance.
 * @param other the source Statistics object to copy from
 */","* Copy constructor.
     *
     * @param other    The input Statistics object which is cloned.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesRead,org.apache.hadoop.fs.FileSystem$Statistics:getBytesRead(),4308,4321,"/**
* Aggregates bytes read from statistics.
* @return total bytes read as long
*/","* Get the total number of bytes read.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesWritten,org.apache.hadoop.fs.FileSystem$Statistics:getBytesWritten(),4327,4340,"/**
 * Aggregates total bytes written using a StatisticsAggregator.
 * @return Total bytes written as a long value
 */","* Get the total number of bytes written.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getReadOps,org.apache.hadoop.fs.FileSystem$Statistics:getReadOps(),4346,4360,"/**
* Aggregates read operations from statistics.
* @return Total number of read operations
*/","* Get the number of file system read operations such as list files.
     * @return number of read operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLargeReadOps,org.apache.hadoop.fs.FileSystem$Statistics:getLargeReadOps(),4367,4380,"/**
* Aggregates large read operations.
* @return Total count of large read operations
*/","* Get the number of large file system read operations such as list files
     * under a large directory.
     * @return number of large read operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getWriteOps,org.apache.hadoop.fs.FileSystem$Statistics:getWriteOps(),4387,4400,"/**
* Aggregates write operations from statistics.
* @return Total number of write operations
*/","* Get the number of file system write operations such as create, append
     * rename etc.
     * @return number of write operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getRemoteReadTime,org.apache.hadoop.fs.FileSystem$Statistics:getRemoteReadTime(),4436,4449,"/**
* Aggregates remote read times from statistics.
* @return Total remote read time in milliseconds
*/","* Get total time taken in ms for bytes read from remote.
     * @return time taken in ms for remote bytes read.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getData,org.apache.hadoop.fs.FileSystem$Statistics:getData(),4456,4469,"/**
* Aggregates statistics data.
* @return Aggregated StatisticsData object
*/","* Get all statistics data.
     * MR or other frameworks can use the method to get all statistics at once.
     * @return the StatisticsData",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesReadErasureCoded,org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadErasureCoded(),4475,4488,"/**
* Aggregates erasure-coded bytes read.
* @return Total aggregated bytes read
*/","* Get the total number of bytes read on erasure-coded files.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,toString,org.apache.hadoop.fs.FileSystem$Statistics:toString(),4490,4504,"/**
* Aggregates statistics and returns result.
* Uses StatisticsAggregator to process data.
* @return aggregated statistics string
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,reset,org.apache.hadoop.fs.FileSystem$Statistics:reset(),4524,4539,"/**
* Aggregates statistics and updates root data.
*/","* Resets all statistics to 0.
     *
     * In order to reset, we add up all the thread-local statistics data, and
     * set rootData to the negative of that.
     *
     * This may seem like a counterintuitive way to reset the statistics.  Why
     * can't we just zero out all the thread-local data?  Well, thread-local
     * data can only be modified by the thread that owns it.  If we tried to
     * modify the thread-local data from this thread, our modification might get
     * interleaved with a read-modify-write operation done by the thread that
     * owns the data.  That would result in our update getting lost.
     *
     * The approach used here avoids this problem because it only ever reads
     * (not writes) the thread-local data.  Both reads and writes to rootData
     * are done under the lock, so we're free to modify rootData from any thread
     * that holds the lock.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,toString,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:toString(),113,116,"/**
 * Generates a masked string combining MD5 and another value.
 * @return Concatenated string of m1() and md5 values
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validate,"org.apache.hadoop.fs.CreateFlag:validate(java.lang.Object,boolean,java.util.EnumSet)",172,187,"/**
 * Handles file operations based on flags.
 * @param path the file path object
 * @param pathExists whether the file already exists
 * @param flag set of operation flags (CREATE, APPEND, OVERWRITE)
 * @throws IOException if file handling fails due to non-existent file or invalid flags
 */","* Validate the CreateFlag for create operation
   * @param path Object representing the path; usually String or {@link Path}
   * @param pathExists pass true if the path exists in the file system
   * @param flag set of CreateFlag
   * @throws IOException on error
   * @throws HadoopIllegalArgumentException if the CreateFlag is invalid",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validateForAppend,org.apache.hadoop.fs.CreateFlag:validateForAppend(java.util.EnumSet),195,201,"/**
* Masks flags and validates APPEND presence.
* @param flag set of creation flags
*/","* Validate the CreateFlag for the append operation. The flag must contain
   * APPEND, and cannot contain OVERWRITE.
   *
   * @param flag enum set flag.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getUri,"org.apache.hadoop.fs.AbstractFileSystem:getUri(java.net.URI,java.lang.String,boolean,int)",316,341,"/**
 * Masks a URI with a supported scheme and adjusts the port.
 * @param uri original URI to mask
 * @param supportedScheme target URI scheme
 * @param authorityNeeded flag indicating if authority is required
 * @param defaultPort default port to use if not specified in URI
 * @return masked URI with adjusted scheme and port
 * @throws URISyntaxException if URI construction fails
 */","* Get the URI for the file system based on the given URI. The path, query
   * part of the given URI is stripped out and default file system port is used
   * to form the URI.
   * 
   * @param uri FileSystem URI.
   * @param authorityNeeded if true authority cannot be null in the URI. If
   *          false authority must be null.
   * @param defaultPort default port to use if port is not specified in the URI.
   * 
   * @return URI of the file system
   * 
   * @throws URISyntaxException <code>uri</code> has syntax error",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecodeImpl,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",85,96,"/**
 * Masks inputs to generate error signatures and corrected outputs.
 * @param inputs array of ByteBuffers containing input data
 * @param erasedIndexes indices of erased blocks in the input
 * @param outputs array of ByteBuffers for storing corrected output data
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,byte[][],byte[][])",36,50,"/**
* Initializes encoding state for byte arrays.
* @param encoder raw erasure encoder instance
* @param inputs array of input data shards
* @param outputs array of output data shards
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,java.nio.ByteBuffer[],java.nio.ByteBuffer[])",35,47,"/**
* Initializes encoding state with given parameters.
* @param encoder raw erasure encoder instance
* @param inputs array of input ByteBuffers
* @param outputs array of output ByteBuffers
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Class),113,116,"/**
* Initializes an ArrayPrimitiveWritable with a specified component type.
* @param componentType the primitive class type of array elements
*/","* Construct an instance of known type but no value yet
   * for use with type-specific wrapper classes.
   *
   * @param componentType componentType.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,set,org.apache.hadoop.io.ArrayPrimitiveWritable:set(java.lang.Object),142,150,"/**
* Masks and processes the input value.
* @param value the object to be processed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/DefaultFailoverProxyProvider.java,close,org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:close(),55,58,"/**
 * Calls remote procedure to mask data.
 * @throws IOException if communication fails
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:close(),49,52,"/**
 * Invokes remote procedure call m1 through rpcProxy.
 * @throws IOException if communication fails
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:close(),54,57,"/**
 * Executes an RPC function.
 * @throws IOException if an I/O error occurs during execution
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:close(),52,55,"/**
 * Invokes remote procedure call m1.
 * @throws IOException if communication fails
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:close(),49,52,"/**
 * Executes RPC call m1 using rpcProxy.
 * @throws IOException if an I/O error occurs during the RPC call
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:close(),47,50,"/**
 * Executes RPC function m1 using rpcProxy.
 * @throws IOException if an I/O error occurs during execution
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:close(),72,75,"/**
 * Invokes remote procedure call through proxy.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:close(),165,168,"/**
 * Invokes remote procedure call m1 through rpcProxy.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,getPermFromString,org.apache.hadoop.util.ZKUtil:getPermFromString(java.lang.String),44,71,"/**
* Converts permission string to bitmask.
* @param permString permission string with characters 'r', 'w', 'c', 'd', 'a'
* @return integer bitmask representing permissions or throws exception for invalid input
*/","* Parse ACL permission string, partially borrowed from
   * ZooKeeperMain private method",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,processChecksumOpt,"org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt)",339,342,"/**
 * Calls overloaded method with default timeout.
 * @param defaultOpt default checksum option
 * @param userOpt user-specified checksum option
 * @return result of the checksum operation
 */","* A helper method for processing user input and default value to 
     * create a combined checksum option. 
     *
     * @param defaultOpt Default checksum option
     * @param userOpt User-specified checksum option
     *
     * @return ChecksumOpt.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getUriDefaultPort,org.apache.hadoop.fs.DelegateToFileSystem:getUriDefaultPort(),174,177,"/**
 * Returns mask value using function implementation.
 * @return integer mask value
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,canonicalizeUri,org.apache.hadoop.fs.HarFileSystem:canonicalizeUri(java.net.URI),323,326,"/**
 * Delegates URI processing to another method.
 * @param uri the URI to process
 * @return the processed URI
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getCanonicalUri,org.apache.hadoop.fs.FileSystem:getCanonicalUri(),383,385,"/**
 * Generates a masked URI.
 * @return Masked URI
 */","* Return a canonicalized form of this FileSystem's URI.
   *
   * The default implementation simply calls {@link #canonicalizeUri(URI)}
   * on the filesystem's own URI, so subclasses typically only need to
   * implement that method.
   *
   * @see #canonicalizeUri(URI)
   * @return the URI of this filesystem.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,canonicalizeUri,org.apache.hadoop.fs.FilterFileSystem:canonicalizeUri(java.net.URI),118,121,"/**
 * Delegates URI processing to file system.
 * @param uri input URI to process
 * @return processed URI from file system
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,"org.apache.hadoop.fs.ContentSummary:<init>(long,long,long)",162,165,"/**
* Constructs a ContentSummary with specified counts.
* @param length total content length in bytes
* @param fileCount number of files
* @param directoryCount number of directories
*/","*  Constructor, deprecated by ContentSummary.Builder
   *  This constructor implicitly set spaceConsumed the same as length.
   *  spaceConsumed and length must be set explicitly with
   *  ContentSummary.Builder.
   *
   * @param length length.
   * @param fileCount file count.
   * @param directoryCount directory count.
   *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,toString,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:toString(),160,166,"/**
* Generates renewal status message for a token.
* @return Status message indicating token renewal or expiration
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,<init>,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:<init>(org.apache.hadoop.fs.FileSystem),71,75,"/**
* Initializes a renewal action with a weak reference to the filesystem.
* @param fs filesystem object to manage renewals
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getStatus,org.apache.hadoop.fs.HarFileSystem:getStatus(org.apache.hadoop.fs.Path),283,286,"/**
 * Checks file status.
 * @param p file path
 * @return FsStatus object representing file status
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFsStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus(org.apache.hadoop.fs.Path),153,156,"/**
 * Applies a mask to a file.
 * @param f file path to apply mask
 * @return status of the operation
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatus,org.apache.hadoop.fs.FileSystem:getStatus(),3026,3028,"/**
 * Calls m1 with null as the argument.
 * @throws IOException if an I/O error occurs
 */","* Returns a status object describing the use and capacity of the
   * filesystem. If the filesystem has multiple partitions, the
   * use and capacity of the root partition is reflected.
   *
   * @return a FsStatus object
   * @throws IOException
   *           see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getStatus,org.apache.hadoop.fs.FilterFileSystem:getStatus(org.apache.hadoop.fs.Path),329,332,"/**
 * Delegates file status retrieval to underlying filesystem.
 * @param p path to the file or directory
 * @return FsStatus object representing the status of the path
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,toString,org.apache.hadoop.fs.impl.CombinedFileRange:toString(),91,96,"/**
* Appends additional info to base method output.
* @return Enhanced string with range count and data size
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,<init>,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:<init>(org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder),151,180,"/**
* Constructs an HttpReferrerAuditHeader using a builder.
* @param builder configuration for constructing the header
*/","* Instantiate.
   * <p>
   * All maps/enums passed down are copied into thread safe equivalents.
   * as their origin is unknown and cannot be guaranteed to
   * not be shared.
   * <p>
   * Context and operationId are expected to be well formed
   * numeric/hex strings, at least adequate to be
   * used as individual path elements in a URL.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,<init>,"org.apache.hadoop.fs.impl.WeakReferenceThreadMap:<init>(java.util.function.Function,java.util.function.Consumer)",36,39,"/**
 * Constructs a WeakReferenceThreadMap with specified value factory and reference lost handler.
 * @param factory function to create values when keys are accessed
 * @param referenceLost consumer called when a key's reference is lost
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,hasCapability,org.apache.hadoop.fs.FSDataOutputStream:hasCapability(java.lang.String),128,131,"/**
 * Checks if the wrapped stream has the specified capability.
 * @param capability the capability to check
 * @return true if the capability is supported, false otherwise
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hasCapability,org.apache.hadoop.crypto.CryptoOutputStream:hasCapability(java.lang.String),316,319,"/**
 * Checks if a capability is supported.
 * @param capability the capability to check
 * @return true if supported, false otherwise
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,hasCapability,org.apache.hadoop.fs.FSDataInputStream:hasCapability(java.lang.String),242,245,"/**
 * Checks if the store has the specified capability.
 * @param capability the capability to check
 * @return true if the store supports the capability, false otherwise
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getPrefetched,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getPrefetched(int),168,172,"/**
* Creates an operation to get a prefetched block.
* @param blockNumber identifier for the block
* @return Operation object representing the prefetch request
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getCached,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getCached(int),174,178,"/**
* Creates and returns an operation to get cached data.
* @param blockNumber identifier for the block of data
* @return Operation object representing the GET_CACHED action
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getRead,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getRead(int),180,184,"/**
* Creates and returns an operation to get read access.
* @param blockNumber identifier for the block
* @return Operation object configured for GET_READ
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,release,org.apache.hadoop.fs.impl.prefetch.BlockOperations:release(int),186,190,"/**
* Masks a block by creating an operation.
* @param blockNumber identifier of the block to mask
* @return Operation object representing the masking action
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestPrefetch(int),192,196,"/**
* Masks block and creates an operation.
* @param blockNumber identifier of the block to be masked
* @return Operation object representing the prefetch request
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,prefetch,org.apache.hadoop.fs.impl.prefetch.BlockOperations:prefetch(int),198,202,"/**
* Creates an operation to prefetch a specified block.
* @param blockNumber identifier of the block to prefetch
* @return Operation object representing the prefetch action
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,cancelPrefetches,org.apache.hadoop.fs.impl.prefetch.BlockOperations:cancelPrefetches(),204,206,"/**
 * Creates and returns an operation to cancel prefetches.
 * @return Operation object configured to cancel prefetches
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,close,org.apache.hadoop.fs.impl.prefetch.BlockOperations:close(),208,210,"/**
 * Creates and returns an Operation to close with an invalid index.
 * @return Operation object configured to close with an index of -1
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,requestCaching,org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestCaching(int),212,216,"/**
 * Masks operation with caching request.
 * @param blockNumber identifier of the block to cache
 * @return Operation object representing caching request
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,addToCache,org.apache.hadoop.fs.impl.prefetch.BlockOperations:addToCache(int),218,222,"/**
 * Masks block number and creates an operation.
 * @param blockNumber block identifier to be masked
 * @return Operation object representing the cache put action
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,end,org.apache.hadoop.fs.impl.prefetch.BlockOperations:end(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),224,226,"/**
 * Masks an operation by wrapping it in an End operation.
 * @param op the original operation to be masked
 * @return the masked operation using m1
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,fromSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations:fromSummary(java.lang.String),377,424,"/**
* Parses a summary string into BlockOperations.
* @param summary formatted operation summary
* @return BlockOperations object representing parsed operations
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,release,org.apache.hadoop.fs.impl.prefetch.BufferPool:release(org.apache.hadoop.fs.impl.prefetch.BufferData),236,255,"/**
* Processes buffer data, releases resources, and updates pool.
* @param data BufferData object to process
*/","* Releases a previously acquired resource.
   * @param data the {@code BufferData} instance to release.
   * @throws IllegalArgumentException if data is null.
   * @throws IllegalArgumentException if data cannot be released due to its state.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,toString,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:toString(),153,158,"/**
* Generates a formatted string with queue statistics.
* @return Formatted string containing size and counts of created, queued, and available items.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getDurationInfo,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getDurationInfo(java.lang.StringBuilder),252,295,"/**
* Processes operations to summarize durations by kind.
* @param sb StringBuilder to append summary results
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,createCache,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:createCache(int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",554,556,"/**
* Creates a block cache with single file per block.
* @param maxBlocksCount maximum number of blocks to cache
* @param trackerFactory factory for duration tracking
* @return BlockCache instance configured with specified parameters
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,<init>,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean)",90,95,"/**
* Constructs a SemaphoredDelegatingExecutor with specified parameters.
* @param executorDelegatee underlying ExecutorService to delegate tasks
* @param permitCount number of permits for semaphore control
* @param fair true if semaphore should be fair in granting permits
*/","* Instantiate without collecting executor aquisition duration information.
   * @param executorDelegatee Executor to delegate to
   * @param permitCount number of permits into the queue permitted
   * @param fair should the semaphore be ""fair""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getEntry,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getEntry(int),297,307,"/**
* Retrieves an entry by block number.
* @param blockNumber the block identifier
* @return Entry object from the cache
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,releaseReadyBlock,org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseReadyBlock(int),205,224,"/**
* Releases the 'ready' block with the highest priority.
* @param blockNumber identifier for the block to process
*/","* If no blocks were released after calling releaseDoneBlocks() a few times,
   * we may end up waiting forever. To avoid that situation, we try releasing
   * a 'ready' block farthest away from the given block.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,toString,org.apache.hadoop.fs.impl.prefetch.BufferPool:toString(),278,292,"/**
* Builds a formatted string from sorted buffer data.
* @return formatted string of sorted buffer data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,buffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:buffer(),130,133,"/**
 * Applies mask to data and returns processed buffer.
 * @return ByteBuffer with masked data
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,data,org.apache.hadoop.fs.impl.prefetch.FilePosition:data(),135,138,"/**
 * Masks buffer data and returns it.
 * @return masked BufferData object
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,relative,org.apache.hadoop.fs.impl.prefetch.FilePosition:relative(),172,175,"/**
 * Calls m1 and returns result of buffer.m2.
 */","* Gets the current position within this file relative to the start of the associated buffer.
   *
   * @return the current position within this file relative to the start of the associated buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,isWithinCurrentBuffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:isWithinCurrentBuffer(long),183,187,"/**
* Checks if position is within buffer range.
* @param pos position to check
* @return true if position is within buffer, false otherwise
*/","* Determines whether the given absolute position lies within the current buffer.
   *
   * @param pos the position to check.
   * @return true if the given absolute position lies within the current buffer, false otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,bufferStartOffset,org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferStartOffset(),231,234,"/**
 * Masks and returns the buffer start offset.
 * Calls m1() before returning the value.
 * @return masked buffer start offset as a long
 */","* Gets the start of the current block's absolute offset.
   *
   * @return the start of the current block's absolute offset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,getCurrentIOStatisticsContext,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:getCurrentIOStatisticsContext(),122,126,"/**
* Returns active I/O statistics context if enabled, otherwise returns empty context.
*/","* Get the current thread's IOStatisticsContext instance. If no instance is
   * present for this thread ID, create one using the factory.
   * @return instance of IOStatisticsContext.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,setThreadIOStatisticsContext,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext),133,145,"/**
* Masks IO statistics context.
* @param statisticsContext the context to be masked or null for default behavior
*/","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,mergeSortedRanges,"org.apache.hadoop.fs.VectoredReadUtils:mergeSortedRanges(java.util.List,int,int,int)",380,398,"/**
 * Combines file ranges into chunks.
 * @param sortedRanges list of sorted FileRange objects
 * @param chunkSize size of each chunk
 * @param minimumSeek minimum seek distance
 * @param maxSize maximum size for combined range
 * @return list of CombinedFileRange objects
 */","* Merge sorted ranges to optimize the access from the underlying file
   * system.
   * The motivations are that:
   * <ul>
   *   <li>Upper layers want to pass down logical file ranges.</li>
   *   <li>Fewer reads have better performance.</li>
   *   <li>Applications want callbacks as ranges are read.</li>
   *   <li>Some file systems want to round ranges to be at checksum boundaries.</li>
   * </ul>
   *
   * @param sortedRanges already sorted list of ranges based on offset.
   * @param chunkSize round the start and end points to multiples of chunkSize
   * @param minimumSeek the smallest gap that we should seek over in bytes
   * @param maxSize the largest combined file range in bytes
   * @return the list of sorted CombinedFileRanges that cover the input",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,findChecksumRanges,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:findChecksumRanges(java.util.List,int,int,int)",344,362,"/**
 * Combines file ranges with CRC offsets.
 * @param dataRanges list of file ranges to process
 * @param bytesPerSum number of bytes per sum calculation
 * @param minSeek minimum seek distance for combining
 * @param maxSize maximum size for combined range
 * @return list of combined file ranges
 */","* Find the checksum ranges that correspond to the given data ranges.
     * @param dataRanges the input data ranges, which are assumed to be sorted
     *                   and non-overlapping
     * @return a list of AsyncReaderUtils.CombinedFileRange that correspond to
     *         the checksum ranges",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name:<init>(),48,50,"/**
 * Constructs a new Name instance with default visibility.
 */",Creates a case sensitive name expression.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,subset,org.apache.hadoop.metrics2.impl.MetricsConfig:subset(java.lang.String),144,147,"/**
 * Creates a new MetricsConfig with a specified prefix.
 * @param prefix configuration prefix string
 * @return new MetricsConfig instance
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/And.java,apply,"org.apache.hadoop.fs.shell.find.And:apply(org.apache.hadoop.fs.shell.PathData,int)",57,68,"/**
* Evaluates expressions recursively.
* @param item PathData to evaluate
* @param depth current recursion depth
* @return Result of evaluation
*/","* Applies child expressions to the {@link PathData} item. If all pass then
   * returns {@link Result#PASS} else returns the result of the first
   * non-passing expression.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getOptions,org.apache.hadoop.fs.shell.find.Find:getOptions(),235,241,"/**
* Returns FindOptions with mask applied.
* @return FindOptions object
*/","Returns the current find options, creating them if necessary.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,parse,org.apache.hadoop.fs.shell.CommandFormat:parse(java.util.List),99,140,"/**
* Parses command-line arguments.
* @param args list of command-line arguments
*/","Parse parameters from the given list of args.  The list is
   *  destructively modified to remove the options.
   * 
   * @param args as a list of input arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getDescription,org.apache.hadoop.fs.shell.Command:getDescription(),547,551,"/**
* Returns masked function description.
* @return Deprecated message or actual command description
*/","* The long usage suitable for help output
   * @return text of the usage",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayWarning,org.apache.hadoop.fs.shell.Command:displayWarning(java.lang.String),510,512,"/**
 * Logs an error message with a prefix.
 * @param message the error message to log
 */","* Display an warning string prefaced with the command name.
   * @param message warning message to display",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getUsage,org.apache.hadoop.fs.shell.Command:getUsage(),537,541,"/**
* Generates a command string based on conditions.
* @return formatted command string
*/","* The short usage suitable for the synopsis
   * @return ""name options""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,long,boolean)",71,88,"/**
 * Initializes a MetricsSourceAdapter with specified parameters.
 * @param prefix metric name prefix
 * @param name adapter name
 * @param description adapter description
 * @param source metrics data source
 * @param injectedTags additional tags to inject
 * @param recordFilter filter for records
 * @param metricFilter filter for metrics
 * @param jmxCacheTTL JMX cache time-to-live in milliseconds
 * @param startMBeans flag to start MBeans
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,addRecord,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(org.apache.hadoop.metrics2.MetricsInfo),42,50,"/**
 * Creates a metrics record builder with filtering.
 * @param info metrics information
 * @return MetricsRecordBuilderImpl instance
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,<init>,org.apache.hadoop.util.ChunkedArrayList:<init>(),94,96,"/**
 * Constructs a new ChunkedArrayList with default initial chunk capacity and max chunk size.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ScopedAclEntries.java,<init>,org.apache.hadoop.fs.permission.ScopedAclEntries:<init>(java.util.List),47,57,"/**
 * Initializes ScopedAclEntries with given ACL entries.
 * @param aclEntries list of Access Control List entries
 */","* Creates a new ScopedAclEntries from the given list.  It is assumed that the
   * list is already sorted such that all access entries precede all default
   * entries.
   *
   * @param aclEntries List&lt;AclEntry&gt; to separate",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,printToStream,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:printToStream(java.io.PrintStream),312,334,"/**
* Formats and prints table data.
* @param out PrintStream to output formatted data
*/","* Render the table to a stream.
     * @param out PrintStream for output",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,moved,org.apache.hadoop.fs.Options$HandleOpt:moved(boolean),432,434,"/**
* Creates a new Location with specified mask setting.
* @param allow determines mask allowance
* @return Location object configured with mask setting
*/","* @param allow If true, resolve references to this entity anywhere in
     *              the namespace.
     * @return Handle option encoding parameter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,changed,org.apache.hadoop.fs.Options$HandleOpt:changed(boolean),423,425,"/**
 * Creates a Data object with mask status.
 * @param allow indicates whether masking is allowed
 * @return Data object initialized with mask status
 */","* @param allow If true, resolve references to this entity even if it has
     *             been modified.
     * @return Handle option encoding parameter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,<init>,"org.apache.hadoop.fs.DF:<init>(java.io.File,long)",54,59,"/**
* Initializes a DataFetcher with a directory path and fetch interval.
* @param path the directory to monitor
* @param dfInterval the time interval for data fetching
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell:<init>(),901,903,"/**
 * Constructs a new Shell instance with default ID.
 */","* Create an instance with no minimum interval between runs; stderr is
   * not merged with stdout.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,<init>,"org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:<init>(org.apache.hadoop.fs.CachingGetSpaceUsed,boolean)",204,207,"/**
* Initializes a thread to refresh space usage.
* @param spaceUsed instance for getting space used
* @param runImmediately if true, starts the thread immediately
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,"org.apache.hadoop.security.token.Token$PrivateToken:<init>(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",255,263,"/**
* Creates a private token from a public token and service.
* @param publicToken the original public token to clone from
* @param newService the new service for the private token
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,generateDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateDelegationToken(org.apache.hadoop.security.token.Token),1144,1153,"/**
 * Masks a delegation token.
 * @param dToken original token to be masked
 * @return masked DelegationTokenAuthenticatedURL.Token object
 */","* Generate a DelegationTokenAuthenticatedURL.Token from the given generic
   * typed delegation token.
   *
   * @param dToken The delegation token.
   * @return The DelegationTokenAuthenticatedURL.Token, with its delegation
   *         token set to the delegation token passed in.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:listLocatedStatus(org.apache.hadoop.fs.Path),847,852,"/**
 * Retrieves file status iterator for a given path.
 * @param f file path to search
 * @return iterator of LocatedFileStatus objects
 * @throws FileNotFoundException if the file is not found
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),284,288,"/**
 * Retrieves file status iterator for a given path.
 * @param f file path to search
 * @return iterator of LocatedFileStatus objects
 */",List files and its block locations in a directory.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setVerifyChecksum,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setVerifyChecksum(boolean),1368,1372,"/**
 * Throws an access control exception when attempting to set checksum verification.
 * @param verifyChecksum flag indicating whether to verify checksums
 * @throws AccessControlException always thrown with a specific message
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileChecksum,org.apache.hadoop.fs.DelegateToFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),124,128,"/**
 * Computes checksum for a file.
 * @param f file path
 * @return FileChecksum object
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileChecksum,org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),502,505,"/**
 * Computes checksum of a file.
 * @param f file path
 * @return FileChecksum object
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processPath,org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData),179,186,"/**
* Masks file or directory in PathData.
* @param item PathData object containing file path and metadata
* @throws IOException if an I/O error occurs during masking
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setXAttr,"org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",623,627,"/**
* Writes data to a file system path.
* @param path file system path
* @param name file name
* @param value data to write
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.DelegateToFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",282,285,"/**
 * Opens a file for reading.
 * @param path the file path
 * @param parameters additional open parameters
 * @return CompletableFuture of FSDataInputStream
 * @throws IOException if an I/O error occurs
 */","* Open a file by delegating to
   * {@link FileSystem#openFileWithOptions(Path, org.apache.hadoop.fs.impl.OpenFileParameters)}.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   *
   * @return a future which will evaluate to the opened file.ControlAlpha
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",721,726,"/**
* Opens a file for reading.
* @param path path to the file
* @param parameters file opening parameters
* @return CompletableFuture of FSDataInputStream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",444,449,"/**
 * Opens a file for reading asynchronously.
 * @param path the path to the file
 * @param parameters additional open options
 * @return CompletableFuture containing FSDataInputStream or exception
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)",728,733,"/**
* Opens file stream for given path handle.
* @param pathHandle file path to open
* @param parameters file opening parameters
* @return CompletableFuture of FSDataInputStream or exception if fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",249,252,"/**
 * Constructs an INodeDirLink with a specified path and user info.
 * @param pathToNode the path to the node directory
 * @param aUgi the UserGroupInformation for authentication
 * @param link the INodeLink object associated with this directory
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addDir,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addDir(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",211,220,"/**
* Creates a new directory.
* @param pathComponent name of the new directory
* @param aUgi user group information
* @throws FileAlreadyExistsException if directory already exists
* @return newly created INodeDir object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getChildFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:getChildFileSystems(),1035,1058,"/**
* Retrieves all child FileSystems.
* @return Array of FileSystem objects
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getFallbackFileSystem,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getFallbackFileSystem(),400,411,"/**
* Retrieves a FileSystem instance.
* @return FileSystem object or null if unavailable
*/","* @return Gets the fallback file system configured. Usually, this will be the
   * default cluster.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,addCall,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:addCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall),118,124,"/**
* Adds an asynchronous call to the processing queue.
* @param call the AsyncCall object to be processed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,update,"org.apache.hadoop.crypto.OpensslCipher:update(java.nio.ByteBuffer,java.nio.ByteBuffer)",231,241,"/**
 * Processes data from input buffer to output buffer.
 * @param input source ByteBuffer containing data
 * @param output destination ByteBuffer for processed data
 * @return length of processed data in bytes
 * @throws ShortBufferException if not enough space in output buffer
 */","* Continues a multiple-part encryption or decryption operation. The data
   * is encrypted or decrypted, depending on how this cipher was initialized.
   * <p>
   * 
   * All <code>input.remaining()</code> bytes starting at 
   * <code>input.position()</code> are processed. The result is stored in
   * the output buffer.
   * <p>
   * 
   * Upon return, the input buffer's position will be equal to its limit;
   * its limit will not have changed. The output buffer's position will have
   * advanced by n, when n is the value returned by this method; the output
   * buffer's limit will not have changed.
   * <p>
   * 
   * If <code>output.remaining()</code> bytes are insufficient to hold the
   * result, a <code>ShortBufferException</code> is thrown.
   * 
   * @param input the input ByteBuffer
   * @param output the output ByteBuffer
   * @return int number of bytes stored in <code>output</code>
   * @throws ShortBufferException if there is insufficient space in the
   * output buffer",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,doFinal,org.apache.hadoop.crypto.OpensslCipher:doFinal(java.nio.ByteBuffer),270,277,"/**
* Encrypts data into the provided ByteBuffer.
* @param output ByteBuffer to store encrypted data
* @return Length of encrypted data written to buffer
* @throws ShortBufferException if output buffer is too small
* @throws IllegalBlockSizeException for invalid block size
* @throws BadPaddingException for padding issues
*/","* Finishes a multiple-part operation. The data is encrypted or decrypted,
   * depending on how this cipher was initialized.
   * <p>
   * The result is stored in the output buffer. Upon return, the output buffer's
   * position will have advanced by n, where n is the value returned by this
   * method; the output buffer's limit will not have changed.
   * </p>
   * If <code>output.remaining()</code> bytes are insufficient to hold the result,
   * a <code>ShortBufferException</code> is thrown.
   * <p>
   * Upon finishing, this method resets this cipher object to the state it was
   * in when previously initialized. That is, the object is available to encrypt
   * or decrypt more data.
   * </p>
   * If any exception is thrown, this cipher object need to be reset before it
   * can be used again.
   *
   * @param output the output ByteBuffer
   * @return int number of bytes stored in <code>output</code>
   * @throws ShortBufferException      if there is insufficient space in the output buffer.
   * @throws IllegalBlockSizeException This exception is thrown when the length
   *                                   of data provided to a block cipher is incorrect.
   * @throws BadPaddingException       This exception is thrown when a particular
   *                                   padding mechanism is expected for the input
   *                                   data but the data is not padded properly.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointInterceptorFactory.java,create,org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorFactory:create(java.lang.String),41,66,"/**
* Parses interceptor settings string and returns a RegexMountPointInterceptor.
* @param interceptorSettingsString configuration string for the interceptor
* @return RegexMountPointInterceptor object or null if parsing fails
*/","* interceptorSettingsString string should be like ${type}:${string},
   * e.g. replaceresolveddstpath:word1,word2.
   *
   * @param interceptorSettingsString
   * @return Return interceptor based on setting or null on bad/unknown config.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,toString,org.apache.hadoop.fs.DF:toString(),129,139,"/**
 * Generates a formatted disk usage string.
 * @return String containing disk usage details
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,normalizePath,"org.apache.hadoop.fs.Path:normalizePath(java.lang.String,java.lang.String)",297,318,"/**
* Masks the given URI path.
* @param scheme URI scheme
* @param path original URI path
* @return masked URI path
*/","* Normalize a path string to use non-duplicated forward slashes as
   * the path separator and remove any trailing path separators.
   *
   * @param scheme the URI scheme. Used to deduce whether we
   * should replace backslashes or not
   * @param path the scheme-specific part
   * @return the normalized path string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isWindowsAbsolutePath,"org.apache.hadoop.fs.Path:isWindowsAbsolutePath(java.lang.String,boolean)",341,348,"/**
* Checks if a path string has a valid separator at the specified position.
* @param pathString the input path string to check
* @param slashed flag indicating if the path should end with a slash
* @return true if the path has a valid separator, false otherwise
*/","* Determine whether a given path string represents an absolute path on
   * Windows. e.g. ""C:/a/b"" is an absolute path. ""C:a/b"" is not.
   *
   * @param pathString the path string to evaluate
   * @param slashed true if the given path is prefixed with ""/""
   * @return true if the supplied path looks like an absolute path with a Windows
   * drive-specifier",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isUriPathAbsolute,org.apache.hadoop.fs.Path:isUriPathAbsolute(),388,391,"/**
* Checks if URI contains separator after mask.
* @return true if separator exists, false otherwise
*/","* Returns true if the path component (i.e. directory) of this URI is
   * absolute.
   *
   * @return whether this URI's path is absolute",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getHarHash,org.apache.hadoop.fs.HarFileSystem:getHarHash(org.apache.hadoop.fs.Path),488,490,"/**
 * Applies mask to extract lower 31 bits from Path.
 * @param p input Path object
 * @return masked integer value
 */","* the hash of the path p inside  the filesystem
   * @param p the path in the harfilesystem
   * @return the hash code of the path.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,build,org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:build(),4694,4714,"/**
* Opens a file for writing with specified flags.
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FileStatus),108,135,"/**
 * Converts a FileStatus object to a FileStatusProto.
 * @param stat the source FileStatus object
 * @return the converted FileStatusProto object
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,<init>,"org.apache.hadoop.fs.sftp.SFTPInputStream:<init>(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem$Statistics)",46,58,"/**
 * Initializes an SFTP input stream for a given path.
 * @param channel the SFTP channel to use
 * @param path the remote file path
 * @param stats file system statistics
 * @throws IOException if there's an error opening the stream
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPath,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPath(org.apache.hadoop.fs.Path),69,73,"/**
* Validates and masks a file path.
* @param path the Path to validate
*/","* Validate a path.
   * @param path path to check.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,getExecString,org.apache.hadoop.fs.Stat:getExecString(),91,108,"/**
 * Returns command to get file stats with dereferencing option.
 * @return String array of command and arguments for stat
 * @throws UnsupportedOperationException if platform is unsupported
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)",103,106,"/**
 * Generates an AccessControlException for a file operation.
 * @param operation type of file operation (e.g., read, write)
 * @param p path to the file
 * @return AccessControlException with details
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)",181,184,"/**
 * Delegates access control check to another method.
 * @param operation type of operation being checked
 * @param p path associated with the operation
 * @return AccessControlException if access is denied
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,getPathToResolve,"org.apache.hadoop.fs.viewfs.RegexMountPoint:getPathToResolve(java.lang.String,boolean)",217,227,"/**
 * Masks the last component of a path.
 * @param srcPath source file path
 * @param resolveLastComponent flag to determine if last component should be resolved
 * @return masked path or null if no valid path found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,checkDependencies,"org.apache.hadoop.fs.FileContext:checkDependencies(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2284,2298,"/**
 * Checks and throws exceptions for invalid source and destination paths.
 * @param qualSrc source path
 * @param qualDst destination path
 * @throws IOException if paths are invalid or src is a parent of dst
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compareTo,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:compareTo(java.lang.Object),3794,3805,"/**
* Compares two SegmentDescriptors based on segment length, offset, and path name.
* @param o the other SegmentDescriptor to compare with
* @return negative if less, positive if greater, zero if equal
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:equals(java.lang.Object),3807,3820,"/**
 * Compares two SegmentDescriptors for equality.
 * @param o the object to compare with
 * @return true if equal, false otherwise
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getNextIdToTry,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNextIdToTry(org.apache.hadoop.fs.Path,int)",731,753,"/**
 * Finds the next available ID by scanning directory files.
 * @param initial path to start scanning from
 * @param lastId last known ID
 * @return next available ID
 * @throws IOException if file system operations fail
 */","* Return the next ID suffix to use when creating the log file. This method
   * will look at the files in the directory, find the one with the highest
   * ID suffix, and 1 to that suffix, and return it. This approach saves a full
   * linear probe, which matters in the case where there are a large number of
   * log files.
   *
   * @param initial the base file path
   * @param lastId the last ID value that was used
   * @return the next ID to try
   * @throws IOException thrown if there's an issue querying the files in the
   * directory",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getPathAsString,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPathAsString(),144,146,"/**
 * Returns masked function name.
 * @return Masked string representation of the function
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createServiceURL,org.apache.hadoop.crypto.key.kms.KMSClientProvider:createServiceURL(org.apache.hadoop.fs.Path),447,453,"/**
 * Constructs a masked URL from the given path.
 * @param path input file path
 * @return modified URL with service version appended
 * @throws IOException if URL creation fails
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,seek,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seek(long),313,319,"/**
 * Moves file pointer to specified position.
 * @param pos target position in the file
 * @throws IOException if position is beyond EOF or other I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,skip,org.apache.hadoop.fs.FSInputChecker:skip(long),405,413,"/**
 * Applies mask function to input value.
 * @param n input number to be masked
 * @return masked value of n, or 0 if n is non-positive
 * @throws IOException if an I/O error occurs during processing
 */","* Skips over and discards <code>n</code> bytes of data from the
   * input stream.
   *
   * <p>This method may skip more bytes than are remaining in the backing
   * file. This produces no exception and the number of bytes skipped
   * may include some number of bytes that were beyond the EOF of the
   * backing file. Attempting to read from the stream after skipping past
   * the end will result in -1 indicating the end of the file.
   *
   *<p>If <code>n</code> is negative, no bytes are skipped.
   *
   * @param      n   the number of bytes to be skipped.
   * @return     the actual number of bytes skipped.
   * @exception  IOException  if an I/O error occurs.
   *             ChecksumException if the chunk to skip to is corrupted",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ByteBufferUtil.java,fallbackRead,"org.apache.hadoop.fs.ByteBufferUtil:fallbackRead(java.io.InputStream,org.apache.hadoop.io.ByteBufferPool,int)",57,117,"/**
 * Reads data from an InputStream into a ByteBuffer.
 * @param stream input source to read from
 * @param bufferPool pool for acquiring ByteBuffers
 * @param maxLength maximum length of data to read
 * @return ByteBuffer containing the read data or null if unsuccessful
 * @throws IOException if an I/O error occurs
 */","* Perform a fallback read.
   *
   * @param stream input stream.
   * @param bufferPool bufferPool.
   * @param maxLength maxLength.
   * @throws IOException raised on errors performing I/O.
   * @return byte buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,reset,org.apache.hadoop.fs.audit.CommonAuditContext:reset(),185,188,"/**
 * Calls methods to evaluate and process entries.
 */","* Rest the context; will set the standard options again.
   * Primarily for testing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,createInstance,org.apache.hadoop.fs.audit.CommonAuditContext:createInstance(),212,216,"/**
* Initializes and configures a CommonAuditContext.
* @return configured CommonAuditContext instance
*/","* Demand invoked to create the instance for this thread.
   * @return an instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,toString,org.apache.hadoop.tools.TableListing:toString(),229,292,"/**
 * Formats table data into a wrapped string.
 * @return formatted table as a single string
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,"org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction)",82,84,"/**
* Constructs a FsPermission with specified actions for user, group, others.
* @param u action for user
* @param g action for group
* @param o action for others
*/","* Construct by the given {@link FsAction}.
   * @param u user action
   * @param g group action
   * @param o other action",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(short),95,95,"/**
 * Constructs an FsPermission object from a short mode.
 * @param mode permission bits represented as a short
 */","* Construct by the given mode.
   * @param mode mode.
   * @see #toShort()",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,readFields,org.apache.hadoop.fs.permission.FsPermission:readFields(java.io.DataInput),185,189,"/**
 * Masks data by delegating to another method.
 * @param in DataInput object containing input data
 * @deprecated Use alternative method instead
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,read,org.apache.hadoop.fs.permission.FsPermission:read(java.io.DataInput),214,218,"/**
* Constructs an FsPermission object from DataInput.
* @param in source of permission data
* @return FsPermission object initialized with input data
*/","* Create and initialize a {@link FsPermission} from {@link DataInput}.
   *
   * @param in data input.
   * @throws IOException raised on errors performing I/O.
   * @return FsPermission.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,getEffectivePermission,org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry),230,232,"/**
 * Processes an ACL entry with default permission.
 * @param entry ACL entry to process
 * @return Resulting FsAction
 */","* Get the effective permission for the AclEntry
   * @param entry AclEntry to get the effective action
   * @return FsAction.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,createImmutable,"org.apache.hadoop.fs.permission.PermissionStatus:createImmutable(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",49,57,"/**
* Creates a masked permission status.
* @param user the owner of the file or directory
* @param group the group associated with the file or directory
* @param permission the file system permissions
* @return PermissionStatus object with overridden FUNC_MASK method
*/","* Create an immutable {@link PermissionStatus} object.
   * @param user user.
   * @param group group.
   * @param permission permission.
   * @return PermissionStatus.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,parseAclSpec,"org.apache.hadoop.fs.permission.AclEntry:parseAclSpec(java.lang.String,boolean)",235,245,"/**
 * Parses ACL specification and creates entries.
 * @param aclSpec comma-separated ACL specification string
 * @param includePermission flag to include permissions in entries
 * @return list of AclEntry objects
 */","* Parses a string representation of an ACL spec into a list of AclEntry
   * objects. Example: ""user::rwx,user:foo:rw-,group::r--,other::---""
   * The expected format of ACL entries in the string parameter is the same
   * format produced by the {@link #toStringStable()} method.
   * 
   * @param aclSpec
   *          String representation of an ACL spec.
   * @param includePermission
   *          for setAcl operations this will be true. i.e. AclSpec should
   *          include permissions.<br>
   *          But for removeAcl operation it will be false. i.e. AclSpec should
   *          not contain permissions.<br>
   *          Example: ""user:foo,group:bar""
   * @return Returns list of {@link AclEntry} parsed",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,create,"org.apache.hadoop.fs.permission.FsCreateModes:create(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",58,63,"/**
 * Creates FsCreateModes with masked and unmasked permissions.
 * @param masked permission object to be masked
 * @param unmasked original permission object
 * @return FsCreateModes instance
 */","* Create from masked and unmasked modes.
   *
   * @param masked masked.
   * @param unmasked unmasked.
   * @return FsCreateModes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,printExtendedAclEntry,"org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printExtendedAclEntry(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.AclEntry)",137,152,"/**
 * Masks ACL entry based on permission and type.
 * @param aclStatus current ACL status
 * @param fsPerm file system permissions
 * @param entry ACL entry to process
 */","* Prints a single extended ACL entry.  If the mask restricts the
     * permissions of the entry, then also prints the restricted version as the
     * effective permissions.  The mask applies to all named entries and also
     * the unnamed group entry.
     * @param aclStatus AclStatus for the path
     * @param fsPerm FsPermission for the path
     * @param entry AclEntry extended ACL entry to print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,toString,org.apache.hadoop.fs.permission.AclEntry:toString(),102,108,"/**
* Returns masked function name.
* @return Masked string representation of the function
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStrings,org.apache.hadoop.util.StringUtils:getStrings(java.lang.String),407,410,"/**
 * Splits input string by commas.
 * @param str input string to split
 * @return array of substrings separated by commas
 */","* Returns an arraylist of strings.
   * @param str the comma separated string values
   * @return the arraylist of the comma separated string values",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/UmaskParser.java,<init>,org.apache.hadoop.fs.permission.UmaskParser:<init>(java.lang.String),41,45,"/**
* Parses umask from string.
* @param modeStr umask string representation
* @throws IllegalArgumentException if input is invalid
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/RawParser.java,<init>,org.apache.hadoop.fs.permission.RawParser:<init>(java.lang.String),35,38,"/**
* Initializes parser with specified mode.
* @param modeStr parsing mode string
* @throws IllegalArgumentException if invalid mode
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ChmodParser.java,<init>,org.apache.hadoop.fs.permission.ChmodParser:<init>(java.lang.String),38,40,"/**
 * Initializes a new ChmodParser with a given mode string.
 * @param modeStr the permission mode as a string
 * @throws IllegalArgumentException if modeStr is invalid
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,create,"org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",530,535,"/**
* Creates a data block with specified limit.
* @param index starting index (unused)
* @param limit size of the data block
* @param statistics upload statistics object
* @return DataBlock initialized with given limit
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,available,org.apache.hadoop.fs.store.ByteBufferInputStream:available(),116,120,"/**
 * Calls m1 and returns result of byteBuffer.m2.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,position,org.apache.hadoop.fs.store.ByteBufferInputStream:position(),126,129,"/**
 * Calls m1 and returns result of byteBuffer.m2().
 */","* Get the current buffer position.
   * @return the buffer position",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,hasRemaining,org.apache.hadoop.fs.store.ByteBufferInputStream:hasRemaining(),135,138,"/**
* Calls m1 and checks buffer status.
* @return true if buffer operation successful, false otherwise
*/","* Check if there is data left.
   * @return true if there is data remaining in the buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,reset,org.apache.hadoop.fs.store.ByteBufferInputStream:reset(),147,152,"/**
* Resets and processes data.
* Logs reset action, calls processing methods.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$DataBlock:startUpload(),454,458,"/**
* Initiates block upload process.
* @throws IOException if an I/O error occurs during upload
*/","* Switch to the upload state and return a stream for uploading.
     * Base class calls {@link #enterState(DestState, DestState)} to
     * manage the state machine.
     *
     * @return the stream.
     * @throws IOException trouble",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,enterClosedState,org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterClosedState(),466,473,"/**
* Checks and sets state to closed.
* @return true if state was changed, false otherwise
*/","* Enter the closed state.
     *
     * @return true if the class was in any other state, implying that
     * the subclass should do its close operations.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$DiskBlock:write(byte[],int,int)",878,885,"/**
* Writes data to output stream.
* @param b byte array containing data
* @param offset starting index in the byte array
* @param len number of bytes to write
* @return number of bytes actually written
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:write(byte[],int,int)",747,753,"/**
* Writes bytes to a buffer.
* @param b byte array containing data
* @param offset starting index in the byte array
* @param len number of bytes to write
* @return number of bytes actually written
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,flush,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:flush(),940,943,"/**
 * Overrides method m1 to extend functionality.
 * Calls superclass and delegate methods.
 */","* Flush operation will flush to disk.
     *
     * @throws IOException IOE raised on FileOutputStream",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:write(byte[],int,int)",612,618,"/**
* Writes data to buffer after processing.
* @param b byte array containing data
* @param offset starting index in the byte array
* @param len number of bytes to write
* @return number of bytes actually written
* @throws IOException if I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toString,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:toString(),767,776,"/**
* Generates a string representation of ByteBufferBlock.
* @return formatted string with block details
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getStatistics,org.apache.hadoop.fs.FileContext:getStatistics(java.net.URI),2396,2398,"/**
 * Retrieves statistics from a given URI.
 * @param uri location of the resource
 * @return Statistics object containing file system details
 */","* Get the statistics for a particular file system
   * 
   * @param uri
   *          the uri to lookup the statistics. Only scheme and authority part
   *          of the uri are used as the key to store and lookup.
   * @return a statistics object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createMultipartUploader,org.apache.hadoop.fs.FilterFs:createMultipartUploader(org.apache.hadoop.fs.Path),457,461,"/**
* Initializes multipart uploader with base path.
* @param basePath root directory for uploads
* @return MultipartUploaderBuilder instance
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getCurrentDirectoryIndex,org.apache.hadoop.fs.LocalDirAllocator:getCurrentDirectoryIndex(),257,260,"/**
* Allocates resources and invokes m2 on the context.
* @return result of context.m2()
*/","* Get the current directory index for the given configuration item.
   * @return the current directory index for the given configuration item.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,getPos,org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:getPos(),49,52,"/**
 * Calls and returns the result of fsOut's m1 method.
 * @return Result from fsOut's m1 method
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$Writer:sync(),1369,1375,"/**
* Writes synchronization data to output stream.
* @throws IOException if an I/O error occurs
*/","* create a sync point.
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getLength,org.apache.hadoop.io.SequenceFile$Writer:getLength(),1530,1532,"/**
 * Returns masked value from output stream.
 * @throws IOException if I/O error occurs
 */","@return Returns the current length of the output file.
     *
     * <p>This always returns a synchronized position.  In other words,
     * immediately after calling {@link SequenceFile.Reader#seek(long)} with a position
     * returned by this method, {@link SequenceFile.Reader#next(Writable)} may be called.  However
     * the key may be earlier in the file than key last written when this
     * method was called (e.g., with block-compression, it may be the first key
     * in the block that was being written when this method was called).</p>
     *
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCurrentPos,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCurrentPos(),156,158,"/**
 * Calculates and returns the masked value from file system outputs.
 * @return long value representing the mask
 * @throws IOException if an I/O error occurs during calculation
 */","* Get the current position in file.
       * 
       * @return The current byte offset in underlying file.
       * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getContentSummary,org.apache.hadoop.fs.FileSystem:getContentSummary(org.apache.hadoop.fs.Path),1923,1945,"/**
 * Computes content summary for a file or directory.
 * @param f path to the file or directory
 * @return ContentSummary object with size, file count, and directory count
 * @throws IOException if an I/O error occurs
 */","Return the {@link ContentSummary} of a given {@link Path}.
   * @param f path to use
   * @throws FileNotFoundException if the path does not resolve
   * @throws IOException IO failure
   * @return content summary.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,buildACL,org.apache.hadoop.security.authorize.AccessControlList:buildACL(java.lang.String[]),107,126,"/**
* Initializes users and groups based on ACL parts.
* @param userGroupStrings array containing user and group strings
*/","* Build ACL from the given array of strings.
   * The strings contain comma separated values.
   *
   * @param userGroupStrings build ACL from array of Strings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfigurationHelper.java,parseEnumSet,"org.apache.hadoop.util.ConfigurationHelper:parseEnumSet(java.lang.String,java.lang.String,java.lang.Class,boolean)",68,99,"/**
* Converts a comma-separated string to an EnumSet.
* @param key configuration key for error messages
* @param valueString comma-separated string of values
* @param enumClass the Enum class to convert to
* @param ignoreUnknown flag to ignore unknown values
* @return EnumSet containing parsed enum values
*/","* Given a comma separated list of enum values,
   * trim the list, map to enum values in the message (case insensitive)
   * and return the set.
   * Special handling of ""*"" meaning: all values.
   * @param key Configuration object key -used in error messages.
   * @param valueString value from Configuration
   * @param enumClass class of enum
   * @param ignoreUnknown should unknown values be ignored?
   * @param <E> enum type
   * @return a mutable set of enum values parsed from the valueString, with any unknown
   * matches stripped if {@code ignoreUnknown} is true.
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   * or there are two entries in the enum which differ only by case.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,ensureCurrentState,org.apache.hadoop.service.ServiceStateModel:ensureCurrentState(org.apache.hadoop.service.Service$STATE),97,104,"/**
* Checks if service is in expected state.
* @param expectedState required state for operation
* @throws ServiceStateException if current state does not match expected
*/","* Verify that that a service is in a given state.
   * @param expectedState the desired state
   * @throws ServiceStateException if the service state is different from
   * the desired state",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,<init>,org.apache.hadoop.service.AbstractService:<init>(java.lang.String),113,116,"/**
 * Initializes a new service with a specified name.
 * @param name the unique identifier for the service
 */","* Construct the service.
   * @param name service name",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,checkStateTransition,"org.apache.hadoop.service.ServiceStateModel:checkStateTransition(java.lang.String,org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)",128,135,"/**
 * Validates state transition for a service.
 * @param name service name
 * @param state current service state
 * @param proposed new proposed state
 * @throws ServiceStateException if transition is invalid
 */","* Check that a state tansition is valid and
   * throw an exception if not
   * @param name name of the service (can be null)
   * @param state current state
   * @param proposed proposed new state",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceCreationFailure,org.apache.hadoop.service.launcher.ServiceLauncher:serviceCreationFailure(java.lang.Exception),745,747,"/**
 * Wraps an exception in a ServiceLaunchException.
 * @param exception the original exception to wrap
 * @return a new ServiceLaunchException with EXIT_SERVICE_CREATION_FAILURE code
 */","* Generate an exception announcing a failure to create the service.
   * @param exception inner exception.
   * @return a new exception, with the exit code
   * {@link LauncherExitCodes#EXIT_SERVICE_CREATION_FAILURE}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,verifyConfigurationFilesExist,org.apache.hadoop.service.launcher.ServiceLauncher:verifyConfigurationFilesExist(java.lang.String[]),989,1003,"/**
* Masks filenames by logging and validating their existence.
* @param filenames array of file paths to be processed
*/","* Verify that all the specified filenames exist.
   * @param filenames a list of files
   * @throws ServiceLaunchException if a file is not found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String,java.lang.Object[])",1088,1092,"/**
* Constructs a KerberosDiagsFailure with formatted message.
* @param category failure category
* @param message format string for the error message
* @param args arguments for the message format
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,convertToExitException,org.apache.hadoop.service.launcher.ServiceLauncher:convertToExitException(java.lang.Throwable),714,737,"/**
* Wraps a Throwable in an ExitException.
* @param thrown the original exception to be wrapped
* @return ExitUtil.ExitException with appropriate exit code and message
*/","* Convert an exception to an {@code ExitException}.
   *
   * This process may just be a simple pass through, otherwise a new
   * exception is created with an exit code, the text of the supplied
   * exception, and the supplied exception as an inner cause.
   * 
   * <ol>
   *   <li>If is already the right type, pass it through.</li>
   *   <li>If it implements {@link ExitCodeProvider#getExitCode()},
   *   the exit code is extracted and used in the new exception.</li>
   *   <li>Otherwise, the exit code
   *   {@link LauncherExitCodes#EXIT_EXCEPTION_THROWN} is used.</li>
   * </ol>
   *  
   * @param thrown the exception thrown
   * @return an {@code ExitException} with a status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,<init>,org.apache.hadoop.service.launcher.ServiceShutdownHook:<init>(org.apache.hadoop.service.Service),52,54,"/**
 * Initializes a shutdown hook with a weak reference to the service.
 * @param service the service to be monitored for shutdown
 */","* Create an instance.
   * @param service the service",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,toString,org.apache.hadoop.service.launcher.InterruptEscalator:toString(),89,101,"/**
* Generates string representation of InterruptEscalator.
* @return formatted string with signal status, owner info, and shutdown details
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,noteFailure,org.apache.hadoop.service.AbstractService:noteFailure(java.lang.Exception),257,272,"/**
* Logs and records the first exception as a service failure.
* @param exception the exception to handle
*/","* Failure handling: record the exception
   * that triggered it -if there was not one already.
   * Services are free to call this themselves.
   * @param exception the exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,recordLifecycleEvent,org.apache.hadoop.service.AbstractService:recordLifecycleEvent(),421,426,"/**
* Records a lifecycle event with current time and state.
*/",* Add a state change event to the lifecycle history,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceInit,org.apache.hadoop.service.CompositeService:serviceInit(org.apache.hadoop.conf.Configuration),104,113,"/**
* Initializes services with configuration.
* @param conf configuration settings
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,stop,"org.apache.hadoop.service.CompositeService:stop(int,boolean)",147,170,"/**
 * Stops services in reverse order.
 * @param numOfServicesStarted number of started services
 * @param stopOnlyStartedServices flag to stop only started services
 */","* Stop the services in reverse order
   *
   * @param numOfServicesStarted index from where the stop should work
   * @param stopOnlyStartedServices flag to say ""only start services that are
   * started, not those that are NOTINITED or INITED.
   * @throws RuntimeException the first exception raised during the
   * stop process -<i>after all services are stopped</i>",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.hadoop.service.Service),65,67,"/**
 * Calls m1 with default LOG and given Service.
 * @param service the Service to process
 * @return an Exception or null if no error occurs
 */","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,progressable,org.apache.hadoop.io.SequenceFile$Writer:progressable(org.apache.hadoop.util.Progressable),1036,1038,"/**
 * Wraps a Progressable in an Option.
 * @param value Progressable object to wrap
 * @return Option containing the Progressable
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,blockSize,org.apache.hadoop.io.SequenceFile$Writer:blockSize(long),1032,1034,"/**
 * Creates an option with a block size.
 * @param value block size value
 * @return Option object representing the block size
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,syncInterval,org.apache.hadoop.io.SequenceFile$Writer:syncInterval(int),1061,1063,"/**
 * Creates an option with a specified interval.
 * @param value the interval value to set
 * @return an Option object representing the interval
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,replication,org.apache.hadoop.io.SequenceFile$Writer:replication(short),1024,1026,"/**
 * Creates a ReplicationOption from a short value.
 * @param value the short value to mask
 * @return a ReplicationOption object
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,bufferSize,org.apache.hadoop.io.SequenceFile$Writer:bufferSize(int),1016,1018,"/**
 * Creates a buffer size option.
 * @param value buffer size in bytes
 * @return Option object representing buffer size
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,write,org.apache.hadoop.io.ObjectWritable$NullInstance:write(java.io.DataOutput),126,129,"/**
 * Writes class name to output stream.
 * @param out DataOutput stream to write to
 * @throws IOException if I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,write,org.apache.hadoop.io.ArrayPrimitiveWritable:write(java.io.DataOutput),171,200,"/**
* Writes array data to a DataOutput stream.
* @param out DataOutput stream to write to
* @throws IOException if an I/O error occurs or unsupported component type
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,valueClass,org.apache.hadoop.io.SequenceFile$Writer:valueClass(java.lang.Class),1044,1046,"/**
 * Creates an option for a class.
 * @param value the class type to be wrapped in an option
 * @return an Option object wrapping the provided class
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,keyClass,org.apache.hadoop.io.MapFile$Writer:keyClass(java.lang.Class),285,287,"/**
 * Creates an option to mask a key class.
 * @param value the key class type
 * @return KeyClassOption object configured with the specified key class
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,keyClass,org.apache.hadoop.io.SequenceFile$Writer:keyClass(java.lang.Class),1040,1042,"/**
 * Creates an option for a key class.
 * @param value Class type to be used as a key
 * @return Option object configured with the given class
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,compareTo,org.apache.hadoop.io.UTF8:compareTo(org.apache.hadoop.io.UTF8),156,160,"/**
* Compares byte arrays using UTF-8 encoding.
* @param o UTF8 object to compare with
* @return comparison result as integer
*/",Compare two UTF8s.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,equals,org.apache.hadoop.io.UTF8:equals(java.lang.Object),193,203,"/**
 * Compares this UTF8 object with another for equality.
 * @param o the object to compare with
 * @return true if equal, false otherwise
 */",Returns true iff <code>o</code> is a UTF8 with the same contents.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,compareTo,org.apache.hadoop.io.MD5Hash:compareTo(org.apache.hadoop.io.MD5Hash),241,245,"/**
* Compares two MD5 hashes.
* @param that another MD5Hash object to compare with
* @return comparison result as an integer
*/",Compares this object with the specified object for order.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,compareTo,org.apache.hadoop.io.BinaryComparable:compareTo(org.apache.hadoop.io.BinaryComparable),50,56,"/**
* Compares this object with another BinaryComparable.
* @param other the other BinaryComparable to compare with
* @return result of comparison: 0 if equal, negative if less, positive if greater
*/","* Compare bytes from {#getBytes()}.
   * @see org.apache.hadoop.io.WritableComparator#compareBytes(byte[],int,int,byte[],int,int)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,compareTo,"org.apache.hadoop.io.BinaryComparable:compareTo(byte[],int,int)",66,69,"/**
* Compares byte arrays using specific comparator methods.
* @param other byte array to compare against
* @param off starting offset in the 'other' array
* @param len length of the segment to compare
* @return result of comparison as integer
*/","* Compare bytes from {#getBytes()} to those provided.
   *
   * @param other other.
   * @param off off.
   * @param len len.
   * @return compareBytes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/CompareUtils.java,compare,"org.apache.hadoop.io.file.tfile.CompareUtils$MemcmpRawComparator:compare(byte[],int,int,byte[],int,int)",89,92,"/**
* Compares two byte arrays using a mask function.
* @param b1 first byte array
* @param s1 start index in the first array
* @param l1 length of bytes to compare in the first array
* @param b2 second byte array
* @param s2 start index in the second array
* @param l2 length of bytes to compare in the second array
* @return result of comparison as an integer
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,hashCode,org.apache.hadoop.io.UTF8:hashCode(),205,208,"/**
 * Computes mask value using bytes and length.
 * @return computed mask as an integer
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,hashCode,org.apache.hadoop.io.BinaryComparable:hashCode(),88,91,"/**
 * Returns mask value using comparator logic.
 * @return integer result of mask calculation
 */","* Return a hash of the bytes returned from {#getBytes()}.
   * @see org.apache.hadoop.io.WritableComparator#hashBytes(byte[],int)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,hashCode,org.apache.hadoop.security.token.Token:hashCode(),402,405,"/**
 * Generates a mask value using identifier.
 * @return integer mask value
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readDouble,"org.apache.hadoop.io.WritableComparator:readDouble(byte[],int)",311,313,"/**
 * Masks and converts byte array to double.
 * @param bytes source byte array
 * @param start starting index in byte array
 * @return masked double value
 */","* Parse a double from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return double from a byte array.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,setSize,org.apache.hadoop.io.BytesWritable:setSize(int),131,138,"/**
* Adjusts array size based on input.
* @param size desired new size for the array
*/","* Change the size of the buffer. The values in the old range are preserved
   * and any new values are undefined. The capacity is changed if it is 
   * necessary.
   * @param size The new number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MergeSort.java,<init>,org.apache.hadoop.util.MergeSort:<init>(java.util.Comparator),38,40,"/**
 * Initializes a new MergeSort instance with a custom comparator.
 * @param comparator Comparator to define sorting order
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canRead,org.apache.hadoop.fs.FileUtil:canRead(java.io.File),1412,1423,"/**
 * Checks file readability.
 * @param f the File object to check
 * @return true if file is readable, false otherwise
 */","* Platform independent implementation for {@link File#canRead()}
   * @param f input file
   * @return On Unix, same as {@link File#canRead()}
   *         On Windows, true if process has read access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canWrite,org.apache.hadoop.fs.FileUtil:canWrite(java.io.File),1431,1442,"/**
* Checks write access for a file.
* @param f the File object to check
* @return true if writable, false otherwise
*/","* Platform independent implementation for {@link File#canWrite()}
   * @param f input file
   * @return On Unix, same as {@link File#canWrite()}
   *         On Windows, true if process has write access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canExecute,org.apache.hadoop.fs.FileUtil:canExecute(java.io.File),1450,1461,"/**
* Checks file execute permission.
* @param f the File object to check
* @return true if file is executable, false otherwise
*/","* Platform independent implementation for {@link File#canExecute()}
   * @param f input file
   * @return On Unix, same as {@link File#canExecute()}
   *         On Windows, true if process has execute access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,assertCodeLoaded,org.apache.hadoop.io.nativeio.NativeIO$POSIX:assertCodeLoaded(),364,368,"/**
* Checks if native IO is loaded.
* Throws IOException if not loaded.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,getInstance,org.apache.hadoop.io.ReadaheadPool:getInstance(),55,62,"/**
* Returns singleton ReadaheadPool instance.
* Initializes if not already created and m1() is true.
* @return ReadaheadPool instance or null if conditions not met
*/",* @return Return the singleton instance for the current process.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,verifyCanMlock,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:verifyCanMlock(),300,302,"/**
 * Calls native I/O function m1.
 * @return result of native function call
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.java,getLoadingFailureReason,org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:getLoadingFailureReason(),53,61,"/**
* Checks for NativeIO availability and UNIX OS.
* @return error message or null if checks pass
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getMemlockLimit,org.apache.hadoop.io.nativeio.NativeIO:getMemlockLimit(),884,886,"/**
 * Returns mask value based on condition.
 * @return long value from m2() if m1() is true, otherwise 0
 */","* Get the maximum number of bytes that can be locked into memory at any
   * given point.
   *
   * @return 0 if no bytes can be locked into memory;
   *         Long.MAX_VALUE if there is no limit;
   *         The number of bytes that can be locked into memory otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,calculateChunkedSums,"org.apache.hadoop.util.DataChecksum:calculateChunkedSums(byte[],int,int,byte[],int)",576,600,"/**
 * Masks data with checksums.
 * @param data input data array
 * @param dataOffset starting offset in data array
 * @param dataLength length of data to process
 * @param sums output array for checksums
 * @param sumsOffset starting offset in sums array
 */","* Implementation of chunked calculation specifically on byte arrays. This
   * is to avoid the copy when dealing with ByteBuffers that have array backing.
   *
   * @param data data.
   * @param dataOffset dataOffset.
   * @param dataLength dataLength.
   * @param sums sums.
   * @param sumsOffset sumsOffset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getCreateForWriteFileOutputStream,"org.apache.hadoop.io.nativeio.NativeIO:getCreateForWriteFileOutputStream(java.io.File,int)",1001,1037,"/**
* Creates a file with specified permissions.
* @param f the File to create
* @param permissions file access permissions
* @return FileOutputStream for the created file
* @throws IOException if file creation fails
*/","* @return Create the specified File for write access, ensuring that it does not exist.
   * @param f the file that we want to create
   * @param permissions we want to have on the file (if security is enabled)
   *
   * @throws AlreadyExistsException if the file already exists
   * @throws IOException if any other error occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,run,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:run(),210,232,"/**
* Performs read-ahead operation for file descriptor.
* @throws IOException if I/O error occurs and not canceled
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,cleanBufferPool,org.apache.hadoop.crypto.CryptoInputStream:cleanBufferPool(),816,821,"/**
* Masks data in buffers from pool.
* Uses CryptoStreamUtils to process each buffer.
*/",Clean direct buffer pool,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,freeBuffers,org.apache.hadoop.crypto.CryptoOutputStream:freeBuffers(),311,314,"/**
 * Applies cryptographic mask to input and output buffers.
 */",Forcibly free the direct buffers.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getFstat,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getFstat(java.io.FileDescriptor),575,596,"/**
 * Retrieves file statistics for a given file descriptor.
 * Handles different OS environments and throws IOException on errors.
 * @param fd FileDescriptor object representing the file
 * @return Stat object containing file statistics or null if not found
 */","* Returns the file stat for a file descriptor.
     *
     * @param fd file descriptor.
     * @return the file descriptor file stat.
     * @throws IOException thrown if there was an IO error while obtaining the file stat.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getStat,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getStat(java.lang.String),606,627,"/**
* Retrieves file statistics for a given path.
* @param path file path to retrieve stats for
* @return Stat object containing file information
* @throws IOException if an I/O error occurs or path is null
*/","* Return the file stat for a file path.
     *
     * @param path  file path
     * @return  the file stat
     * @throws IOException  thrown if there is an IO error while obtaining the
     * file stat",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,"org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int,int)",55,57,"/**
 * Initializes a bounded byte array output stream.
 * @param capacity initial size of the buffer
 * @param limit maximum number of bytes that can be written
 */","* Create a BoundedByteArrayOutputStream with the specified
   * capacity and limit.
   * @param capacity The capacity of the underlying byte array
   * @param limit The maximum limit upto which data can be written",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,<init>,org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet),80,82,"/**
 * Constructs an EnumSetWritable with the specified EnumSet.
 * @param value the EnumSet to be wrapped
 */","* Construct a new EnumSetWritable. Argument <tt>value</tt> should not be null
   * or empty.
   * 
   * @param value enumSet value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,write,"org.apache.hadoop.io.OutputBuffer:write(java.io.InputStream,int)",107,109,"/**
 * Reads data from input stream into buffer.
 * @param in InputStream to read from
 * @param length number of bytes to read
 * @throws IOException if an I/O error occurs
 */","* Writes bytes from a InputStream directly into the buffer.
   * @param in input in.
   * @param length input length.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,nextBytes,org.apache.hadoop.crypto.random.OsSecureRandom:nextBytes(byte[]),97,108,"/**
 * Masks input bytes using a reservoir.
 * @param bytes input byte array to be masked
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,next,org.apache.hadoop.crypto.random.OsSecureRandom:next(int),110,118,"/**
* Generates a mask value with specified bits.
* @param nbits number of bits for the mask
* @return integer mask with set bits
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,fromString,org.apache.hadoop.io.DefaultStringifier:fromString(java.lang.String),75,81,"/**
* Deserializes a string from Base64 and returns an object.
* @param str Base64 encoded string
* @return Deserialized object of type T
* @throws IOException if I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKeyStream,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyStream(),1806,1809,"/**
 * Masks data using key buffer.
 * @return DataInputStream with masked data
 */","* Streaming access to the key. Useful for desrializing the key into
         * user objects.
         * 
         * @return The input stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeWritable,"org.apache.hadoop.security.token.Token:decodeWritable(org.apache.hadoop.io.Writable,java.lang.String)",355,366,"/**
 * Masks an object with a Base64-decoded value.
 * @param obj the Writable object to mask
 * @param newValue the Base64-encoded string to decode and apply
 * @throws IOException if decoding fails or I/O error occurs
 */","* Modify the writable to the value from the newValue.
   * @param obj the object to read into
   * @param newValue the string with the url-safe base64 encoded bytes
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeUncompressedBytes,org.apache.hadoop.io.SequenceFile$CompressedBytes:writeUncompressedBytes(java.io.DataOutputStream),699,715,"/**
* Writes compressed data to output stream.
* @param outStream destination for writing data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compare,"org.apache.hadoop.io.WritableComparator:compare(byte[],int,int,byte[],int,int)",177,192,"/**
* Processes data using two keys and a buffer.
* @param b1 first byte array
* @param s1 start index for b1
* @param l1 length of data in b1
* @param b2 second byte array
* @param s2 start index for b2
* @param l2 length of data in b2
* @return result of processing with keys
*/","Optimization hook.  Override this to make SequenceFile.Sorter's scream.
   *
   * <p>The default implementation reads the data into two {@link
   * WritableComparable}s (using {@link
   * Writable#readFields(DataInput)}, then calls {@link
   * #compare(WritableComparable,WritableComparable)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,36,"/**
 * Constructs an RSErasureCodec with given configuration and options.
 * @param conf Configuration object
 * @param options ErasureCodecOptions for codec settings
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,36,"/**
* Initializes an HHXORErasureCodec with given configuration and options.
* @param conf Configuration object for codec settings
* @param options ErasureCodecOptions specifying codec behavior
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",32,34,"/**
 * Constructs a DummyErasureCodec with specified configuration and options.
 * @param conf Configuration settings for the codec
 * @param options Options for configuring the erasure codec
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,37,"/**
* Initializes an XORErasureCodec with configuration and options.
* @param conf Configuration object for codec settings
* @param options Options specifying codec behavior
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawCoderFactory,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawCoderFactory(java.lang.String,java.lang.String)",154,161,"/**
 * Retrieves a raw erasure coder factory.
 * @param coderName name of the coder
 * @param codecName name of the codec
 * @return RawErasureCoderFactory instance or null if not found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,anyRecoverable,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:anyRecoverable(org.apache.hadoop.io.erasurecode.ECBlockGroup),86,90,"/**
* Checks if the block group has an acceptable number of erased blocks.
* @param blockGroup the ECBlockGroup to check
* @return true if erased count is greater than 0 and less than or equal to max allowed, false otherwise
*/","* Given a BlockGroup, tell if any of the missing blocks can be recovered,
   * to be called by ECManager
   * @param blockGroup a blockGroup that may contain erased blocks but not sure
   *                   recoverable or not
   * @return true if any erased block recoverable, false otherwise",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getNumErasedBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),132,136,"/**
* Recursively counts blocks in group.
* @param blockGroup ECBlockGroup to process
* @return Total count of blocks
*/","* Get the number of erased blocks in the block group.
   * @param blockGroup blockGroup.
   * @return number of erased blocks",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getErasedIndexes,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getErasedIndexes(org.apache.hadoop.io.erasurecode.ECBlock[]),159,174,"/**
* Identifies indexes of erased blocks.
* @param inputBlocks array of ECBlock objects
* @return array of indexes for erased blocks
*/","* Get indexes of erased blocks from inputBlocks
   * @param inputBlocks inputBlocks.
   * @return indexes of erased blocks from inputBlocks",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,checkInputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkInputBuffers(java.nio.ByteBuffer[]),98,122,"/**
 * Validates input buffers for decoding.
 * @param buffers array of ByteBuffers to validate
 */","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,checkInputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkInputBuffers(byte[][]),95,115,"/**
 * Validates input buffers for decoding.
 * @param buffers array of byte arrays to be validated
 */","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,<init>,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],int[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",52,65,"/**
* Initializes erasure decoding step for Reed-Solomon and XOR.
* @param inputBlocks array of input data blocks
* @param erasedIndexes indices of erased blocks
* @param outputBlocks array of output data blocks
* @param rawDecoder Reed-Solomon decoder instance
* @param rawEncoder XOR encoder instance
*/","* The constructor with all the necessary info.
   * @param inputBlocks inputBlocks.
   * @param erasedIndexes the indexes of erased blocks in inputBlocks array
   * @param outputBlocks outputBlocks.
   * @param rawDecoder underlying RS decoder for hitchhiker decoding
   * @param rawEncoder underlying XOR encoder for hitchhiker decoding",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingState.java,checkParameters,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingState:checkParameters(java.lang.Object[],int[],java.lang.Object[])",38,54,"/**
 * Masks input array elements based on erased indexes.
 * @param <T> generic type of array elements
 * @param inputs original array to be masked
 * @param erasedIndexes indices of elements to be erased
 * @param outputs resulting array after masking
 * @throws IllegalArgumentException if inputs length is invalid
 * @throws HadoopIllegalArgumentException if lengths mismatch or too many erasures
 */","* Check and validate decoding parameters, throw exception accordingly. The
   * checking assumes it's a MDS code. Other code  can override this.
   * @param inputs input buffers to check
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,<init>,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",48,57,"/**
* Initializes a new encoding step for erasure coding.
* @param inputBlocks array of input data blocks
* @param outputBlocks array of output encoded blocks
* @param rsRawEncoder Reed-Solomon raw encoder
* @param xorRawEncoder XOR raw encoder
*/","* The constructor with all the necessary info.
   *
   * @param inputBlocks inputBlocks.
   * @param outputBlocks outputBlocks.
   * @param rsRawEncoder  underlying RS encoder for hitchhiker encoding
   * @param xorRawEncoder underlying XOR encoder for hitchhiker encoding",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/EncodingState.java,checkParameters,"org.apache.hadoop.io.erasurecode.rawcoder.EncodingState:checkParameters(java.lang.Object[],java.lang.Object[])",36,43,"/**
* Validates input and output arrays against encoder requirements.
* @param inputs array of input elements
* @param outputs array of output elements
*/","* Check and validate decoding parameters, throw exception accordingly.
   * @param inputs input buffers to check
   * @param outputs output buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
 * Creates an XOR-based raw erasure decoder.
 * @param coderOptions configuration options for the erasure coder
 * @return XORRawDecoder instance configured with the given options
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),36,45,"/**
* Creates an erasure decoding step for a block group.
* @param blockGroup the block group to process
* @return an ErasureCodingStep object configured with input blocks and decoder
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,39,"/**
 * Creates a dummy raw erasure decoder.
 * @param coderOptions configuration options for the decoder
 * @return instance of DummyRawDecoder
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
* Initializes NativeXORRawDecoder with given options.
* @param coderOptions configuration for erasure coding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
* Initializes a NativeRSRawDecoder with given ErasureCoderOptions.
* @param coderOptions configuration for erasure coding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,getPiggyBackForDecode,"org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBackForDecode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,int,int)",150,201,"/**
 * Processes input and output buffers for Reed-Solomon coding.
 * @param inputs 2D array of input ByteBuffers
 * @param outputs 2D array of output ByteBuffers
 * @param pbParityIndex index of parity unit to process
 * @param numDataUnits number of data units
 * @param numParityUnits number of parity units
 * @param pbIndex index of piggyback buffer
 * @return ByteBuffer containing processed data
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,add,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int[],int[])",371,384,"/**
* Merges two integer arrays up to the length of the shorter array.
* @param p first input array
* @param q second input array
* @return merged array containing elements from both inputs
*/","* Compute the sum of two polynomials. The index in the array corresponds to
   * the power of the entry. For example p[0] is the constant term of the
   * polynomial p.
   *
   * @param p input polynomial
   * @param q input polynomial
   * @return polynomial represents p+q",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,multiply,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int[],int[])",327,340,"/**
* Multiplies two integer arrays representing polynomials.
* @param p first polynomial array
* @param q second polynomial array
* @return resulting polynomial array after multiplication
*/","* Compute the multiplication of two polynomials. The index in the array
   * corresponds to the power of the entry. For example p[0] is the constant
   * term of the polynomial p.
   *
   * @param p input polynomial
   * @param q input polynomial
   * @return polynomial represents p*q",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,gaussianElimination,org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:gaussianElimination(int[][]),549,588,"/**
* Performs Gaussian elimination on a given matrix.
* @param matrix 2D array representing the matrix to be processed
*/","* Perform Gaussian elimination on the given matrix. This matrix has to be a
   * fat matrix (number of rows &gt; number of columns).
   *
   * @param matrix matrix.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,getPrimitivePower,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:getPrimitivePower(int,int)",38,45,"/**
* Generates an array of powers of a primitive root.
* @param numDataUnits number of data units
* @param numParityUnits number of parity units
* @return array of powers for Reed-Solomon encoding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/DumpUtil.java,dumpChunks,"org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunks(java.lang.String,org.apache.hadoop.io.erasurecode.ECChunk[])",80,87,"/**
* Masks data in chunks under a given header.
* @param header descriptive header for the masking process
* @param chunks array of ECChunk objects to be masked
*/","* Print data in hex format in an array of chunks.
   * @param header header.
   * @param chunks chunks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
* Initializes a NativeXORRawEncoder with given options.
* @param coderOptions configuration for erasure coding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
* Initializes the NativeRSRawEncoder with given options.
* @param coderOptions configuration for erasure coding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
 * Creates an XOR-based raw erasure encoder.
 * @param coderOptions configuration options for the encoder
 * @return XORRawEncoder instance
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),36,44,"/**
* Creates an encoding step for a block group.
* @param blockGroup the block group to encode
* @return ErasureCodingStep configured with input blocks and encoder
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),31,34,"/**
 * Creates a dummy raw encoder.
 * @param coderOptions configuration options for the encoder
 * @return DummyRawEncoder instance initialized with coderOptions
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetOutputBuffers,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(java.nio.ByteBuffer[],int)",87,91,"/**
 * Masks data in each buffer using specified length.
 * @param buffers array of ByteBuffers to mask
 * @param dataLen length of data to mask in each buffer
 */",* Initialize the output buffers with ZERO bytes.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,toBuffers,org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[]),108,125,"/**
 * Processes ECChunk array to create ByteBuffer array.
 * @param chunks input ECChunk array
 * @return processed ByteBuffer array
 */","* Convert an array of this chunks to an array of ByteBuffers
   * @param chunks chunks to convertToByteArrayState into buffers
   * @return an array of ByteBuffers",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetOutputBuffers,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(byte[][],int[],int)",96,101,"/**
* Masks data in multiple buffers.
* @param buffers array of byte arrays containing data to mask
* @param offsets array of starting offsets for masking in each buffer
* @param dataLen length of data to mask in each buffer
*/",* Initialize the output buffers with ZERO bytes.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),83,96,"/**
* Converts encoding state to ByteBuffer and processes outputs.
* @param encodingState current encoding state to be converted
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),86,99,"/**
* Converts to ByteBufferDecodingState and processes outputs.
* @param decodingState current state with byte array inputs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),57,71,"/**
* Initializes RSRawDecoder with given coder options.
* @param coderOptions configuration for erasure coding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),42,61,"/**
* Initializes RSRawEncoder with given coder options.
* @param coderOptions configuration for erasure coding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,generateDecodeMatrix,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:generateDecodeMatrix(int[]),143,176,"/**
* Masks and decodes matrix using GF256 operations.
* @param erasedIndexes indexes of erased data units
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,getInstance,org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance(),121,123,"/**
 * Creates a default Galois Field.
 * @return GaloisField object with default size and primitive polynomial
 */","* Get the object performs Galois field arithmetic with default setting.
   * @return GaloisField.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsR,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsR(long),616,638,"/**
 * Masks bits from buffer.
 * @param n number of bits to mask
 * @return masked bits as a long
 * @throws IOException if end of stream is reached unexpectedly
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetBit,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetBit(),640,658,"/**
 * Reads and processes the next bit from the input stream.
 * @return true if the bit is 1, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,init,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:init(),756,772,"/**
* Initializes data and performs setup operations.
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,endCompression,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endCompression(),833,849,"/**
 * Executes a series of mask operations on data.
 * @throws IOException if an I/O error occurs during processing
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues(),951,992,"/**
* Masks data for compression.
* Initializes and processes data arrays for encoding.
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,mainSort,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainSort(),1738,1901,"/**
* Initializes and processes data for compression.
* Uses block, ftab, and other internal arrays for calculations.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,"org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",49,51,"/**
 * Constructs a new CompressorStream with default buffer size.
 * @param out output stream to compress data into
 * @param compressor the compressor to use for compression
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int,int)",54,58,"/**
* Initializes a block compressor stream.
* @param out output stream to write compressed data
* @param compressor compressor to use for compression
* @param bufferSize size of the buffer
* @param compressionOverhead overhead for compression
*/","* Create a {@link BlockCompressorStream}.
   * 
   * @param out stream
   * @param compressor compressor to be used
   * @param bufferSize size of buffer
   * @param compressionOverhead maximum 'overhead' of the compression 
   *                            algorithm with given bufferSize",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,write,org.apache.hadoop.io.compress.CompressorStream:write(int),115,119,"/**
* Writes an integer as a byte to a buffer and calls another method.
* @param b the integer value to write
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",68,72,"/**
 * Creates a DecompressorStream with specified buffer size.
 * @param in input stream to read compressed data from
 * @param decompressor decompressor implementation
 * @param bufferSize size of the internal buffer
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,<init>,org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:<init>(java.io.InputStream),162,166,"/**
 * Initializes a new decompression stream.
 * @param input compressed data input stream
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream),64,66,"/**
 * Initializes a new BlockDecompressorStream.
 * @param in InputStream to decompress data from
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,decompress,"org.apache.hadoop.io.compress.DecompressorStream:decompress(byte[],int,int)",108,173,"/**
* Decompresses data into a byte array.
* @param b destination buffer
* @param off offset in the buffer
* @param len length of bytes to decompress
* @return number of bytes read or -1 if end of file
* @throws IOException on I/O error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,finish,org.apache.hadoop.io.compress.BlockCompressorStream:finish(),136,145,"/**
* Handles compression logic with error handling.
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>(int),78,85,"/**
* Initializes a ZStandard decompressor with a specified buffer size.
* @param bufferSize size of the direct buffers for compression and decompression
*/","* Creates a new decompressor.
   * @param bufferSize bufferSize.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,finalize,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:finalize(),248,251,"/**
 * Calls method m1 to perform masking functionality.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:reset(),307,311,"/**
* Overrides parent's m1 method and sets endOfInput to true.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int,int)",94,103,"/**
 * Initializes a ZStandardCompressor with specified parameters.
 * @param level compression level
 * @param inputBufferSize size of the input buffer
 * @param outputBufferSize size of the output buffer
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,decompress,"org.apache.hadoop.io.compress.BlockDecompressorStream:decompress(byte[],int,int)",68,112,"/**
 * Reads data from a compressed source into a byte array.
 * @param b destination byte array
 * @param off offset in the byte array to start writing
 * @param len maximum number of bytes to read
 * @return number of bytes read, or -1 if end of file is reached
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(),102,104,"/**
* Initializes a new instance with default buffer size.
*/",* Creates a new compressor with the default buffer size.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecClassByName,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClassByName(java.lang.String),274,281,"/**
* Retrieves compression codec class by name.
* @param codecName name of the codec
* @return Class of the compression codec or null if not found
*/","* Find the relevant compression codec for the codec's canonical class name
   * or by codec alias and returns its implemetation class.
   * <p>
   * Codec aliases are case insensitive.
   * <p>
   * The code alias is the short class name (without the package name).
   * If the short class name ends with 'Codec', then there are two aliases for
   * the codec, the complete short class name and the short class name without
   * the 'Codec' ending. For example for the 'GzipCodec' codec class name the
   * alias are 'gzip' and 'gzipcodec'.
   *
   * @param codecName the canonical class name of the codec
   * @return the codec class",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getCompressor,"org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration)",149,165,"/**
* Retrieves or creates a compressor for the given codec.
* @param codec compression codec to use
* @param conf configuration settings
* @return Compressor instance or null if creation fails
*/","* Get a {@link Compressor} for the given {@link CompressionCodec} from the 
   * pool or a new one.
   *
   * @param codec the <code>CompressionCodec</code> for which to get the 
   *              <code>Compressor</code>
   * @param conf the <code>Configuration</code> object which contains confs for creating or reinit the compressor
   * @return <code>Compressor</code> for the given 
   *         <code>CompressionCodec</code> from the pool or a new one",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getDecompressor,org.apache.hadoop.io.compress.CodecPool:getDecompressor(org.apache.hadoop.io.compress.CompressionCodec),180,195,"/**
 * Retrieves a decompressor for the given codec.
 * @param codec compression codec to use
 * @return Decompressor object or null if unavailable
 */","* Get a {@link Decompressor} for the given {@link CompressionCodec} from the
   * pool or a new one.
   *  
   * @param codec the <code>CompressionCodec</code> for which to get the 
   *              <code>Decompressor</code>
   * @return <code>Decompressor</code> for the given 
   *         <code>CompressionCodec</code> the pool or a new one",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,returnCompressor,org.apache.hadoop.io.compress.CodecPool:returnCompressor(org.apache.hadoop.io.compress.Compressor),202,215,"/**
* Applies masking to a compressor.
* @param compressor the Compressor object to mask
*/","* Return the {@link Compressor} to the pool.
   * 
   * @param compressor the <code>Compressor</code> to be returned to the pool",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,returnDecompressor,org.apache.hadoop.io.compress.CodecPool:returnDecompressor(org.apache.hadoop.io.compress.Decompressor),223,236,"/**
* Masks a decompressor by checking conditions and invoking methods.
* @param decompressor the Decompressor object to be masked
*/","* Return the {@link Decompressor} to the pool.
   * 
   * @param decompressor the <code>Decompressor</code> to be returned to the 
   *                     pool",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressorType,org.apache.hadoop.io.compress.ZStandardCodec:getCompressorType(),151,155,"/**
 * Returns the compressor class.
 * @return Class of ZStandardCompressor
 */","* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of compressor needed by this codec.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getDecompressorType,org.apache.hadoop.io.compress.ZStandardCodec:getDecompressorType(),209,213,"/**
 * Returns the decompressor class.
 * @return ZStandardDecompressor class
 */","* Get the type of {@link Decompressor} needed by
   * this {@link CompressionCodec}.
   *
   * @return the type of decompressor needed by this codec.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,getCompressorType,org.apache.hadoop.io.compress.DefaultCodec:getCompressorType(),69,72,"/**
 * Returns compressor class based on configuration.
 * @return Class of Compressor or null if not found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,getDecompressorType,org.apache.hadoop.io.compress.DefaultCodec:getDecompressorType(),95,98,"/**
 * Returns decompressor class based on configuration.
 * @return Class of decompressor or null if not found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:<init>(org.apache.hadoop.conf.Configuration),66,68,"/**
 * Initializes Gzip compressor with configuration.
 * @param conf configuration settings for compression
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,reinit,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:reinit(org.apache.hadoop.conf.Configuration),167,175,"/**
 * Initializes configuration and resets internal state.
 * @param conf Configuration object to be applied
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>(),122,126,"/**
 * Initializes a GzipZlibCompressor with default settings.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>(org.apache.hadoop.conf.Configuration),128,133,"/**
* Initializes a GzipZlibCompressor with configuration settings.
* @param conf configuration object containing compression parameters
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(),234,239,"/**
* Initializes a new ZlibCompressor with default settings.
*/","* Creates a new compressor with the default compression level.
   * Compressed data will be generated in ZLIB format.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.conf.Configuration),245,250,"/**
* Initializes ZlibCompressor with configuration settings.
* @param conf Configuration object containing compression parameters
*/","* Creates a new compressor, taking settings from the configuration.
   * @param conf configuration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,reinit,org.apache.hadoop.io.compress.zlib.ZlibCompressor:reinit(org.apache.hadoop.conf.Configuration),283,298,"/**
* Masks configuration and initializes compressor.
* @param conf Configuration object for compression settings
*/","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's compression level
   * and compression strategy.
   * 
   * @param conf Configuration storing new settings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>(),352,354,"/**
 * Initializes a new ZlibDirectDecompressor with default header and no compression level.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)",356,358,"/**
 * Initializes a new zlib decompressor with specified header and buffer size.
 * @param header compression header containing configuration details
 * @param directBufferSize size of the direct buffer for decompression
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>(),117,119,"/**
 * Initializes a new ZlibDecompressor with default settings.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibDecompressor:<init>(),137,139,"/**
 * Initializes GzipZlibDecompressor with autodetect header and 64KB buffer size.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:reset(),365,369,"/**
 * Overrides m1 to set endOfInput to true after calling superclass's m1.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,executeHeaderState,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeHeaderState(),257,358,"/**
 * Processes gzip header and transitions to deflate stream.
 * Throws IOException on CRC mismatch.
 */","* Parse the gzip header (assuming we're in the appropriate state).
   * In order to deal with degenerate cases (e.g., user buffer is one byte
   * long), we copy (some) header bytes to another buffer.  (Filename,
   * comment, and extra-field bytes are simply skipped.)</p>
   *
   * See http://www.ietf.org/rfc/rfc1952.txt for the gzip spec.  Note that
   * no version of gzip to date (at least through 1.4.0, 2010-01-20) supports
   * the FHCRC header-CRC16 flagbit; instead, the implementation treats it
   * as a multi-file continuation flag (which it also doesn't support). :-(
   * Sun's JDK v6 (1.6) supports the header CRC, however, and so do we.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readCompressedStringArray,org.apache.hadoop.io.WritableUtils:readCompressedStringArray(java.io.DataInput),181,189,"/**
* Reads array of strings from input.
* @param in DataInput source
* @return String array or null if no data
* @throws IOException on read errors
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,write,org.apache.hadoop.io.VIntWritable:write(java.io.DataOutput),54,57,"/**
 * Writes value to DataOutput using custom serialization.
 * @param out destination for serialized data
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,write,org.apache.hadoop.io.Text:write(java.io.DataOutput),395,399,"/**
* Writes data to output stream.
* @param out DataOutput object to write to
* @throws IOException if an I/O error occurs
*/","* Serialize. Write this object to out length uses zero-compressed encoding.
   *
   * @see Writable#write(DataOutput)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,write,"org.apache.hadoop.io.Text:write(java.io.DataOutput,int)",401,409,"/**
* Writes data to output stream with length check.
* @param out DataOutput object to write to
* @param maxLength maximum allowed length of data
* @throws IOException if data exceeds maxLength
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,write,org.apache.hadoop.security.token.delegation.DelegationKey:write(java.io.DataOutput),94,104,"/**
* Writes object data to DataOutput.
* @param out the DataOutput stream to write to
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,write,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:write(java.io.DataOutput),737,747,"/**
 * Writes object data to output stream.
 * @param out DataOutput stream for writing
 * @throws IOException if I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVInt,org.apache.hadoop.io.WritableUtils:readVInt(java.io.DataInput),334,340,"/**
* Reads a long value and casts it to an int, throwing an exception if out of range.
* @param stream input data source
* @return int representation of the read long value
* @throws IOException if value is too large for an integer or IO error occurs
*/","* Reads a zero-compressed encoded integer from input stream and returns it.
   * @param stream Binary input stream
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer from stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVIntInRange,"org.apache.hadoop.io.WritableUtils:readVIntInRange(java.io.DataInput,int,int)",354,370,"/**
* Reads a long from stream, checks bounds, and returns as int.
* @param stream data input source
* @param lower minimum allowed value
* @param upper maximum allowed value
* @return integer within specified bounds
* @throws IOException if value is out of bounds or read fails
*/","* Reads an integer from the input stream and returns it.
   *
   * This function validates that the integer is between [lower, upper],
   * inclusive.
   *
   * @param stream Binary input stream
   * @param lower input lower.
   * @param upper input upper.
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer from stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,readFields,org.apache.hadoop.io.VLongWritable:readFields(java.io.DataInput),49,52,"/**
 * Reads and sets the value from a DataInput stream.
 * @param in DataInput source to read from
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(java.lang.String),95,97,"/**
 * Constructs a Text object with the given string.
 * @param string the initial text content
 */","* Construct from a string.
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,find,"org.apache.hadoop.io.Text:find(java.lang.String,int)",187,221,"/**
 * Searches for a substring in byte array starting from given index.
 * @param what substring to search
 * @param start starting index
 * @return position of first occurrence or -1 if not found
 */","* Finds any occurrence of <code>what</code> in the backing
   * buffer, starting as position <code>start</code>. The starting
   * position is measured in bytes and the return value is in
   * terms of byte position in the buffer. The backing buffer is
   * not converted to a string for this operation.
   *
   * @param what input what.
   * @param start input start.
   * @return byte position of the first occurrence of the search
   *         string in the UTF-8 buffer or -1 if not found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,writeString,"org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)",582,588,"/**
* Writes masked string to DataOutput.
* @param out destination for writing data
* @param s input string to mask and write
* @return number of bytes written
* @throws IOException if I/O error occurs
*/","* Write a UTF8 encoded string to out.
   *
   * @param out input out.
   * @param s input s.
   * @throws IOException raised on errors performing I/O.
   * @return a UTF8 encoded string to out.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,writeString,"org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String,int)",598,610,"/**
* Writes a string to DataOutput with a length check.
* @param out destination for writing data
* @param s string to write
* @param maxLength maximum allowed length of the string in bytes
* @return number of bytes written
* @throws IOException if string exceeds maxLength
*/","* @return Write a UTF8 encoded string with a maximum size to out.
   *
   * @param out input out.
   * @param s input s.
   * @param maxLength input maxLength.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(byte[]),246,254,"/**
 * Initializes the object with UTF-8 encoded data.
 * @param utf8 byte array containing UTF-8 encoded data
 */","* Set to a utf8 byte array. If the length of <code>utf8</code> is
   * <em>zero</em>, actually clear {@link #bytes} and any existing
   * data is lost.
   *
   * @param utf8 input utf8.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(org.apache.hadoop.io.Text),260,263,"/**
* Updates text content and length from another Text object.
* @param other source Text object to copy data from
*/","* Copy a text.
   * @param other other.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readDefaultLine,"org.apache.hadoop.util.LineReader:readDefaultLine(org.apache.hadoop.io.Text,int,int)",197,263,"/**
 * Reads and processes text from input, appending to given Text object.
 * @param str Text object to append processed text
 * @param maxLineLength Maximum length of a line to process
 * @param maxBytesToConsume Maximum number of bytes to consume
 * @return Number of bytes consumed or throws IOException if too many bytes
 */","* Read a line terminated by one of CR, LF, or CRLF.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readCustomLine,"org.apache.hadoop.util.LineReader:readCustomLine(org.apache.hadoop.io.Text,int,int)",268,371,"/**
 * Reads and processes text up to a specified delimiter or byte limit.
 * @param str Text object to process
 * @param maxLineLength Maximum length of the line to read
 * @param maxBytesToConsume Maximum number of bytes to consume
 * @return Number of bytes consumed
 * @throws IOException if an I/O error occurs
 */",* Read a line terminated by a custom delimiter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,toString,org.apache.hadoop.io.Text:toString(),337,344,"/**
 * Masks data using specified function.
 * @return masked string or throws runtime exception on error
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,<init>,org.apache.hadoop.io.SortedMapWritable:<init>(),45,48,"/**
 * Constructs a new instance of SortedMapWritable.
 * Initializes with a TreeMap to maintain sorted order.
 */",default constructor.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,<init>,org.apache.hadoop.io.MapWritable:<init>(),43,46,"/**
* Constructs a new MapWritable instance.
* Initializes an internal HashMap to store key-value pairs.
*/",Default constructor.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,write,org.apache.hadoop.io.SortedMapWritable:write(java.io.DataOutput),182,198,"/**
* Writes data to output stream.
* @param out DataOutput object for writing
* @throws IOException if I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,write,org.apache.hadoop.io.MapWritable:write(java.io.DataOutput),147,163,"/**
* Writes data to output stream.
* @param out DataOutput object for writing
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getDeserializer,"org.apache.hadoop.io.SequenceFile$Reader:getDeserializer(org.apache.hadoop.io.serializer.SerializationFactory,java.lang.Class)",2163,2166,"/**
 * Retrieves a deserializer for a given class.
 * @param sf Serialization factory instance
 * @param c Class to get deserializer for
 * @return Deserializer object
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,<init>,org.apache.hadoop.util.bloom.Key:<init>(byte[]),87,89,"/**
 * Constructs a key with the specified byte array and default weight.
 * @param value byte array representing the key's value
 */","* Constructor.
   * <p>
   * Builds a key with a default weight.
   * @param value The byte value of <i>this</i> key.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compression,org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType),1052,1054,"/**
 * Creates a compression option from a given type.
 * @param value the compression type to be used
 * @return an Option object representing the compression setting
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,compression,"org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",302,306,"/**
* Creates a SequenceFile writer option with specified compression.
* @param type compression type to use
* @param codec compression codec to apply
* @return SequenceFile writer option
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/DeserializerComparator.java,<init>,org.apache.hadoop.io.serializer.DeserializerComparator:<init>(org.apache.hadoop.io.serializer.Deserializer),52,57,"/**
 * Initializes with a deserializer and opens it.
 * @param deserializer the deserializer to use
 * @throws IOException if opening fails
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/DeserializerComparator.java,compare,"org.apache.hadoop.io.serializer.DeserializerComparator:compare(byte[],int,int,byte[],int,int)",59,73,"/**
* Processes two byte arrays and compares their deserialized keys.
* @param b1 first byte array
* @param s1 start index of b1
* @param l1 length of b1
* @param b2 second byte array
* @param s2 start index of b2
* @param l2 length of b2
* @return result of comparing deserialized keys
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,readFields,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:readFields(java.io.DataInput),99,104,"/**
* Reads CRC and MD5 configuration from input.
* @param in DataInput source for reading values
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(byte[]),118,120,"/**
 * Computes MD5 hash of byte array.
 * @param data input byte array
 * @return MD5Hash object representing the hash
 */","* Construct a hash value for a byte array.
   * @param data data.
   * @return MD5Hash.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(org.apache.hadoop.io.UTF8),195,197,"/**
* Computes MD5 hash for UTF-8 string.
* @param utf8 UTF8 object containing input data
* @return MD5Hash of the input data
*/","* Construct a hash value for a String.
   * @param utf8 utf8.
   * @return MD5Hash.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,<init>,org.apache.hadoop.io.MD5Hash:<init>(java.lang.String),61,63,"/**
 * Constructs an MD5 hash from a hexadecimal string.
 * @param hex hexadecimal representation of the MD5 hash
 */","* Constructs an MD5Hash from a hex string.
   * @param hex input hex.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,stream,org.apache.hadoop.io.SequenceFile$Writer:stream(org.apache.hadoop.fs.FSDataOutputStream),1020,1022,"/**
 * Wraps FSDataOutputStream in a StreamOption.
 * @param value output stream to be wrapped
 * @return StreamOption object containing the input stream
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendIfExists,org.apache.hadoop.io.SequenceFile$Writer:appendIfExists(boolean),1028,1030,"/**
 * Creates an option to append if exists.
 * @param value boolean indicating whether to append
 * @return AppendIfExistsOption object
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,file,org.apache.hadoop.io.SequenceFile$Writer:file(org.apache.hadoop.fs.Path),994,996,"/**
 * Creates an option from a file path.
 * @param value file path
 * @return Option object representing the file
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:shouldRetry(java.lang.Exception,int,int,boolean)",421,436,"/**
* Determines retry action based on exception and retry count.
* @param e the exception encountered
* @param curRetry current retry attempt number
* @param failovers number of failover attempts
* @param isIdempotentOrAtMostOnce flag indicating if operation is idempotent or at most once
* @return RetryAction with decision and sleep time
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryForeverWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryForeverWithFixedSleep(long,java.util.concurrent.TimeUnit)",82,86,"/**
* Creates an infinite retry policy with fixed sleep.
* @param sleepTime duration to wait between retries
* @param timeUnit unit of time for sleep duration
* @return RetryPolicy that retries indefinitely with specified sleep
*/","* <p>
   * Keep trying forever with a fixed time between attempts.
   * </p>
   *
   * @param sleepTime sleepTime.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumCountWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithFixedSleep(int,long,java.util.concurrent.TimeUnit)",99,101,"/**
* Creates a retry policy with fixed sleep.
* @param maxRetries maximum number of retries
* @param sleepTime duration to wait between retries
* @param timeUnit unit of time for sleep duration
* @return RetryPolicy object configured with max retries and sleep time
*/","* <p>
   * Keep trying a limited number of times, waiting a fixed time between attempts,
   * and then fail by re-throwing the exception.
   * </p>
   *
   * @param maxRetries maxRetries.
   * @param sleepTime sleepTime.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:<init>(long,long,java.util.concurrent.TimeUnit)",346,351,"/**
* Constructs a retry mechanism with a fixed sleep duration.
* @param maxTime maximum total time for retries
* @param sleepTime time to wait between retries
* @param timeUnit unit of time for maxTime and sleepTime
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,exponentialBackoffRetry,"org.apache.hadoop.io.retry.RetryPolicies:exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)",148,151,"/**
* Creates an exponential backoff retry policy.
* @param maxRetries maximum number of retries
* @param sleepTime base sleep time between retries
* @param timeUnit unit for the sleep time
* @return RetryPolicy with exponential backoff
*/","* <p>
   * Keep trying a limited number of times, waiting a growing amount of time between attempts,
   * and then fail by re-throwing the exception.
   * The time between attempts is <code>sleepTime</code> mutliplied by a random
   * number in the range of [0, 2 to the number of retries)
   * </p>
   *
   *
   * @param timeUnit timeUnit.
   * @param maxRetries maxRetries.
   * @param sleepTime sleepTime.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumCountWithProportionalSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)",130,132,"/**
* Creates a retry policy with exponential backoff.
* @param maxRetries maximum number of retries
* @param sleepTime base sleep duration between retries
* @param timeUnit time unit for sleep duration
* @return RetryPolicy object configured with specified parameters
*/","* <p>
   * Keep trying a limited number of times, waiting a growing amount of time between attempts,
   * and then fail by re-throwing the exception.
   * The time between attempts is <code>sleepTime</code> mutliplied by the number of tries so far.
   * </p>
   *
   * @param sleepTime sleepTime.
   * @param maxRetries maxRetries.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)",210,215,"/**
* Creates a retry policy with network exception failover.
* @param fallbackPolicy base retry policy to fall back on
* @param maxFailovers maximum number of failover attempts
* @param delayMillis initial delay between retries in milliseconds
* @param maxDelayBase maximum base delay for exponential backoff
* @return RetryPolicy configured for network exception handling
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,newAsyncCall,"org.apache.hadoop.io.retry.AsyncCallHandler:newAsyncCall(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler)",322,327,"/**
* Creates an AsyncCall instance.
* @param method the target method to invoke
* @param args arguments for the method
* @param isRpc true if it's an RPC call
* @param callId unique identifier for the call
* @param retryInvocationHandler handler for retry logic
* @return new AsyncCall object configured with provided parameters
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,newRetryInfo,"org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:newRetryInfo(org.apache.hadoop.io.retry.RetryPolicy,java.lang.Exception,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,boolean,long)",273,302,"/**
 * Determines retry information based on policy and exceptions.
 * @param policy the retry policy to apply
 * @param e the exception encountered
 * @param counters current retry and failover counts
 * @param idempotentOrAtMostOnce indicates if operation is idempotent or at most once
 * @param expectedFailoverCount expected number of failovers
 * @return RetryInfo with max delay, action, and failure exception
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,tryStop,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStop(org.apache.hadoop.util.Daemon),186,190,"/**
 * Checks queue and processes daemon if within grace period.
 * @param d Daemon object to process
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,read,org.apache.hadoop.security.Groups$TimerToTickerAdapter:read(),292,296,"/**
* Converts timer value to milliseconds.
* @return time in milliseconds
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,unlock,org.apache.hadoop.util.InstrumentedWriteLock:unlock(),59,69,"/**
* Handles write lock release and reporting.
* Updates timestamp and reports if necessary.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedWriteLock:startLockTiming(),74,79,"/**
* Updates write lock timestamp if held.
*/",* Starts timing for the instrumented write lock.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long,org.apache.hadoop.util.Timer)",87,99,"/**
* Initializes an instrumented lock with logging and timing.
* @param name lock identifier
* @param logger for logging lock events
* @param lock underlying lock mechanism
* @param minLoggingGapMs minimum gap between log entries
* @param lockWarningThresholdMs threshold for warning logs
* @param clock timer for measuring elapsed time
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedLock:startLockTiming(),177,179,"/**
 * Records the current timestamp for acquiring a lock.
 */",* Starts timing for the instrumented lock.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,unlock,org.apache.hadoop.util.InstrumentedReadLock:unlock(),65,75,"/**
 * Executes lock operations and reports status if needed.
 * @param needReport flag to determine if reporting is required
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedReadLock:startLockTiming(),81,86,"/**
* Checks lock status and records timestamp.
* Updates timestamp if read lock is held.
*/","* Starts timing for the instrumented read lock.
   * It records the time to ThreadLocal.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",327,330,"/**
* Constructs a new RetryInvocationHandler with specified proxy provider and retry policy.
* @param proxyProvider provider for failover proxies
* @param retryPolicy policy defining retry behavior
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,java.util.Map,org.apache.hadoop.io.retry.RetryPolicy)",100,110,"/**
* Creates a proxy with retry logic for interface.
* @param iface target interface class
* @param proxyProvider provider for proxy objects
* @param methodNameToPolicyMap method-specific retry policies
* @param defaultPolicy default retry policy
* @return proxy object implementing the interface
*/","* Create a proxy for an interface of implementations of that interface using
   * the given {@link FailoverProxyProvider} and the a set of retry policies
   * specified by method name. If no retry policy is defined for a method then a
   * default of {@link RetryPolicies#TRY_ONCE_THEN_FAIL} is used.
   * 
   * @param iface the interface that the retry will implement
   * @param proxyProvider provides implementation instances whose methods should be retried
   * @param methodNameToPolicyMap map of method names to retry policies
   * @param defaultPolicy defaultPolicy.
   * @param <T> T.
   * @return the retry proxy",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,failover,"org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:failover(long,java.lang.reflect.Method,int)",217,229,"/**
* Handles failover logic based on expected count.
* @param expectedFailoverCount expected number of failovers
* @param method method being called
* @param callId unique identifier for the call
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,log,"org.apache.hadoop.io.retry.RetryInvocationHandler:log(java.lang.reflect.Method,boolean,int,int,long,java.lang.Exception)",398,430,"/**
* Logs error details for method invocation.
* @param method Method being invoked
* @param isFailover Indicates if failover is attempted
* @param failovers Number of failover attempts
* @param retries Current retry count
* @param delay Delay in milliseconds before retrying
* @param ex Exception occurred during invocation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,invokeMethod,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])",48,65,"/**
 * Handles method invocation with retry logic.
 * @param method Method being invoked
 * @param args Arguments for the method
 * @return Result of the method invocation or throws exception if conditions met
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,hashCode,org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:hashCode(),150,153,"/**
 * Delegates to multipleLinearRandomRetry's m1 method.
 * @return Result of calling m1 on multipleLinearRandomRetry instance
 */","* Similarly, remoteExceptionToRetry is ignored as part of hashCode since it
     * does not affect connection failure handling.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,equals,org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:equals(java.lang.Object),135,144,"/**
* Checks equality with another object.
* @param obj the object to compare
* @return true if equal, false otherwise
*/","* remoteExceptionToRetry is ignored as part of equals since it does not
     * affect connection failure handling.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:shouldRetry(java.lang.Exception,int,int,boolean)",283,291,"/**
* Determines retry action based on exception and retry count.
* @param e the caught exception
* @param retries current retry attempt count
* @param failovers number of failover attempts
* @param isIdempotentOrAtMostOnce flag indicating idempotency
* @return RetryAction with decision, delay, and message
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,mayThrow,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:mayThrow(java.util.List),314,324,"/**
* Throws an IOException if replication is below minimum.
* @param ioExceptions list of IOExceptions to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:getFileStatus(org.apache.hadoop.fs.Path),875,915,"/**
 * Retrieves file status from nodes, handling exceptions.
 * @param f file path to check
 * @return FileStatus object or throws exception if not found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MultipleIOException.java,build,org.apache.hadoop.io.MultipleIOException$Builder:build(),83,85,"/**
 * Masks exceptions and returns an IOException.
 * @param exceptions array of Throwable objects to mask
 * @return IOException object masking provided exceptions
 */","* @return null if nothing is added to this builder;
     *         otherwise, return an {@link IOException}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.lang.String),878,883,"/**
* Initializes DataIndex with a default compression algorithm.
* @param defaultCompressionAlgorithmName name of the default compression algorithm
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getSupportedCompressionAlgorithms,org.apache.hadoop.io.file.tfile.TFile:getSupportedCompressionAlgorithms(),198,200,"/**
 * Returns an array of strings from compression utility.
 * @return Array of compressed string data
 */","* Get names of supported compression algorithms. The names are acceptable by
   * TFile.Writer.
   * 
   * @return Array of strings, each represents a supported compression
   *         algorithm. Currently, the following compression algorithms are
   *         supported.
   *         <ul>
   *         <li>""none"" - No compression.
   *         <li>""lzo"" - LZO compression.
   *         <li>""gz"" - GZIP compression.
   *         </ul>",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressionName(),576,578,"/**
 * Retrieves block state.
 * @return block state string from rBlkState
 */","* Get the name of the compression algorithm used to compress the block.
       * 
       * @return name of the compression algorithm.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,register,"org.apache.hadoop.io.file.tfile.BCFile$Writer$MetaBlockRegister:register(long,long,long)",451,455,"/**
* Masks data by adding a MetaIndex entry.
* @param raw original data value
* @param begin start position of the mask
* @param end end position of the mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,readString,org.apache.hadoop.io.file.tfile.Utils:readString(java.io.DataInput),276,282,"/**
* Reads and masks data from input.
* @param in DataInput source
* @return masked string or null if no data
*/","* Read a String as a VInt n, followed by n Bytes in Text format.
   * 
   * @param in
   *          The input stream.
   * @return The string
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:<init>(java.io.DataInput),2302,2307,"/**
* Reads file index entry from input stream.
* @param in DataInput source
* @throws IOException if reading fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,readLength,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:readLength(),102,109,"/**
* Reads remaining data and sets chunk status.
* @throws IOException if an I/O error occurs
*/","* Reading the length of next chunk.
     * 
     * @throws java.io.IOException
     *           when no more data is available.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.lang.String),2051,2057,"/**
* Initializes a new TFileMeta with the given comparator.
* @param comparator string used for comparison logic
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,makeComparator,org.apache.hadoop.io.file.tfile.TFile:makeComparator(java.lang.String),176,178,"/**
 * Returns a comparator for RawComparable based on file metadata.
 * @param name metadata field to compare by
 * @return Comparator object for sorting
 */","* Make a raw comparator from a string name.
   * 
   * @param name
   *          Comparator name
   * @return A RawComparable comparator.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,writeChunk,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeChunk(byte[],int,int,boolean)",253,266,"/**
 * Masks and writes a chunk of data to output.
 * @param chunk byte array containing data to process
 * @param offset starting index in the chunk
 * @param len length of data to mask and write
 * @param last flag indicating if it's the last chunk
 * @throws IOException if an I/O error occurs
 */","* Write out a chunk.
     * 
     * @param chunk
     *          The chunk buffer.
     * @param offset
     *          Offset to chunk buffer for the beginning of chunk.
     * @param len
     * @param last
     *          Is this the last call to flushBuffer?",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,writeBufData,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeBufData(byte[],int,int)",279,287,"/**
 * Masks data by writing to output stream.
 * @param data byte array containing data to mask
 * @param offset starting index in the data array
 * @param len number of bytes to mask
 * @throws IOException if an I/O error occurs
 */","* Write out a chunk that is a concatenation of the internal buffer plus
     * user supplied data. This will never be the last block.
     * 
     * @param data
     *          User supplied data buffer.
     * @param offset
     *          Offset to user data buffer.
     * @param len
     *          User data buffer size.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:write(java.io.DataOutput),2335,2339,"/**
* Writes data to output stream.
* @param out DataOutput object for writing
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,<init>,"org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:<init>(java.io.DataOutputStream,int)",377,382,"/**
* Initializes encoder with output stream and chunk size.
* @param out data output stream to write to
* @param size size of the chunk
* @throws IOException if I/O error occurs
*/","* Constructor.
     * 
     * @param out
     *          the underlying output stream.
     * @param size
     *          The total # of bytes to be written as a single chunk.
     * @throws java.io.IOException
     *           if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getEntryComparator,org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryComparator(),925,944,"/**
* Returns a comparator for Scanner entries.
* @throws RuntimeException if entries are not comparable for unsorted TFiles
*/","* Get a Comparator object to compare Entries. It is useful when you want
     * stores the entries in a collection (such as PriorityQueue) and perform
     * sorting or comparison among entries based on the keys without copying out
     * the key.
     * 
     * @return An Entry Comparator..",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareKeys,"org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(byte[],int,int,byte[],int,int)",1007,1012,"/**
* Compares two byte arrays.
* @param a first byte array
* @param o1 offset in the first array
* @param l1 length of the first array segment
* @param b second byte array
* @param o2 offset in the second array
* @param l2 length of the second array segment
* @return comparison result as an integer
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareKeys,"org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1014,1019,"/**
* Compares two RawComparable objects.
* @param a first RawComparable object
* @param b second RawComparable object
* @return comparison result as integer
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,setFirstKey,"org.apache.hadoop.io.file.tfile.TFile$TFileIndex:setFirstKey(byte[],int,int)",2250,2253,"/**
* Masks data using a given key.
* @param key the masking key array
* @param offset starting index in the key array
* @param length number of bytes to mask
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLastKey,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLastKey(),2255,2260,"/**
* Returns a RawComparable object based on index conditions.
* @return RawComparable object or null if index condition not met
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,atEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:atEnd(),1588,1590,"/**
 * Checks if current location has reached or exceeded end location.
 * @return true if reached or exceeded, false otherwise
 */","* Is cursor at the end location?
       * 
       * @return true if the cursor is at the end location.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationNear(long),1031,1035,"/**
 * Masks location based on offset.
 * @param offset byte offset in the file
 * @return Location object or end if block not found
 */","* Get the location pointing to the beginning of the first key-value pair in
     * a compressed block whose byte offset in the TFile is greater than or
     * equal to the specified offset.
     * 
     * @param offset
     *          the user supplied offset.
     * @return the location to the corresponding entry; or end() if no such
     *         entry exists.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,clone,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:clone(),759,762,"/**
 * Creates a new location based on block and record indices.
 * @return Location object representing the current position
 */",* @see java.lang.Object#clone(),,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationByRecordNum,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLocationByRecordNum(long),2238,2242,"/**
* Determines location for a given record number.
* @param recNum the record number to locate
* @return Location object with index and offset
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),713,715,"/**
 * Copies properties from another Location instance.
 * @param other source Location to copy from
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[]),1840,1842,"/**
 * Calls overloaded method with default offset.
 * @param buf input byte array
 * @return result from overloaded method
 * @throws IOException if an I/O error occurs
 */","* Copy value into user-supplied buffer. User supplied buffer must be
         * large enough to hold the whole value. The value part of the key-value
         * pair pointed by the current cursor is not cached and can only be
         * examined once. Calling any of the following functions more than once
         * without moving the cursor will result in exception:
         * {@link #getValue(byte[])}, {@link #getValue(byte[], int)},
         * {@link #getValueStream}.
         *
         * @param buf buf.
         * @return the length of the value. Does not require
         *         isValueLengthKnown() to be true.
         * @throws IOException raised on errors performing I/O.
         *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,set,org.apache.hadoop.io.UTF8:set(java.lang.String),96,118,"/**
* Masks and processes a string, ensuring it fits within length limits.
* @param string input string to be processed
*/","* Set to contain the contents of a string.
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,toByteArray,org.apache.hadoop.io.WritableUtils:toByteArray(org.apache.hadoop.io.Writable[]),461,472,"/**
* Converts Writable objects to a byte array.
* @param writables variable number of Writable objects
* @return byte array representation of the Writable objects
*/","* Convert writables to a byte array.
   * @param writables input writables.
   * @return ByteArray.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,getBytes,org.apache.hadoop.io.UTF8:getBytes(java.lang.String),238,249,"/**
 * Masks input string and returns byte array.
 * @param string input to be masked
 * @return masked data as byte array
 */","* @return Convert a string to a UTF-8 encoded byte array.
   * @see String#getBytes(String)
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeBuffer,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:writeBuffer(org.apache.hadoop.io.DataOutputBuffer),1623,1635,"/**
* Compresses data and writes it to output.
* @param uncompressedDataBuffer contains raw data to compress
*/",Workhorse to check and write out compressed data/lengths,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,byteArrayForBloomKey,org.apache.hadoop.io.BloomMapFile:byteArrayForBloomKey(org.apache.hadoop.io.DataOutputBuffer),71,79,"/**
* Masks data buffer to specified length.
* @param buf input DataOutputBuffer
* @return masked byte array of specified length
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,lessThan,"org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:lessThan(java.lang.Object,java.lang.Object)",3537,3548,"/**
* Compares two segment descriptors.
* @param a first SegmentDescriptor object
* @param b second SegmentDescriptor object
* @return true if a is less than b, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,copy,org.apache.hadoop.io.AbstractMapWritable:copy(org.apache.hadoop.io.Writable),125,142,"/**
* Masks this object with another Writable.
* @param other the source Writable to mask with
*/","* Used by child copy constructors.
   * @param other other.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$RecordCompressWriter:append(java.lang.Object,java.lang.Object)",1545,1575,"/**
 * Masks and serializes key-value pair to output stream.
 * @param key the key object to be serialized
 * @param val the value object to be serialized
 * @throws IOException if class mismatch or negative length occurs
 */",Append a key/value pair.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,toString,org.apache.hadoop.io.DefaultStringifier:toString(java.lang.Object),83,90,"/**
* Serializes object to Base64 string.
* @param obj object to serialize
* @return Base64 encoded string of the serialized object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,checkKey,org.apache.hadoop.io.MapFile$Writer:checkKey(org.apache.hadoop.io.WritableComparable),418,429,"/**
* Masks and writes a key to output buffer.
* @param key the key to process
* @throws IOException if key is out of order or I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/TokenIdentifier.java,getBytes,org.apache.hadoop.security.token.TokenIdentifier:getBytes(),60,68,"/**
 * Generates a masked byte array.
 * @return byte array representing the mask
 */","* Get the bytes for the token identifier
   * @return the bytes of the identifier",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,encodeWritable,org.apache.hadoop.security.token.Token:encodeWritable(org.apache.hadoop.io.Writable),340,347,"/**
* Serializes object to Base64 string.
* @param obj Writable object to serialize
* @return Base64 encoded string representation of the object
*/","* Generate a string with the url-quoted base64 encoded serialized form
   * of the Writable.
   * @param obj the object to serialize
   * @return the encoded string
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,moveData,org.apache.hadoop.util.ReflectionUtils$CopyInCopyOutBuffer:moveData(),314,316,"/**
 * Masks data from input buffer to output buffer.
 */",* Move the data from the output buffer to the input buffer.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNSDomainNameResolver.java,getAllResolvedHostnameByDomainName,"org.apache.hadoop.net.DNSDomainNameResolver:getAllResolvedHostnameByDomainName(java.lang.String,boolean)",64,80,"/**
* Resolves domain name to hostnames.
* @param domainName the domain to resolve
* @param useFQDN whether to use fully qualified domain names
* @return array of hostnames or IP addresses
* @throws UnknownHostException if domain cannot be resolved
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDistanceByPath,"org.apache.hadoop.net.NetworkTopology:getDistanceByPath(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",376,400,"/**
* Computes distance between two nodes.
* @param node1 first node in the comparison
* @param node2 second node in the comparison
* @return integer distance value or Integer.MAX_VALUE if null pointer is encountered
*/","Return the distance between two nodes by comparing their network paths
   * without checking if they belong to the same ancestor node by reference.
   * It is assumed that the distance from one node to its parent is 1
   * The distance between two nodes is calculated by summing up their distances
   * to their closest common ancestor.
   * @param node1 one node
   * @param node2 another node
   * @return the distance between node1 and node2",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:equals(java.lang.Object),108,112,"/**
 * Overrides to call superclass method.
 * @param o object to process
 * @return result of superclass method
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,equals,org.apache.hadoop.net.InnerNodeImpl:equals(java.lang.Object),316,319,"/**
 * Overrides and delegates call to superclass method.
 * @param to object to be passed to superclass method
 * @return result of superclass method invocation
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:hashCode(),114,118,"/**
 * Calls the superclass implementation of m1.
 * @return The result of the superclass's m1 method.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,hashCode,org.apache.hadoop.net.InnerNodeImpl:hashCode(),311,314,"/**
 * Calls the superclass's m1 method.
 * @return result of super.m1()
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getNodeForNetworkLocation,org.apache.hadoop.net.NetworkTopology:getNodeForNetworkLocation(org.apache.hadoop.net.Node),193,195,"/**
 * Applies mask to node.
 * @param node input node
 * @return masked node result
 */","* Return a reference to the node given its string representation.
   * Default implementation delegates to {@link #getNode(String)}.
   * 
   * <p>To be overridden in subclasses for specific NetworkTopology 
   * implementations, as alternative to overriding the full {@link #add(Node)}
   *  method.
   * 
   * @param node The string representation of this node's network location is
   * used to retrieve a Node object. 
   * @return a reference to the node; null if the node is not in the tree
   * 
   * @see #add(Node)
   * @see #getNode(String)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getLeaves,org.apache.hadoop.net.NetworkTopology:getLeaves(java.lang.String),643,655,"/**
* Retrieves leaf nodes within a specified scope.
* @param scope the scope to search for nodes
* @return list of leaf Node objects
*/","return leaves in <i>scope</i>
   * @param scope a path string
   * @return leaves nodes under specific scope",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,countNumOfAvailableNodes,"org.apache.hadoop.net.NetworkTopology:countNumOfAvailableNodes(java.lang.String,java.util.Collection)",664,711,"/**
 * Calculates node count within a scope, excluding certain nodes.
 * @param scope the scope string to evaluate
 * @param excludedNodes collection of nodes to exclude from counting
 * @return count of nodes in scope minus excluded nodes
 */","return the number of leaves in <i>scope</i> but not in <i>excludedNodes</i>
   * if scope starts with ~, return the number of nodes that are not
   * in <i>scope</i> and <i>excludedNodes</i>; 
   * @param scope a path string that may start with ~
   * @param excludedNodes a list of nodes
   * @return number of available nodes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,interRemoveNodeWithEmptyRack,org.apache.hadoop.net.NetworkTopology:interRemoveNodeWithEmptyRack(org.apache.hadoop.net.Node),1104,1122,"/**
* Masks a node by removing it from its rack's node set.
* @param node the node to be masked
*/","* Internal function for update empty rack number
   * for remove or decommission a node.
   * @param node node to be removed; can be null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer,boolean)",970,1000,"/**
* Sorts nodes based on weights and applies a secondary sort if needed.
* @param reader node used for reading data
* @param nodes array of nodes to be sorted
* @param activeLen number of active nodes
* @param secondarySort optional secondary sorting logic
* @param nonDataNodeReader flag indicating if using non-data node reader
*/","* Sort nodes array by network distance to <i>reader</i>.
   * <p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   * And it helps choose node with more fast storage type.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param nonDataNodeReader True if the reader is not a datanode",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,recommissionNode,org.apache.hadoop.net.NetworkTopology:recommissionNode(org.apache.hadoop.net.Node),1040,1055,"/**
 * Masks a node by removing it and handling associated resources.
 * @param node the node to be masked
 * @throws IllegalArgumentException if the node is an inner node
 */","* Update empty rack number when add a node like recommission.
   * @param node node to be added; can be null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream$Reader:<init>(java.nio.channels.ReadableByteChannel,long)",50,53,"/**
* Initializes a new reader with a channel and timeout.
* @param channel the readable byte channel to use
* @param timeout the maximum time to wait for I/O operations
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream$Writer:<init>(java.nio.channels.WritableByteChannel,long)",55,58,"/**
* Initializes a writer with a specified channel and timeout.
* @param channel writable byte channel to write data
* @param timeout maximum time to wait for I/O operations
* @throws IOException if an I/O error occurs during initialization
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,write,org.apache.hadoop.net.SocketOutputStream:write(int),101,109,"/**
 * Converts an integer to a byte and calls another method.
 * @param b the integer value to convert
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,transferToFully,"org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int)",264,267,"/**
* Reads bytes from a file channel.
* @param fileCh FileChannel to read from
* @param position starting position in the file
* @param count number of bytes to read
*/","* Call
   * {@link #transferToFully(FileChannel, long, int, LongWritable, LongWritable)
   * }
   * with null <code>waitForWritableTime</code> and <code>transferToTime</code>.
   *
   * @param fileCh input fileCh.
   * @param position input position.
   * @param count input count.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/CachedDNSToSwitchMapping.java,resolve,org.apache.hadoop.net.CachedDNSToSwitchMapping:resolve(java.util.List),106,125,"/**
* Processes a list of names, caching and resolving them as needed.
* @param names list of names to process
* @return processed list of names
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,wrapException,"org.apache.hadoop.net.NetUtils:wrapException(java.lang.String,int,java.lang.String,int,java.io.IOException)",850,949,"/**
* Handles various socket exceptions by masking them with detailed messages.
* @param destHost destination host address
* @param destPort destination port number
* @param localHost local host address
* @param localPort local port number
* @param exception original IOException to be masked
* @return new IOException with a detailed message
*/","* Take an IOException , the local host port and remote host port details and
   * return an IOException with the input exception as the cause and also
   * include the host details. The new exception provides the stack trace of the
   * place where the exception is thrown and some extra diagnostics information.
   * If the exception is of type BindException, ConnectException,
   * UnknownHostException, SocketTimeoutException or has a String constructor,
   * return a new one of the same type; Otherwise return an IOException.
   *
   * @param destHost target host (nullable)
   * @param destPort target port
   * @param localHost local host (nullable)
   * @param localPort local port
   * @param exception the caught exception.
   * @return an exception to throw",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,<init>,org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:<init>(),144,144,"/**
* Constructs a new instance of RawScriptBasedMappingWithDependency.
*/","* Constructor. The mapping is not ready to use until
     * {@link #setConf(Configuration)} has been called",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,<init>,org.apache.hadoop.net.TableMapping:<init>(),63,65,"/**
* Initializes a new TableMapping with a default RawTableMapping.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping),95,97,"/**
 * Constructs a ScriptBasedMapping with a given DNSToSwitchMapping.
 * @param rawMap the initial mapping configuration
 */","* Create an instance from the given raw mapping
   * @param rawMap raw DNSTOSwithMapping",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,<init>,org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String),48,50,"/**
 * Constructs an InnerNodeImpl with a specified path.
 * @param path the node's path in the hierarchy
 */","* Construct an InnerNode from a path-like string.
   * @param path input path.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.ChRootedFileSystem)",99,102,"/**
* Constructs an NflyNode with specified host, rack, and file system.
* @param hostName name of the host node
* @param rackName name of the rack where the node is located
* @param fs ChRootedFileSystem instance associated with this node
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,<init>,"org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.InnerNode,int)",60,63,"/**
* Constructs an InnerNodeImpl.
* @param name node name
* @param location node location
* @param parent parent node
* @param level hierarchy level
*/","* Construct an InnerNode
   * from its name, its network location, its parent, and its level.
   * @param name input name.
   * @param location input location.
   * @param parent input parent.
   * @param level input level.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,kick,org.apache.hadoop.net.unix.DomainSocketWatcher:kick(),359,374,"/**
* Masks functionality with assertions and socket operations.
* Asserts lock, checks kick status, writes to socket, handles exceptions.
*/",* Wake up the DomainSocketWatcher thread.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,handle,org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler:handle(org.apache.hadoop.net.unix.DomainSocket),103,130,"/**
* Handles socket read operations.
* @param sock the DomainSocket to read from
* @return false if read succeeds, true if an IOException occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,bindAndListen,org.apache.hadoop.net.unix.DomainSocket:bindAndListen(java.lang.String),191,200,"/**
* Creates a DomainSocket from the specified path.
* @param path socket file path
* @return DomainSocket object
* @throws IOException if an I/O error occurs
*/","* Create a new DomainSocket listening on the given path.
   *
   * @param path         The path to bind and listen on.
   * @return             The new DomainSocket.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,socketpair,org.apache.hadoop.net.unix.DomainSocket:socketpair(),210,216,"/**
* Creates and returns an array of two anonymous DomainSockets.
* @return Array containing two DomainSocket instances
* @throws IOException if there is an I/O error during socket creation
*/","* Create a pair of UNIX domain sockets which are connected to each other
   * by calling socketpair(2).
   *
   * @return                An array of two UNIX domain sockets connected to
   *                        each other.
   * @throws IOException    on error.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,connect,org.apache.hadoop.net.unix.DomainSocket:connect(java.lang.String),255,261,"/**
* Creates a DomainSocket from the specified path.
* @param path file path for the socket
* @return DomainSocket object initialized with the given path and file descriptor
* @throws IOException if an I/O error occurs during socket creation
*/","* Create a new DomainSocket connected to the given path.
   *
   * @param path              The path to connect to.
   * @throws IOException      If there was an I/O error performing the connect.
   *
   * @return                  The new DomainSocket.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,sendCallbackAndRemove,"org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallbackAndRemove(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)",435,440,"/**
 * Handles masking operations for file descriptors.
 * @param caller identifier of the calling entity
 * @param entries map of integer keys to Entry objects
 * @param fdSet set containing file descriptor information
 * @param fd file descriptor number
 */","* Send callback, and if the domain socket was closed as a result of
   * processing, then also remove the entry for the file descriptor.
   *
   * @param caller reason for call
   * @param entries mapping of file descriptor to entry
   * @param fdSet set of file descriptors
   * @param fd file descriptor",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,isOpen,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:isOpen(),592,595,"/**
 * Delegates call to m1 on DomainSocket instance.
 * @return result of m1 invocation
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:close(),597,600,"/**
 * Calls the overridden m1 method of the enclosing class.
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:close(),558,561,"/**
 * Delegates call to parent's m1 method.
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,close,org.apache.hadoop.net.unix.DomainSocketWatcher:close(),266,284,"/**
 * Closes the resource and notifies sockets.
 * @throws IOException if an I/O error occurs
 */","* Close the DomainSocketWatcher and wait for its thread to terminate.
   *
   * If there is more than one close, all but the first will be ignored.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:close(),547,550,"/**
 * Calls the overridden m1 method of the DomainSocket class.
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,get,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:get(java.nio.channels.SelectableChannel),387,403,"/**
* Retrieves or creates a SelectorInfo for a channel.
* @param channel the SelectableChannel to process
* @return SelectorInfo associated with the channel's provider
* @throws IOException if an I/O error occurs
*/","* Takes one selector from end of LRU list of free selectors.
     * If there are no selectors awailable, it creates a new selector.
     * Also invokes trimIdleSelectors(). 
     * 
     * @param channel
     * @return 
     * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,release,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:release(org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo),411,417,"/**
* Updates selector activity time and processes it.
* @param info SelectorInfo object to update
*/","* puts selector back at the end of LRU list of free selectos.
     * Also invokes trimIdleSelectors().
     * 
     * @param info",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,getLeaf,"org.apache.hadoop.net.InnerNodeImpl:getLeaf(int,org.apache.hadoop.net.Node)",253,300,"/**
 * Retrieves a node by leaf index, excluding a specified node.
 * @param leafIndex index of the leaf node to retrieve
 * @param excludedNode node to exclude from consideration
 * @return Node object or null if not found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,remove,org.apache.hadoop.net.InnerNodeImpl:remove(org.apache.hadoop.net.Node),189,234,"/**
* Removes a node from the tree.
* @param n node to be removed
* @return true if node is successfully removed, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPs,org.apache.hadoop.net.DNS:getIPs(java.lang.String),152,155,"/**
 * Calls m1 with interface and default flag.
 * @param strInterface network interface name
 * @return array of IP addresses or empty if none found
 * @throws UnknownHostException if interface is invalid
 */","* @return Like {@link DNS#getIPs(String, boolean)}, but returns all
   * IPs associated with the given interface and its subinterfaces.
   *
   * @param strInterface input strInterface.
   * @throws UnknownHostException
   * If no IP address for the local host could be found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getHosts,"org.apache.hadoop.net.DNS:getHosts(java.lang.String,java.lang.String,boolean)",248,277,"/**
* Resolves hostnames for given network interface.
* @param strInterface network interface name
* @param nameserver optional DNS server to use
* @param tryfallbackResolution flag to attempt fallback resolution
* @return array of resolved hostnames or cached hostname if none found
* @throws UnknownHostException if no addresses are found
*/","* Returns all the host names associated by the provided nameserver with the
   * address bound to the specified network interface
   *
   * @param strInterface
   *            The name of the network interface or subinterface to query
   *            (e.g. eth0 or eth0:0)
   * @param nameserver
   *            The DNS host name
   * @param tryfallbackResolution
   *            if true and if reverse DNS resolution fails then attempt to
   *            resolve the hostname with
   *            {@link InetAddress#getCanonicalHostName()} which includes
   *            hosts file resolution.
   * @return A string vector of all host names associated with the IPs tied to
   *         the specified interface
   * @throws UnknownHostException if the given interface is invalid",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,read,org.apache.hadoop.net.SocketInputStream:read(),112,127,"/**
* Reads a single byte from the input source.
* @return integer representation of the byte or -1 if end of stream is reached
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,listBeans,"org.apache.hadoop.jmx.JMXJsonServlet:listBeans(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)",235,328,"/**
 * Generates JSON response listing beans and their attributes.
 * @param jg JsonGenerator for writing JSON output
 * @param qry ObjectName query to find MBeans
 * @param attribute specific attribute to fetch, or null for all
 * @param response HttpServletResponse to set status code
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseArguments,org.apache.hadoop.log.LogLevel$CLI:parseArguments(java.lang.String[]),141,170,"/**
* Parses command-line arguments for setting and getting levels with a specified protocol.
* @param args array of command-line arguments
* @throws HadoopIllegalArgumentException if required arguments are missing or invalid
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.util.Map)",117,132,"/**
* Prints help entries to the provided PrintStream.
* @param pStr stream to print output
* @param helpEntries map of command names to UsageInfo
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,<init>,org.apache.hadoop.log.LogThrottlingHelper:<init>(long),145,147,"/**
 * Initializes log throttling with a specified minimum period.
 * @param minLogPeriodMs minimum time between logs in milliseconds
 */","* Create a log helper without any primary recorder.
   *
   * @see #LogThrottlingHelper(long, String)
   * @param minLogPeriodMs input minLogPeriodMs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,record,org.apache.hadoop.log.LogThrottlingHelper:record(double[]),195,197,"/**
 * Records action with default recorder and current time.
 * @param values variable number of double values to log
 * @return LogAction object representing the recorded action
 */","* Record some set of values at the current time into this helper. Note that
   * this does <i>not</i> actually write information to any log. Instead, this
   * will return a LogAction indicating whether or not the caller should write
   * to its own log. The LogAction will additionally contain summary information
   * about the values specified since the last time the caller was expected to
   * write to its log.
   *
   * <p>Specifying multiple values will maintain separate summary statistics
   * about each value. For example:
   * <pre>{@code
   *   helper.record(1, 0);
   *   LogAction action = helper.record(3, 100);
   *   action.getStats(0); // == 2
   *   action.getStats(1); // == 50
   * }</pre>
   *
   * @param values The values about which to maintain summary information. Every
   *               time this method is called, the same number of values must
   *               be specified.
   * @return A LogAction indicating whether or not the caller should write to
   *         its log.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,getEvent,org.apache.hadoop.http.ProfileServlet:getEvent(javax.servlet.http.HttpServletRequest),366,373,"/**
* Retrieves or defaults to CPU event from request.
* @param req HTTP servlet request containing event parameter
* @return Event object, defaulting to CPU if none found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,main,org.apache.hadoop.http.HtmlQuoting:main(java.lang.String[]),215,224,"/**
 * Processes command line arguments by quoting and then unquoting them.
 * @param args array of command line arguments
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameter,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameter(java.lang.String),1804,1808,"/**
* Quotes HTML special characters in request parameter.
* @param name raw input string from request
* @return quoted string safe for HTML display
*/",* Unquote the name and quote the value.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameterValues,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterValues(java.lang.String),1810,1822,"/**
 * Processes and unquotes a name, then retrieves and processes values.
 * @param name the input name to process
 * @return an array of processed strings or null if no values found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameterMap,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterMap(),1824,1838,"/**
 * Processes and sanitizes request data.
 * @return Map of sanitized key-value pairs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getRequestURL,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getRequestURL(),1844,1848,"/**
* Constructs a StringBuffer with HTML-quoted URL from request.
* @return StringBuffer containing quoted URL
*/","* Quote the url so that users specifying the HOST HTTP header
       * can't inject attacks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getServerName,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getServerName(),1854,1857,"/**
 * Returns HTML-quoted request parameter.
 * @return quoted string from raw request
 */","* Quote the server name so that users specifying the HOST HTTP header
       * can't inject attacks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addAsyncProfilerServlet,"org.apache.hadoop.http.HttpServer2:addAsyncProfilerServlet(org.eclipse.jetty.server.handler.ContextHandlerCollection,org.apache.hadoop.conf.Configuration)",797,816,"/**
* Configures profiling endpoints for a given context.
* @param contexts collection of web application contexts
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addNoCacheFilter,org.apache.hadoop.http.HttpServer2:addNoCacheFilter(org.eclipse.jetty.servlet.ServletContextHandler),888,891,"/**
* Applies no-cache filter to all paths.
* @param ctxt Servlet context handler
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,makeConfigurationChangeMonitor,"org.apache.hadoop.http.HttpServer2$Builder:makeConfigurationChangeMonitor(long,org.eclipse.jetty.util.ssl.SslContextFactory$Server)",634,663,"/**
* Sets up a timer to periodically reload SSL keystores and truststores.
* @param reloadInterval interval in milliseconds for reloading
* @param sslContextFactory factory for creating SSL contexts
* @return Timer object managing the periodic task
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileMonitoringTimerTask.java,<init>,"org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.nio.file.Path,java.util.function.Consumer,java.util.function.Consumer)",61,64,"/**
* Initializes file monitoring task.
* @param filePath path to monitor for changes
* @param onFileChange action to perform on file change
* @param onChangeFailure action to take on failure
*/","* See {@link #FileMonitoringTimerTask(List, Consumer, Consumer)}.
   *
   * @param filePath The file to monitor.
   * @param onFileChange What to do when the file changes.
   * @param onChangeFailure What to do when <code>onFileChange</code>
   *                        throws an exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,close,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:close(),877,892,"/**
* Synchronizes access to the output stream and handles closing resources.
* Calls methods on currentOutStream if not null, then closes it.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRatesWithAggregation,org.apache.hadoop.metrics2.lib.MetricsRegistry:newRatesWithAggregation(java.lang.String),331,337,"/**
* Creates and registers a new MutableRatesWithAggregation for a given name.
* @param name unique identifier for the aggregation
* @return newly created MutableRatesWithAggregation instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,add,"org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,org.apache.hadoop.metrics2.lib.MutableMetric)",348,351,"/**
 * Updates metric in map under given name.
 * @param name unique identifier for the metric
 * @param metric the MutableMetric object to update
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsFactory.java,getAnnotatedMetricsFactory,org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getAnnotatedMetricsFactory(),33,35,"/**
 * Returns an instance of MutableMetricsFactory.
 * @return MutableMetricsFactory object
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,flush,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:flush(),863,875,"/**
* Masks the current file stream by flushing it.
* @throws IOException if an I/O error occurs during flushing
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,currentConfig,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:currentConfig(),348,358,"/**
* Serializes configuration to a string.
* @return serialized configuration as String
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getPluginLoader,org.apache.hadoop.metrics2.impl.MetricsConfig:getPluginLoader(),228,264,"/**
* Returns a ClassLoader for plugins.
* @return ClassLoader instance or null if none found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,toString,org.apache.hadoop.metrics2.impl.MetricsConfig:toString(org.apache.commons.configuration2.Configuration),287,298,"/**
* Masks configuration properties.
* @param c Configuration object to be masked
* @return Masked configuration as a UTF-8 string
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,consume,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:consume(org.apache.hadoop.metrics2.impl.MetricsBuffer),172,200,"/**
* Processes metrics buffer entries, filtering and pushing records to sink.
* @param buffer MetricsBuffer containing entries to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,tag,"org.apache.hadoop.metrics2.MetricStringBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",81,84,"/**
 * Masks and records metric information.
 * @param info metric metadata
 * @param value metric value
 * @return MetricsRecordBuilder instance for further operations
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.AbstractMetric),91,95,"/**
* Adds metrics to builder using AbstractMetric.
* @param metric source of metric data
* @return updated MetricsRecordBuilder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",102,105,"/**
 * Masks and records metric information.
 * @param info metric information
 * @param value metric value
 * @return MetricsRecordBuilder instance for further operations
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",107,110,"/**
 * Sets a metric value with masking.
 * @param info metric information
 * @param value metric value to set
 * @return updated MetricsRecordBuilder instance
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",112,115,"/**
 * Masks a metric with given info and value.
 * @param info metric information
 * @param value integer value to mask
 * @return updated MetricsRecordBuilder instance
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",117,120,"/**
 * Masks and records metric information.
 * @param info metric information
 * @param value metric value
 * @return updated MetricsRecordBuilder instance
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",122,125,"/**
 * Masks and records a metric.
 * @param info metric information
 * @param value metric value
 * @return updated MetricsRecordBuilder
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",127,130,"/**
 * Records a metric with masking.
 * @param info metric information
 * @param value numeric value of the metric
 * @return updated MetricsRecordBuilder instance
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,update,org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord),184,186,"/**
 * Calls overloaded method with default logging flag.
 * @param mr MetricsRecord to process
 * @return processed Record object
 */","* Update the cache and return the current cache record
   * @param mr the update record
   * @return the updated cache record",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,flush,org.apache.hadoop.metrics2.sink.GraphiteSink:flush(),110,122,"/**
* Flushes and closes metrics connection to Graphite.
* Logs errors during flush and rethrows exception on close failure.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,write,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:write(java.lang.String),167,174,"/**
* Processes message based on condition.
* @param msg message to be processed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,init,org.apache.hadoop.metrics2.sink.GraphiteSink:init(org.apache.commons.configuration2.SubsetConfiguration),54,68,"/**
* Initializes Graphite with configuration.
* @param conf configuration object for subset settings
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,writeMetrics,org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:writeMetrics(java.io.Writer),111,163,"/**
* Writes Prometheus metrics to a Writer.
* @param writer destination for metric output
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,consume,org.apache.hadoop.metrics2.impl.SinkQueue:consume(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer),65,75,"/**
* Processes an element using a consumer and handles cleanup.
* @param consumer function to process the element
* @throws InterruptedException if thread is interrupted during processing
*/","* Consume one element, will block if queue is empty
   * Only one consumer at a time is allowed
   * @param consumer  the consumer callback object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,consumeAll,org.apache.hadoop.metrics2.impl.SinkQueue:consumeAll(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer),82,94,"/**
* Executes a masked operation with a consumer.
* @param consumer function to apply to each element
*/","* Consume all the elements, will block if queue is empty
   * @param consumer  the consumer callback object
   * @throws InterruptedException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheHit,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheHit(),64,66,"/**
 * Calls m1 on cacheHit to handle cache hit.
 */",* One cache hit event,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheCleared,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheCleared(),71,73,"/**
 * Clears the cache by invoking m1 on cacheCleared.
 */",* One cache cleared,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheUpdated,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheUpdated(),78,80,"/**
 * Updates cache state.
 */",* One cache updated,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthenticationFailures,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationFailures(),215,217,"/**
 * Calls authentication failure handling method.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthenticationSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationSuccesses(),223,225,"/**
 * Calls authentication success method.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthorizationSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationSuccesses(),231,233,"/**
* Handles successful RPC authorization.
* Invokes m1 on rpcAuthorizationSuccesses object.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthorizationFailures,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationFailures(),239,241,"/**
 * Handles RPC authorization failures.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrClientBackoff,org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoff(),343,345,"/**
 * Initiates RPC client backoff process.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrClientBackoffDisconnected,org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoffDisconnected(),350,352,"/**
 * Calls m1 on rpcClientBackoffDisconnected to handle disconnection.
 */",* Client was disconnected due to backoff,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrSlowRpc,org.apache.hadoop.ipc.metrics.RpcMetrics:incrSlowRpc(),366,368,"/**
 * Executes slow RPC call m1.
 */",* Increments the Slow RPC counter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrRequeueCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:incrRequeueCalls(),373,375,"/**
 * Requeues RPC calls.
 */",* Increments the Requeue Calls counter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrRpcCallSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrRpcCallSuccesses(),380,382,"/**
* Executes RPC call to mask data.
*/",* One RPC call success event.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,channelWrite,"org.apache.hadoop.ipc.Server:channelWrite(java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)",3923,3932,"/**
* Writes data from ByteBuffer to channel.
* @param channel target WritableByteChannel
* @param buffer source ByteBuffer
* @return number of bytes written or -1 if error occurs
*/","* This is a wrapper around {@link WritableByteChannel#write(ByteBuffer)}.
   * If the amount of data is large, it writes to channel in smaller chunks. 
   * This is to avoid jdk from creating many direct buffers as the size of 
   * buffer increases. This also minimizes extra copies in NIO layer
   * as a result of multiple write operations required to write a large 
   * buffer.  
   *
   * @see WritableByteChannel#write(ByteBuffer)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,channelRead,"org.apache.hadoop.ipc.Server:channelRead(java.nio.channels.ReadableByteChannel,java.nio.ByteBuffer)",3943,3952,"/**
* Reads data from a channel into a buffer.
* @param channel source of the data
* @param buffer destination for the read data
* @return number of bytes read or -1 if end of stream
*/","* This is a wrapper around {@link ReadableByteChannel#read(ByteBuffer)}.
   * If the amount of data is large, it writes to channel in smaller chunks. 
   * This is to avoid jdk from creating many direct buffers as the size of 
   * ByteBuffer increases. There should not be any performance degredation.
   * 
   * @see ReadableByteChannel#read(ByteBuffer)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,getRecord,org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:getRecord(),152,157,"/**
* Creates a metrics record if conditions are met.
* @return MetricsRecordImpl object or null if not acceptable or filter fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,newAttrInfo,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",58,60,"/**
 * Creates MBeanAttributeInfo using MetricsInfo and type.
 * @param info source of metric information
 * @param type attribute data type
 * @return MBeanAttributeInfo instance
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,get,org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:get(),96,112,"/**
 * Generates MBeanInfo for metrics records.
 * Initializes current record number and processes each record's tags and metrics.
 * Logs the attributes and returns MBeanInfo object.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateAttrCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateAttrCache(java.lang.Iterable),250,268,"/**
* Processes and updates metric records.
* @param lastRecs iterable of the latest metric records
* @return total number of processed metrics and tags
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newObjectName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newObjectName(java.lang.String),128,137,"/**
* Creates and returns an ObjectName for a given metric name.
* @param name metric name to be converted
* @return ObjectName instance if valid, throws exception otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newSourceName,"org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newSourceName(java.lang.String,boolean)",147,156,"/**
 * Masks the given metric source name.
 * @param name the metric source name
 * @param dupOK flag to allow duplicates
 * @return masked name or original if allowed duplicates
 * @throws MetricsException if duplicate not allowed and name exists
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,putMetrics,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,long)",96,107,"/**
 * Enqueues buffer if logical time is a multiple of period.
 * @param buffer the MetricsBuffer to enqueue
 * @param logicalTimeMs the current logical time in milliseconds
 * @return true if buffer was enqueued or not applicable, false if dropped
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,putMetricsImmediate,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer),109,126,"/**
 * Processes metrics buffer by adding it to the queue and waiting for fulfillment.
 * @param buffer metrics buffer to process
 * @return true if processing is successful, false otherwise
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,diskCheckFailed,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:diskCheckFailed(),146,149,"/**
 * Increments failure count and records last failure time.
 */",* Increase the failure count and update the last failure timestamp.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,fetchGroupSet,org.apache.hadoop.security.Groups$GroupCacheLoader:fetchGroupSet(java.lang.String),413,424,"/**
* Retrieves user groups with performance monitoring.
* @param user username for which to fetch groups
* @return set of group names associated with the user
*/","* Queries impl for groups belonging to the user.
     * This could involve I/O and take awhile.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,shutdown,org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:shutdown(),66,69,"/**
 * Resets metrics and implementation.
 * Clears JvmMetrics and sets impl to null.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,registerIfNeeded,org.apache.hadoop.metrics2.source.JvmMetrics:registerIfNeeded(),72,79,"/**
* Initializes JVM metrics if not already present.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,create,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:create(),147,149,"/**
 * Creates and returns an instance of UgiMetrics.
 * @return UgiMetrics object initialized by metrics system
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:shutdown(),105,107,"/**
* Masks metrics system with specified name.
* @param name identifier for the metric system
*/",* Shutdown the instrumentation process.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:shutdown(),108,110,"/**
 * Registers a metric with the default metrics system.
 * @param name the name of the metric to register
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.RpcMetrics:shutdown(),247,249,"/**
 * Masks the given name using metrics system.
 * @param name the name to be masked
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,registerMetrics2Source,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:registerMetrics2Source(java.lang.String),891,894,"/**
* Registers metrics system with specified namespace.
* @param namespace unique identifier for metric grouping
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,unregister,org.apache.hadoop.metrics2.util.MBeans:unregister(javax.management.ObjectName),136,149,"/**
* Unregisters an MBean and logs errors.
* @param mbeanName the name of the MBean to unregister
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2Metrics.java,remove,org.apache.hadoop.http.HttpServer2Metrics:remove(),161,163,"/**
 * Initializes metrics system with HTTP server port.
 * @param port HTTP server port number
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,snapshot,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",228,230,"/**
 * Delegates to registry's m1 method.
 * @param rb MetricsRecordBuilder instance
 * @param all flag indicating whether to include all metrics
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRates:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",80,83,"/**
 * Delegates MetricsRecordBuilder processing to registry.
 * @param rb builder for metrics record
 * @param all flag to include all metrics
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,setQuantiles,"org.apache.hadoop.metrics2.lib.MutableQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)",117,125,"/**
 * Generates and processes percentile-based metrics.
 * @param ucName prefix for metric name
 * @param uvName suffix for metric name
 * @param desc description of the metric
 * @param lvName level name for the metric
 * @param pDecimalFormat formatter for percentiles
 */","* Sets quantileInfo.
   *
   * @param ucName capitalized name of the metric
   * @param uvName capitalized type of the values
   * @param desc uncapitalized long-form textual description of the metric
   * @param lvName uncapitalized type of the values
   * @param pDecimalFormat Number formatter for percentile value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,setQuantiles,"org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)",75,83,"/**
 * Generates and processes quantile data.
 * @param ucName prefix for name template
 * @param uvName suffix for name template
 * @param desc description text
 * @param lvName level name
 * @param df DecimalFormat instance for formatting
 */","* Sets quantileInfo.
   *
   * @param ucName capitalized name of the metric
   * @param uvName capitalized type of the values
   * @param desc uncapitalized long-form textual description of the metric
   * @param lvName uncapitalized type of the values
   * @param df Number formatter for inverse percentile value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,<init>,org.apache.hadoop.metrics2.lib.MetricsRegistry:<init>(java.lang.String),49,51,"/**
 * Initializes a new MetricsRegistry with a given name.
 * @param name the name of the registry
 */","* Construct the registry with a record name
   * @param name  of the record of the metrics",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",169,197,"/**
 * Adds average metrics to the builder.
 * @param builder MetricsRecordBuilder to add data to
 * @param all whether to include all metrics
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,tag,"org.apache.hadoop.metrics2.lib.Interns:tag(java.lang.String,java.lang.String,java.lang.String)",162,164,"/**
* Creates and caches a metrics tag.
* @param name tag name
* @param description tag description
* @param value tag value
* @return MetricsTag object
*/","* Get a metrics tag.
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @return an interned metrics tag",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(java.lang.Class,org.apache.hadoop.metrics2.annotation.Metrics)",141,146,"/**
* Generates MetricsInfo from class and annotation.
* @param cls the class to process
* @param annotation the Metrics annotation
* @return MetricsInfo object with processed name and description
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.String)",162,174,"/**
* Processes metric annotation to create MetricsInfo.
* @param annotation Metric annotation containing metric data
* @param defaultName Default name if annotation value is missing
* @return MetricsInfo object with processed metric details
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)",64,85,"/**
 * Initializes a MutableStat with various statistical information.
 * @param name base name of the statistic
 * @param description detailed description of the statistic
 * @param sampleName name for samples in statistics
 * @param valueName name for values in statistics
 * @param extended flag indicating if extended stats are enabled
 */","* Construct a sample statistics metric
   * @param name        of the metric
   * @param description of the metric
   * @param sampleName  of the metric (e.g. ""Ops"")
   * @param valueName   of the metric (e.g. ""Time"", ""Latency"")
   * @param extended    create extended stats (stdev, min/max etc.) by default.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,metricInfo,org.apache.hadoop.metrics2.lib.MethodMetric:metricInfo(java.lang.reflect.Method),145,147,"/**
* Generates metrics info for a given method.
* @param method the Method object to process
* @return MetricsInfo object containing metric details
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getGcInfo,org.apache.hadoop.metrics2.source.JvmMetrics:getGcInfo(java.lang.String),209,222,"/**
* Retrieves or creates GC metrics info.
* @param gcName name of the garbage collector
* @return MetricsInfo array containing GC count and time metrics
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addUniqueIdentityCount,org.apache.hadoop.ipc.DecayRpcScheduler:addUniqueIdentityCount(org.apache.hadoop.metrics2.MetricsRecordBuilder),1030,1033,"/**
* Adds metric for unique callers to record builder.
* @param rb MetricsRecordBuilder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addDecayedCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1036,1039,"/**
* Adds decayed call volume metric to record builder.
* @param rb MetricsRecordBuilder instance to add metric to
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addRawCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1041,1044,"/**
 * Records call volume metric.
 * @param rb MetricsRecordBuilder instance to add the metric to
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addServiceUserDecayedCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1047,1051,"/**
* Records decayed call volume metric.
* @param rb MetricsRecordBuilder instance to record metrics
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addServiceUserRawCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1054,1058,"/**
* Records service user call volume metric.
* @param rb MetricsRecordBuilder instance to add the metric to
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addCallVolumePerPriority,org.apache.hadoop.ipc.DecayRpcScheduler:addCallVolumePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder),1061,1067,"/**
* Records call volumes for each priority level.
* @param rb MetricsRecordBuilder to record metrics
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addAvgResponseTimePerPriority,org.apache.hadoop.ipc.DecayRpcScheduler:addAvgResponseTimePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder),1070,1076,"/**
 * Records average response times for each priority level.
 * @param rb MetricsRecordBuilder instance to record metrics
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSystem,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSystem(),488,490,"/**
 * Masks function by injecting tags with host information.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,tag,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",73,79,"/**
* Adds a metric tag with info and value.
* @param info metric information
* @param value metric value
* @return this MetricsRecordBuilderImpl instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String,boolean)",415,420,"/**
* Registers metrics with a registry.
* @param info metrics information
* @param value metric value
* @param override flag to override existing metrics
* @return MetricsRegistry instance
*/","* Add a tag to the metrics
   * @param info  metadata of the tag
   * @param value of the tag
   * @param override existing tag if true
   * @return the registry (for keep adding tags etc.)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr(),42,45,"/**
 * Calls overloaded method m1 with default float value.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:decr(),47,50,"/**
 * Calls function m1 with -1.0f as argument.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,add,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:add(double),178,180,"/**
 * Calls synchronized method on static instance.
 * @param x double value to pass to method
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,add,org.apache.hadoop.metrics2.lib.MutableStat:add(long),132,136,"/**
 * Updates statistics with given value.
 * @param value the value to update statistics with
 */","* Add a snapshot to the metric.
   * @param value of the metric",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,"org.apache.hadoop.metrics2.util.SampleStat:reset(long,double,double,org.apache.hadoop.metrics2.util.SampleStat$MinMax)",48,53,"/**
* Updates statistics with new sample data.
* @param numSamples1 number of samples
* @param mean1 mean value of samples
* @param s1 standard deviation of samples
* @param minmax1 MinMax object containing min and max values
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,snapshotInto,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:snapshotInto(org.apache.hadoop.metrics2.lib.MutableRate),182,187,"/**
* Updates metric with statistical data.
* @param metric the MutableRate object to update
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newImpl,org.apache.hadoop.metrics2.lib.MethodMetric:newImpl(org.apache.hadoop.metrics2.annotation.Metric$Type),55,70,"/**
* Returns a MutableMetric based on the metric type.
* @param metricType type of metric to create
* @return MutableMetric object or null if unsupported
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,toString,org.apache.hadoop.metrics2.util.SampleStat:toString(),145,156,"/**
* Generates a summary string with statistical data.
* @return formatted string containing samples, min, mean, std dev, and max values
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingStdDev,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingStdDev(),412,414,"/**
 * Retrieves processing time from RPC.
 * @return Processing time value as a double
 */","* Return Standard Deviation of the Processing Time.
   * @return  double",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingStdDev,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingStdDev(),445,447,"/**
 * Retrieves processing time from deferred RPC.
 * @return Processing time as a double value
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,insert,org.apache.hadoop.metrics2.util.SampleQuantiles:insert(long),113,123,"/**
* Adds value to buffer and processes it if full.
* @param v the value to add
*/","* Add a new value from the stream.
   * 
   * @param v v.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,snapshot,org.apache.hadoop.metrics2.util.SampleQuantiles:snapshot(),236,250,"/**
* Computes and returns a map of Quantile to Long.
* @return Map containing quantiles mapped to their respective long values, or null if samples are invalid
*/","* Get a snapshot of the current values of all the tracked quantiles.
   * 
   * @return snapshot of the tracked quantiles. If no items are added
   * to the estimator, returns null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTopTokenRealOwners,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTopTokenRealOwners(int),880,898,"/**
* Generates a sorted list of top N NameValuePairs based on token owner stats.
* @param n number of top entries to retrieve
* @return sorted List of NameValuePair objects
*/","* Return top token real owners list as well as the tokens count.
   *
   * @param n top number of users
   * @return map of owners to counts",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getTopCallers,org.apache.hadoop.ipc.DecayRpcScheduler:getTopCallers(int),1099,1112,"/**
 * Retrieves top N callers based on call costs.
 * @param n number of top callers to retrieve
 * @return TopN object containing top callers and their costs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,cacheGroupsAdd,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List),90,103,"/**
* Processes groups by checking conditions and updating cache.
* @param groups list of group identifiers to process
*/","* Add a group to cache, only netgroups are cached
   *
   * @param groups list of group names to add to cache",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getTokens,org.apache.hadoop.security.UserGroupInformation:getTokens(),1724,1729,"/**
* Retrieves and masks tokens.
* @return Collection of masked Token objects
*/","* Obtain the collection of tokens associated with this user.
   * 
   * @return an unmodifiable collection of tokens associated with user",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/User.java,<init>,org.apache.hadoop.security.User:<init>(java.lang.String),42,44,"/**
 * Constructs a new User with a given name.
 * @param name user's full name
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroups,org.apache.hadoop.security.Groups:getGroups(java.lang.String),213,217,"/**
 * Masks user data.
 * @param user input user string
 * @return list of masked strings
 * @deprecated use alternative method
 */","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * Note this method can be expensive as it involves Set {@literal ->} List
   * conversion. For user with large group membership
   * (i.e., {@literal >} 1000 groups), we recommend using getGroupSet
   * to avoid the conversion and fast membership look up via contains().
   * @param user User's name
   * @return the group memberships of the user as list
   * @throws IOException if user does not exist
   * @deprecated Use {@link #getGroupsSet(String user)} instead.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroupsSet,org.apache.hadoop.security.Groups:getGroupsSet(java.lang.String),231,233,"/**
 * Masks sensitive information from a user string.
 * @param user input user data
 * @return set of masked strings
 * @throws IOException if an I/O error occurs
 */","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * This provide better performance when user has large group membership via
   * <br>
   * 1) avoid {@literal set->list->set} conversion for the caller
   * UGI/PermissionCheck <br>
   * 2) fast lookup using contains() via Set instead of List
   * @param user User's name
   * @return the group memberships of the user as set
   * @throws IOException if user does not exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,getGroups,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String),67,73,"/**
* Retrieves and caches user's group memberships.
* @param user the username to query
* @return list of group names associated with the user
* @throws IOException if an I/O error occurs during retrieval
*/","* Gets unix groups and netgroups for the user.
   *
   * It gets all unix groups as returned by id -Gn but it
   * only returns netgroups that are used in ACLs (there is
   * no way to get all netgroups for a given user, see
   * documentation for getent netgroup)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,println,org.apache.hadoop.security.KDiag:println(),868,870,"/**
 * Recursively calls itself with an empty string.
 */",* Print a new line,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printSysprop,org.apache.hadoop.security.KDiag:printSysprop(java.lang.String),897,900,"/**
 * Logs the value of a system property.
 * @param property name of the system property to log
 */","* Print a system property, or {@link #UNSET} if unset.
   * @param property property to print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printEnv,org.apache.hadoop.security.KDiag:printEnv(java.lang.String),916,919,"/**
* Masks environment variable value.
* @param variable name of the environment variable
*/","* Print an environment variable's name and value; printing
   * {@link #UNSET} if it is not set.
   * @param variable environment variable",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dump,org.apache.hadoop.security.KDiag:dump(java.io.File),926,932,"/**
* Reads and prints each line from a file.
* @param file the File to read from
*/","* Dump any file to standard out.
   * @param file file to dump
   * @throws IOException IO problems",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,error,"org.apache.hadoop.security.KDiag:error(java.lang.String,java.lang.String,java.lang.Object[])",1011,1013,"/**
* Logs an error message with a category and formatted arguments.
* @param category error category
* @param message error message template
* @param args optional arguments for the message template
*/","* Print a message as an error
   * @param category error category
   * @param message format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,warn,"org.apache.hadoop.security.KDiag:warn(java.lang.String,java.lang.String,java.lang.Object[])",1020,1022,"/**
* Logs a warning message with formatted details.
* @param category type of warning category
* @param message base message to log
* @param args additional arguments for formatting the message
*/","* Print a message as an warning
   * @param category error category
   * @param message format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullUserMap,org.apache.hadoop.security.ShellBasedIdMapping:loadFullUserMap(),359,370,"/**
 * Updates user ID to name mapping.
 * @throws IOException if command execution fails
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullGroupMap,org.apache.hadoop.security.ShellBasedIdMapping:loadFullGroupMap(),372,384,"/**
* Updates group ID to name mapping.
* Initializes bidirectional map with OS-specific commands.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod),1844,1846,"/**
 * Processes authentication using specified method.
 * @param authMethod authentication method to use
 */","* Sets the authentication method in the subject
   * 
   * @param authMethod authMethod.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,org.apache.hadoop.security.SaslOutputStream:write(int),123,131,"/**
* Writes a single byte to the output stream.
* @param b byte value to write
* @throws IOException if an I/O error occurs
*/","* Writes the specified byte to this output stream.
   * 
   * @param b
   *          the <code>byte</code>.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,org.apache.hadoop.security.SaslOutputStream:write(byte[]),148,151,"/**
 * Calls overloaded method with full array range.
 * @param b byte array to process
 */","* Writes <code>b.length</code> bytes from the specified byte array to this
   * output stream.
   * <p>
   * The <code>write</code> method of <code>SASLOutputStream</code> calls the
   * <code>write</code> method of three arguments with the three arguments
   * <code>b</code>, <code>0</code>, and <code>b.length</code>.
   * 
   * @param b
   *          the data.
   * @exception NullPointerException
   *              if <code>b</code> is null.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createKeyManagers,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyManagers(),1078,1088,"/**
* Initializes KeyManagers from keystore.
* @return array of KeyManager objects or null if keystore location is invalid
* @throws IOException on I/O errors accessing the keystore
* @throws GeneralSecurityException on security issues
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createTrustManagers,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createTrustManagers(),1090,1100,"/**
* Initializes and returns a TrustManager array.
* @throws IOException if there is an I/O error
* @throws GeneralSecurityException if security setup fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,doFilter,"org.apache.hadoop.security.http.RestCsrfPreventionFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",205,212,"/**
* Processes HTTP requests and responses.
* @param request incoming servlet request
* @param response outgoing servlet response
* @param chain filter chain for subsequent filters
* @throws IOException if an I/O error occurs
* @throws ServletException if a servlet exception occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,doFilter,"org.apache.hadoop.security.http.CrossOriginFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",92,98,"/**
* Filters and processes requests.
* @param req incoming HTTP request object
* @param res outgoing HTTP response object
* @param chain filter chain for further processing
* @throws IOException if an I/O error occurs
* @throws ServletException if a servlet exception occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,init,org.apache.hadoop.security.http.CrossOriginFilter:init(javax.servlet.FilterConfig),84,90,"/**
* Initializes filter configuration.
* @param filterConfig configuration settings for the filter
* @throws ServletException if initialization fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,getKey,org.apache.hadoop.security.token.delegation.DelegationKey:getKey(),75,82,"/**
* Generates a SecretKey from keyBytes.
* @return SecretKey object or null if keyBytes is invalid
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,checkToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),528,546,"/**
 * Validates a token and returns its information.
 * @param identifier token identifier to validate
 * @return DelegationTokenInformation if valid
 * @throws InvalidToken if token is not found or expired
 */","* Find the DelegationTokenInformation for the given token id, and verify that
   * if the token is expired. Note that this method should be called with 
   * acquiring the secret manager's monitor.
   *
   * @param identifier identifier.
   * @throws InvalidToken invalid token exception.
   * @return DelegationTokenInformation.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(),703,705,"/**
 * Constructs a new instance with default values.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,logExpireTokens,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logExpireTokens(java.util.Collection),786,793,"/**
* Processes and removes expired tokens.
* @param expiredTokens collection of expired TokenIdent objects
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,setExternalDelegationTokenSecretManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager),143,146,"/**
 * Delegates call to token manager.
 * @param secretManager AbstractDelegationTokenSecretManager instance
 */","* Sets an external <code>DelegationTokenSecretManager</code> instance to
   * manage creation and verification of Delegation Tokens.
   * <p>
   * This is useful for use cases where secrets must be shared across multiple
   * services.
   *
   * @param secretManager a <code>DelegationTokenSecretManager</code> instance",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,destroy,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:destroy(),189,193,"/**
 * Delegates tasks to token manager and auth handler.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,updateCurrentKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateCurrentKey(),440,456,"/**
 * Updates the current master key for delegation tokens.
 * @throws IOException if an I/O error occurs during key update
 */","* Update the current master key 
   * This is called once by startThreads before tokenRemoverThread is created, 
   * and only by tokenRemoverThread afterwards.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,<init>,org.apache.hadoop.security.token.delegation.DelegationKey:<init>(),47,49,"/**
 * Constructs a new DelegationKey with default values.
 */",Default constructore required for Writable,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",183,188,"/**
* Initializes a new instance with given authenticator and configurator.
* @param authenticator the token authenticator to use
* @param connConfigurator configuration for connections
*/","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   *
   * @param authenticator the {@link DelegationTokenAuthenticator} instance to
   * use, if <code>null</code> the default one will be used.
   * @param connConfigurator a connection configurator.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,renew,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:renew(),126,151,"/**
 * Checks and renews delegation token if valid.
 * @return true if token is valid, false otherwise
 * @throws IOException if an I/O error occurs
 * @throws InterruptedException if thread is interrupted
 */","* Renew or replace the delegation token for this file system.
     * It can only be called when the action is not in the queue.
     * @return
     * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,cancel,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:cancel(),153,158,"/**
* Processes file system operations.
* @throws IOException if an I/O error occurs
* @throws InterruptedException if the thread is interrupted
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(),568,573,"/**
 * Reads a single byte from input.
 * @return the byte read or -1 if end of stream
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[]),575,578,"/**
 * Calls overloaded method with full byte array.
 * @param b byte array to process
 * @return result of processing
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setSaslClient,org.apache.hadoop.ipc.Client$IpcStreams:setSaslClient(org.apache.hadoop.security.SaslRpcClient),1905,1910,"/**
* Masks data using SASL client.
* @param client SASL RPC client instance
* @throws IOException if I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,noPasswordWarning,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordWarning(),348,352,"/**
* Retrieves password using environment variable and file key.
* @return Password string or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,noPasswordWarning,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordWarning(),315,319,"/**
 * Retrieves keystore password from environment variable or file.
 * @return keystore password as a string
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,noPasswordError,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordError(),354,358,"/**
 * Retrieves a credential password using environment variable and file key.
 * @return credential password as a string
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,noPasswordError,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordError(),321,325,"/**
 * Retrieves keystore password from environment variable or file.
 * @return keystore password as a string
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(),193,207,"/**
* Reads the next byte from input stream.
* @return next byte as an integer or -1 if end of stream
* @throws IOException if an I/O error occurs
*/","* Reads the next byte of data from this input stream. The value byte is
   * returned as an <code>int</code> in the range <code>0</code> to
   * <code>255</code>. If no byte is available because the end of the stream has
   * been reached, the value <code>-1</code> is returned. This method blocks
   * until input data is available, the end of the stream is detected, or an
   * exception is thrown.
   * <p>
   * 
   * @return the next byte of data, or <code>-1</code> if the end of the stream
   *         is reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,"org.apache.hadoop.security.SaslInputStream:read(byte[],int,int)",248,275,"/**
* Reads bytes from input stream into buffer.
* @param b destination byte array
* @param off offset in the buffer
* @param len number of bytes to read
* @return number of bytes read or -1 if end of stream
*/","* Reads up to <code>len</code> bytes of data from this input stream into an
   * array of bytes. This method blocks until some input is available. If the
   * first argument is <code>null,</code> up to <code>len</code> bytes are read
   * and discarded.
   * 
   * @param b
   *          the buffer into which the data is read.
   * @param off
   *          the start offset of the data.
   * @param len
   *          the maximum number of bytes read.
   * @return the total number of bytes read into the buffer, or <code>-1</code>
   *         if there is no more data because the end of the stream has been
   *         reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ImpersonationProvider.java,authorize,"org.apache.hadoop.security.authorize.ImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",51,58,"/**
 * Calls m2 with user and resolved InetAddress.
 * @param user UserGroupInformation object
 * @param remoteAddress IP address as string
 * @throws AuthorizationException if resolution fails or access is denied
 */","* Authorize the superuser which is doing doAs.
   * {@link #authorize(UserGroupInformation, InetAddress)} should
   *             be preferred to avoid possibly re-resolving the ip address.
   * @param user ugi of the effective or proxy user which contains a real user.
   * @param remoteAddress the ip address of client.
   * @throws AuthorizationException Authorization Exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKeytab,org.apache.hadoop.security.UserGroupInformation:getKeytab(),814,819,"/**
* Retrieves masked value using Hadoop login context.
* @return Masked string or null if login fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isHadoopLogin,org.apache.hadoop.security.UserGroupInformation:isHadoopLogin(),825,828,"/**
 * Checks if m1() returns a non-null value.
 * @return true if m1() is not null, false otherwise
 */","* Is the ugi managed by the UGI or an external subject?
   * @return true if managed by UGI.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createProxyUser,"org.apache.hadoop.security.UserGroupInformation:createProxyUser(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",1522,1537,"/**
* Creates a UserGroupInformation object for proxy users.
* @param user the proxy user's name
* @param realUser the actual user being proxied
* @return UserGroupInformation representing the proxy setup
*/","* Create a proxy user using username of the effective user and the ugi of the
   * real user.
   * @param user user.
   * @param realUser realUser.
   * @return proxyUser ugi",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getName,org.apache.hadoop.security.UserGroupInformation$RealUser:getName(),472,475,"/**
 * Returns masked string from real user.
 * @return Masked string representation
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,shouldBackOff,org.apache.hadoop.ipc.DecayRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable),720,745,"/**
* Determines if a task should be backed off based on response times.
* @param obj the schedulable task to evaluate
* @return true if task should be backed off, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealUserOrSelf,org.apache.hadoop.security.UserGroupInformation:getRealUserOrSelf(org.apache.hadoop.security.UserGroupInformation),1558,1564,"/**
* Masks user information, returning the underlying user if available.
* @param user the original UserGroupInformation object
* @return masked UserGroupInformation or original if masking fails
*/","* If this is a proxy user, get the real user. Otherwise, return
   * this user.
   * @param user the user to check
   * @return the real user or self",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,toString,org.apache.hadoop.security.UserGroupInformation:toString(),1819,1827,"/**
 * Constructs a detailed message string.
 * @return formatted message including authentication and source information
 */",* Return the username.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod(),1863,1869,"/**
 * Retrieves authentication method for user group info.
 * @return AuthenticationMethod object
 */","* Get the authentication method from the real user's subject.  If there
   * is no real user, return the given user's authentication method.
   * 
   * @return AuthenticationMethod in the subject, null if not present.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation),1878,1885,"/**
* Determines the authentication method for a user group.
* @param ugi UserGroupInformation object representing the user group
* @return AuthenticationMethod used by the user group
*/","* Returns the authentication method of a ugi. If the authentication method is
   * PROXY, returns the authentication method of the real user.
   * 
   * @param ugi ugi.
   * @return AuthenticationMethod",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeIpcConnectionContext,"org.apache.hadoop.util.ProtoUtil:makeIpcConnectionContext(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",91,122,"/**
* Creates an IPC connection context.
* @param protocol communication protocol
* @param ugi user group information
* @param authMethod authentication method
* @return IpcConnectionContextProto object
*/","* This method creates the connection context  using exactly the same logic
   * as the old connection context as was done for writable where
   * the effective and real users are set based on the auth method.
   *
   * @param protocol protocol.
   * @param ugi ugi.
   * @param authMethod authMethod.
   * @return IpcConnectionContextProto.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,close,org.apache.hadoop.ipc.Server$ConnectionManager:close(org.apache.hadoop.ipc.Server$Connection),4110,4126,"/**
* Checks if a connection exists and disconnects it.
* @param connection the database connection to check and disconnect
* @return true if the connection existed and was disconnected, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UserIdentityProvider.java,makeIdentity,org.apache.hadoop.ipc.UserIdentityProvider:makeIdentity(org.apache.hadoop.ipc.Schedulable),28,35,"/**
* Masks scheduling object.
* @param obj Schedulable object to be masked
* @return Masked string or null if UserGroupInformation is null
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,verify,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:verify(java.lang.String,javax.net.ssl.SSLSession)",262,273,"/**
* Validates SSL session certificate for given host.
* @param host target hostname
* @param session SSL session object
* @return true if validation passes, false otherwise
*/","* The javax.net.ssl.HostnameVerifier contract.
         *
         * @param host    'hostname' we used to create our socket
         * @param session SSLSession with the remote server
         * @return true if the host matched the one in the certificate.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,java.security.cert.X509Certificate)",280,284,"/**
 * Calls overloaded method with single host.
 * @param host target server host
 * @param cert SSL certificate for authentication
 * @throws SSLException if SSL handshake fails
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],javax.net.ssl.SSLSocket)",292,347,"/**
 * Verifies SSL session certificates against host.
 * @param host array of hostnames to verify
 * @param ssl SSLSocket for the connection
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadResource,org.apache.hadoop.util.FindClass:loadResource(java.lang.String),172,180,"/**
* Masks a function by name.
* @param name function identifier
* @return status code (SUCCESS or E_NOT_FOUND)
*/","* Load a resource
   * @param name resource name
   * @return the status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,<init>,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:<init>(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),132,151,"/**
 * Initializes DelegatingSSLSocketFactory with specified SSL channel mode.
 * @param preferredChannelMode desired SSL channel mode
 * @throws IOException if initialization fails due to algorithm or key management issues
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,<init>,org.apache.hadoop.fs.shell.Command:<init>(org.apache.hadoop.conf.Configuration),86,88,"/**
 * Constructs a new Command instance with the given configuration.
 * @param conf Configuration settings for the command
 */","* Constructor.
   *
   * @param conf configuration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,<init>,org.apache.hadoop.fs.shell.CommandFactory:<init>(org.apache.hadoop.conf.Configuration),53,55,"/**
 * Constructs a CommandFactory with the given configuration.
 * @param conf Configuration object containing settings
 */","* Factory constructor for commands
   * @param conf the hadoop configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem:<init>(),777,779,"/**
 * Constructs a new FileSystem instance with no parent.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell:<init>(org.apache.hadoop.conf.Configuration),75,77,"/**
 * Constructs an FsShell instance.
 * @param conf configuration settings for the shell
 */","* Construct a FsShell with the given configuration.  Commands can be
   * executed via {@link #run(String[])}
   * @param conf the hadoop configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,org.apache.hadoop.io.ObjectWritable$NullInstance:<init>(),109,109,"/**
 * Constructs a NullInstance with a null superclass reference.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,"org.apache.hadoop.io.ObjectWritable$NullInstance:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)",110,113,"/**
 * Constructs a NullInstance with specified class and configuration.
 * @param declaredClass the class to be associated with this instance
 * @param conf the configuration settings for this instance
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintWriter,java.io.File,java.lang.String,long,boolean)",171,184,"/**
* Initializes KDiag with configuration and security settings.
* @param conf Configuration object
* @param out PrintWriter for output
* @param keytab File containing Kerberos keytab data
* @param principal Kerberos principal name
* @param minKeyLength Minimum key length requirement
* @param securityRequired Flag indicating if security is required
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,<init>,org.apache.hadoop.util.FindClass:<init>(org.apache.hadoop.conf.Configuration),131,133,"/**
 * Constructs a new instance of FindClass.
 * @param conf Configuration object to initialize with
 */","* Create a class with a specified configuration
   * @param conf configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,<init>,"org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintStream)",53,56,"/**
 * Constructs a new instance with configuration and output stream.
 * @param conf Configuration object
 * @param out PrintStream for output
 */","* Used exclusively for testing.
   * 
   * @param conf The configuration to use.
   * @param out The PrintStream to write to, instead of System.out",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configured.java,<init>,org.apache.hadoop.conf.Configured:<init>(),32,34,"/**
 * Default constructor initializes with no configuration.
 */",Construct a Configured.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,<init>,org.apache.hadoop.ha.HAAdmin:<init>(org.apache.hadoop.conf.Configuration),103,105,"/**
* Constructs an HAAdmin instance with the given configuration.
* @param conf Hadoop configuration settings
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByNameWithSearch,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByNameWithSearch(java.lang.String),706,718,"/**
 * Resolves InetAddress from host name.
 * @param host the hostname to resolve
 * @return InetAddress object or null if resolution fails
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getAppConfigurationEntry,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getAppConfigurationEntry(java.lang.String),2211,2230,"/**
 * Generates configuration entries for an application.
 * @param appName name of the application
 * @return array of AppConfigurationEntry objects
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,parseStaticMap,org.apache.hadoop.security.ShellBasedIdMapping:parseStaticMap(java.io.File),588,630,"/**
* Parses static mapping from file.
* @param staticMapFile file containing user and group ID mappings
* @return StaticMapping object with parsed mappings
* @throws IOException if an I/O error occurs while reading the file
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getAclString,org.apache.hadoop.security.authorize.AccessControlList:getAclString(),301,312,"/**
* Masks or formats a string based on allowed status.
* @return Masked or formatted string
*/","* Returns the access control list as a String that can be used for building a
   * new instance by sending it to the constructor of {@link AccessControlList}.
   * @return acl string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,createCredentialEntry,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:createCredentialEntry(java.lang.String,char[])",228,244,"/**
* Adds a new credential to the keystore.
* @param alias unique identifier for the credential
* @param credential character array representing the credential
* @return CredentialEntry object for the added credential
* @throws IOException if credential already exists or an error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,execute,org.apache.hadoop.security.alias.CredentialShell$CreateCommand:execute(),438,465,"/**
* Handles credential creation process.
* @throws IOException if I/O error occurs
* @throws NoSuchAlgorithmException if algorithm not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getTGT,org.apache.hadoop.security.UserGroupInformation:getTGT(),852,861,"/**
* Retrieves a valid Kerberos ticket from the subject.
* @return KerberosTicket object or null if none is found
*/","* Get the Kerberos TGT
   * @return the user's TGT or null if none was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setSslConfiguration,"org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",819,823,"/**
* Initializes client configuration with provided settings and utility.
* @param zkClientConfig ZooKeeper client configuration
* @param truststoreKeystore Truststore and keystore configuration
* @throws ConfigurationException if configuration fails
*/","* Configure ZooKeeper Client with SSL/TLS connection.
   * @param zkClientConfig ZooKeeper Client configuration
   * @param truststoreKeystore truststore keystore, that we use to set the SSL configurations
   * @throws ConfigurationException if the SSL configs are empty",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,unprotectedRelogin,"org.apache.hadoop.security.UserGroupInformation:unprotectedRelogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)",1347,1377,"/**
 * Handles user logout and re-login.
 * @param login Hadoop login context
 * @param ignoreLastLoginTime flag to skip last login time check
 * @throws IOException if an I/O error occurs during the process
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,getServerProperties,org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.lang.String),124,129,"/**
* Retrieves SASL properties for a given client address.
* @param clientAddress IP address of the client
* @return Map of SASL properties or default if address is null
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,handle,org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:handle(javax.security.auth.callback.Callback[]),294,348,"/**
 * Handles SASL DIGEST-MD5 callbacks for authentication.
 * @param callbacks array of Callback objects to process
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long,boolean)",97,118,"/**
* Initializes a CryptoOutputStream for data encryption.
* @param out underlying output stream
* @param codec encryption codec to use
* @param bufferSize size of the buffer
* @param key encryption key
* @param iv initialization vector
* @param streamOffset offset in the stream
* @param closeOutputStream whether to close the underlying stream on close
* @throws IOException if an I/O error occurs or security exception is thrown
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createEncryptor(),54,58,"/**
* Creates an SM4 encryption instance.
* @return Encryptor object configured for encryption
* @throws GeneralSecurityException if encryption setup fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createDecryptor(),60,64,"/**
* Creates a decryptor using SM4 algorithm.
* @return Decryptor instance configured for decryption
* @throws GeneralSecurityException if setup fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createEncryptor(),54,58,"/**
* Creates an AES encryptor using CTR mode.
* @return Encryptor instance configured for encryption
* @throws GeneralSecurityException if setup fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createDecryptor(),60,64,"/**
* Creates a decryptor instance.
* @return Decryptor object configured with AES cipher in CTR mode
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,getInstance,"org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String,java.lang.String)",131,140,"/**
* Initializes an OpenSSL cipher with specified transformation and engine.
* @param transformation encryption algorithm and mode
* @param engineId optional identifier for the cryptographic engine
* @return OpensslCipher instance configured with given parameters
*/","* Return an <code>OpensslCipher</code> object that implements the specified
   * transformation.
   * 
   * @param transformation the name of the transformation, e.g., 
   * AES/CTR/NoPadding.
   * @param engineId the openssl engine to use.if not set,
   * defalut engine will be used.
   * @return OpensslCipher an <code>OpensslCipher</code> object
   * @throws NoSuchAlgorithmException if <code>transformation</code> is null, 
   * empty, in an invalid format, or if Openssl doesn't implement the 
   * specified algorithm.
   * @throws NoSuchPaddingException if <code>transformation</code> contains 
   * a padding scheme that is not available.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,isSupported,org.apache.hadoop.crypto.OpensslCipher:isSupported(org.apache.hadoop.crypto.CipherSuite),181,193,"/**
* Checks if cipher suite is valid.
* @param suite CipherSuite to validate
* @return true if valid, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersions(java.lang.String),376,398,"/**
 * Retrieves all key versions for a given name.
 * @param name the key identifier
 * @return List of KeyVersion objects or empty list if none found
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,createKey,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",433,460,"/**
* Creates a new key version with the specified name and material.
* @param name unique key identifier
* @param material cryptographic material for the key
* @param options configuration options for the key
* @return KeyVersion object representing the created key version
* @throws IOException if key already exists or other I/O errors occur
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:rollNewVersion(java.lang.String,byte[])",508,527,"/**
* Creates a new key version with the specified name and material.
* @param name key identifier
* @param material cryptographic material for the key
* @return KeyVersion object representing the new key version
* @throws IOException if key is not found or length mismatch occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSEncryptedKeyVersion:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,byte[])",246,250,"/**
* Constructs a KMSEncryptedKeyVersion.
* @param keyName name of the encryption key
* @param keyVersionName version identifier for the key
* @param iv initialization vector
* @param encryptedVersionName encrypted version name
* @param keyMaterial raw key material data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONKeyVersion,org.apache.hadoop.util.KMSUtil:parseJSONKeyVersion(java.util.Map),185,202,"/**
* Creates a KeyVersion from a map.
* @param valueMap contains key material and metadata
* @return KeyVersion object or null if map is empty
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONMetadata,org.apache.hadoop.util.KMSUtil:parseJSONMetadata(java.util.Map),204,218,"/**
 * Creates KeyProvider.Metadata from a map.
 * @param valueMap input map containing metadata fields
 * @return KeyProvider.Metadata object or null if map is empty
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,getCurrentKey,org.apache.hadoop.crypto.key.KeyProviderExtension:getCurrentKey(java.lang.String),66,69,"/**
 * Retrieves a key version by name.
 * @param name key identifier
 * @return KeyVersion object or null if not found
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,createKey,"org.apache.hadoop.crypto.key.KeyProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",567,571,"/**
* Generates a key version.
* @param name key identifier
* @param options configuration options
* @return KeyVersion object
* @throws NoSuchAlgorithmException if algorithm is not supported
* @throws IOException if I/O error occurs
*/","* Create a new key generating the material for it.
   * The given key must not already exist.
   * <p>
   * This implementation generates the key material and calls the
   * {@link #createKey(String, byte[], Options)} method.
   *
   * @param name the base name of the key
   * @param options the options for the new key.
   * @return the version name of the first version of the key.
   * @throws IOException raised on errors performing I/O.
   * @throws NoSuchAlgorithmException no such algorithm exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.KeyProvider:rollNewVersion(java.lang.String),612,621,"/**
 * Retrieves KeyVersion by name.
 * @param name key identifier
 * @return KeyVersion object
 * @throws NoSuchAlgorithmException if algorithm is not found
 * @throws IOException if metadata or key retrieval fails
 */","* Roll a new version of the given key generating the material for it.
   * <p>
   * This implementation generates the key material and calls the
   * {@link #rollNewVersion(String, byte[])} method.
   *
   * @param name the basename of the key
   * @return the name of the new version of the key
   * @throws IOException              raised on errors performing I/O.
   * @throws NoSuchAlgorithmException This exception is thrown when a particular
   *                                  cryptographic algorithm is requested
   *                                  but is not available in the environment.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String,byte[])",140,146,"/**
* Generates and registers a new key version.
* @param name key identifier
* @param material key material data
* @return KeyVersion object representing the new key
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,toJSON,org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),110,122,"/**
* Converts EncryptedKeyVersion to JSON map.
* @param encryptedKeyVersion the key version to convert
* @return Map containing key details or empty map if null
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,warmUpEncryptedKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:warmUpEncryptedKeys(java.lang.String[]),288,308,"/**
 * Warms up specified keys across all configured providers.
 * @param keyNames variable number of key names to warm up
 * @throws IOException if no provider succeeds in warming up the keys
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,close,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:close(),544,554,"/**
* Closes all KMSClientProviders, logging errors if any occur.
* @throws IOException if an I/O error occurs while closing providers
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,readLock,org.apache.hadoop.crypto.key.kms.ValueQueue:readLock(java.lang.String),115,117,"/**
 * Calls methods in sequence on objects returned by other methods.
 * @param keyName identifier used to fetch an object
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,readUnlock,org.apache.hadoop.crypto.key.kms.ValueQueue:readUnlock(java.lang.String),119,121,"/**
 * Performs operations on an object identified by keyName.
 * @param keyName identifier for the target object
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,writeUnlock,org.apache.hadoop.crypto.key.kms.ValueQueue:writeUnlock(java.lang.String),123,125,"/**
 * Calls chain of methods with given key.
 * @param keyName key to process
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,writeLock,org.apache.hadoop.crypto.key.kms.ValueQueue:writeLock(java.lang.String),127,129,"/**
 * Calls methods on an object obtained from m1.
 * @param keyName key to retrieve object from m1
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String),139,141,"/**
 * Initializes a Builder with context and default separator.
 * @param context the initial context string
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,unpack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto),70,80,"/**
* Converts a proto collection to a list of refresh responses.
* @param collection the proto collection to process
* @return a list of RefreshResponse objects
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolServerSideTranslatorPB.java,refresh,"org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:refresh(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto)",44,62,"/**
* Processes generic refresh request.
* @param controller RPC controller
* @param request refresh request proto
* @return collection of refresh responses
* @throws ServiceException on invalid request or IO error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,newEntry,"org.apache.hadoop.ipc.RetryCache:newEntry(long,byte[],int)",335,339,"/**
* Creates a cache entry with an expiration time.
* @param expirationTime duration until the cache entry expires
* @param clientId identifier for the client
* @param callId unique identifier for the call
* @return CacheEntry object configured with expiration and identifiers
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long)",155,159,"/**
* Constructs a cache entry with a payload.
* @param clientId unique client identifier
* @param callId unique call identifier
* @param payload data to be cached
* @param expirationTime time when the entry expires
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long,boolean)",84,88,"/**
* Constructs a cache entry with success status.
* @param clientId identifier for the client
* @param callId unique call identifier
* @param expirationTime time when the entry expires
* @param success indicates if the operation was successful
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isServerFailOverEnabledByQueue,org.apache.hadoop.ipc.Server:isServerFailOverEnabledByQueue(),3885,3888,"/**
 * Checks if the queue is empty.
 * @return true if the queue has no elements, false otherwise
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,put,org.apache.hadoop.ipc.CallQueueManager:put(org.apache.hadoop.ipc.Schedulable),289,299,"/**
* Handles event E based on conditions.
* @param e event to be processed
*/","* Insert e into the backing queue or block until we can.  If client
   * backoff is enabled this method behaves like add which throws if
   * the queue overflows.
   * If we block and the queue changes on us, we will insert while the
   * queue is drained.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,add,org.apache.hadoop.ipc.CallQueueManager:add(org.apache.hadoop.ipc.Schedulable),301,304,"/**
 * Masks an element by invoking a helper method.
 * @param e the element to be masked
 * @return result of masking operation
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,callQueueLength,org.apache.hadoop.ipc.metrics.RpcMetrics:callQueueLength(),167,169,"/**
 * Retrieves length of call queue.
 * @return integer representing queue size
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,initialize,org.apache.hadoop.ipc.WritableRpcEngine:initialize(),80,84,"/**
* Initializes RPC server with writable invoker.
*/",* Register the rpcRequest deserializer for WritableRpcEngine,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,registerProtocolEngine,org.apache.hadoop.ipc.ProtobufRpcEngine2:registerProtocolEngine(),70,77,"/**
* Initializes RPC server for Protocol Buffers.
* Sets up the server if not already configured.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,setExpirationTime,"org.apache.hadoop.util.LightWeightCache:setExpirationTime(org.apache.hadoop.util.LightWeightCache$Entry,long)",147,149,"/**
 * Updates entry expiration time.
 * @param e Entry to update
 * @param expirationPeriod Time in milliseconds to add to current timer value
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,start,org.apache.hadoop.util.StopWatch:start(),58,65,"/**
* Starts the stopwatch.
* Throws exception if already started.
* @return current Stopwatch instance
*/","* Start to measure times and make the state of stopwatch running.
   * @return this instance of StopWatch.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,stop,org.apache.hadoop.util.StopWatch:stop(),71,79,"/**
* Stops the StopWatch and calculates elapsed time.
* Throws IllegalStateException if not started.
*/","* Stop elapsed time and make the state of stopwatch stop.
   * @return this instance of StopWatch.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,now,org.apache.hadoop.util.StopWatch:now(),105,109,"/**
* Calculates elapsed time in nanoseconds.
* @return Total elapsed time if started, otherwise just current elapsed time
*/",* @return current elapsed time in nanosecond.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,sendResponse,org.apache.hadoop.ipc.Server$Call:sendResponse(),1086,1094,"/**
* Masks function with response handling.
* @throws IOException on I/O error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,abortResponse,org.apache.hadoop.ipc.Server$Call:abortResponse(java.lang.Throwable),1096,1103,"/**
* Handles exceptions and logs them.
* @param t the Throwable to handle
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getFingerprint,org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(java.lang.reflect.Method[]),140,142,"/**
 * Recursively processes an array of Method objects.
 * @param methods array of Method objects to process
 * @return result of recursive processing
 */","* Get the hash code of an array of methods
   * Methods are sorted before hashcode is calculated.
   * So the returned value is irrelevant of the method order in the array.
   * 
   * @param methods an array of methods
   * @return the hash code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getSigFingerprint,"org.apache.hadoop.ipc.ProtocolSignature:getSigFingerprint(java.lang.Class,long)",186,200,"/**
* Generates or retrieves protocol fingerprint.
* @param protocol Class representing the protocol
* @param serverVersion Version of the server
* @return ProtocolSigFingerprint object for the given protocol and version
*/","* Return a protocol's signature and finger print from cache
   * 
   * @param protocol a protocol class
   * @param serverVersion protocol version
   * @return its signature and finger print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,valueOf,org.apache.hadoop.ipc.RemoteException:valueOf(org.xml.sax.Attributes),131,134,"/**
 * Creates a RemoteException with class and message from Attributes.
 * @param attrs object containing error details
 * @return RemoteException instance with specified class and message
 */","* Create RemoteException from attributes.
   * @param attrs may not be null.
   * @return RemoteException.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,readResponse,org.apache.hadoop.ipc.Client$IpcStreams:readResponse(),1922,1944,"/**
* Reads and returns a ByteBuffer from input stream.
* @throws IOException if I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldFailoverOnException,org.apache.hadoop.io.retry.RetryPolicies:shouldFailoverOnException(java.lang.Exception),773,781,"/**
* Checks if exception is a StandbyException after unwrapping.
* @param e the original exception to check
* @return true if unwrapped exception is StandbyException, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getWrappedRetriableException,org.apache.hadoop.io.retry.RetryPolicies:getWrappedRetriableException(java.lang.Exception),795,803,"/**
* Masks a RemoteException to a RetriableException.
* @param e the original exception
* @return RetriableException if applicable, otherwise null
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,monitorHealth,"org.apache.hadoop.ha.HAServiceProtocolHelper:monitorHealth(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",34,42,"/**
* Calls remote service method m2.
* @param svc HAServiceProtocol instance
* @param reqInfo request information
* @throws IOException if communication fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToActive,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",44,52,"/**
* Calls remote service method m2.
* @param svc HAServiceProtocol instance
* @param reqInfo StateChangeRequestInfo for the request
* @throws IOException if communication fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToStandby,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",54,62,"/**
* Invokes service method m2 with request info.
* @param svc HAServiceProtocol instance
* @param reqInfo StateChangeRequestInfo object
* @throws IOException if remote call fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToObserver,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",64,71,"/**
* Invokes service method m2 with request info.
* @param svc HAServiceProtocol instance
* @param reqInfo StateChangeRequestInfo object
* @throws IOException if remote call fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,isHealthCheckFailedException,org.apache.hadoop.ha.HealthMonitor:isHealthCheckFailedException(java.lang.Throwable),229,235,"/**
* Checks if a Throwable is a HealthCheckFailedException or related.
* @param t the Throwable to check
* @return true if it's a HealthCheckFailedException or related, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,get,org.apache.hadoop.fs.PartialListing:get(),67,72,"/**
* Returns a list of items.
* @throws IOException if an I/O error occurs
* @return List of T or empty list if no data available
*/","* Partial listing of the path being listed. In the case where the path is
   * a file. The list will be a singleton with the file itself.
   *
   * @return Partial listing of the path being listed.
   * @throws IOException if there was an exception getting the listing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,org.apache.hadoop.ipc.Server$Call:<init>(org.apache.hadoop.ipc.Server$Call),985,988,"/**
* Constructs a new Call instance by copying properties from another Call.
* @param call existing Call object to copy properties from
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[])",990,992,"/**
 * Constructs an RPC call with optional parameters.
 * @param id unique identifier for the call
 * @param retryCount number of retries allowed
 * @param kind type of RPC
 * @param clientId client identifier bytes
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,java.lang.Void,java.lang.Void,org.apache.hadoop.ipc.RPC$RpcKind,byte[])",994,998,"/**
* Constructs a new Call object.
* @param id unique call identifier
* @param retryCount number of retries attempted
* @param kind type of RPC request
* @param clientId client identifier bytes
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcScheduler.java,addResponseTime,"org.apache.hadoop.ipc.RpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",65,78,"/**
* Calls m3 with timing metrics from ProcessingDetails.
* @param callName name of the call
* @param schedulable Schedulable instance
* @param details ProcessingDetails containing timing information
*/","* Store a processing time value for an RPC call into this scheduler.
   *
   * @param callName The name of the call.
   * @param schedulable The schedulable representing the incoming call.
   * @param details The details of processing time.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numDroppedConnections,org.apache.hadoop.ipc.metrics.RpcMetrics:numDroppedConnections(),171,173,"/**
 * Returns the number of dropped connections.
 * @return count of dropped connections
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,register,"org.apache.hadoop.ipc.Server$ConnectionManager:register(java.nio.channels.SocketChannel,int,boolean)",4097,4108,"/**
* Creates a new server connection.
* @param channel the socket channel for communication
* @param ingressPort the port number of incoming connection
* @param isOnAuxiliaryPort flag indicating if connected to auxiliary port
* @return Connection object or null if creation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numOpenConnections,org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnections(),153,155,"/**
* Returns number of open connections.
* @Metric tag for monitoring
* @return integer count of open connections
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offerQueues,"org.apache.hadoop.ipc.FairCallQueue:offerQueues(int,org.apache.hadoop.ipc.Schedulable,boolean)",257,267,"/**
* Checks conditions for a given priority and element.
* @param priority the current priority level
* @param e the element to check against
* @param includeLast whether to include the last priority level
* @return true if conditions are met, otherwise false
*/","* Offer the element to queue of the given or lower priority.
   * @param priority - starting queue priority
   * @param e - element to add
   * @param includeLast - whether to attempt last queue
   * @return boolean if added to a queue",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,"org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)",269,279,"/**
* Adds an element to a queue with a timeout.
* @param e the element to add
* @param timeout maximum time to wait
* @param unit time unit for timeout
* @return true if added, false otherwise
* @throws InterruptedException if interrupted while waiting
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object),281,290,"/**
* Adds element to queue based on its priority.
* @param e element to add
* @return true if added successfully, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,populateResponseParamsOnError,"org.apache.hadoop.ipc.Server$RpcCall:populateResponseParamsOnError(java.lang.Throwable,org.apache.hadoop.ipc.Server$RpcCall$ResponseParams)",1279,1302,"/**
 * Handles exceptions by logging and setting response parameters.
 * @param t the Throwable to process
 * @param responseParams object to store response details
 */","* @param t              the {@link java.lang.Throwable} to use to set
     *                       errorInfo
     * @param responseParams the {@link ResponseParams} instance to populate",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolVersionForRpcKind,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersionForRpcKind(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",106,120,"/**
 * Fetches protocol versions for a given RPC kind and protocol.
 * @param rpcKind type of RPC
 * @param protocol name of the protocol
 * @return array of version numbers or null if none found
 * @throws ClassNotFoundException if protocol class not found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcNoSuchProtocolException.java,<init>,org.apache.hadoop.ipc.RpcNoSuchProtocolException:<init>(java.lang.String),29,31,"/**
 * Constructs an exception with a specified detail message.
 * @param message the detail message
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcNoSuchMethodException.java,<init>,org.apache.hadoop.ipc.RpcNoSuchMethodException:<init>(java.lang.String),30,32,"/**
 * Constructs an exception indicating no such RPC method.
 * @param message descriptive error message
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,<init>,"org.apache.hadoop.ipc.RPC$VersionMismatch:<init>(java.lang.String,long,long)",248,255,"/**
* Constructs a VersionMismatch exception.
* @param interfaceName name of the interface with version mismatch
* @param clientVersion client's protocol version
* @param serverVersion server's protocol version
*/","* Create a version mismatch exception
     * @param interfaceName the name of the protocol mismatch
     * @param clientVersion the client's version of the protocol
     * @param serverVersion the server's version of the protocol",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.io.IOException)",1986,1989,"/**
* Constructs a new FatalRpcServerException with an error code and IO exception.
* @param errCode RPC error code
* @param ioe underlying IO exception
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:<init>(int),78,81,"/**
 * Initializes a framed buffer with specified capacity.
 * @param capacity buffer capacity without framing bytes
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,reset,org.apache.hadoop.ipc.ResponseBuffer:reset(),70,74,"/**
* Resets write count and flushes buffer.
* @return current ResponseBuffer instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,writeTo,org.apache.hadoop.ipc.ResponseBuffer:writeTo(java.io.OutputStream),48,50,"/**
 * Writes data to an output stream.
 * @param out OutputStream to write data to
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,toByteArray,org.apache.hadoop.ipc.ResponseBuffer:toByteArray(),52,54,"/**
 * Calls m1 and then invokes m2 on its result.
 * @return byte array from m2 of the object returned by m1
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,recomputeScheduleCache,org.apache.hadoop.ipc.DecayRpcScheduler:recomputeScheduleCache(),545,560,"/**
* Updates cache with new levels based on call costs.
*/",* Update the scheduleCache to match current conditions in callCosts.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,cachedOrComputedPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:cachedOrComputedPriorityLevel(java.lang.Object),642,661,"/**
* Computes or retrieves priority for a given identity.
* @param identity unique identifier for the item
* @return computed priority as an integer
*/","* Returns the priority level for a given identity by first trying the cache,
   * then computing it.
   * @param identity an object responding to toString and hashCode
   * @return integer scheduling decision from 0 to numLevels - 1",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,setPriorityLevel,"org.apache.hadoop.ipc.CallQueueManager:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",272,276,"/**
* Delegates task to DecayRpcScheduler if applicable.
* @param user UserGroupInformation object representing the user
* @param priority Task priority level
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getCallVolumeSummary,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getCallVolumeSummary(),914,922,"/**
* Retrieves status from scheduler or indicates none active.
* @return Status string from scheduler or ""No Active Scheduler""
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,constructRpcRequest,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:constructRpcRequest(java.lang.reflect.Method,com.google.protobuf.Message)",296,299,"/**
* Creates an RPC request with headers.
* @param method target method for invocation
* @param theRequest message containing request details
* @return Writable object representing the request
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,constructRpcRequest,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:constructRpcRequest(java.lang.reflect.Method,org.apache.hadoop.thirdparty.protobuf.Message)",306,309,"/**
* Creates an RPC request with a protobuf header.
* @param method method to create the request for
* @param theRequest original message
* @return Writable object representing the RPC request
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,refreshServiceAcl,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:refreshServiceAcl(),54,58,"/**
* Calls remote method m1 through RPC proxy.
* @throws IOException if communication fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,refreshUserToGroupsMappings,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshUserToGroupsMappings(),59,63,"/**
* Invokes remote procedure call for user group mapping.
* @throws IOException if communication fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshSuperUserGroupsConfiguration(),65,69,"/**
* Executes RPC call to refresh superuser groups configuration.
* @throws IOException if an I/O error occurs during execution
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,refreshCallQueue,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:refreshCallQueue(),54,58,"/**
* Executes an RPC call and handles potential exceptions.
* @throws IOException if an I/O error occurs during execution
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,getGroupsForUser,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:getGroupsForUser(java.lang.String),52,59,"/**
* Retrieves groups for a user.
* @param user username
* @return array of group names or empty if none found
* @throws IOException on network error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,cedeActive,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:cedeActive(int),56,63,"/**
* Sends a request to cede active status for a specified duration.
* @param millisToCede milliseconds to cede the active status
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,gracefulFailover,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:gracefulFailover(),65,69,"/**
* Calls remote procedure with failover handling.
* @throws IOException if communication fails
* @throws AccessControlException if access is denied
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,monitorHealth,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:monitorHealth(),84,87,"/**
 * Executes remote procedure call for monitoring health.
 * @throws IOException if communication fails
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,getServiceStatus,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:getServiceStatus(),114,128,"/**
 * Retrieves and processes service status.
 * @return HAServiceStatus object representing the current service status
 * @throws IOException if an I/O error occurs during the RPC call
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeMethod,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod(),165,171,"/**
* Executes method with potential RPC call.
* @return result of method execution
* @throws Throwable if any error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,close,org.apache.hadoop.ipc.WritableRpcEngine$Invoker:close(),264,270,"/**
* Marks the function as closed and notifies clients.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,close,org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:close(),325,331,"/**
* Marks this resource as closed and notifies clients.
* @throws IOException if an I/O error occurs during notification
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,close,org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:close(),335,341,"/**
* Marks client as closed and processes it.
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getMetrics,"org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",462,477,"/**
* Collects metrics for FairCallQueue.
* @param collector MetricsCollector instance to record metrics
* @param all unused parameter
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,wrap,org.apache.hadoop.ipc.RpcWritable:wrap(java.lang.Object),40,53,"/**
 * Wraps an object in an RpcWritable.
 * @param o the object to be wrapped
 * @return RpcWritable instance or throws exception if unsupported type
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,hashCode,org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:hashCode(),174,177,"/**
 * Calls the superclass method m1.
 * @return result of super.m1()
 */",Override hashcode to avoid findbugs warnings,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,getAndAdvanceCurrentIndex,org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getAndAdvanceCurrentIndex(),145,149,"/**
 * Returns index after executing mask functions.
 * @return Index from m1 function
 */",* Use the mux by getting and advancing index.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,registerForDeferredResponse,org.apache.hadoop.ipc.ProtobufRpcEngine$Server:registerForDeferredResponse(),421,426,"/**
* Creates and configures a ProtobufRpcEngineCallback.
* @return configured ProtobufRpcEngineCallback instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,registerForDeferredResponse2,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:registerForDeferredResponse2(),453,458,"/**
* Creates and configures a ProtobufRpcEngineCallback.
* @return configured ProtobufRpcEngineCallback instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,writeTo,org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:writeTo(org.apache.hadoop.ipc.ResponseBuffer),110,116,"/**
* Writes message to output buffer.
* @param out buffer for writing data
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,writeTo,org.apache.hadoop.ipc.RpcWritable$Buffer:writeTo(org.apache.hadoop.ipc.ResponseBuffer),159,163,"/**
* Masks response data.
* @param out buffer to write masked data
* @throws IOException if I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufWrapperLegacy.java,writeTo,org.apache.hadoop.ipc.ProtobufWrapperLegacy:writeTo(org.apache.hadoop.ipc.ResponseBuffer),63,70,"/**
* Writes message to response buffer.
* @param out ResponseBuffer object to write to
* @throws IOException if I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,cleanupCalls,org.apache.hadoop.ipc.Client$Connection:cleanupCalls(),1307,1314,"/**
* Iterates and processes call entries.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteAddress,org.apache.hadoop.ipc.Server:getRemoteAddress(),436,439,"/**
* Masks IP address using helper methods.
* @return masked IP string or null if unavailable
*/","@return Returns remote address as a string when invoked inside an RPC.
   *  Returns null in case of an error.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numOpenConnectionsPerUser,org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnectionsPerUser(),162,165,"/**
 * Retrieves metric data for open connections.
 * @return JSON string containing connection metrics
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doAsyncWrite,org.apache.hadoop.ipc.Server$Responder:doAsyncWrite(java.nio.channels.SelectionKey),1798,1821,"/**
 * Handles asynchronous write operation for an RPC call.
 * @param key SelectionKey associated with the I/O event
 * @throws IOException if there's a channel mismatch or other I/O error
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRespond,org.apache.hadoop.ipc.Server$Responder:doRespond(org.apache.hadoop.ipc.Server$RpcCall),1927,1939,"/**
* Handles RPC call processing and response queuing.
* @param call the RpcCall object to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsTracer.java,get,org.apache.hadoop.fs.FsTracer:get(org.apache.hadoop.conf.Configuration),39,47,"/**
* Initializes and returns a singleton Tracer instance.
* @param conf configuration settings for the tracer
* @return Tracer instance configured with provided settings
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,"org.apache.hadoop.util.MachineList:<init>(java.lang.String,org.apache.hadoop.util.MachineList$InetAddressFactory)",77,79,"/**
* Initializes MachineList with trimmed host entries.
* @param hostEntries comma-separated host names
* @param addressFactory factory for creating InetAddress objects
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,org.apache.hadoop.util.MachineList:<init>(java.util.Collection),85,87,"/**
 * Initializes a new MachineList with given host entries.
 * @param hostEntries collection of host names or IP addresses
 */","*
   * @param hostEntries collection of separated ip/cidr/host addresses",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,isIn,org.apache.hadoop.util.FileBasedIPList:isIn(java.lang.String),71,77,"/**
* Checks if IP address is masked.
* @param ipAddress IP address to check
* @return true if IP is masked, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,<init>,org.apache.hadoop.util.SysInfoLinux:<init>(),180,183,"/**
* Initializes system information with default Linux procfs files.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcMemInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(),215,217,"/**
 * Calls overloaded method with default false parameter.
 */","* Read /proc/meminfo, parse and compute memory information only once.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getAvailablePhysicalMemorySize,org.apache.hadoop.util.SysInfoLinux:getAvailablePhysicalMemorySize(),609,616,"/**
* Calculates total free memory size.
* @return Total free memory in bytes
*/",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCumulativeCpuTime,org.apache.hadoop.util.SysInfoLinux:getCumulativeCpuTime(),646,650,"/**
 * Calls m1 and returns CPU time from tracker.
 */",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCpuUsagePercentage,org.apache.hadoop.util.SysInfoLinux:getCpuUsagePercentage(),653,661,"/**
* Calculates masked CPU usage.
* @return Masked CPU usage percentage or UNAVAILABLE if data is unavailable
*/",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumVCoresUsed,org.apache.hadoop.util.SysInfoLinux:getNumVCoresUsed(),664,672,"/**
* Calculates and returns the CPU usage percentage.
* @return CPU usage as a percentage or UNAVAILABLE if not applicable
*/",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getStorageBytesRead,org.apache.hadoop.util.SysInfoLinux:getStorageBytesRead(),688,692,"/**
 * Returns the number of disk bytes read.
 * @return Number of disk bytes read as a long value
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getStorageBytesWritten,org.apache.hadoop.util.SysInfoLinux:getStorageBytesWritten(),694,698,"/**
 * Calls m1 and returns the number of bytes written.
 * @return Number of bytes written to disks
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,<init>,org.apache.hadoop.util.IdentityHashStore:<init>(int),62,72,"/**
* Initializes IdentityHashStore with specified capacity.
* @param capacity desired storage capacity, rounded up to nearest power of 2
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,put,"org.apache.hadoop.util.IdentityHashStore:put(java.lang.Object,java.lang.Object)",118,126,"/**
* Adds a key-value pair to the buffer.
* @param k key to be added
* @param v value to be added
*/","* Add a new (key, value) mapping.
   *
   * Inserting a new (key, value) never overwrites a previous one.
   * In other words, you can insert the same key multiple times and it will
   * lead to multiple entries.
   *
   * @param k Generics Type k.
   * @param v Generics Type v.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,releaseBuffer,org.apache.hadoop.fs.FSDataInputStream:releaseBuffer(java.nio.ByteBuffer),222,235,"/**
* Releases a ByteBuffer to the appropriate pool or delegates.
* @param buffer ByteBuffer to be released
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,hasNext,org.apache.hadoop.util.LightWeightGSet$SetIterator:hasNext(),329,333,"/**
 * Checks if the next element is available.
 * @return true if next element exists, false otherwise
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,next,org.apache.hadoop.util.LightWeightGSet$SetIterator:next(),335,344,"/**
* Advances iterator and returns current element.
* Throws exception if no more elements.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,put,org.apache.hadoop.util.LightWeightGSet:put(java.lang.Object),149,177,"/**
 * Masks an element by replacing it in the entries array.
 * @param element the element to mask
 * @return the existing element at the masked index, or null if not found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,org.apache.hadoop.util.LightWeightGSet:remove(java.lang.Object),221,228,"/**
* Retrieves an element by key, throwing NPE for null keys.
* @param key the key to search for
* @return the element associated with the key
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:sort(int),3253,3256,"/**
 * Sorts an array using a custom sort function.
 * @param count number of elements to sort
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,newSecureTransformerFactory,org.apache.hadoop.util.XMLUtils:newSecureTransformerFactory(),148,154,"/**
* Creates a secure TransformerFactory instance.
* @return configured TransformerFactory with secure processing enabled
* @throws TransformerConfigurationException if configuration fails
*/","* This method should be used if you need a {@link TransformerFactory}. Use this method
   * instead of {@link TransformerFactory#newInstance()}. The factory that is returned has
   * secure configuration enabled.
   *
   * @return a {@link TransformerFactory} with secure configuration enabled
   * @throws TransformerConfigurationException if the {@code JAXP} transformer does not
   * support the secure configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,newSecureSAXTransformerFactory,org.apache.hadoop.util.XMLUtils:newSecureSAXTransformerFactory(),165,171,"/**
* Creates and configures a secure SAXTransformerFactory.
* @return configured SAXTransformerFactory instance
*/","* This method should be used if you need a {@link SAXTransformerFactory}. Use this method
   * instead of {@link SAXTransformerFactory#newInstance()}. The factory that is returned has
   * secure configuration enabled.
   *
   * @return a {@link SAXTransformerFactory} with secure configuration enabled
   * @throws TransformerConfigurationException if the {@code JAXP} transformer does not
   * support the secure configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,formatSize,"org.apache.hadoop.fs.ContentSummary:formatSize(long,boolean)",477,481,"/**
* Formats file size.
* @param size file size in bytes
* @param humanReadable true for human-readable format, false for raw bytes
* @return formatted size string
*/","* Formats a size to be human readable or in bytes.
   * @param size value to be formatted
   * @param humanReadable flag indicating human readable or not
   * @return String representation of the size",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,formatSize,org.apache.hadoop.fs.shell.Ls:formatSize(long),126,130,"/**
* Converts size to human-readable format.
* @param size file size in bytes
* @return formatted size string
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,formatSize,org.apache.hadoop.fs.shell.FsUsage:formatSize(long),55,59,"/**
 * Converts size to human-readable format.
 * @param size file size in bytes
 * @return formatted size string
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,formatSize,"org.apache.hadoop.fs.QuotaUsage:formatSize(long,boolean)",394,398,"/**
* Converts size to a formatted string.
* @param size file size in bytes
* @param humanReadable true for human-readable format, false for raw bytes
* @return formatted size string
*/","* Formats a size to be human readable or in bytes.
   * @param size value to be formatted
   * @param humanReadable flag indicating human readable or not
   * @return String representation of the size",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,humanReadableInt,org.apache.hadoop.util.StringUtils:humanReadableInt(long),132,135,"/**
 * Masks a number using traditional binary prefix.
 * @param number the number to mask
 * @return masked string representation of the number
 */","* Given an integer, return a string that is in an approximate, but human 
   * readable format. 
   * @param number the number to format
   * @return a human readable form of the integer
   *
   * @deprecated use {@link TraditionalBinaryPrefix#long2String(long, String, int)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteDesc,org.apache.hadoop.util.StringUtils:byteDesc(long),1022,1024,"/**
 * Masks a length with binary prefix.
 * @param len the length to mask
 * @return masked length as string with binary unit
 */","* a byte description of the given long interger value.
   *
   * @param len len.
   * @return a byte description of the given long interger value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,computeCapacity,"org.apache.hadoop.util.LightWeightGSet:computeCapacity(long,double,java.lang.String)",379,417,"/**
 * Calculates memory capacity for a map based on max memory and percentage.
 * @param maxMemory maximum available memory in bytes
 * @param percentage percentage of max memory to use
 * @param mapName name of the map
 * @return calculated capacity as an integer
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,addToUsagesTable,"org.apache.hadoop.fs.shell.FsUsage$Df:addToUsagesTable(java.net.URI,org.apache.hadoop.fs.FsStatus,java.lang.String)",114,127,"/**
 * Masks file system status and logs it.
 * @param uri URI of the file system
 * @param fsStatus file system status object
 * @param mountedOnPath path where the file system is mounted
 */","* Add a new row to the usages table for the given FileSystem URI.
     *
     * @param uri - FileSystem URI
     * @param fsStatus - FileSystem status
     * @param mountedOnPath - FileSystem mounted on path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,readChecksumChunk,"org.apache.hadoop.fs.FSInputChecker:readChecksumChunk(byte[],int,int)",293,334,"/**
 * Reads data from a chunk with checksum validation and retry mechanism.
 * @param b byte array to store the read data
 * @param off offset in the byte array to start writing
 * @param len number of bytes to read
 * @return number of bytes read or 0 if no data is available
 * @throws IOException if an I/O error occurs or checksum validation fails after retries
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,readChars,"org.apache.hadoop.io.UTF8:readChars(java.io.DataInput,java.lang.StringBuilder,int)",279,332,"/**
 * Reads and masks UTF-8 bytes into a StringBuilder.
 * @param in DataInput source
 * @param buffer StringBuilder to store masked characters
 * @param nBytes number of bytes to process
 * @throws UTFDataFormatException if invalid UTF-8 sequence is found
 * @throws IOException if I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,org.apache.hadoop.util.StringUtils:byteToHexString(byte[]),199,201,"/**
 * Calls overloaded method with full byte array range.
 * @param bytes input byte array
 * @return result from processing byte array
 */","* Same as byteToHexString(bytes, 0, bytes.length).
   * @param bytes bytes.
   * @return byteToHexString.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HeapSort.java,sort,"org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)",51,54,"/**
* Calls overloaded method with default comparator.
* @param s indexed and sortable object
* @param p starting index
* @param r ending index
*/","* Sort the given range of items using heap sort.
   * {@inheritDoc}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exit,org.apache.hadoop.service.launcher.ServiceLauncher:exit(org.apache.hadoop.util.ExitUtil$ExitException),874,876,"/**
 * Handles exit exception by delegating to utility method.
 * @param ee ExitException to be processed
 */","* Exit the JVM using an exception for the exit code and message,
   * invoking {@link ExitUtil#terminate(ExitUtil.ExitException)}.
   *
   * This is the standard way a launched service exits.
   * An error code of 0 means success -nothing is printed.
   *
   * If {@link ExitUtil#disableSystemExit()} has been called, this
   * method will throw the exception.
   *
   * The method <i>may</i> be subclassed for testing
   * @param ee exit exception
   * @throws ExitUtil.ExitException if ExitUtil exceptions are disabled",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exitWithMessage,"org.apache.hadoop.service.launcher.ServiceLauncher:exitWithMessage(int,java.lang.String)",1024,1026,"/**
* Throws an exception with given status and message.
* @param status error code
* @param message descriptive error message
*/","* Exit with a printed message. 
   * @param status status code
   * @param message message message to print before exiting
   * @throws ExitUtil.ExitException if exceptions are disabled",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,"org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.Throwable)",338,344,"/**
* Handles exceptions by rethrowing as ExitException.
* @param status error code
* @param t original throwable
*/","* Like {@link #terminate(int, String)} but uses the given throwable to
   * build the message to display or throw as an
   * {@link ExitException}.
   * <p>
   * @param status exit code to use if the exception is not an ExitException.
   * @param t throwable which triggered the termination. If this exception
   * is an {@link ExitException} its status overrides that passed in.
   * @throws ExitException if {@link System#exit(int)}  is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,"org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.String)",380,382,"/**
 * Throws an ExitException with given status and message.
 * @param status error code
 * @param msg descriptive message
 */","* Terminate the current process. Note that terminate is the *only* method
   * that should be used to terminate the daemon processes.
   *
   * @param status exit code
   * @param msg message used to create the {@code ExitException}
   * @throws ExitException if {@link System#exit(int)} is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,"org.apache.hadoop.util.ExitUtil:halt(int,java.lang.Throwable)",354,360,"/**
* Handles exceptions by rethrowing as HaltException.
* @param status error code
* @param t original Throwable
*/","* Forcibly terminates the currently running Java virtual machine.
   *
   * @param status exit code to use if the exception is not a HaltException.
   * @param t throwable which triggered the termination. If this exception
   * is a {@link HaltException} its status overrides that passed in.
   * @throws HaltException if {@link System#exit(int)}  is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,"org.apache.hadoop.util.ExitUtil:halt(int,java.lang.String)",399,401,"/**
* Throws a HaltException with given status and message.
* @param status error code
* @param message descriptive error message
*/","* Forcibly terminates the currently running Java virtual machine.
   * @param status status code
   * @param message message
   * @throws HaltException if {@link Runtime#halt(int)} is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,unregister,org.apache.hadoop.service.launcher.ServiceShutdownHook:unregister(),69,75,"/**
* Registers a shutdown hook for the current instance.
* Handles IllegalStateException if registration fails.
*/",* Unregister the hook.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sort,"org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)",63,67,"/**
* Sorts indexed elements using a custom sort function.
* @param s indexed data to be sorted
* @param p start index of the range to sort
* @param r end index (exclusive) of the range to sort
* @param rep progress reporter, can be null
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,org.apache.hadoop.util.LightWeightResizableGSet:<init>(),83,85,"/**
 * Constructs a new LightWeightResizableGSet with default capacity and load factor.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,org.apache.hadoop.util.LightWeightResizableGSet:<init>(int),87,89,"/**
 * Constructs a new LightWeightResizableGSet with an initial capacity.
 * @param initCapacity initial capacity of the set
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.lang.Iterable),91,98,"/**
* Converts Iterable to ArrayList.
* @param elements iterable collection of elements
* @return ArrayList containing the elements
*/","* Creates a <i>mutable</i> {@code ArrayList} instance containing the
   * given elements; a very thin shortcut for creating an empty list then
   * calling Iterables#addAll.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newLinkedList,org.apache.hadoop.util.Lists:newLinkedList(java.lang.Iterable),185,190,"/**
* Creates a linked list from an iterable.
* @param elements collection of elements to add
* @return LinkedList containing the elements
*/","* Creates a <i>mutable</i> {@code LinkedList} instance containing the given
   * elements; a very thin shortcut for creating an empty list then calling
   * Iterables#addAll.
   *
   * <p><b>Performance note:</b> {@link ArrayList} and
   * {@link java.util.ArrayDeque} consistently
   * outperform {@code LinkedList} except in certain rare and specific
   * situations. Unless you have spent a lot of time benchmarking your
   * specific needs, use one of those instead.</p>
   *
   * @param elements elements.
   * @param <E> Generics Type E.
   * @return Generics Type E List.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclUtil.java,getAclFromPermAndEntries,"org.apache.hadoop.fs.permission.AclUtil:getAclFromPermAndEntries(org.apache.hadoop.fs.permission.FsPermission,java.util.List)",42,90,"/**
* Applies ACL mask and other permissions.
* @param perm file system permission object
* @param entries list of existing ACL entries
* @return modified list of ACL entries with applied mask and other permissions
*/","* Given permissions and extended ACL entries, returns the full logical ACL.
   *
   * @param perm FsPermission containing permissions
   * @param entries List&lt;AclEntry&gt; containing extended ACL entries
   * @return List&lt;AclEntry&gt; containing full logical ACL",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,addChunk,org.apache.hadoop.util.ChunkedArrayList:addChunk(int),156,160,"/**
* Initializes a new chunk with given capacity.
* @param capacity size of the chunk to be created
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.lang.Object[]),70,80,"/**
* Creates an ArrayList from variable arguments.
* @param elements array of elements to add to the list
* @return ArrayList containing the provided elements
*/","* Creates a <i>mutable</i> {@code ArrayList} instance containing the given
   * elements.
   *
   * <p>Note that even when you do need the ability to add or remove,
   * this method provides only a tiny bit of syntactic sugar for
   * {@code newArrayList(}
   * {@link Arrays#asList asList}
   * {@code (...))}, or for creating an empty list then calling
   * {@link Collections#addAll}.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayListWithExpectedSize,org.apache.hadoop.util.Lists:newArrayListWithExpectedSize(int),148,151,"/**
* Creates an ArrayList with initial capacity.
* @param estimatedSize expected number of elements
* @return ArrayList initialized with specified capacity
*/","* Creates an {@code ArrayList} instance to hold {@code estimatedSize}
   * elements, <i>plus</i> an unspecified amount of padding;
   * you almost certainly mean to call {@link
   * #newArrayListWithCapacity} (see that method for further advice on usage).
   *
   * @param estimatedSize an estimate of the eventual {@link List#size()}
   *     of the new list.
   * @return a new, empty {@code ArrayList}, sized appropriately to hold the
   *     estimated number of elements.
   * @throws IllegalArgumentException if {@code estimatedSize} is negative.
   *
   * @param <E> Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,loadClass,org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String),155,158,"/**
 * Loads class by name.
 * @param name class name to load
 * @throws ClassNotFoundException if class is not found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,save,"org.apache.hadoop.util.JsonSerialization:save(java.io.File,java.lang.Object)",201,204,"/**
 * Applies mask to file content.
 * @param file File to be masked
 * @param instance Instance used for masking logic
 * @throws IOException if an I/O error occurs
 */","* Save to a local file. Any existing file is overwritten unless
   * the OS blocks that.
   * @param file file
   * @param instance instance
   * @throws IOException IO exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,<init>,"org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String,long)",72,81,"/**
 * Initializes a StatisticDurationTracker.
 * @param iostats store for I/O statistics
 * @param key identifier for the statistic
 * @param count initial count to increment
 */","* Constructor.
   * If the supplied count is greater than zero, the counter
   * of the key name is updated.
   * @param iostats statistics to update
   * @param key Key to use as prefix of values.
   * @param count #of times to increment the matching counter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,<init>,"org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])",69,83,"/**
 * Initializes a DurationInfo with logging capabilities.
 * @param log the Logger instance to use for logging
 * @param logAtInfo true to log at INFO level, false for DEBUG
 * @param format the message format string
 * @param args arguments to be formatted into the message
 */","* Create the duration text from a {@code String.format()} code call
   * and log either at info or debug.
   * @param log log to write to
   * @param logAtInfo should the log be at info, rather than debug
   * @param format format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,toString,org.apache.hadoop.util.OperationDuration:toString(),91,94,"/**
 * Returns masked function name.
 * @return Masked function name as a string
 */","* Return the duration as {@link #humanTime(long)}.
   * @return a printable duration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,newStripedCrcComposer,"org.apache.hadoop.util.CrcComposer:newStripedCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long,long)",83,92,"/**
* Creates a CrcComposer for given checksum type and parameters.
* @param type checksum algorithm type
* @param bytesPerCrcHint hint for bytes per CRC calculation
* @param stripeLength length of the data stripe
* @return CrcComposer instance configured with specified parameters
* @throws IOException if an I/O error occurs during setup
*/","* Returns a CrcComposer which will collapse CRCs for every combined
   * underlying data size which aligns with the specified stripe boundary. For
   * example, if ""update"" is called with 20 CRCs and bytesPerCrc == 5, and
   * stripeLength == 10, then every two (10 / 5) consecutive CRCs will be
   * combined with each other, yielding a list of 10 CRC ""stripes"" in the
   * final digest, each corresponding to 10 underlying data bytes. Using
   * a stripeLength greater than the total underlying data size is equivalent
   * to using a non-striped CrcComposer.
   *
   * @param type type.
   * @param bytesPerCrcHint bytesPerCrcHint.
   * @param stripeLength stripeLength.
   * @return a CrcComposer which will collapse CRCs for every combined.
   * underlying data size which aligns with the specified stripe boundary.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,compose,"org.apache.hadoop.util.CrcUtil:compose(int,int,long,int)",102,105,"/**
* Applies CRC mask to combine two CRCs.
* @param crcA first CRC value
* @param crcB second CRC value
* @param lengthB length of data for crcB
* @param mod modulus value
* @return combined CRC result
*/","* compose.
   *
   * @param crcA crcA.
   * @param crcB crcB.
   * @param lengthB length of content corresponding to {@code crcB}, in bytes.
   * @param mod mod.
   * @return compose result.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,getBytes,org.apache.hadoop.fs.CompositeCrcFileChecksum:getBytes(),64,67,"/**
 * Generates a masked CRC value.
 * @return byte array representing the masked CRC result
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,digest,org.apache.hadoop.util.CrcComposer:digest(),204,213,"/**
* Generates and resets the current stripe's digest.
* @return byte array containing the digest value
*/","* Returns byte representation of composed CRCs; if no stripeLength was
   * specified, the digest should be of length equal to exactly one CRC.
   * Otherwise, the number of CRCs in the returned array is equal to the
   * total sum bytesPerCrc divided by stripeLength. If the sum of bytesPerCrc
   * is not a multiple of stripeLength, then the last CRC in the array
   * corresponds to totalLength % stripeLength underlying data bytes.
   *
   * @return byte representation of composed CRCs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJarAndSave,"org.apache.hadoop.util.RunJar:unJarAndSave(java.io.InputStream,java.io.File,java.lang.String,java.util.regex.Pattern)",168,178,"/**
* Masks and processes input stream.
* @param inputStream source data stream
* @param toDir target directory for processing
* @param name file name for output
* @param unpackRegex pattern for unpacking files
*/","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped. Keep also a copy
   * of the entire jar in the same directory for backward compatibility.
   * TODO remove this feature in a new release and do only unJar
   *
   * @param inputStream the jar stream to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   * @param name name.
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File)",104,106,"/**
* Extracts files from a JAR archive.
* @param jarFile source JAR file
* @param toDir target directory for extracted files
*/","* Unpack a jar file into a directory.
   *
   * This version unpacks all files inside the jar regardless of filename.
   *
   * @param jarFile the .jar file to unpack
   * @param toDir the destination directory into which to unpack the jar
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/UTF8ByteArrayUtils.java,findNthByte,"org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],byte,int)",98,100,"/**
* Calls overloaded m1 with default start and end indices.
* @param utf byte array containing UTF data
* @param b byte to search for
* @param n number of times to find the byte
* @return result of the overloaded m1 method
*/","* Find the nth occurrence of the given byte b in a UTF-8 encoded string
   * @param utf a byte array containing a UTF-8 encoded string
   * @param b the byte to find
   * @param n the desired occurrence of the given byte
   * @return position that nth occurrence of the given byte if exists; otherwise -1",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,get,org.apache.hadoop.util.WeakReferenceMap:get(java.lang.Object),147,171,"/**
* Retrieves value for key using weak reference caching.
* @param key unique identifier for the value
* @return cached value or newly fetched value if not found in cache
*/","* Get the value, creating if needed.
   * @param key key.
   * @return an instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,check,"org.apache.hadoop.util.InstrumentedLock:check(long,long,boolean)",190,226,"/**
 * Logs lock warning if held time exceeds threshold.
 * @param acquireTime timestamp when lock was acquired
 * @param releaseTime timestamp when lock was released
 * @param checkLockHeld flag to check if lock is currently held
 */","* Log a warning if the lock was held for too long.
   *
   * Should be invoked by the caller immediately AFTER releasing the lock.
   *
   * @param acquireTime  - timestamp just after acquiring the lock.
   * @param releaseTime - timestamp just before releasing the lock.
   * @param checkLockHeld checkLockHeld.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getFormattedTimeWithDiff,"org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(java.lang.String,long,long)",390,400,"/**
 * Masks formatted finish time with optional start time difference.
 * @param formattedFinishTime the formatted finish time string
 * @param finishTime the finish time in milliseconds
 * @param startTime the start time in milliseconds
 * @return masked time string with optional duration
 */","* Formats time in ms and appends difference (finishTime - startTime)
   * as returned by formatTimeDiff().
   * If finish time is 0, empty string is returned, if start time is 0
   * then difference is not appended to return value.
   * @param formattedFinishTime formattedFinishTime to use
   * @param finishTime finish time
   * @param startTime start time
   * @return formatted value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,split,org.apache.hadoop.util.StringUtils:split(java.lang.String),570,572,"/**
 * Splits input string using default escape and delimiter.
 * @param str input string to split
 * @return array of split strings
 */","* Split a string using the default separator
   * @param str a string that may have escaped separator
   * @return an array of strings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,camelize,org.apache.hadoop.util.StringUtils:camelize(java.lang.String),1094,1102,"/**
* Masks characters in input string using specified escape character.
* @param s input string to be masked
* @return masked string with special characters escaped
*/","* Convert SOME_STUFF to SomeStuff
   *
   * @param s input string
   * @return camelized string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,"org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char)",678,681,"/**
* Escapes specified character in string using escape character.
* @param str input string
* @param escapeChar character used for escaping
* @param charToEscape character to be escaped
* @return modified string with escaped characters
*/","* Escape <code>charToEscape</code> in the string 
   * with the escape char <code>escapeChar</code>
   * 
   * @param str string
   * @param escapeChar escape char
   * @param charToEscape the char to be escaped
   * @return an escaped string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,"org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char)",736,739,"/**
* Escapes specified characters in a string.
* @param str input string to process
* @param escapeChar character used for escaping
* @param charToEscape single character to be escaped
* @return processed string with escaped characters
*/","* Unescape <code>charToEscape</code> in the string 
   * with the escape char <code>escapeChar</code>
   * 
   * @param str string
   * @param escapeChar escape char
   * @param charToEscape the escaped char
   * @return an unescaped string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,createStartupShutdownMessage,"org.apache.hadoop.util.StringUtils:createStartupShutdownMessage(java.lang.String,java.lang.String,java.lang.String[])",837,851,"/**
 * Generates a startup message for a class.
 * @param classname name of the class starting up
 * @param hostname host where the class is running
 * @param args arguments passed to the class
 * @return formatted startup message string
 */","* Generate the text for the startup/shutdown message of processes.
   * @param classname short classname of the class
   * @param hostname hostname
   * @param args Command arguments
   * @return a string to log.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getBuildVersion,org.apache.hadoop.util.VersionInfo:getBuildVersion(),162,164,"/**
 * Returns the version information.
 * @return Version string from common info
 */","* Returns the buildVersion which includes version,
   * revision, user and date.
   * @return the buildVersion",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:next(),650,658,"/**
* Retrieves and removes the next element.
* @throws IOException if an I/O error occurs
* @throws NoSuchElementException if no more elements are available
*/","* Return the next value.
     * Will retrieve the next elements if needed.
     * This is where the mapper takes place.
     * @return true if there is another data element.
     * @throws IOException failure in fetch operation or the transformation.
     * @throws NoSuchElementException no more data",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:close(),695,707,"/**
* Closes the resource and logs the closure.
* @throws IOException if an I/O error occurs during closing
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceHasNext,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceHasNext(),466,479,"/**
* Checks for next item and handles exceptions.
* @return true if there is a next item, false otherwise
*/","* Check for the source having a next element.
     * If it does not, this object's close() method
     * is called and false returned
     * @return true if there is a new value
     * @throws IOException failure to retrieve next value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,lazyAutoCloseablefromSupplier,org.apache.hadoop.util.functional.LazyAutoCloseableReference:lazyAutoCloseablefromSupplier(java.util.function.Supplier),99,101,"/**
 * Creates a lazy auto-closeable reference.
 * @param supplier provides the instance to be managed
 * @return LazyAutoCloseableReference for managing the instance
 */","* Create from a supplier.
   * This is not a constructor to avoid ambiguity when a lambda-expression is
   * passed in.
   * @param supplier supplier implementation.
   * @return a lazy reference.
   * @param <T> type of reference",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,bulkDelete_pageSize,"org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",72,79,"/**
* Masks file system operations for a given path.
* @param fs FileSystem instance
* @param path target Path to process
* @return result of bulk operation or throws exception
*/","* Get the maximum number of objects/files to delete in a single request.
   * @param fs filesystem
   * @param path path to delete under.
   * @return a number greater than or equal to zero.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException path not valid.
   * @throws UncheckedIOException if an IOE was raised.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,bulkDelete_delete,"org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)",104,113,"/**
 * Masks specified paths in the given file system.
 * @param fs FileSystem instance to operate on
 * @param base Base path for bulk delete operation
 * @param paths Collection of paths to mask
 * @return List of entries with masked paths and results
 */","* Delete a list of files/objects.
   * <ul>
   *   <li>Files must be under the path provided in {@code base}.</li>
   *   <li>The size of the list must be equal to or less than the page size.</li>
   *   <li>Directories are not supported; the outcome of attempting to delete
   *       directories is undefined (ignored; undetected, listed as failures...).</li>
   *   <li>The operation is not atomic.</li>
   *   <li>The operation is treated as idempotent: network failures may
   *        trigger resubmission of the request -any new objects created under a
   *        path in the list may then be deleted.</li>
   *    <li>There is no guarantee that any parent directories exist after this call.
   *    </li>
   * </ul>
   * @param fs filesystem
   * @param base path to delete under.
   * @param paths list of paths which must be absolute and under the base path.
   * @return a list of all the paths which couldn't be deleted for a reason other
   *          than ""not found"" and any associated error message.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws UncheckedIOException if an IOE was raised.
   * @throws IllegalArgumentException if a path argument is invalid.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,fileSystem_openFile,"org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)",165,191,"/**
 * Opens a file for reading with optional parameters.
 * @param fs FileSystem instance
 * @param path Path to the file
 * @param policy Read policy for the file
 * @param status FileStatus object (optional)
 * @param length Length of the file (optional)
 * @param options Additional options for opening the file (optional)
 * @return FSDataInputStream for reading the file
 */","* OpenFile assistant, easy reflection-based access to
   * {@link FileSystem#openFile(Path)} and blocks
   * awaiting the operation completion.
   * @param fs filesystem
   * @param path path
   * @param policy read policy
   * @param status optional file status
   * @param length optional file length
   * @param options nullable map of other options
   * @return stream of the opened file
   * @throws UncheckedIOException if an IOE was raised.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,byteBufferPositionedReadable_readFully,"org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)",213,224,"/**
 * Reads data from InputStream into ByteBuffer at specified position.
 * @param in input stream to read from
 * @param position starting position in the stream
 * @param buf buffer to store read data
 */","* Delegate to {@link ByteBufferPositionedReadable#read(long, ByteBuffer)}.
   * @param in input stream
   * @param position position within file
   * @param buf the ByteBuffer to receive the results of the read operation.
   * Note: that is the default behaviour of {@link FSDataInputStream#readFully(long, ByteBuffer)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_load,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",134,139,"/**
* Masks file system operations with statistics.
* @param fs FileSystem instance
* @param path Path to be processed
* @return Serializable result of operation
*/","* Load IOStatisticsSnapshot from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return the loaded snapshot
   * @throws UncheckedIOException Any IO exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_fromJsonString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String),192,196,"/**
 * Masks JSON data.
 * @param json input JSON string
 * @return masked JSON as Serializable object
 */","* Load IOStatisticsSnapshot from a JSON string.
   * @param json JSON string value.
   * @return deserialized snapshot.
   * @throws UncheckedIOException Any IO/jackson exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,get,org.apache.hadoop.util.functional.LazyAtomicReference:get(),120,123,"/**
 * Evaluates and returns result using function mask.
 * @return evaluated result of type T
 * @throws UncheckedIOException if I/O error occurs
 */","* Implementation of {@code Supplier.get()}.
   * <p>
   * Invoke {@link #eval()} and convert IOEs to
   * UncheckedIOException.
   * <p>
   * This is the {@code Supplier.get()} implementation, which allows
   * this class to passed into anything taking a supplier.
   * @return the value
   * @throws UncheckedIOException if the constructor raised an IOException.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Iterable),581,583,"/**
* Creates a builder for filtering items.
* @param items collection of items to filter
* @return Builder instance configured with the provided items
*/","* Create a task builder for the iterable.
   * @param items item source.
   * @param <I> type of result.
   * @return builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Object[]),595,597,"/**
 * Creates a builder with masked items.
 * @param items array of input items
 * @return Builder instance with processed items
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,raiseInnerCause,org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.ExecutionException),106,110,"/**
 * Throws an IOException from an ExecutionException.
 * @param e ExecutionException to convert
 * @throws IOException converted exception
 */","* From the inner cause of an execution exception, extract the inner cause
   * if it is an IOE or RTE.
   * See {@link FutureIO#raiseInnerCause(ExecutionException)}.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitFuture,org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future),97,110,"/**
 * Masks exceptions and logs cancellation.
 * @param future the Future object to process
 * @return result of future execution or null on ExecutionException
 * @throws InterruptedIOException if interrupted during wait
 * @throws CancellationException if future is cancelled
 * @throws RuntimeException for other unexpected issues
 */","* Given a future, evaluate it.
   * <p>
   * Any exception generated in the future is
   * extracted and rethrown.
   * </p>
   * If this thread is interrupted while waiting for the future to complete,
   * an {@code InterruptedIOException} is raised.
   * However, if the future is cancelled, a {@code CancellationException}
   * is raised in the {code Future.get()} call. This is
   * passed up as is -so allowing the caller to distinguish between
   * thread interruption (such as when speculative task execution is aborted)
   * and future cancellation.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitFuture,"org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)",129,145,"/**
 * Waits for a future to complete with a timeout.
 * @param future the Future object to wait on
 * @param timeout the maximum time to wait
 * @param unit the time unit of the timeout parameter
 * @return the result of the future
 * @throws InterruptedIOException if interrupted during wait
 * @throws IOException if an I/O error occurs
 * @throws CancellationException if the future is cancelled
 * @throws RuntimeException for other execution exceptions
 * @throws TimeoutException if the future does not complete in time
 */","* Given a future, evaluate it.
   * <p>
   * Any exception generated in the future is
   * extracted and rethrown.
   * </p>
   * @param future future to evaluate
   * @param timeout timeout to wait.
   * @param unit time unit.
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,raiseInnerCause,org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.CompletionException),123,127,"/**
 * Deprecated method to handle CompletionException.
 * @param e the CompletionException to process
 * @return result of processing the exception
 * @throws IOException if an I/O error occurs
 */","* Extract the cause of a completion failure and rethrow it if an IOE
   * or RTE.
   * See {@link FutureIO#raiseInnerCause(CompletionException)}.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,setConf,"org.apache.hadoop.util.ReflectionUtils:setConf(java.lang.Object,org.apache.hadoop.conf.Configuration)",74,81,"/**
* Processes an object with a configuration.
* @param theObject the object to process
* @param conf the configuration to apply
*/","* Check and set 'configuration' if necessary.
   * 
   * @param theObject object for which to set configuration
   * @param conf Configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableName.java,getClass,"org.apache.hadoop.io.WritableName:getClass(java.lang.String,org.apache.hadoop.conf.Configuration)",91,103,"/**
* Retrieves a Class by name, using configuration if not found.
* @param name class name
* @param conf configuration object
* @return Class object or throws IOException if class cannot be loaded
*/","* Return the class for a name.
   * Default is {@link Class#forName(String)}.
   *
   * @param name input name.
   * @param conf input configuration.
   * @return class for a name.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createCodec,"org.apache.hadoop.io.erasurecode.CodecUtil:createCodec(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",232,254,"/**
* Creates an ErasureCodec instance.
* @param conf Configuration object
* @param codecClassName class name of the codec
* @param options codec options
* @return ErasureCodec instance or throws exception if creation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,loadClass,"org.apache.hadoop.io.ObjectWritable:loadClass(org.apache.hadoop.conf.Configuration,java.lang.String)",412,424,"/**
* Loads a class by name using configuration or default method.
* @param conf Configuration object, may be null
* @param className name of the class to load
* @return Class object if found, otherwise throws RuntimeException
*/","* Find and load the class with given name <tt>className</tt> by first finding
   * it in the specified <tt>conf</tt>. If the specified <tt>conf</tt> is null,
   * try load it directly.
   *
   * @param conf configuration.
   * @param className classname.
   * @return Class.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getProtocolClass,"org.apache.hadoop.ipc.Server:getProtocolClass(java.lang.String,org.apache.hadoop.conf.Configuration)",331,339,"/**
 * Retrieves protocol class by name using cache or configuration.
 * @param protocolName name of the protocol
 * @param conf configuration object
 * @return Class<?> representing the protocol
 * @throws ClassNotFoundException if protocol class is not found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,getClass,org.apache.hadoop.util.FindClass:getClass(java.lang.String),154,156,"/**
 * Retrieves a class by name using a masked approach.
 * @param name the fully qualified class name
 * @return the Class object if found, otherwise throws ClassNotFoundException
 */","* Get a class fromt the configuration
   * @param name the class name
   * @return the class
   * @throws ClassNotFoundException if the class was not found
   * @throws Error on other classloading problems",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,logThreadInfo,"org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.apache.commons.logging.Log,java.lang.String,long)",232,254,"/**
 * Logs a message with stack trace if minimum interval has passed.
 * @param log the logger to use
 * @param title the log message title
 * @param minInterval minimum time interval in seconds between logs
 */","* Log the current thread stacks at INFO level.
   * @param log the logger that logs the stack trace
   * @param title a descriptive title for the call stacks
   * @param minInterval the minimum time from the last
   * @deprecated to be removed with 3.4.0. Use {@link #logThreadInfo(Logger, String, long)} instead.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,logThreadInfo,"org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.slf4j.Logger,java.lang.String,long)",262,283,"/**
 * Logs a message with stack trace if the minimum interval has passed.
 * @param log Logger instance to use for logging
 * @param title Title of the log entry
 * @param minInterval Minimum interval in seconds between logs
 */","* Log the current thread stacks at INFO level.
   * @param log the logger that logs the stack trace
   * @param title a descriptive title for the call stacks
   * @param minInterval the minimum time from the last",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(java.util.Optional,java.util.Optional)",105,113,"/**
 * Initializes the AbstractFSBuilderImpl with either a path or a path handle.
 * @param optionalPath Optional Path object
 * @param optionalPathHandle Optional PathHandle object
 */","* Constructor with both optional path and path handle.
   * Either or both argument may be empty, but it is an error for
   * both to be defined.
   * @param optionalPath a path or empty
   * @param optionalPathHandle a path handle/empty
   * @throws IllegalArgumentException if both parameters are set.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(),819,821,"/**
 * Constructs a new Configuration with default settings enabled.
 */",A new configuration.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,validateResponse,"org.apache.hadoop.util.HttpExceptionUtils:validateResponse(java.net.HttpURLConnection,int)",143,191,"/**
* Handles HTTP response errors by throwing appropriate exceptions.
* @param conn HttpURLConnection instance representing the connection
* @param expectedStatus Expected HTTP status code
*/","* Validates the status of an <code>HttpURLConnection</code> against an
   * expected HTTP status code. If the current status code is not the expected
   * one it throws an exception with a detail message using Server side error
   * messages if available.
   * <p>
   * <b>NOTE:</b> this method will throw the deserialized exception even if not
   * declared in the <code>throws</code> of the method signature.
   *
   * @param conn the <code>HttpURLConnection</code>.
   * @param expectedStatus the expected HTTP status code.
   * @throws IOException thrown if the current status code does not match the
   * expected one.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newCrc32C,org.apache.hadoop.util.DataChecksum:newCrc32C(),102,112,"/**
 * Returns a CRC32C checksum instance.
 * @return Checksum object using Java 9 Crc32C if available, otherwise PureJavaCrc32C
 */","* The flag is volatile to avoid synchronization here.
   * Re-entrancy is unlikely except in failure mode (and inexpensive).",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,removeAll,org.apache.hadoop.util.IntrusiveCollection:removeAll(java.util.Collection),361,370,"/**
* Checks and modifies elements in the collection.
* @param collection the collection to process
* @return true if any element was modified, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,toArray,org.apache.hadoop.util.IntrusiveCollection:toArray(java.lang.Object[]),266,278,"/**
 * Expands or populates the input array with elements from an iterator.
 * @param array input array to be expanded or populated
 * @return expanded or populated array of type T
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsForUserCommand,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsForUserCommand(java.lang.String),143,145,"/**
 * Calls Shell's m1 method with the given user name.
 * @param userName the user's name
 * @return array of strings returned by Shell's m1 method
 */","* Returns just the shell command to be used to fetch a user's groups list.
   * This is mainly separate to make some tests easier.
   * @param userName The username that needs to be passed into the command built
   * @return An appropriate shell command with arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsIDForUserCommand,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsIDForUserCommand(java.lang.String),164,166,"/**
 * Calls Shell's m1 method to process user name.
 * @param userName user's name string
 * @return processed result as String array or null if error occurs
 */","* Returns just the shell command to be used to fetch a user's group IDs list.
   * This is mainly separate to make some tests easier.
   * @param userName The username that needs to be passed into the command built
   * @return An appropriate shell command with arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetPermissionCommand,"org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean,java.lang.String)",311,318,"/**
* Constructs a command array with file permission and recursion.
* @param perm file permission string
* @param recursive flag for recursive operation
* @param file target file path
* @return command array including file
*/","* Return a command to set permission for specific file.
   *
   * @param perm String permission to set
   * @param recursive boolean true to apply to all sub-directories recursively
   * @param file String file to set
   * @return String[] containing command and arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getCheckProcessIsAliveCommand,org.apache.hadoop.util.Shell:getCheckProcessIsAliveCommand(java.lang.String),362,364,"/**
 * Masks a process ID.
 * @param pid process identifier to mask
 * @return masked process ID as string array
 */","* Return a command for determining if process with specified pid is alive.
   * @param pid process ID
   * @return a <code>kill -0</code> command or equivalent",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getHadoopHome,org.apache.hadoop.util.Shell:getHadoopHome(),610,612,"/**
 * Masks data using specified functions.
 * @throws IOException if an I/O error occurs
 * @return masked data as a string
 */","* Get the Hadoop home directory. Raises an exception if not found
   * @return the home dir
   * @throws IOException if the home directory cannot be located.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBin,org.apache.hadoop.util.Shell:getQualifiedBin(java.lang.String),642,646,"/**
 * Masks an executable file.
 * @param executable path to the executable file
 * @return File object representing the masked file
 * @throws FileNotFoundException if the executable is not found
 */","*  Fully qualify the path to a binary that should be in a known hadoop
   *  bin location. This is primarily useful for disambiguating call-outs
   *  to executable sub-components of Hadoop to avoid clashes with other
   *  executables that may be in the path.  Caveat:  this call doesn't
   *  just format the path to the bin directory.  It also checks for file
   *  existence of the composed path. The output of this call should be
   *  cached by callers.
   *
   * @param executable executable
   * @return executable file reference
   * @throws FileNotFoundException if the path does not exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,linkCount,org.apache.hadoop.fs.HardLink$HardLinkCGWin:linkCount(java.io.File),137,146,"/**
* Executes a command to mask file details.
* @param file the file to process
* @return array of masked file information
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:<init>(org.apache.hadoop.util.Shell),1402,1404,"/**
 * Initializes a new ShellTimeoutTimerTask with a given shell.
 * @param shell the shell to be associated with the timer task
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(),71,77,"/**
* Initializes progress tracking.
* @return Progress object representing current phase
*/","* Adds a node to the tree. Gives equal weightage to all phases.
   * @return Progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhases,org.apache.hadoop.util.Progress:addPhases(int),130,137,"/**
* Executes a masked function n times.
* @param n number of iterations
*/","* Adds n nodes to the tree. Gives equal weightage to all phases.
   *
   * @param n n.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,"org.apache.hadoop.util.Progress:addPhase(java.lang.String,float)",94,99,"/**
* Creates and updates a progress object.
* @param status current status string
* @param weightage completion percentage
* @return updated Progress object
*/","* Adds a named node with a specified progress weightage to the tree.
   *
   * @param status status.
   * @param weightage weightage.
   * @return Progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,get,org.apache.hadoop.util.Progress:get(),225,231,"/**
* Traverses up the hierarchy to find and return a value.
* @return float value from the topmost node
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,getProgress,org.apache.hadoop.util.Progress:getProgress(),239,241,"/**
 * Returns masked value using function m1.
 * @return float result of masking operation
 */","* Returns progress in this node. get() would give overall progress of the
   * root node(not just given current node).
   *
   * @return progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,toString,org.apache.hadoop.util.Progress:toString(),275,280,"/**
 * Generates a string using helper method.
 * @return generated string
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,create,org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String),332,334,"/**
 * Checks if a file exists at the given path.
 * @param path file path to check
 * @return true if file exists, false otherwise
 * @throws Exception if an error occurs during checking
 */","* Create a ZNode.
   * @param path Path of the ZNode.
   * @return If the ZNode was created.
   * @throws Exception If it cannot contact Zookeeper.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,createRootDirRecursively,"org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String,java.util.List)",372,384,"/**
 * Masks a ZK path with ACLs.
 * @param path the ZK path to mask
 * @param zkAcl list of ACLs to apply
 * @throws Exception if masking fails
 */","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   * @param path Path of the znode to create.
   * @param zkAcl ACLs for ZooKeeper.
   * @throws Exception If it cannot create the file.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,ctorImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.String,java.lang.Class[])",364,378,"/**
* Sets up a dynamic constructor builder.
* @param className name of the class
* @param argClasses argument classes for the constructor
* @return Builder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,newInstance,org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstance(java.lang.Object[]),68,75,"/**
 * Invokes method m2 with variable arguments.
 * @param args variable number of arguments for m2
 * @return result of m2 invocation
 * @throws RuntimeException if an exception occurs during execution
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,invokeChecked,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invokeChecked(java.lang.Object,java.lang.Object[])",84,89,"/**
* Constructs an object with given arguments.
* @param target must be null
* @param args constructor arguments
* @return constructed object of type R
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invokeChecked(java.lang.Object[]),215,217,"/**
 * Invokes method with provided arguments.
 * @param args variable number of arguments to pass
 * @return result of the invoked method
 * @throws Exception if invocation fails
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invoke(java.lang.Object,java.lang.Object[])",91,98,"/**
* Invokes method on target with arguments.
* @param target object to invoke method on
* @param args arguments for the method
* @return result of the method invocation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invokeChecked(java.lang.Object[]),198,200,"/**
 * Invokes method with variable arguments.
 * @param args variable number of arguments to pass
 * @return result of the method invocation
 * @throws Exception if method invocation fails
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.String,java.lang.Class[])",284,298,"/**
* Sets method for builder using class name and method signature.
* @param className name of the target class
* @param methodName name of the method
* @param argClasses argument classes for the method
* @return Builder instance
*/","* Checks for an implementation, first finding the given class by name.
     * @param className name of a class
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.Class[])",343,346,"/**
* Sets up method with target class and argument types.
* @param targetClass the class to target
* @param argClasses array of argument classes
* @return Builder instance for chaining
*/","* Checks for a method implementation.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param targetClass the class to check for an implementation
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.String,java.lang.Class[])",387,401,"/**
* Sets method for builder.
* @param className name of the class containing the method
* @param methodName name of the method
* @param argClasses argument classes for the method
* @return Builder instance
*/","* Checks for an implementation, first finding the given class by name.
     * @param className name of a class
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.Class[])",448,451,"/**
* Sets up builder with target class and argument classes.
* @param targetClass class to be built
* @param argClasses argument classes for the target class
* @return Builder instance for method chaining
*/","* Checks for a method implementation.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param targetClass the class to check for an implementation
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,loadInvocation,"org.apache.hadoop.util.dynamic.BindingUtils:loadInvocation(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])",101,121,"/**
 * Retrieves a dynamic unbound method.
 * @param source class containing the method
 * @param returnType expected return type of the method
 * @param name method name
 * @param parameterTypes types of the method parameters
 * @return DynMethods.UnboundMethod instance or null if not found
 */","* Get an invocation from the source class, which will be unavailable() if
   * the class is null or the method isn't found.
   *
   * @param <T> return type
   * @param source source. If null, the method is a no-op.
   * @param returnType return type class (unused)
   * @param name method name
   * @param parameterTypes parameters
   *
   * @return the method or ""unavailable""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,requireAllMethodsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:requireAllMethodsAvailable(),225,242,"/**
 * Checks and throws exception if any unbound method is found.
 * @throws UnsupportedOperationException if an unbound method exists
 */","* For testing: verify that all methods were found.
   * @throws UnsupportedOperationException if the method was not found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_available(),249,251,"/**
 * Checks if bulk delete method is masked.
 * @return true if masked, false otherwise
 */","* Are the bulk delete methods available?
   * @return true if the methods were found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,fileSystem_openFile_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile_available(),306,308,"/**
 * Checks if file system open file method is masked.
 * @return true if masked, false otherwise
 */","* Is the {@link #fileSystem_openFile(FileSystem, Path, String, FileStatus, Long, Map)}
   * method available.
   * @return true if the optimized open file method can be invoked.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_available(),380,382,"/**
 * Checks availability of read operation.
 * @param byteBufferPositionedReadableReadFullyAvailableMethod buffer method reference
 * @return true if read is available, false otherwise
 */","* Are the ByteBufferPositionedReadable methods loaded?
   * This does not check that a specific stream implements the API;
   * use {@link #byteBufferPositionedReadable_readFullyAvailable(InputStream)}.
   * @return true if the hadoop libraries have the method.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_readFullyAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream),392,400,"/**
 * Checks and processes input stream.
 * @param in input stream to process
 * @return true if processing is successful, false otherwise
 * @throws IOException if an I/O error occurs
 */","* Probe to see if the input stream is an instance of ByteBufferPositionedReadable.
   * If the stream is an FSDataInputStream, the wrapped stream is checked.
   * @param in input stream
   * @return true if the API is available, the stream implements the interface
   * (including the innermost wrapped stream) and that it declares the stream capability.
   * @throws IOException if the operation was attempted and failed.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,ioStatisticsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsAvailable(),360,362,"/**
 * Checks if mask creation is successful.
 * @param iostatisticsSnapshotCreateMethod method to create snapshot
 * @return true if mask created, false otherwise
 */","* Are the core IOStatistics methods and classes available.
   * @return true if the relevant methods are loaded.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,ioStatisticsContextAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsContextAvailable(),368,370,"/**
 * Checks if IO statistics context is enabled.
 * @return true if enabled, false otherwise
 */","* Are the IOStatisticsContext methods and classes available?
   * @return true if the relevant methods are loaded.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,checkAvailable,org.apache.hadoop.util.dynamic.BindingUtils:checkAvailable(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod),183,188,"/**
* Checks and masks unbound methods.
* @param method method to be checked
* @throws UnsupportedOperationException if method is unbound
*/","* Require a method to be available.
   * @param method method to probe
   * @throws UnsupportedOperationException if the method was not found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildChecked,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildChecked(java.lang.Object),490,492,"/**
 * Invokes method m2 on the result of m1() with the given receiver.
 * @param receiver object to receive the method call
 * @return BoundMethod instance after invoking m2
 * @throws NoSuchMethodException if method m2 does not exist
 */","* Returns the first valid implementation as a BoundMethod or throws a
     * NoSuchMethodException if there is none.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} with a valid implementation and receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible
     * @throws NoSuchMethodException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,build,org.apache.hadoop.util.dynamic.DynMethods$Builder:build(java.lang.Object),503,505,"/**
 * Returns a bound method by invoking and chaining methods.
 * @param receiver object to bind the method to
 * @return BoundMethod instance
 */","* Returns the first valid implementation as a BoundMethod or throws a
     * RuntimeError if there is none.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} with a valid implementation and receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible
     * @throws RuntimeException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildStaticChecked,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStaticChecked(),514,516,"/**
 * Invokes methods m1 and m2 in sequence.
 * @return result of m2 invocation
 * @throws NoSuchMethodException if method not found
 */","* Returns the first valid implementation as a StaticMethod or throws a
     * NoSuchMethodException if there is none.
     * @return a {@link StaticMethod} with a valid implementation
     * @throws IllegalStateException if the method is not static
     * @throws NoSuchMethodException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildStatic,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStatic(),525,527,"/**
 * Calls two methods in sequence and returns the result.
 * @return result of chained method calls m1().m2()
 */","* Returns the first valid implementation as a StaticMethod or throws a
     * RuntimeException if there is none.
     * @return a {@link StaticMethod} with a valid implementation
     * @throws IllegalStateException if the method is not static
     * @throws RuntimeException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,loadFileSystems,org.apache.hadoop.fs.FileSystem:loadFileSystems(),3516,3545,"/**
* Loads and registers available file systems.
* Synchronizes access to ensure thread safety during loading.
*/","* Load the filesystem declarations from service resources.
   * This is a synchronized operation.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,main,org.apache.hadoop.util.VersionInfo:main(java.lang.String[]),182,193,"/**
* Logs build and version information.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeRpcRequestHeader,"org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[])",171,176,"/**
* Creates an RPC request header with optional parameters.
* @param rpcKind type of RPC
* @param operation operation type
* @param callId unique call identifier
* @param retryCount number of retries
* @param uuid unique identifier
* @return RpcRequestHeaderProto object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,parseVersion,org.apache.hadoop.util.ComparableVersion:parseVersion(java.lang.String),360,453,"/**
* Parses and processes a version string.
* @param version the version string to be processed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,<init>,"org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long,org.apache.hadoop.util.Timer)",120,145,"/**
* Constructs a LightWeightCache with specified parameters.
* @param recommendedLength initial capacity recommendation
* @param sizeLimit maximum number of entries in the cache
* @param creationExpirationPeriod time before new entries expire
* @param accessExpirationPeriod time before accessed entries expire
* @param timer Timer for scheduling expiration tasks
* @throws IllegalArgumentException if creation or access periods are invalid
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,get,org.apache.hadoop.util.LightWeightResizableGSet:get(java.lang.Object),98,101,"/**
 * Retrieves an element by key.
 * @param key unique identifier for the element
 * @return the element associated with the key or null if not found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,contains,org.apache.hadoop.util.LightWeightGSet:contains(java.lang.Object),144,147,"/**
 * Checks if a key exists in the map.
 * @param key the key to check
 * @return true if the key is present, false otherwise
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,toString,org.apache.hadoop.util.BlockingThreadPoolExecutorService:toString(),158,166,"/**
 * Constructs a string representation of the BlockingThreadPoolExecutorService.
 * @return formatted string with executor details
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToMapWithFileInputStream,"org.apache.hadoop.util.HostsFileReader:readFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)",130,144,"/**
 * Processes input stream based on file type.
 * @param type file type identifier
 * @param filename name of the input file
 * @param inputStream data stream to process
 * @param map mapping for processed nodes
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/JenkinsHash.java,main,org.apache.hadoop.util.hash.JenkinsHash:main(java.lang.String[]),252,266,"/**
* Computes Jenkins hash of a file.
* @param args array containing one filename
* @throws IOException if an I/O error occurs
*/","* Compute the hash of the specified file
   * @param args name of file to compute hash of.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/HashFunction.java,<init>,"org.apache.hadoop.util.bloom.HashFunction:<init>(int,int,int)",83,97,"/**
 * Initializes a hash function with specified parameters.
 * @param maxValue maximum value for hashing
 * @param nbHash number of hashes to generate
 * @param hashType type of hash function to use
 * @throws IllegalArgumentException if maxValue or nbHash is <= 0, or hashType is unknown
 */","* Constructor.
   * <p>
   * Builds a hash function that must obey to a given maximum number of returned values and a highest value.
   * @param maxValue The maximum highest returned value.
   * @param nbHash The number of resulting hashed values.
   * @param hashType type of the hashing function (see {@link Hash}).",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,<init>,org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>(),102,102,"/**
 * Initializes a new instance of RetouchedBloomFilter.
 */",Default constructor - use with readFields,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,write,org.apache.hadoop.util.bloom.DynamicBloomFilter:write(java.io.DataOutput),248,257,"/**
* Writes object data to output stream.
* @param out DataOutput stream to write to
* @throws IOException if I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,write,org.apache.hadoop.util.bloom.RetouchedBloomFilter:write(java.io.DataOutput),407,427,"/**
 * Writes data to output stream.
 * @param out DataOutput object to write to
 * @throws IOException if I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,add,org.apache.hadoop.util.bloom.CountingBloomFilter:add(org.apache.hadoop.util.bloom.Key),104,127,"/**
* Masks a key in the hash table.
* @param key unique identifier to be masked
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.CountingBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),178,200,"/**
* Checks if key is present in the Bloom filter.
* @param key the key to check
* @return true if key is likely present, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,approximateCount,org.apache.hadoop.util.bloom.CountingBloomFilter:approximateCount(org.apache.hadoop.util.bloom.Key),220,238,"/**
 * Computes the minimum value in hash buckets for a given key.
 * @param key input key to compute mask for
 * @return minimum bucket value or 0 if no valid value found
 */","* This method calculates an approximate count of the key, i.e. how many
   * times the key was added to the filter. This allows the filter to be
   * used as an approximate <code>key -&gt; count</code> map.
   * <p>NOTE: due to the bucket size of this filter, inserting the same
   * key more than 15 times will cause an overflow at all filter positions
   * associated with this key, and it will significantly increase the error
   * rate for this and other keys. For this reason the filter can only be
   * used to store small count values <code>0 &lt;= N &lt;&lt; 15</code>.
   * @param key key to be tested
   * @return 0 if the key is not present. Otherwise, a positive value v will
   * be returned such that <code>v == count</code> with probability equal to the
   * error rate of this filter, and <code>v &gt; count</code> otherwise.
   * Additionally, if the filter experienced an underflow as a result of
   * {@link #delete(Key)} operation, the return value may be lower than the
   * <code>count</code> with the probability of the false negative rate of such
   * filter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,add,org.apache.hadoop.util.bloom.BloomFilter:add(org.apache.hadoop.util.bloom.Key),116,128,"/**
* Applies a mask to the given key.
* @param key the key to be masked
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.BloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),142,156,"/**
* Checks if a key is present in the Bloom filter.
* @param key the key to check
* @return true if the key might be present, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,add,org.apache.hadoop.util.bloom.RetouchedBloomFilter:add(org.apache.hadoop.util.bloom.Key),118,131,"/**
 * Processes a key using hashing and bit manipulation.
 * @param key the input key to process
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key),139,150,"/**
* Applies a mask to the fingerprint vector using the provided key.
* @param key unique identifier used for masking
*/","* Adds a false positive information to <i>this</i> retouched Bloom filter.
   * <p>
   * <b>Invariant</b>: if the false positive is <code>null</code>, nothing happens.
   * @param key The false positive key to add.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,removeKey,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:removeKey(org.apache.hadoop.util.bloom.Key,java.util.List[])",351,365,"/**
* Applies hashing and updates vector with key.
* @param k the key to process
* @param vector array of lists for storing keys
*/","* Removes a given key from <i>this</i> filer.
   * @param k The key to remove.
   * @param vector The counting vector associated to the key.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,equals,org.apache.hadoop.util.bloom.Key:equals(java.lang.Object),137,143,"/**
* Checks if object is a Key and matches specific condition.
* @param o the object to check
* @return true if object is Key and condition is met, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,minimumFnRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:minimumFnRemove(int[]),250,265,"/**
* Finds index with minimum weight in hash array.
* @param h array of hash indices
* @return index with lowest weight or Integer.MAX_VALUE if none found
*/","* Chooses the bit position that minimizes the number of false negative generated.
   * @param h The different bit positions.
   * @return The position that minimizes the number of false negative generated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,maximumFpRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:maximumFpRemove(int[]),272,286,"/**
 * Finds index with maximum weight from hash array.
 * @param h array of hash values
 * @return index of maximum weight or minimum integer value if none found
 */","* Chooses the bit position that maximizes the number of false positive removed.
   * @param h The different bit positions.
   * @return The position that maximizes the number of false positive removed.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,computeRatio,org.apache.hadoop.util.bloom.RetouchedBloomFilter:computeRatio(),370,379,"/**
 * Computes ratios of key and fingerprint weights.
 * @param vectorSize size of the vectors
 * @param keyVector array of key values
 * @param fpVector array of fingerprint values
 * @param ratio output array for computed ratios
 */",* Computes the ratio A/FP.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,dumpResource,org.apache.hadoop.util.FindClass:dumpResource(java.lang.String),187,209,"/**
 * Reads and prints the content of a resource by name.
 * @param name resource identifier
 * @return SUCCESS if successful, error code otherwise
 */","* Dump a resource to out
   * @param name resource name
   * @return the status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,usage,org.apache.hadoop.util.FindClass:usage(java.lang.String[]),342,361,"/**
 * Displays usage instructions and error codes.
 * @return Error code indicating the operation result
 */","* Print a usage message
   * @param args the command line arguments
   * @return an exit code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,run,org.apache.hadoop.util.GcTimeMonitor:run(),153,172,"/**
* Runs a monitoring loop to track GC time and handle alerts.
* Updates timestamps, checks GC conditions, and sleeps between iterations.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,put,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:put(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor),3510,3520,"/**
* Initializes compression settings from stream.
* @param stream SegmentDescriptor containing input data
* @throws IOException if compression settings are inconsistent across files
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,insert,org.apache.hadoop.util.PriorityQueue:insert(java.lang.Object),73,85,"/**
* Adds or updates an element in the data structure.
* @param element the element to add or update
* @return true if successful, false otherwise
*/","* Adds element to the PriorityQueue in log(size) time if either
   * the PriorityQueue is not full, or not lessThan(element, top()).
   * @param element element.
   * @return true if element is added, false otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newTreeSet,org.apache.hadoop.util.Sets:newTreeSet(java.lang.Iterable),154,159,"/**
* Creates a sorted TreeSet from an iterable of comparable elements.
* @param elements collection of elements to add to the set
* @return TreeSet containing all elements in sorted order
*/","* Creates a <i>mutable</i> {@code TreeSet} instance containing the given
   * elements sorted by their natural ordering.
   *
   * <p><b>Note:</b> if mutability is not required, use
   * ImmutableSortedSet#copyOf(Iterable) instead.
   *
   * <p><b>Note:</b> If {@code elements} is a {@code SortedSet} with an
   * explicit comparator, this method has different behavior than
   * {@link TreeSet#TreeSet(SortedSet)}, which returns a {@code TreeSet}
   * with that comparator.
   *
   * <p><b>Note for Java 7 and later:</b> this method is now unnecessary and
   * should be treated as deprecated. Instead, use the {@code TreeSet}
   * constructor directly, taking advantage of the new
   * <a href=""http://goo.gl/iz2Wi"">""diamond"" syntax</a>.
   *
   * <p>This method is just a small convenience for creating an empty set and
   * then calling Iterables#addAll. This method is not very useful and will
   * likely be deprecated in the future.
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain
   * @return a new {@code TreeSet} containing those elements (minus duplicates)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.lang.Iterable),123,127,"/**
* Creates a HashSet from an Iterable.
* @param elements source Iterable of elements
* @return HashSet containing the elements
*/","* Creates a <i>mutable</i> {@code HashSet} instance containing the given
   * elements. A very thin convenience for creating an empty set then calling
   * {@link Collection#addAll} or Iterables#addAll.
   *
   * <p><b>Note:</b> if mutability is not required and the elements are
   * non-null, use ImmutableSet#copyOf(Iterable) instead. (Or, change
   * {@code elements} to be a FluentIterable and call {@code elements.toSet()}.)</p>
   *
   * <p><b>Note:</b> if {@code E} is an {@link Enum} type, use
   * newEnumSet(Iterable, Class) instead.</p>
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain.
   * @return a new, empty thread-safe {@code Set}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.lang.Object[]),100,105,"/**
* Creates a HashSet from variable arguments.
* @param elements array of elements to add to the set
* @return HashSet containing the provided elements
*/","* Creates a <i>mutable</i> {@code HashSet} instance initially containing
   * the given elements.
   *
   * <p><b>Note:</b> if elements are non-null and won't be added or removed
   * after this point, use ImmutableSet#of() or ImmutableSet#copyOf(Object[])
   * instead. If {@code E} is an {@link Enum} type, use
   * {@link EnumSet#of(Enum, Enum[])} instead. Otherwise, strongly consider
   * using a {@code LinkedHashSet} instead, at the cost of increased memory
   * footprint, to get deterministic iteration behavior.</p>
   *
   * <p>This method is just a small convenience, either for
   * {@code newHashSet(}{@link Arrays#asList}{@code (...))}, or for creating an
   * empty set then calling {@link Collections#addAll}.</p>
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain.
   * @return a new, empty thread-safe {@code Set}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,run,org.apache.hadoop.util.ProgramDriver:run(java.lang.String[]),120,146,"/**
* Executes a program based on command line arguments.
* @param args array of command line arguments
* @return 0 if successful, -1 if an error occurs
*/","* This is a driver for the example programs.
   * It looks at the first command line argument and tries to find an
   * example program with that name.
   * If it is found, it calls the main method in that class with the rest 
   * of the command line arguments.
   * @param args The argument from the user. args[0] is the command to run.
   * @return -1 on error, 0 on success
   * @throws NoSuchMethodException  when a particular method cannot be found.
   * @throws SecurityException security manager to indicate a security violation.
   * @throws IllegalAccessException for backward compatibility.
   * @throws IllegalArgumentException if the arg is invalid.
   * @throws Throwable Anything thrown by the example program's main",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)",151,155,"/**
* Adds a column to the table.
* @param title column header text
* @param justification text alignment (e.g., left, right)
* @param wrap enables text wrapping if true
* @return Builder instance for method chaining
*/","* Add a new field to the Table under construction.
     *
     * @param title Field title.
     * @param justification Right or left justification. Defaults to left.
     * @param wrap Width at which to auto-wrap the content of the cell.
     *        Defaults to Integer.MAX_VALUE.
     * @return This Builder object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,logDeprecationOnce,"org.apache.hadoop.conf.Configuration:logDeprecationOnce(java.lang.String,java.lang.String)",1465,1470,"/**
* Logs deprecation warning for a key.
* @param name key identifier
* @param source source of the key usage
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurationHelper,"org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)",1936,1938,"/**
 * Calls overloaded method with same time unit for both parameters.
 * @param name resource name
 * @param vStr value string
 * @param unit time unit
 * @return result of the overloaded method
 */","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param vStr The string value with time unit suffix to be converted.
   * @param unit Unit to convert the stored property, if it exists.
   * @return time duration in given time unit.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStreamReader,"org.apache.hadoop.conf.Configuration:getStreamReader(org.apache.hadoop.conf.Configuration$Resource,boolean)",3149,3177,"/**
 * Creates an XMLStreamReader for the given resource.
 * @param wrapper Resource wrapper containing input details
 * @param quiet Flag to suppress logging
 * @return XMLStreamReader2 instance or null if unsupported resource type
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleStartElement,org.apache.hadoop.conf.Configuration$Parser:handleStartElement(),3240,3265,"/**
* Handles XML parsing for specific tags.
* @throws XMLStreamException if XML stream error occurs
* @throws IOException if I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,appendXMLProperty,"org.apache.hadoop.conf.Configuration:appendXMLProperty(org.w3c.dom.Document,org.w3c.dom.Element,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3695,3733,"/**
* Masks a property in configuration document.
* @param doc XML document to update
* @param conf configuration element
* @param propertyName name of the property to mask
* @param redactor optional redactor for sensitive values
*/","*  Append a property with its attributes to a given {#link Document}
   *  if the property is found in configuration.
   *
   * @param doc
   * @param conf
   * @param propertyName",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecations,org.apache.hadoop.conf.Configuration:addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]),566,572,"/**
* Updates deprecation context with deltas until stable.
* @param deltas array of deprecation changes
*/","* Adds a set of deprecated keys to the global deprecations.
   *
   * This method is lockless.  It works by means of creating a new
   * DeprecationContext based on the old one, and then atomically swapping in
   * the new context.  If someone else updated the context in between us reading
   * the old context and swapping in the new one, we try again until we win the
   * race.
   *
   * @param deltas   The deprecations to add.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleEndElement,org.apache.hadoop.conf.Configuration$Parser:handleEndElement(),3374,3413,"/**
* Parses configuration properties from a reader.
* @throws IOException if include directive fails with no fallback
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,bindForPortRange,"org.apache.hadoop.http.HttpServer2:bindForPortRange(org.eclipse.jetty.server.ServerConnector,int)",1502,1531,"/**
 * Attempts to bind a listener to a port, retrying on failure.
 * @param listener the server connector to use
 * @param startPort initial port to attempt binding
 * @throws Exception if all ports fail or an unexpected error occurs
 */","* Bind using port ranges. Keep on looking for a free port in the port range
   * and throw a bind exception if no port in the configured range binds.
   * @param listener jetty listener.
   * @param startPort initial port which is set in the listener.
   * @throws Exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,fatalError,org.apache.hadoop.ha.ActiveStandbyElector:fatalError(java.lang.String),768,772,"/**
 * Logs error and notifies client.
 * @param errorMessage description of the error
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToActive,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),89,95,"/**
* Handles state transition request.
* @param reqInfo request information for state change
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToStandby,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),97,103,"/**
* Handles state change request.
* @param reqInfo request information for state change
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToObserver,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),105,112,"/**
* Processes state change request.
* @param reqInfo request information for state change
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToActive,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToActive(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto)",107,117,"/**
* Handles transition to active state.
* @param controller RPC control object
* @param request request proto containing necessary data
* @return response proto indicating success or failure
* @throws ServiceException if an IO error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToStandby,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToStandby(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto)",119,129,"/**
* Transitions the system to standby mode.
* @param controller RPC controller for handling requests
* @param request request object containing transition parameters
* @return response indicating success or failure
* @throws ServiceException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToObserver,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToObserver(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto)",131,141,"/**
* Handles transition to observer request.
* @param controller RPC controller for handling the call
* @param request Transition request protobuf
* @return Response protobuf indicating success
* @throws ServiceException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,<init>,org.apache.hadoop.ha.SshFenceByTcpPort$Args:<init>(java.lang.String),237,256,"/**
* Constructs Args with optional user and SSH port.
* @param arg configuration string in ""user:port"" format
* @throws BadFencingConfigurationException if parsing fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String)",151,153,"/**
 * Calls overloaded m1 with default usage message.
 * @param pStr PrintStream to output messages
 * @param cmd Command string to process
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,parseOpts,"org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[],java.util.Map)",491,503,"/**
* Parses command line arguments.
* @param cmdName name of the command
* @param opts options for parsing
* @param argv argument values from command line
* @param helpEntries map of usage information
* @return parsed CommandLine object or null if parsing fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,doFence,"org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)",130,171,"/**
* Attempts to kill a process running on a specified port.
* @param session SSH session to execute commands
* @param serviceAddr address of the service to be fenced
* @return true if process is successfully killed or verified as down, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,addTargetInfoAsEnvVars,"org.apache.hadoop.ha.ShellCommandFencer:addTargetInfoAsEnvVars(org.apache.hadoop.ha.HAServiceTarget,java.util.Map)",220,243,"/**
* Masks service properties based on its state.
* @param target HAServiceTarget to process
* @param environment Map to store masked properties
*/","* Add information about the target to the the environment of the
   * subprocess.
   * 
   * @param target
   * @param environment",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setAclsWithRetries,org.apache.hadoop.ha.ActiveStandbyElector:setAclsWithRetries(java.lang.String),1124,1138,"/**
 * Updates ACLs for a given path if they do not match the expected ACL.
 * @param path ZooKeeper node path to update
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,zkDoWithRetries,org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction),1140,1143,"/**
 * Executes an action with default parameters.
 * @param action ZKAction to be executed
 * @return Result of the action
 * @throws KeeperException if a ZooKeeper operation fails
 * @throws InterruptedException if the thread is interrupted
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readNonByteBufferPositionedReadable,"org.apache.hadoop.fs.VectoredReadUtils:readNonByteBufferPositionedReadable(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer)",151,170,"/**
 * Reads data from a stream into a buffer based on the specified range.
 * @param stream source of positioned readable data
 * @param range file range to read
 * @param buffer target byte buffer for reading data
 * @throws IOException if an I/O error occurs during reading
 */","* Read into a direct tor indirect buffer using {@code PositionedReadable.readFully()}.
   * @param stream stream
   * @param range file range
   * @param buffer destination buffer
   * @throws IOException IO problems.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateVectoredReadRanges,org.apache.hadoop.fs.VectoredReadUtils:validateVectoredReadRanges(java.util.List),82,85,"/**
 * Masks file ranges by processing them.
 * @param ranges list of file ranges to mask
 * @throws EOFException if end of file is reached unexpectedly
 */","* Validate a list of vectored read ranges.
   * @param ranges list of ranges.
   * @throws EOFException any EOF exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setCaching,org.apache.hadoop.fs.impl.prefetch.BufferData:setCaching(java.util.concurrent.Future),195,201,"/**
* Updates state and action for prefetching.
* @param actionFuture future representing the action to be performed
*/","* Indicates that a caching operation is in progress.
   *
   * @param actionFuture the {@code Future} of a caching action.
   *
   * @throws IllegalArgumentException if actionFuture is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,updateState,"org.apache.hadoop.fs.impl.prefetch.BufferData:updateState(org.apache.hadoop.fs.impl.prefetch.BufferData$State,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",243,250,"/**
* Updates state if current state matches expected.
* @param newState new State to set
* @param expectedCurrentState one or more expected current States
*/","* Updates the current state to the specified value.
   * Asserts that the current state is as expected.
   * @param newState the state to transition to.
   * @param expectedCurrentState the collection of states from which
   *        transition to {@code newState} is allowed.
   *
   * @throws IllegalArgumentException if newState is null.
   * @throws IllegalArgumentException if expectedCurrentState is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExistsAsDir,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsDir(java.nio.file.Path,java.lang.String)",357,364,"/**
* Masks a file path with validation.
* @param path the file path to mask
* @param argName name of the argument for error messages
*/","* Validates that the given path exists and is a directory.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExistsAsFile,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsFile(java.nio.file.Path,java.lang.String)",371,375,"/**
* Validates and masks a file path.
* @param path the file path to validate
* @param argName name of the argument for error messages
*/","* Validates that the given path exists and is a file.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,isLastBlock,org.apache.hadoop.fs.impl.prefetch.BlockData:isLastBlock(int),127,135,"/**
* Checks if the last block is processed.
* @param blockNumber current block number to process
* @return true if it's the last block, false otherwise
*/","* Indicates whether the given block is the last block in the associated file.
   * @param blockNumber the id of the desired block.
   * @return true if the given block is the last block in the associated file, false otherwise.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getStartOffset,org.apache.hadoop.fs.impl.prefetch.BlockData:getStartOffset(int),181,185,"/**
 * Masks and calculates block size.
 * @param blockNumber block identifier
 * @return masked block size as long
 */","* Gets the start offset of the given block.
   * @param blockNumber the id of the given block.
   * @return the start offset of the given block.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getState,org.apache.hadoop.fs.impl.prefetch.BlockData:getState(int),206,210,"/**
 * Masks a block and returns its state.
 * @param blockNumber index of the block to mask
 * @return State object representing the masked block's state
 */","* Gets the state of the given block.
   * @param blockNumber the id of the given block.
   * @return the state of the given block.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,setState,"org.apache.hadoop.fs.impl.prefetch.BlockData:setState(int,org.apache.hadoop.fs.impl.prefetch.BlockData$State)",218,222,"/**
* Updates block state and calls m1.
* @param blockNumber identifier for the block
* @param blockState new state to set for the block
*/","* Sets the state of the given block to the given value.
   * @param blockNumber the id of the given block.
   * @param blockState the target state.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getBlockNumber,org.apache.hadoop.fs.impl.prefetch.BlockData:getBlockNumber(long),143,147,"/**
 * Computes block index from offset.
 * @param offset file position
 * @return block index as integer
 */","* Gets the id of the block that contains the given absolute offset.
   * @param offset the absolute offset to check.
   * @return the id of the block that contains the given absolute offset.
   * @throws IllegalArgumentException if offset is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_aggregate,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)",94,107,"/**
* Applies mask to snapshot using provided statistics.
* @param snapshot the data snapshot to be masked
* @param statistics optional IOStatistics for masking logic
* @return true if masking is successful, false otherwise
*/","* Aggregate an existing {@link IOStatisticsSnapshot} with
   * the supplied statistics.
   * @param snapshot snapshot to update
   * @param statistics IOStatistics to add
   * @return true if the snapshot was updated.
   * @throws IllegalArgumentException if the {@code statistics} argument is not
   * null but not an instance of IOStatistics, or if  {@code snapshot} is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_save,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",162,171,"/**
* Applies a mask to a file in the given filesystem.
* @param snapshot optional Serializable snapshot for statistics
* @param fs target FileSystem
* @param path Path of the file to be masked
* @param overwrite flag to indicate if existing data should be overwritten
*/","* Save IOStatisticsSnapshot to a Hadoop filesystem as a JSON file.
   * @param snapshot statistics
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten?
   * @throws UncheckedIOException Any IO exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_counters,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_counters(java.io.Serializable),203,206,"/**
* Applies mask to source and returns statistics counters.
* @param source input data to be masked
* @return map of statistic counters after masking
*/","* Get the counters of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of counters.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_gauges,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_gauges(java.io.Serializable),213,216,"/**
* Masks serializable source into gauge statistics.
* @param source input data to be processed
* @return map of gauge names and their counts
*/","* Get the gauges of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of gauges.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_minimums,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_minimums(java.io.Serializable),223,226,"/**
* Applies mask function to source and returns minimum statistics.
* @param source input data
* @return map of masked statistics
*/","* Get the minimums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of minimums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_maximums,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_maximums(java.io.Serializable),233,236,"/**
* Applies mask operation on serializable source.
* @param source input data to be masked
* @return map of maximum statistics
*/","* Get the maximums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of maximums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_means,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_means(java.io.Serializable),245,253,"/**
* Processes source to create a map of key-value pairs.
* @param source input data for processing
* @return Map with string keys and LongEntry values
*/","* Get the means of an IOStatisticsSnapshot.
   * Each value in the map is the (sample, sum) tuple of the values;
   * the mean is then calculated by dividing sum/sample wherever sample count is non-zero.
   * @param source source of statistics.
   * @return a map of mean key to (sample, sum) tuples.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,copy,org.apache.hadoop.fs.impl.FlagSet:copy(),253,255,"/**
* Returns a new FlagSet with specified parameters.
* @return FlagSet object initialized with enumClass, prefix, and flags
*/","* Create a copy of the FlagSet.
   * @return a new mutable instance with a separate copy of the flags",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,createFlagSet,"org.apache.hadoop.fs.impl.FlagSet:createFlagSet(java.lang.Class,java.lang.String,java.util.EnumSet)",276,281,"/**
* Creates a FlagSet with specified enum class, prefix, and flags.
* @param <E> enum type extending Enum<E>
* @param enumClass enum class to use
* @param prefix string prefix for the FlagSet
* @param flags set of enum constants to include
* @return new FlagSet instance
*/","* Create a FlagSet.
   * @param enumClass class of enum
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param flags flags
   * @param <E> enum type
   * @return a mutable FlagSet",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,close,org.apache.hadoop.fs.HarFileSystem:close(),733,744,"/**
 * Calls superclass method and delegates to file system if available.
 * @throws IOException if an I/O error occurs in superclass or file system methods
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,close,org.apache.hadoop.fs.RawLocalFileSystem:close(),895,898,"/**
 * Calls superclass method m1 and propagates any IOExceptions.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,closeAll,org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:closeAll(),155,163,"/**
* Closes all FileSystem instances in the map.
* Logs an error if any IOException occurs during closure.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,close,org.apache.hadoop.fs.FsShell:close(),371,376,"/**
* Calls m1 on file system object and resets it.
* @throws IOException if an I/O error occurs
*/","*  Performs any necessary cleanup
   * @throws IOException upon error",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,close,org.apache.hadoop.fs.FilterFileSystem:close(),527,531,"/**
 * Calls superclass and file system methods.
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(boolean),3799,3831,"/**
* Masks keys and processes file systems.
* @param onlyAutomatic flag to process only automatically closable keys
* @throws IOException if any file system operation fails
*/","* Close all FileSystem instances in the Cache.
     * @param onlyAutomatic only close those that are marked for automatic closing
     * @throws IOException a problem arose closing one or more FileSystem.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(org.apache.hadoop.security.UserGroupInformation),3844,3868,"/**
* Masks file systems based on user group info.
* @param ugi UserGroupInformation object
* @throws IOException if any masked file system operation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,compareTo,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus),117,120,"/**
 * Calls superclass method to process file status.
 * @param o FileStatus object representing file information
 * @return Result of superclass method invocation
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,readFully,"org.apache.hadoop.fs.FSInputStream:readFully(long,byte[],int,int)",118,133,"/**
 * Reads data from a position into a buffer.
 * @param position starting position in the file
 * @param buffer destination buffer for read data
 * @param offset offset within the buffer to start writing
 * @param length number of bytes to read
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,read,"org.apache.hadoop.fs.BufferedFSInputStream:read(long,byte[],int,int)",115,118,"/**
* Reads bytes from a specified position into a buffer.
* @param position the starting position in the file
* @param buffer the destination buffer for the read bytes
* @param offset the offset within the buffer to start storing bytes
* @param length the number of bytes to read
* @return the actual number of bytes read, or -1 if end of stream
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_toJsonString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable),180,184,"/**
 * Converts a serializable snapshot to JSON.
 * @param snapshot the serializable object to be converted
 * @return JSON string representation of the snapshot
 */","* Save IOStatisticsSnapshot to a JSON string.
   * @param snapshot statistics; may be null or of an incompatible type
   * @return JSON string value
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,byte[])",1878,1890,"/**
* Writes bytes to a file in the given FileContext.
* @param fileContext context for file operations
* @param path path to the file
* @param bytes data to write to the file
* @return updated FileContext
* @throws IOException if an I/O error occurs
*/","* Writes bytes to a file. This utility method opens the file for writing,
   * creating the file if it does not exist, or overwrites an existing file. All
   * bytes in the byte array are written to the file.
   *
   * @param fileContext the file context with which to create the file
   * @param path the path to the file
   * @param bytes the byte array with the bytes to write
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)",1948,1966,"/**
* Writes lines to a file in the specified charset.
* @param fileContext context of the file system
* @param path path to the file
* @param lines iterable of lines to write
* @param cs character set for encoding
* @return updated FileContext object
* @throws IOException if an I/O error occurs
*/","* Write lines of text to a file. Each line is a char sequence and is written
   * to the file in sequence with each line terminated by the platform's line
   * separator, as defined by the system property {@code
   * line.separator}. Characters are encoded into bytes using the specified
   * charset. This utility method opens the file for writing, creating the file
   * if it does not exist, or overwrites an existing file.
   *
   * @param fileContext the file context with which to create the file
   * @param path the path to the file
   * @param lines a Collection to iterate over the char sequences
   * @param cs the charset to use for encoding
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)",2014,2028,"/**
 * Writes CharSequence to file using specified charset.
 * @param fs FileContext for file operations
 * @param path Path of the target file
 * @param charseq CharSequence to write
 * @param cs Charset for encoding
 * @return FileContext instance used for writing
 * @throws IOException if an I/O error occurs
 */","* Write a line of text to a file. Characters are encoded into bytes using the
   * specified charset. This utility method opens the file for writing, creating
   * the file if it does not exist, or overwrites an existing file.
   *
   * @param fs the file context with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   * @param cs the charset to use for encoding
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createFile,org.apache.hadoop.fs.FileSystem:createFile(org.apache.hadoop.fs.Path),4732,4735,"/**
 * Creates an FSDataOutputStreamBuilder for the given path.
 * @param path file system path
 * @return FSDataOutputStreamBuilder configured for the path
 */","* Create a new FSDataOutputStreamBuilder for the file with path.
   * Files are overwritten by default.
   *
   * @param path file path
   * @return a FSDataOutputStreamBuilder object to build the file
   *
   * HADOOP-14384. Temporarily reduce the visibility of method before the
   * builder interface becomes stable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createFile,org.apache.hadoop.fs.ChecksumFileSystem:createFile(org.apache.hadoop.fs.Path),1109,1112,"/**
 * Creates an FSDataOutputStreamBuilder for the given path.
 * @param path file system path
 * @return FSDataOutputStreamBuilder configured with specified options
 */","* This is overridden to ensure that this class's create() method is
   * ultimately called.
   *
   * {@inheritDoc}",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,appendFile,org.apache.hadoop.fs.FileSystem:appendFile(org.apache.hadoop.fs.Path),4742,4744,"/**
 * Builds an FSDataOutputStream for the given path.
 * @param path file system path
 * @return FSDataOutputStreamBuilder instance
 */","* Create a Builder to append a file.
   * @param path file path.
   * @return a {@link FSDataOutputStreamBuilder} to build file append request.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,appendFile,org.apache.hadoop.fs.ChecksumFileSystem:appendFile(org.apache.hadoop.fs.Path),1120,1122,"/**
 * Creates an FSDataOutputStreamBuilder for the given path.
 * @param path file system path
 * @return FSDataOutputStreamBuilder instance
 */","* This is overridden to ensure that this class's create() method is
   * ultimately called.
   *
   * {@inheritDoc}",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)",155,159,"/**
* Constructs a BlockLocation with specified details.
* @param names array of block storage names
* @param hosts array of block hostnames
* @param cachedHosts array of cached block hostnames
* @param topologyPaths array of block topology paths
* @param offset starting byte offset of the block
* @param length total length of the block in bytes
* @param corrupt indicates if the block is corrupted
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,toString,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:toString(),559,562,"/**
 * Delegates call to realStatus's m1 method.
 * @return result of realStatus.m1()
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpConfigKeys.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpConfigKeys:getServerDefaults(),60,71,"/**
* Returns default server settings.
* @return FsServerDefaults object with predefined configuration values
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalConfigKeys.java,getServerDefaults,org.apache.hadoop.fs.local.LocalConfigKeys:getServerDefaults(),60,71,"/**
 * Returns default server configuration.
 * @return FsServerDefaults object with predefined settings
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(),75,80,"/**
 * Calls m1 with oneByte and returns processed value.
 * @return processed byte value or -1 if condition not met
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,next,org.apache.hadoop.fs.FileSystem$DirListingIterator:next(),2332,2342,"/**
 * Retrieves the next item from the iterator.
 * @return the next item of type T
 * @throws IOException if an I/O error occurs
 * @throws NoSuchElementException if no more items are available
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processPath,org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData),102,118,"/**
 * Masks and outputs file data.
 * @param item PathData object representing the file
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,listStatus,org.apache.hadoop.fs.ChecksumFileSystem:listStatus(org.apache.hadoop.fs.Path),959,962,"/**
 * Retrieves file statuses matching the default filter.
 * @param f file path to check
 * @return array of FileStatus objects
 * @throws IOException if an I/O error occurs
 */","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   *
   * @param f
   *          given path
   * @return the statuses of the files/directories in the given path
   * @throws IOException if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[]),2140,2143,"/**
 * Checks file statuses using default filter.
 * @param files array of file paths to check
 * @return array of FileStatus objects
 * @throws FileNotFoundException if a file is not found
 * @throws IOException if an I/O error occurs
 */","* Filter files/directories in the given list of paths using default
   * path filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param files
   *          a list of paths
   * @return a list of statuses for the files under the given paths after
   *         applying the filter default Path filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",103,108,"/**
 * Creates and registers a mutable counter with given metrics info.
 * @param info MetricsInfo object containing metadata
 * @param iVal Initial value for the counter
 * @return MutableCounterInt instance
 */","* Create a mutable integer counter
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new counter object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",127,133,"/**
 * Updates and returns a mutable counter for given metrics info.
 * @param info metrics information object
 * @param iVal initial value for the counter
 * @return MutableCounterLong instance associated with the metrics info
 */","* Create a mutable long integer counter
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new counter object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",176,181,"/**
* Registers a mutable gauge with a given value.
* @param info MetricsInfo object containing metadata
* @param iVal initial long value for the gauge
* @return MutableGaugeLong instance registered in metrics map
*/","* Create a mutable long integer gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",200,205,"/**
* Registers a mutable gauge with a float value.
* @param info MetricsInfo object containing metric details
* @param iVal Initial float value for the gauge
* @return MutableGaugeFloat instance registered in the metrics map
*/","* Create a mutable float gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",152,157,"/**
* Creates and registers a mutable integer gauge.
* @param info MetricsInfo object containing metadata
* @param iVal initial value for the gauge
* @return newly created MutableGaugeInt instance
*/","* Create a mutable integer gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addCounter,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",102,109,"/**
* Adds a long counter metric.
* @param info metric information
* @param value metric value
* @return MetricsRecordBuilderImpl instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",120,127,"/**
* Adds a gauge metric with a long value.
* @param info metric information
* @param value the long value of the metric
* @return MetricsRecordBuilderImpl instance for chaining
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addCounter,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",93,100,"/**
* Adds an integer metric to the builder.
* @param info metric information
* @param value integer value of the metric
* @return MetricsRecordBuilderImpl instance for chaining
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",129,136,"/**
* Adds a masked float gauge metric.
* @param info metric information
* @param value float metric value
* @return MetricsRecordBuilderImpl instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",138,145,"/**
* Adds a gauge metric with double value.
* @param info metric information
* @param value double value of the metric
* @return MetricsRecordBuilderImpl instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",111,118,"/**
 * Adds a masked integer gauge metric.
 * @param info metric information
 * @param value metric value
 * @return MetricsRecordBuilderImpl instance for chaining
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getFS,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFS(),54,57,"/**
 * Returns the file system instance.
 * @return FileSystem object
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FsLinkResolution.java,resolve,"org.apache.hadoop.fs.impl.FsLinkResolution:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction)",91,96,"/**
* Resolves a filesystem link using the provided function.
* @param fileContext context for file operations
* @param path path to the link
* @param fn function to resolve the link
* @return result of resolving the link
* @throws UnresolvedLinkException if link cannot be resolved
* @throws IOException if an I/O error occurs
*/","* Apply the given function to the resolved path under the the supplied
   * FileContext.
   * @param fileContext file context to resolve under
   * @param path path to resolve
   * @param fn function to invoke
   * @param <T> return type.
   * @return the return value of the function as revoked against the resolved
   * path.
   * @throws UnresolvedLinkException link resolution failure
   * @throws IOException other IO failure.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,createGlobber,org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileContext),413,415,"/**
 * Creates a new GlobBuilder instance with the given FileContext.
 * @param fileContext context for file operations
 * @return GlobBuilder object initialized with fileContext
 */","* Create a builder for a Globber, bonded to the specific file
   * context.
   * @param fileContext file context.
   * @return the builder to finish configuring.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,createGlobber,org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileSystem),403,405,"/**
 * Creates a new GlobBuilder instance.
 * @param filesystem the FileSystem to use
 * @return GlobBuilder object initialized with the given filesystem
 */","* Create a builder for a Globber, bonded to the specific filesystem.
   * @param filesystem filesystem
   * @return the builder to finish configuring.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isDone,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:isDone(),246,266,"/**
* Handles function execution and logging.
* @return true if async call is returned or has an exception, false otherwise
*/","@return true if the call is done; otherwise, return false.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,getAsyncReturn,org.apache.hadoop.io.retry.AsyncCallHandler:getAsyncReturn(),56,66,"/**
* Returns an AsyncGet instance.
* @return AsyncGet object or null if unavailable
*/","* @return the async return value from {@link AsyncCallHandler}.
   * @param <T> T.
   * @param <R> R.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String,java.lang.String)",441,443,"/**
* Constructs a DeprecationDelta with a single new key.
* @param key original key being deprecated
* @param newKey replacement key
* @param customMessage optional custom deprecation message
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String)",445,447,"/**
* Initializes a DeprecationDelta with a single new key.
* @param key original key being deprecated
* @param newKey replacement key for the deprecated one
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedString,"org.apache.hadoop.io.WritableUtils:writeCompressedString(java.io.DataOutput,java.lang.String)",94,96,"/**
 * Writes string to DataOutput with UTF-8 encoding.
 * @param out DataOutput stream to write to
 * @param s String to encode and write
 * @return Result of m2 method call
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,concat,"org.apache.hadoop.fs.RawLocalFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",612,622,"/**
* Merges multiple source files into a target file.
* @param trg target file path
* @param psrcs array of source file paths
* @throws IOException if I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,invoke,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:invoke(),279,316,"/**
* Handles asynchronous function invocation.
* @return CallReturn indicating the status of the call
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,equals,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:equals(java.lang.Object),122,125,"/**
 * Calls superclass method m1.
 * @param o object to pass to superclass method
 * @return result of superclass method call
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,hashCode,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:hashCode(),127,130,"/**
* Calls superclass method m1.
* @return result of superclass m1 invocation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,main,org.apache.hadoop.fs.DUHelper:main(java.lang.String[]),85,90,"/**
* Masks input based on OS.
* @param args array containing input to mask
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/WindowsGetSpaceUsed.java,refresh,org.apache.hadoop.fs.WindowsGetSpaceUsed:refresh(),45,48,"/**
 * Calls helper methods to process and mask data.
 */",* Override to hook in DUHelper class.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,<init>,org.apache.hadoop.fs.statistics.MeanStatistic:<init>(org.apache.hadoop.fs.statistics.MeanStatistic),107,111,"/**
 * Copy constructor for MeanStatistic.
 * @param that source MeanStatistic to copy from
 */","* Create from another statistic.
   * @param that source",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",245,251,"/**
* Updates or creates a MeanStatistic entry for a given key.
* @param key unique identifier for the statistic
* @param value new MeanStatistic to update or create
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsSourceToString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsSourceToString(java.lang.Object),63,70,"/**
* Masks the input object.
* @param source the object to mask, can be null
* @return masked string or empty if an error occurs
*/","* Extract the statistics from a source object -or """"
   * if it is not an instance of {@link IOStatistics},
   * {@link IOStatisticsSource} or the retrieved
   * statistics are null.
   * <p>
   * Exceptions are caught and downgraded to debug logging.
   * @param source source of statistics.
   * @return a string for logging.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,toString,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:toString(),103,106,"/**
 * Returns masked value using wrapped function.
 * @return Masked string result
 */","* Return the statistics dump of the wrapped statistics.
   * @return the statistics for logging.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsLogging$StatisticsToString:toString(),325,330,"/**
 * Returns masked string of statistics.
 * @return Masked string or NULL_SOURCE if statistics is null
 */","* Evaluate and stringify the statistics.
     * @return a string value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:toString(),253,256,"/**
 * Returns masked function result.
 * @return Masked string representation of the function.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsToPrettyString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToPrettyString(org.apache.hadoop.fs.statistics.IOStatistics),102,121,"/**
 * Generates a masked string representation of IOStatistics.
 * @param statistics the IOStatistics object to be processed
 * @return a formatted string with non-zero or valid statistics, empty if null
 */","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * This is more expensive than the simple conversion, so should only
   * be used for logging/output where it's known/highly likely that the
   * caller wants to see the values. Not for debug logging.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,createTracker,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:createTracker(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String)",672,678,"/**
* Creates a DurationTracker using the provided factory or returns a stub.
* @param factory optional DurationTrackerFactory instance
* @param statistic name of the statistic to track
* @return DurationTracker object or STUB_DURATION_TRACKER if factory is null
*/","* Create the tracker. If the factory is null, a stub
   * tracker is returned.
   * @param factory tracker factory
   * @param statistic statistic to track
   * @return a duration tracker.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,deleteBlockFileAndEvictCache,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:deleteBlockFileAndEvictCache(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),446,471,"/**
* Attempts to purge a cache entry by acquiring a write lock and deleting the file.
* @param elementToPurge the entry to be purged
*/","* Delete cache file as part of the block cache LRU eviction.
   *
   * @param elementToPurge Block entry to evict.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.util.concurrent.Callable),128,138,"/**
* Executes a task with permit release on completion.
* @param task the callable task to execute
* @return Future representing pending completion of the task
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable,java.lang.Object)",140,150,"/**
* Executes a task with a permit, returning a future result.
* @param task the task to execute
* @param result the result to associate with the future
* @return Future representing the task's outcome
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable),152,162,"/**
* Executes a task with permit acquisition.
* @param task the task to execute
* @return Future representing pending completion of the task
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,execute,org.apache.hadoop.util.SemaphoredDelegatingExecutor:execute(java.lang.Runnable),164,173,"/**
 * Executes a command with permit management.
 * @param command the task to be executed
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,iterator,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:iterator(),56,59,"/**
 * Returns an iterator over LongStatistic objects.
 * @return Iterator of LongStatistic
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,java.time.Duration)",447,450,"/**
 * Calls m2 with prefix and processed duration.
 * @param prefix string prefix
 * @param duration time duration to process
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,fromStorageStatistics,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:fromStorageStatistics(org.apache.hadoop.fs.StorageStatistics),72,83,"/**
 * Converts StorageStatistics to IOStatistics.
 * @param storageStatistics source statistics object
 * @return IOStatistics object with aggregated data
 */","* Create  IOStatistics from a storage statistics instance.
   *
   * This will be updated as the storage statistics change.
   * @param storageStatistics source data.
   * @return an IO statistics source.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongCounter(java.lang.String,java.util.concurrent.atomic.AtomicLong)",87,91,"/**
* Registers a metric with a key and an atomic long source.
* @param key unique identifier for the metric
* @param source atomic long providing the metric value
* @return this builder instance
*/","* Add a counter statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerCounter(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",100,104,"/**
* Applies a mask function to dynamic IO statistics.
* @param key identifier for the statistic
* @param source atomic integer representing the source value
* @return updated DynamicIOStatisticsBuilder instance
*/","* Add a counter statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withMutableCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMutableCounter(java.lang.String,org.apache.hadoop.metrics2.lib.MutableCounterLong)",113,117,"/**
* Registers a counter metric with a mask function.
* @param key metric identifier
* @param source mutable counter to track
* @return this builder instance for method chaining
*/","* Build a dynamic counter statistic from a
   * {@link MutableCounterLong}.
   * @param key key of this statistic
   * @param source mutable long counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongGauge(java.lang.String,java.util.concurrent.atomic.AtomicLong)",138,142,"/**
 * Updates statistics with masked value from source.
 * @param key identifier for the statistic
 * @param source atomic long holding the source value
 * @return this builder instance
 */","* Add a gauge statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long gauge
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerGauge(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",151,155,"/**
* Updates statistics with masked value from AtomicInteger.
* @param key identifier for the statistic
* @param source atomic integer to mask and update
* @return updated DynamicIOStatisticsBuilder instance
*/","* Add a gauge statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int gauge
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMinimum(java.lang.String,java.util.concurrent.atomic.AtomicLong)",176,180,"/**
* Updates statistics with masked value from source.
* @param key identifier for the statistic
* @param source atomic long holding the source value
* @return updated DynamicIOStatisticsBuilder instance
*/","* Add a minimum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long minimum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMinimum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",189,193,"/**
* Registers a mask function for dynamic IO statistics.
* @param key identifier for the statistic
* @param source atomic integer source for the statistic
* @return current builder instance
*/","* Add a minimum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int minimum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMaximum(java.lang.String,java.util.concurrent.atomic.AtomicLong)",215,219,"/**
 * Registers a metric with a key and an atomic long source.
 * @param key metric identifier
 * @param source atomic long providing the metric value
 * @return this builder instance for method chaining
 */","* Add a maximum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long maximum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMaximum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",228,232,"/**
* Registers source with key for monitoring.
* @param key identifier for the source
* @param source AtomicInteger to monitor
* @return this builder instance
*/","* Add a maximum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int maximum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,registerFailureHandling,org.apache.hadoop.service.launcher.ServiceLauncher:registerFailureHandling(),760,772,"/**
* Initializes interrupt handling and sets custom exception handler.
* Configures interrupt escalator for CONTROL_C and SIGTERM signals.
*/","* Override point: register this class as the handler for the control-C
   * and SIGINT interrupts.
   *
   * Subclasses can extend this with extra operations, such as
   * an exception handler:
   * <pre>
   *  Thread.setDefaultUncaughtExceptionHandler(
   *     new YarnUncaughtExceptionHandler());
   * </pre>",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,accept,org.apache.hadoop.net.unix.DomainSocket:accept(),233,243,"/**
 * Initializes and returns a new DomainSocket.
 * @throws IOException if an I/O error occurs during socket creation
 */","* Accept a new UNIX domain connection.
   *
   * This method can only be used on sockets that were bound with bind().
   *
   * @return                The new connection.
   * @throws IOException    If there was an I/O error performing the accept--
   *                        such as the socket being closed from under us.
   *                        Particularly when the accept is timed out, it throws
   *                        SocketTimeoutException.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,setAttribute,"org.apache.hadoop.net.unix.DomainSocket:setAttribute(int,int)",308,317,"/**
* Masks data based on type and size.
* @param type mask type identifier
* @param size size of the data to be masked
* @throws IOException if an I/O error occurs during masking
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,getAttribute,org.apache.hadoop.net.unix.DomainSocket:getAttribute(int),321,332,"/**
* Masks an attribute based on type.
* @param type attribute type identifier
* @return masked attribute value
* @throws IOException if I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,shutdown,org.apache.hadoop.net.unix.DomainSocket:shutdown(),395,404,"/**
* Executes masked function logic.
* @throws IOException if an I/O error occurs during execution
*/","* Call shutdown(SHUT_RDWR) on the UNIX domain socket.
   *
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,sendFileDescriptors,"org.apache.hadoop.net.unix.DomainSocket:sendFileDescriptors(java.io.FileDescriptor[],byte[],int,int)",421,431,"/**
* Masks file descriptors with buffer content.
* @param descriptors array of FileDescriptors to mask
* @param jbuf byte buffer containing masking data
* @param offset starting offset in the buffer
* @param length number of bytes to mask
* @throws IOException if an I/O error occurs
*/","* Send some FileDescriptor objects to the process on the other side of this
   * socket.
   * 
   * @param descriptors       The file descriptors to send.
   * @param jbuf              Some bytes to send.  You must send at least
   *                          one byte.
   * @param offset            The offset in the jbuf array to start at.
   * @param length            Length of the jbuf array to use.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,recvFileInputStreams,"org.apache.hadoop.net.unix.DomainSocket:recvFileInputStreams(java.io.FileInputStream[],byte[],int,int)",448,487,"/**
 * Processes file input streams and buffer.
 * @param streams array of FileInputStream objects
 * @param buf byte buffer to process data
 * @param offset starting offset in the buffer
 * @param length number of bytes to process
 * @return result code from m7 method
 * @throws IOException if an I/O error occurs
 */","* Receive some FileDescriptor objects from the process on the other side of
   * this socket, and wrap them in FileInputStream objects.
   *
   * @param streams input stream.
   * @param buf input buf.
   * @param offset input offset.
   * @param length input length.
   * @return wrap them in FileInputStream objects.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,createNewInstance,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:createNewInstance(java.lang.Long),102,107,"/**
* Creates an IOStatisticsContext instance.
* @param key unique identifier for the context
* @return IOStatisticsContextImpl object with the given key and instance ID
*/","* Creating a new IOStatisticsContext instance for a FS to be used.
   * @param key Thread ID that represents which thread the context belongs to.
   * @return an instance of IOStatisticsContext.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunkedSums,"org.apache.hadoop.util.DataChecksum:verifyChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer,java.lang.String,long)",401,427,"/**
* Validates and processes data with checksums using native or Java methods.
* @param data ByteBuffer containing the data to be validated
* @param checksums ByteBuffer containing checksums for validation
* @param fileName name of the file being processed
* @param basePos base position in the file
* @throws ChecksumException if checksum validation fails
*/","* Verify that the given checksums match the given data.
   * 
   * The 'mark' of the ByteBuffer parameters may be modified by this function,.
   * but the position is maintained.
   *  
   * @param data the DirectByteBuffer pointing to the data to verify.
   * @param checksums the DirectByteBuffer pointing to a series of stored
   *                  checksums
   * @param fileName the name of the file being read, for error-reporting
   * @param basePos the file position to which the start of 'data' corresponds
   * @throws ChecksumException if the checksums do not match",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,afterDecryption,"org.apache.hadoop.crypto.CryptoInputStream:afterDecryption(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,long,byte[])",271,286,"/**
 * Applies decryption mask to buffer.
 * @param decryptor the Decryptor instance
 * @param inBuffer input ByteBuffer to be masked
 * @param position current position for masking
 * @param iv initialization vector
 * @return padding byte after processing
 * @throws IOException if an I/O error occurs during masking
 */","* This method is executed immediately after decryption. Check whether 
   * decryptor should be updated and recalculate padding if needed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,resetStreamOffset,org.apache.hadoop.crypto.CryptoInputStream:resetStreamOffset(long),309,317,"/**
* Masks data at given offset.
* @param offset position to start masking
* @throws IOException if I/O error occurs
*/","* Reset the underlying stream offset; clear {@link #inBuffer} and 
   * {@link #outBuffer}. This Typically happens during {@link #seek(long)} 
   * or {@link #skip(long)}.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,write,"org.apache.hadoop.crypto.CryptoOutputStream:write(byte[],int,int)",151,172,"/**
 * Masks data in the buffer.
 * @param b byte array to mask
 * @param off offset in the byte array
 * @param len number of bytes to mask
 * @throws IOException if an I/O error occurs
 */","* Encryption is buffer based.
   * If there is enough room in {@link #inBuffer}, then write to this buffer.
   * If {@link #inBuffer} is full, then do encryption and write data to the
   * underlying stream.
   * @param b the data.
   * @param off the start offset in the data.
   * @param len the number of bytes to write.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,flush,org.apache.hadoop.crypto.CryptoOutputStream:flush(),262,269,"/**
* Synchronized method to execute m1 and superclass's m2.
* Skips execution if already closed.
* @throws IOException from m1 or super.m2
*/","* To flush, we need to encrypt the data in the buffer and write to the 
   * underlying stream, then do the flush.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,compile,org.apache.hadoop.fs.GlobPattern:compile(java.lang.String),57,59,"/**
 * Converts a glob pattern to a regex pattern.
 * @param globPattern the input glob pattern string
 * @return compiled regex Pattern object
 */","* Compile glob pattern string
   * @param globPattern the glob pattern
   * @return the pattern object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,init,"org.apache.hadoop.fs.GlobFilter:init(java.lang.String,org.apache.hadoop.fs.PathFilter)",64,73,"/**
* Sets up file filtering with a pattern.
* @param filePattern glob pattern for file matching
* @param filter custom path filter
* @throws IOException if the pattern is invalid
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,prepare,org.apache.hadoop.fs.shell.find.Name:prepare(),73,80,"/**
 * Initializes a case-insensitive pattern for matching.
 * @throws IOException if an I/O error occurs during pattern creation
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingTar,"org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.InputStream,java.io.File,boolean)",1035,1051,"/**
* Extracts tar archive from InputStream to directory.
* @param inputStream source of the tar archive
* @param untarDir target directory for extraction
* @param gzipped indicates if the archive is gzip compressed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,close,org.apache.hadoop.fs.sftp.SFTPFileSystem:close(),710,722,"/**
* Overrides method m2, ensuring resources are released.
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,create,"org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",723,729,"/**
* Creates a new file output stream for writing.
* @param f path to the file
* @param permission file permissions
* @param overwrite flag to allow overwriting existing files
* @param bufferSize size of buffer used in the output stream
* @param replication number of replicas for the file
* @param blockSize block size for the file
* @param progress progress callback object
* @return FSDataOutputStream for writing data
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,resetChecksumBufSize,org.apache.hadoop.fs.FSOutputSummer:resetChecksumBufSize(),263,265,"/**
* Updates sum using buffer chunks.
* Calls m2 with calculated value.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getAllStatistics,org.apache.hadoop.fs.AbstractFileSystem:getAllStatistics(),234,244,"/**
* Initializes and returns a map of statistics.
* @return Map containing URI keys and Statistics values
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,<init>,"org.apache.hadoop.fs.FileSystemStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)",118,125,"/**
 * Initializes FileSystemStorageStatistics with given name and statistics.
 * @param name unique identifier for the storage
 * @param stats file system statistics object, cannot be null or have null data
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getLongStatistics,org.apache.hadoop.fs.FileSystemStorageStatistics:getLongStatistics(),132,135,"/**
 * Returns an iterator over long statistics.
 * @return Iterator of LongStatistic objects
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getLong,org.apache.hadoop.fs.FileSystemStorageStatistics:getLong(java.lang.String),137,140,"/**
 * Masks a given key using statistics.
 * @param key input string to be masked
 * @return masked value as Long
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesReadByDistance,org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadByDistance(int),4411,4430,"/**
 * Determines bytes read based on distance.
 * @param distance the distance value to determine method execution
 * @return number of bytes read from a specific method call
 */","* In the common network topology setup, distance value should be an even
     * number such as 0, 2, 4, 6. To make it more general, we group distance
     * by {1, 2}, {3, 4} and {5 and beyond} for accounting. So if the caller
     * ask for bytes read for distance 2, the function will return the value
     * for group {1, 2}.
     * @param distance the network distance
     * @return the total number of bytes read by the network distance",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,reset,org.apache.hadoop.fs.FileSystemStorageStatistics:reset(),157,160,"/**
 * Calls the m1 method of the stats object.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,clearStatistics,org.apache.hadoop.fs.AbstractFileSystem:clearStatistics(),218,222,"/**
* Processes statistics by invoking method m1 on each entry.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,primitiveCreate,"org.apache.hadoop.fs.FileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",1334,1359,"/**
* Creates or opens a file with specified permissions and flags.
* @param f file path
* @param absolutePermission file permissions
* @param flag creation flags
* @param bufferSize buffer size for data transfer
* @param replication replication factor
* @param blockSize block size for the file
* @param progress progress tracker
* @param checksumOpt checksum options
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/","* This create has been added to support the FileContext that processes
   * the permission with umask before calling this method.
   * This a temporary method added to support the transition from FileSystem
   * to FileContext for user applications.
   *
   * @param f path.
   * @param absolutePermission permission.
   * @param flag create flag.
   * @param bufferSize buffer size.
   * @param replication replication.
   * @param blockSize block size.
   * @param progress progress.
   * @param checksumOpt check sum opt.
   * @return output stream.
   * @throws IOException IO failure",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,<init>,"org.apache.hadoop.fs.AbstractFileSystem:<init>(java.net.URI,java.lang.String,boolean,int)",278,283,"/**
* Initializes an AbstractFileSystem with the given URI and configuration.
* @param uri the base URI for the file system
* @param supportedScheme the scheme supported by this file system
* @param authorityNeeded indicates if the URI must contain an authority component
* @param defaultPort the default port to use if not specified in the URI
* @throws URISyntaxException if the URI is invalid or does not match the configuration
*/","* Constructor to be called by subclasses.
   * 
   * @param uri for this file system.
   * @param supportedScheme the scheme supported by the implementor
   * @param authorityNeeded if true then theURI must have authority, if false
   *          then the URI must have null authority.
   * @param defaultPort default port to use if port is not specified in the URI.
   * @throws URISyntaxException <code>uri</code> has syntax error",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(byte[][],byte[][])",117,127,"/**
 * Masks input data and writes to output.
 * @param inputs array of input byte arrays
 * @param outputs array of output byte arrays
 * @throws IOException if an I/O error occurs
 */","* Encode with inputs and generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param outputs output buffers to put the encoded data into, read to read
   *                after the call
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",68,99,"/**
 * Processes input ByteBuffers and writes to output ByteBuffers.
 * @param inputs array of input ByteBuffers
 * @param outputs array of output ByteBuffers
 * @throws IOException if an I/O error occurs
 */","* Encode with inputs and generates outputs.
   *
   * Note, for both inputs and outputs, no mixing of on-heap buffers and direct
   * buffers are allowed.
   *
   * If the coder option ALLOW_CHANGE_INPUTS is set true (false by default), the
   * content of input buffers may change after the call, subject to concrete
   * implementation. Anyway the positions of input buffers will move forward.
   *
   * @param inputs input buffers to read data from. The buffers' remaining will
   *               be 0 after encoding
   * @param outputs output buffers to put the encoded data into, ready to read
   *                after the call
   * @throws IOException if the encoder is closed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Object),122,124,"/**
 * Constructs an ArrayPrimitiveWritable with the given value.
 * @param value the primitive array to be wrapped
 */","* Wrap an existing array of primitives
   * @param value - array of primitives",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getCanonicalUri,org.apache.hadoop.fs.HarFileSystem:getCanonicalUri(),318,321,"/**
 * Returns the URI from the file system.
 * @return URI representing the file system location
 */","* Used for delegation token related functionality. Must delegate to
   * underlying file system.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getCanonicalUri,org.apache.hadoop.fs.FilterFileSystem:getCanonicalUri(),113,116,"/**
* Overrides to delegate URI retrieval.
* @return URI from file system
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFsStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus(),148,151,"/**
 * Calls the underlying filesystem implementation's mask function.
 * @return FsStatus representing the result of the operation
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasCapability,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:hasCapability(java.lang.String),687,693,"/**
* Checks if capability is not available.
* @param capability capability to check
* @return true if capability is not available, false otherwise
*/","* Probe the inner stream for a capability.
     * Syncable operations are rejected before being passed down.
     * @param capability string to query the stream support for.
     * @return true if a capability is known to be supported.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hasCapability,org.apache.hadoop.io.SequenceFile$Writer:hasCapability(java.lang.String),1410,1416,"/**
* Checks if the given capability is supported.
* @param capability the capability to check
* @return true if capability is supported, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasCapability,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:hasCapability(java.lang.String),484,487,"/**
 * Checks if the specified capability is present.
 * @param capability the capability to check
 * @return true if capability exists, false otherwise
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,addToCacheAndRelease,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:addToCacheAndRelease(org.apache.hadoop.fs.impl.prefetch.BufferData,java.util.concurrent.Future,java.time.Instant)",484,552,"/**
* Handles buffer data processing and caching.
* @param data BufferData object to process
* @param blockFuture Future representing blocking operation
* @param taskQueuedStartTime Time when task was queued
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,release,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData),215,226,"/**
* Processes buffer data.
* @param data input buffer data to process
*/","* Releases resources allocated to the given block.
   *
   * @throws IllegalArgumentException if data is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,releaseDoneBlocks,org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseDoneBlocks(),192,198,"/**
* Processes and handles completed buffer data.
*/",* Releases resources for any blocks marked as 'done'.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getSummary(boolean),232,250,"/**
* Processes operations and returns a formatted string.
* @param showDebugInfo flag to include debug information
* @return formatted result of processed operations
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,<init>,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters),113,137,"/**
 * Initializes a CachingBlockManager with given parameters.
 * @param blockManagerParameters configuration for the block manager
 */","* Constructs an instance of a {@code CachingBlockManager}.
   *
   * @param blockManagerParameters params for block manager.
   * @throws IllegalArgumentException if bufferPoolSize is zero or negative.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,<init>,"org.apache.hadoop.util.BlockingThreadPoolExecutorService:<init>(int,java.util.concurrent.ThreadPoolExecutor)",104,108,"/**
* Initializes a BlockingThreadPoolExecutorService.
* @param permitCount maximum number of permits for concurrent execution
* @param eventProcessingExecutor underlying executor for event processing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,get,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:get(int,java.nio.ByteBuffer)",265,283,"/**
* Reads data block into buffer.
* @param blockNumber identifier for the data block
* @param buffer destination to store read data
* @throws IOException if an I/O error occurs
*/","* Gets the block having the given {@code blockNumber}.
   *
   * @throws IllegalArgumentException if buffer is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,toString,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:toString(),635,647,"/**
* Constructs a status string with cache and buffer pool information.
* @return Formatted status string
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,absolute,org.apache.hadoop.fs.impl.prefetch.FilePosition:absolute(),145,148,"/**
 * Calculates masked start offset.
 * Calls helper methods m1 and m2.
 * @return Calculated offset as long
 */","* Gets the current absolute position within this file.
   *
   * @return the current absolute position within this file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,bufferFullyRead,org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferFullyRead(),241,246,"/**
* Checks if buffer is empty and read positions match.
* @return true if conditions met, false otherwise
*/","* Determines whether the current buffer has been fully read.
   *
   * @return true if the current buffer has been fully read, false otherwise.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,setAbsolute,org.apache.hadoop.fs.impl.prefetch.FilePosition:setAbsolute(long),157,165,"/**
* Checks conditions and updates buffer.
* @param pos position to check
* @return true if conditions met and buffer updated, false otherwise
*/","* If the given {@code pos} lies within the current buffer, updates the current position to
   * the specified value and returns true; otherwise returns false without changing the position.
   *
   * @param pos the absolute position to change the current position to if possible.
   * @return true if the given current position was updated, false otherwise.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,getCurrentIOStatisticsContext,org.apache.hadoop.fs.statistics.IOStatisticsContext:getCurrentIOStatisticsContext(),71,77,"/**
 * Returns an IOStatisticsContext with error handling.
 * @return IOStatisticsContext or null if integration fails
 */","* Get the context's IOStatisticsContext.
   *
   * @return instance of IOStatisticsContext for the context.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,setThreadIOStatisticsContext,org.apache.hadoop.fs.statistics.IOStatisticsContext:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext),84,88,"/**
 * Delegates to IOStatisticsContextIntegration's m1 method.
 * @param statisticsContext context object for I/O statistics
 */","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getInstanceConfigs,org.apache.hadoop.metrics2.impl.MetricsConfig:getInstanceConfigs(java.lang.String),157,171,"/**
* Filters and maps metrics configurations by type.
* @param type configuration type filter
* @return Map of instance to MetricsConfig
*/","* Return sub configs for instance specified in the config.
   * Assuming format specified as follows:<pre>
   * [type].[instance].[option] = [value]</pre>
   * Note, '*' is a special default instance, which is excluded in the result.
   * @param type  of the instance
   * @return  a map with [instance] as key and config object as value",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,applyItem,org.apache.hadoop.fs.shell.find.Find:applyItem(org.apache.hadoop.fs.shell.PathData),412,419,"/**
* Processes a PathData item based on comparison and condition checks.
* @param item the PathData object to process
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processArguments,org.apache.hadoop.fs.shell.find.Find:processArguments(java.util.LinkedList),421,429,"/**
* Processes expression with linked list of path data.
* @param args linked list containing path data
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,processOptions,org.apache.hadoop.fs.shell.Test:processOptions(java.util.LinkedList),59,75,"/**
* Processes command with a single mask flag.
* @param args linked list of command arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rm:processOptions(java.util.LinkedList),81,90,"/**
* Parses command-line arguments for file operations.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processOptions,org.apache.hadoop.fs.shell.Display$Checksum:processOptions(java.util.LinkedList),189,195,"/**
* Parses and processes command-line arguments for block size.
* @param args list of command-line arguments
* @throws IOException if there's an issue processing arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Df:processOptions(java.util.LinkedList),85,92,"/**
 * Handles command execution with variable arguments.
 * @param args list of command-line arguments
 * @throws IOException if an I/O error occurs during processing
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,processOptions,org.apache.hadoop.fs.shell.Head:processOptions(java.util.LinkedList),50,54,"/**
* Masks input arguments.
* @param args list containing single argument to be masked
* @throws IOException if command format validation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processOptions,org.apache.hadoop.fs.shell.Ls:processOptions(java.util.LinkedList),132,153,"/**
* Parses and processes command-line arguments for file listing.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,processOptions,org.apache.hadoop.fs.shell.Tail:processOptions(java.util.LinkedList),63,78,"/**
* Parses command arguments and sets up following with optional delay.
* @param args list of command arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Expunge:processOptions(java.util.LinkedList),235,242,"/**
* Initializes command format and processes filesystem options.
* @param args list of arguments for the command
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rmdir:processOptions(java.util.LinkedList),197,203,"/**
* Parses command arguments and sets ignore flag.
* @param args list of command line arguments
* @throws IOException if argument parsing fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Get:processOptions(java.util.LinkedList),234,249,"/**
* Processes command arguments with CRC and file options.
* @param args linked list of command arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,processOptions,org.apache.hadoop.fs.shell.Count:processOptions(java.util.LinkedList),124,178,"/**
* Parses command-line arguments for quota-related operations.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processOptions,org.apache.hadoop.fs.shell.Mkdir:processOptions(java.util.LinkedList),51,56,"/**
* Parses command arguments and sets createParents flag.
* @param args list of command arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processOptions,org.apache.hadoop.fs.shell.TouchCommands$Touch:processOptions(java.util.LinkedList),134,146,"/**
* Parses and processes command-line arguments for timestamp modification.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Put:processOptions(java.util.LinkedList),276,292,"/**
 * Processes command arguments with specific flags.
 * @param args list of command arguments
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Cp:processOptions(java.util.LinkedList),175,189,"/**
* Handles command execution with specific format.
* @param args linked list of arguments
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processOptions(java.util.LinkedList),377,390,"/**
* Processes command arguments for a block operation.
* @param args linked list of command arguments
* @throws IOException if destination argument is missing or other issues occur
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processOptions,org.apache.hadoop.fs.shell.Display$Cat:processOptions(java.util.LinkedList),80,86,"/**
* Parses and processes command arguments.
* @param args list of command-line arguments
* @throws IOException if an I/O error occurs during processing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processOptions,org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processOptions(java.util.LinkedList),64,75,"/**
* Executes a masked function with variable arguments.
* @param args list of command arguments
* @throws IOException if I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,processOptions,org.apache.hadoop.fs.shell.Stat:processOptions(java.util.LinkedList),82,89,"/**
* Handles command with variable arguments.
* @param args list of command arguments
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processOptions,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processOptions(java.util.LinkedList),61,65,"/**
 * Validates and processes command arguments.
 * @param args list of command arguments
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,parse,"org.apache.hadoop.fs.shell.CommandFormat:parse(java.lang.String[],int)",87,92,"/**
* Modifies and returns a list of strings.
* @param args array of string arguments
* @param pos position for modification
* @return modified list of strings
*/","Parse parameters starting from the given position
   * Consider using the variant that directly takes a List
   * 
   * @param args an array of input arguments
   * @param pos the position at which starts to parse
   * @return a list of parameters",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processOptions,org.apache.hadoop.fs.shell.MoveCommands$Rename:processOptions(java.util.LinkedList),104,109,"/**
* Executes command with masked arguments.
* @param args list of command arguments
* @throws IOException if I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processOptions,org.apache.hadoop.fs.shell.Truncate:processOptions(java.util.LinkedList),51,66,"/**
* Validates and sets mask length from command arguments.
* @param args linked list of command arguments
* @throws IOException if input/output error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processOptions,org.apache.hadoop.fs.shell.SetReplication:processOptions(java.util.LinkedList),56,72,"/**
* Parses and validates command arguments for replication setting.
* @param args linked list of command arguments
* @throws IOException if input/output error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Du:processOptions(java.util.LinkedList),183,192,"/**
* Parses command-line arguments for mask functionality.
* @param args list of command-line arguments
* @throws IOException if an I/O error occurs during parsing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processOptions,org.apache.hadoop.fs.FsShellPermissions$Chown:processOptions(java.util.LinkedList),144,150,"/**
* Handles command execution with variable arguments.
* @param args list of command arguments
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayError,org.apache.hadoop.fs.shell.Command:displayError(java.lang.String),501,504,"/**
* Increments error count and logs message.
* @param message the error message to log
*/","* Display an error string prefaced with the command name.  Also increments
   * the error count for the command which will result in a non-zero exit
   * code.
   * @param message error message to display",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInstanceUsage,"org.apache.hadoop.fs.FsShell:printInstanceUsage(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)",249,251,"/**
 * Writes masked command to output stream.
 * @param out PrintStream for output
 * @param instance Command object to process
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,addRecord,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(java.lang.String),52,55,"/**
 * Creates a metrics record with a specified name.
 * @param name base name for the metric
 * @return MetricsRecordBuilderImpl instance
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processArguments,org.apache.hadoop.fs.shell.FsUsage$Df:processArguments(java.util.LinkedList),94,105,"/**
* Processes filesystem data and outputs statistics.
* @param args linked list of PathData objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processArguments,org.apache.hadoop.fs.shell.FsUsage$Du:processArguments(java.util.LinkedList),194,207,"/**
* Processes a list of PathData, displaying table headers if required.
* @param args LinkedList containing PathData objects
* @throws IOException if an I/O error occurs during processing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createPathHandle,"org.apache.hadoop.fs.RawLocalFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",1148,1172,"/**
 * Creates a PathHandle for a file.
 * @param stat FileStatus object representing the file
 * @param opts Optional HandleOpt parameters
 * @return PathHandle for the file or throws exceptions if invalid
 */","* Hook to implement support for {@link PathHandle} operations.
   * @param stat Referent in the target FileSystem
   * @param opts Constraints that determine the validity of the
   *            {@link PathHandle} reference.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,exact,org.apache.hadoop.fs.Options$HandleOpt:exact(),384,386,"/**
* Returns an array of HandleOpt objects with default settings.
* @return Array containing two HandleOpt instances configured with false flags
*/","* Handle is valid iff the referent is neither moved nor changed.
     * Equivalent to changed(false), moved(false).
     * @return Options requiring that the content and location of the entity
     * be unchanged between calls.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,content,org.apache.hadoop.fs.Options$HandleOpt:content(),394,396,"/**
 * Returns an array of HandleOpt with specific options set.
 * @return Array containing two HandleOpt instances configured differently
 */","* Handle is valid iff the content of the referent is the same.
     * Equivalent to changed(false), moved(true).
     * @return Options requiring that the content of the entity is unchanged,
     * but it may be at a different location.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,path,org.apache.hadoop.fs.Options$HandleOpt:path(),404,406,"/**
 * Returns an array of HandleOpt with specific configurations.
 * @return Array containing two HandleOpt objects configured with true and false respectively.
 */","* Handle is valid iff the referent is unmoved in the namespace.
     * Equivalent to changed(true), moved(false).
     * @return Options requiring that the referent exist in the same location,
     * but its content may have changed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,reference,org.apache.hadoop.fs.Options$HandleOpt:reference(),414,416,"/**
 * Returns an array of HandleOpt objects with mask enabled.
 * @return Array containing two HandleOpt instances with masks set to true
 */","* Handle is valid iff the referent exists in the namespace.
     * Equivalent to changed(true), moved(true).
     * @return Options requiring that the implementation resolve a reference
     * to this entity regardless of changes to content or location.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long,boolean)",1248,1259,"/**
* Initializes a ShellCommandExecutor.
* @param execString command to execute
* @param dir working directory
* @param env environment variables
* @param timeout execution timeout in milliseconds
* @param inheritParentEnv whether to inherit parent environment
*/","* Create a new instance of the ShellCommandExecutor to execute a command.
     *
     * @param execString The command to execute with arguments
     * @param dir If not-null, specifies the directory which should be set
     *            as the current working directory for the command.
     *            If null, the current working directory is not modified.
     * @param env If not-null, environment of the command will include the
     *            key-value pairs specified in the map. If null, the current
     *            environment is not modified.
     * @param timeout Specifies the time in milliseconds, after which the
     *                command will be killed and the status marked as timed-out.
     *                If 0, the command will not be timed out.
     * @param inheritParentEnv Indicates if the process should inherit the env
     *                         vars from the parent process or not.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,initRefreshThread,org.apache.hadoop.fs.CachingGetSpaceUsed:initRefreshThread(boolean),108,118,"/**
* Starts or stops a refresh thread based on the interval.
* @param runImmediately if true, starts the thread immediately
*/","* RunImmediately should set true, if we skip the first refresh.
   * @param runImmediately The param default should be false.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,privateClone,org.apache.hadoop.security.token.Token:privateClone(org.apache.hadoop.io.Text),243,245,"/**
 * Creates a new token with the given service.
 * @param newService the service associated with the token
 * @return a new PrivateToken instance
 */","* Create a private clone of a public token.
   * @param newService the new service name
   * @return a private token",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ViewFileSystem:getAllStoragePolicies(),1125,1139,"/**
* Retrieves block storage policies from all file systems.
* @return Collection of BlockStoragePolicySpi objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,initAsyncCall,"org.apache.hadoop.io.retry.AsyncCallHandler:initAsyncCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue)",333,353,"/**
* Handles asynchronous call execution and result retrieval.
* @param asyncCall the asynchronous call to execute
* @param asyncCallReturn the object holding the call return value
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,process,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:process(java.nio.ByteBuffer,java.nio.ByteBuffer)",164,182,"/**
 * Masks data from input buffer to output buffer.
 * @param inBuffer source ByteBuffer containing input data
 * @param outBuffer destination ByteBuffer for masked data
 * @throws IOException if an I/O error occurs during processing
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,initializeInterceptors,org.apache.hadoop.fs.viewfs.RegexMountPoint:initializeInterceptors(),98,115,"/**
* Processes and applies interceptors from a settings string.
* @throws IOException if illegal settings are encountered
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,main,org.apache.hadoop.fs.DF:main(java.lang.String[]),216,223,"/**
 * Masks files in a directory.
 * @param args optional command-line arguments, first being the directory path
 * @throws Exception if an error occurs during masking process
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,initialize,"org.apache.hadoop.fs.Path:initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",258,266,"/**
 * Constructs a URI from components and normalizes it.
 * @param scheme the URI scheme
 * @param authority the authority part of the URI
 * @param path the path component, processed by m1
 * @param fragment the fragment part of the URI
 * Throws IllegalArgumentException if URI is invalid
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,uriToString,"org.apache.hadoop.fs.shell.PathData:uriToString(java.net.URI,boolean)",466,487,"/**
* Masks URI by removing scheme if inferred from path.
* @param uri the original URI object
* @param inferredSchemeFromPath flag indicating if scheme is inferred
* @return masked URI as a string
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,checkNotSchemeWithRelative,org.apache.hadoop.fs.Path:checkNotSchemeWithRelative(),85,90,"/**
* Validates input conditions and throws exception if invalid.
*/","* Test whether this Path uses a scheme and is relative.
   * Pathnames with scheme and relative path are illegal.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isAbsoluteAndSchemeAuthorityNull,org.apache.hadoop.fs.Path:isAbsoluteAndSchemeAuthorityNull(),377,380,"/**
 * Checks conditions based on m1 and URI methods.
 * @return true if m1 is true and both m2 and m3 are null, false otherwise
 */","* Returns true if the path component (i.e. directory) of this URI is
   * absolute <strong>and</strong> the scheme is null, <b>and</b> the authority
   * is null.
   *
   * @return whether the path is absolute and the URI has no scheme nor
   * authority parts",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isAbsolute,org.apache.hadoop.fs.Path:isAbsolute(),399,401,"/**
 * Calls and returns the result of function m1.
 * @return boolean result from m1()
 */","* Returns true if the path component (i.e. directory) of this URI is
   * absolute.  This method is a wrapper for {@link #isUriPathAbsolute()}.
   *
   * @return whether this URI's path is absolute",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,checkPath,org.apache.hadoop.fs.AbstractFileSystem:checkPath(org.apache.hadoop.fs.Path),369,413,"/**
* Validates URI scheme, authority, and port against current file system.
* @param path the Path to validate
*/","* Check that a Path belongs to this FileSystem.
   * 
   * If the path is fully qualified URI, then its scheme and authority
   * matches that of this file system. Otherwise the path must be 
   * slash-relative name.
   * @param path the path.
   * @throws InvalidPathException if the path is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,write,org.apache.hadoop.fs.FileStatus:write(java.io.DataOutput),530,537,"/**
* Writes file status to DataOutput.
* @param out destination for writing data
* @deprecated Use alternative method instead
*/","* Write instance encoded as protobuf to stream.
   * @param out Output stream
   * @see PBHelper#convert(FileStatus)
   * @deprecated Use the {@link PBHelper} and protobuf serialization directly.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPutArguments,"org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPutArguments(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)",114,124,"/**
* Validates and processes file upload parameters.
* @param filePath path to the file being uploaded
* @param inputStream stream containing file data
* @param partNumber current part number of the upload
* @param uploadId unique identifier for the upload session
* @param lengthInBytes size of the current part in bytes
*/","* Check all the arguments to the
   * {@link MultipartUploader#putPart(UploadHandle, int, Path, InputStream, long)}
   * operation.
   * @param filePath Target path for upload (as {@link #startUpload(Path)}).
   * @param inputStream Data for this part. Implementations MUST close this
   * stream after reading in the data.
   * @param partNumber Index of the part relative to others.
   * @param uploadId Identifier from {@link #startUpload(Path)}.
   * @param lengthInBytes Target length to read from the stream.
   * @throws IllegalArgumentException invalid argument",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,abortUploadsUnderPath,org.apache.hadoop.fs.impl.AbstractMultipartUploader:abortUploadsUnderPath(org.apache.hadoop.fs.Path),132,138,"/**
* Processes file at given path and returns result asynchronously.
* @param path file path to process
* @return CompletableFuture with processed integer result
*/","* {@inheritDoc}.
   * @param path path to abort uploads under.
   * @return a future to -1.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",1452,1456,"/**
* Throws an exception indicating append operation is not supported.
* @param f file path to append data
* @param bufferSize buffer size for the output stream
* @param progress progress monitor for the operation
* @throws IOException always thrown with a message about unsupported operation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)",1498,1503,"/**
* Masks file operations by throwing an exception.
* @param f target file path
* @param recursive flag for recursive operation
* @throws AccessControlException if access is denied
* @throws IOException if I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1739,1745,"/**
 * Masks and renames a file.
 * @param src source file path
 * @param dst destination file path
 * @return false, always throws exception
 * @throws AccessControlException if access is denied
 * @throws IOException if I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)",1747,1750,"/**
 * Throws an exception as truncating is not supported.
 * @param f file path to truncate
 * @param newLength desired length of the file
 * @throws IOException always thrown with a message
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1752,1757,"/**
 * Masks file ownership by setting owner and throwing an exception.
 * @param f Path to the file
 * @param username Username to set as owner
 * @param groupname Group name to set for the file
 * @throws AccessControlException if access control fails
 * @throws IOException if I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1759,1764,"/**
 * Masks file permissions.
 * @param f file path
 * @throws AccessControlException if access is denied
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)",1766,1771,"/**
 * Sets file replication.
 * @param f file path
 * @param replication desired replication factor
 * @throws AccessControlException if access is denied
 * @throws IOException on I/O error
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)",1773,1778,"/**
* Masks file times and throws an exception.
* @param f file path
* @param mtime modification time in milliseconds
* @param atime access time in milliseconds
* @throws AccessControlException if access is denied
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1800,1805,"/**
 * Applies ACL changes to a file or directory.
 * @param path the file or directory path
 * @param aclSpec list of access control entries to apply
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1807,1812,"/**
 * Masks file or directory by applying ACL entries.
 * @param path file or directory path
 * @param aclSpec list of ACL entries to apply
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),1814,1818,"/**
* Applies mask to file path.
* @param path file path to apply mask
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path),1820,1824,"/**
 * Masks a file by removing its ACL.
 * @param path the file path to mask
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",1826,1830,"/**
* Applies ACL mask to specified file path.
* @param path file path to modify
* @param aclSpec list of ACL entries to apply
* @throws IOException if operation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",1841,1846,"/**
* Sets an extended attribute on a file.
* @param path the file path
* @param name the attribute name
* @param value the attribute value
* @param flag the attribute set flags
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1869,1873,"/**
 * Masks a file attribute by removing it.
 * @param path file path to modify
 * @param name attribute name to remove
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1875,1880,"/**
 * Masks a file path by creating a snapshot.
 * @param path original file path
 * @param snapshotName name of the snapshot
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1882,1887,"/**
* Masks and renames a snapshot.
* @param path file system path to the snapshot
* @param snapshotOldName current name of the snapshot
* @param snapshotNewName new name for the snapshot
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1889,1894,"/**
 * Masks a file path by deleting its snapshot.
 * @param path the file path to mask
 * @param snapshotName name of the snapshot to delete
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1901,1905,"/**
 * Processes file path and throws an exception.
 * @param src source file path to process
 * @throws IOException if storage policy is not satisfied
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1907,1912,"/**
* Applies storage policy to a file.
* @param src path to the source file
* @param policyName name of the storage policy
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),1914,1918,"/**
* Masks file by setting storage policy.
* @param src source file path
* @throws IOException if operation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,delete,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)",1044,1049,"/**
* Masks file operations by throwing an exception.
* @param f target file path
* @param recursive whether to apply recursively
* @return never returns true
* @throws AccessControlException on operation attempt
* @throws IOException if file access fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)",1308,1313,"/**
* Masks file by truncating it to a specified length.
* @param f file path to be truncated
* @param newLength new length of the file
* @throws FileNotFoundException if file is not found
* @throws IOException on I/O errors
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1315,1321,"/**
* Masks file operations by renaming source to destination.
* @param src source file path
* @param dst destination file path
* @throws AccessControlException if access is denied
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1328,1332,"/**
* Throws an access control exception when attempting to create a symlink.
* @param target path where the symlink would point
* @param link path of the symlink to be created
* @param createParent flag indicating if parent directory should be created
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1340,1345,"/**
* Masks file ownership and throws an access control exception.
* @param f file path to be masked
* @param username user name (unused)
* @param groupname group name (unused)
* @throws AccessControlException always thrown after masking
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1347,1352,"/**
* Sets file permissions.
* @param f file path
* @param permission new file permissions
* @throws AccessControlException if access is denied
* @throws IOException on I/O errors
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)",1354,1359,"/**
* Sets file replication and throws an exception.
* @param f file path
* @param replication desired replication factor
* @throws AccessControlException if access is denied
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)",1361,1366,"/**
 * Masks file times and throws an exception.
 * @param f file path
 * @param mtime modified time in milliseconds
 * @param atime accessed time in milliseconds
 * @throws AccessControlException if access is denied
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1374,1379,"/**
* Applies ACL mask to specified file path.
* @param path file path to modify
* @param aclSpec list of ACL entries to apply
* @throws IOException if operation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1381,1386,"/**
* Masks file permissions by applying ACL entries.
* @param path file path to apply ACLs
* @param aclSpec list of ACL entries to mask permissions
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),1388,1392,"/**
 * Masks a file by removing its default ACL.
 * @param path the file path to mask
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path),1394,1398,"/**
 * Masks file by removing its ACL.
 * @param path file path to mask
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",1400,1404,"/**
 * Masks file permissions using specified ACL entries.
 * @param path file path to apply ACLs
 * @param aclSpec list of ACL entries to mask the file
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",1415,1420,"/**
* Masks and sets an extended attribute on a file.
* @param path file path
* @param name attribute name
* @param value attribute value
* @param flag set flags
* @throws IOException if operation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1443,1447,"/**
 * Masks a file by removing an attribute.
 * @param path file path to mask
 * @param name attribute name to remove
 * @throws IOException if operation fails
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1449,1454,"/**
 * Creates a snapshot of the specified path.
 * @param path the path to create a snapshot from
 * @param snapshotName name for the snapshot
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1456,1461,"/**
* Renames a snapshot file.
* @param path path to the snapshot directory
* @param snapshotOldName original snapshot name
* @param snapshotNewName new snapshot name
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1463,1468,"/**
 * Masks a file by deleting its snapshot.
 * @param path Path to the file
 * @param snapshotName Name of the snapshot to delete
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1470,1473,"/**
 * Throws an exception to satisfy storage policy.
 * @param path file path to check policy for
 * @throws IOException if policy not satisfied
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1475,1479,"/**
 * Masks storage policy for a given path.
 * @param path file system path to apply policy
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",340,364,"/**
* Creates a file with specified options.
* @param f file path
* @param flag creation flags
* @param absolutePermission file permissions
* @param bufferSize buffer size for data transfer
* @param replication number of replicas
* @param blockSize block size for storage
* @param progress monitor for progress updates
* @param checksumOpt checksum options
* @param createParent if true, creates parent directories
* @return FSDataOutputStream for writing to the file
* @throws various exceptions on failure
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",651,667,"/**
* Creates a symbolic link.
* @param target path to the target file or directory
* @param link path where the symbolic link should be created
* @param createParent flag to create parent directories if necessary
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if the link cannot be resolved
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:equals(java.lang.Object),3923,3929,"/**
* Checks if object is an instance of LinkedSegmentsDescriptor.
* @param o object to check
* @return true if o is LinkedSegmentsDescriptor, otherwise false
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getCredentialEntry,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getCredentialEntry(java.lang.String),173,198,"/**
* Retrieves a credential entry by alias.
* @param alias the key alias to fetch
* @return CredentialEntry object or null if not found
* @throws IOException if an error occurs during retrieval
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getAliases,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getAliases(),206,226,"/**
* Retrieves all aliases from the keystore.
* @return List of alias names
* @throws IOException if an error occurs while accessing the keystore
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,skip,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:skip(long),291,299,"/**
* Adjusts read length based on current position and file length.
* @param n requested number of bytes to read
* @return adjusted number of bytes that can be read
* @throws IOException if an I/O error occurs
*/","* Skips over and discards <code>n</code> bytes of data from the
     * input stream.
     *
     * The <code>skip</code> method skips over some smaller number of bytes
     * when reaching end of file before <code>n</code> bytes have been skipped.
     * The actual number of bytes skipped is returned.  If <code>n</code> is
     * negative, no bytes are skipped.
     *
     * @param      n   the number of bytes to be skipped.
     * @return     the actual number of bytes skipped.
     * @exception  IOException  if an I/O error occurs.
     *             ChecksumException if the chunk to skip to is corrupted",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getPermissions,org.apache.hadoop.fs.ftp.FTPFileSystem:getPermissions(org.apache.commons.net.ftp.FTPFile),455,461,"/**
* Converts FTP file permissions to FsPermission.
* @param ftpFile FTP file object
* @return FsPermission representing the file's access rights
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,applyUMask,org.apache.hadoop.fs.permission.FsPermission:applyUMask(org.apache.hadoop.fs.permission.FsPermission),297,301,"/**
* Applies a mask to an FsPermission.
* @param umask the permission mask to apply
* @return new FsPermission with masked permissions
*/","* Apply a umask to this permission and return a new one.
   *
   * The umask is used by create, mkdir, and other Hadoop filesystem operations.
   * The mode argument for these operations is modified by removing the bits
   * which are set in the umask.  Thus, the umask limits the permissions which
   * newly created files and directories get.
   *
   * @param umask              The umask to use
   * 
   * @return                   The effective permission",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FsPermissionProto),38,41,"/**
 * Converts a protocol buffer permission to an FsPermission object.
 * @param proto protocol buffer containing permission data
 * @return FsPermission object representing the permission
 * @throws IOException if conversion fails
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getPermissions,org.apache.hadoop.fs.sftp.SFTPFileSystem:getPermissions(com.jcraft.jsch.ChannelSftp$LsEntry),305,307,"/**
* Converts SFTP file permissions to FsPermission.
* @param sftpFile SFTP entry representing a file
* @return FsPermission object
*/","* Return file permission.
   *
   * @param sftpFile
   * @return file permission",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processPath,org.apache.hadoop.fs.FsShellPermissions$Chmod:processPath(org.apache.hadoop.fs.shell.PathData),98,110,"/**
 * Updates file permissions if necessary.
 * @param item PathData object representing the file
 * @throws IOException if permission change fails
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(int),127,129,"/**
 * Constructs an FsPermission with specified mode.
 * @param mode file permission mode (e.g., 0755)
 */","* Construct by the given mode.
   *
   * octal mask is applied.
   *
   *<pre>
   *              before mask     after mask    file type   sticky bit
   *
   *    octal     100644            644         file          no
   *    decimal    33188            420
   *
   *    octal     101644           1644         file          yes
   *    decimal    33700           1420
   *
   *    octal      40644            644         directory     no
   *    decimal    16804            420
   *
   *    octal      41644           1644         directory     yes
   *    decimal    17316           1420
   *</pre>
   *
   * 100644 becomes 644 while 644 remains as 644
   *
   * @param mode Mode is supposed to come from the result of native stat() call.
   *             It contains complete permission information: rwxrwxrwx, sticky
   *             bit, whether it is a directory or a file, etc. Upon applying
   *             mask, only permission and sticky bit info will be kept because
   *             they are the only parts to be used for now.
   * @see #FsPermission(short mode)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getDefault,org.apache.hadoop.fs.permission.FsPermission:getDefault(),416,418,"/**
* Returns full permissions mask.
* @return FsPermission object with all permissions set
*/","* Get the default permission for directory and symlink.
   * In previous versions, this default permission was also used to
   * create files, so files created end up with ugo+x permission.
   * See HADOOP-9155 for detail. 
   * Two new methods are added to solve this, please use 
   * {@link FsPermission#getDirDefault()} for directory, and use
   * {@link FsPermission#getFileDefault()} for file.
   * This method is kept for compatibility.
   *
   * @return Default FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getDirDefault,org.apache.hadoop.fs.permission.FsPermission:getDirDefault(),425,427,"/**
 * Returns a permission mask with full read/write/execute permissions.
 * @return FsPermission object representing 00777 permissions
 */","* Get the default permission for directory.
   *
   * @return DirDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getFileDefault,org.apache.hadoop.fs.permission.FsPermission:getFileDefault(),434,436,"/**
 * Returns file system permissions with read and write access for all users.
 * @return FsPermission object representing 00666 permissions
 */","* Get the default permission for file.
   *
   * @return FileDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getCachePoolDefault,org.apache.hadoop.fs.permission.FsPermission:getCachePoolDefault(),443,445,"/**
 * Returns default file permissions.
 * @return FsPermission object with mask 00755
 */","* Get the default permission for cache pools.
   *
   * @return CachePoolDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,valueOf,org.apache.hadoop.fs.permission.FsPermission:valueOf(java.lang.String),452,475,"/**
 * Converts Unix symbolic permission to FsPermission.
 * @param unixSymbolicPermission Unix permission string
 * @return FsPermission object or null if input is null
 * @throws IllegalArgumentException if permission length is incorrect
 */","* Create a FsPermission from a Unix symbolic permission string
   * @param unixSymbolicPermission e.g. ""-rw-rw-rw-""
   * @return FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission:<init>(short),479,481,"/**
 * Constructs an immutable file system permission.
 * @param permission numeric representation of permissions
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processOptions,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processOptions(java.util.LinkedList),188,248,"/**
 * Processes ACL commands with validation.
 * @param args command arguments
 * @throws IOException on I/O errors
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,printAclEntriesForSingleScope,"org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printAclEntriesForSingleScope(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,java.util.List)",112,126,"/**
 * Processes ACL entries based on conditions.
 * @param aclStatus current ACL status
 * @param fsPerm file system permissions
 * @param entries list of ACL entries to process
 */","* Prints all the ACL entries in a single scope.
     * @param aclStatus AclStatus for the path
     * @param fsPerm FsPermission for the path
     * @param entries List<AclEntry> containing ACL entries of file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,aclSpecToString,org.apache.hadoop.fs.permission.AclEntry:aclSpecToString(java.util.List),330,337,"/**
* Masks ACL entries into a comma-separated string.
* @param aclSpec list of AclEntry objects
* @return masked string representation of ACL entries
*/","* Convert a List of AclEntries into a string - the reverse of parseAclSpec.
   * @param aclSpec List of AclEntries to convert
   * @return String representation of aclSpec",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(java.lang.String),148,150,"/**
* Constructs FsPermission from a string mode.
* @param mode permission string representation
*/","* Construct by given mode, either in octal or symbolic format.
   * @param mode mode as a string, either in octal or symbolic format
   * @throws IllegalArgumentException if <code>mode</code> is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processOptions,org.apache.hadoop.fs.FsShellPermissions$Chmod:processOptions(java.util.LinkedList),81,96,"/**
* Parses and applies chmod command with specified mode.
* @param args list of arguments including mode string
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,read,org.apache.hadoop.fs.store.ByteBufferInputStream:read(),94,100,"/**
* Masks and returns value from ByteBuffer.
* @return masked byte value or -1 if condition not met
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,skip,org.apache.hadoop.fs.store.ByteBufferInputStream:skip(long),102,114,"/**
 * Calculates and sets the new position for reading/writing.
 * @param offset bytes to move from current position
 * @return new position
 * @throws IOException if seek is out of bounds
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,mark,org.apache.hadoop.fs.store.ByteBufferInputStream:mark(int),140,145,"/**
* Marks and processes buffer up to read limit.
* @param readlimit maximum bytes to process
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,read,"org.apache.hadoop.fs.store.ByteBufferInputStream:read(byte[],int,int)",169,189,"/**
* Reads bytes from buffer into array.
* @param b destination byte array
* @param offset start position in the array
* @param length number of bytes to read
* @return actual number of bytes read or -1 if not available
* @throws IOException on I/O error
*/","* Read in data.
   * @param b destination buffer.
   * @param offset offset within the buffer.
   * @param length length of bytes to read.
   * @throws EOFException if the position is negative
   * @throws IndexOutOfBoundsException if there isn't space for the
   * amount of data requested.
   * @throws IllegalArgumentException other arguments are invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:startUpload(),887,897,"/**
* Handles block upload process.
* @throws IOException if an I/O error occurs
* @return BlockUploadData object containing upload details
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:startUpload(),593,600,"/**
* Overrides parent method to process block upload data.
* @return BlockUploadData object containing processed data
* @throws IOException if an I/O error occurs during processing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:startUpload(),722,731,"/**
* Initializes and returns block upload data.
* @throws IOException if I/O operations fail
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,close,org.apache.hadoop.fs.store.DataBlocks$DataBlock:close(),475,481,"/**
* Executes masked function logic.
* Closes resource and performs cleanup if condition met.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,checkAndWriteSync,org.apache.hadoop.io.SequenceFile$Writer:checkAndWriteSync(),1445,1450,"/**
* Checks and performs synchronization if needed.
* @throws IOException if an I/O error occurs during sync
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCompressedSize(),170,173,"/**
 * Calculates mask value based on position difference.
 * @return calculated mask value as long
 * @throws IOException if an I/O error occurs
 */","* Current size of compressed data.
       * 
       * @return
       * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getFileLength,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:getFileLength(),507,512,"/**
 * Returns the file length.
 * @return Length of the file in bytes.
 * @throws IOException if an I/O error occurs.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPathArgument,org.apache.hadoop.fs.shell.Ls:processPathArgument(org.apache.hadoop.fs.shell.PathData),232,246,"/**
* Handles file system operations, checks for erasure coding support.
* @param item PathData object representing the file or directory
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,adjustColumnWidths,org.apache.hadoop.fs.shell.Ls:adjustColumnWidths(org.apache.hadoop.fs.shell.PathData[]),327,355,"/**
 * Updates format string for path data based on item statistics.
 * @param items array of PathData objects
 * @throws IOException if an I/O error occurs
 */","* Compute column widths and rebuild the format string
   * @param items to find the max field width for each column",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPath,org.apache.hadoop.fs.shell.FsUsage$Du:processPath(org.apache.hadoop.fs.shell.PathData),219,230,"/**
 * Masks file data by adjusting size and space consumed.
 * @param item PathData object representing the file
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getQuotaUsage,org.apache.hadoop.fs.FileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),1952,1954,"/**
 * Retrieves quota usage for a given file path.
 * @param f file path to check quota usage
 * @return QuotaUsage object representing the quota information
 * @throws IOException if an I/O error occurs while accessing the file
 */","Return the {@link QuotaUsage} of a given {@link Path}.
   * @param f path to use
   * @return the quota usage
   * @throws IOException IO failure",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUsed,org.apache.hadoop.fs.FileSystem:getUsed(org.apache.hadoop.fs.Path),2730,2732,"/**
 * Masks file path.
 * @param path file path to mask
 * @return masked value as long
 * @throws IOException if I/O error occurs
 */","* Return the total size of all files from a specified path.
   * @param path the path.
   * @throws IOException IO failure
   * @return the number of path content summary.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/AbstractLaunchableService.java,<init>,org.apache.hadoop.service.launcher.AbstractLaunchableService:<init>(java.lang.String),48,50,"/**
 * Initializes an AbstractLaunchableService with a given name.
 * @param name service identifier
 */","* Construct an instance with the given name.
   *
   * @param name input name.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,<init>,org.apache.hadoop.service.CompositeService:<init>(java.lang.String),53,55,"/**
 * Constructs a new CompositeService instance.
 * @param name service name
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,<init>,org.apache.hadoop.util.JvmPauseMonitor:<init>(),70,72,"/**
 * Constructs a new JvmPauseMonitor instance.
 * Initializes with the class name as the identifier.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,enterState,org.apache.hadoop.service.ServiceStateModel:enterState(org.apache.hadoop.service.Service$STATE),113,119,"/**
* Updates service state.
* @param proposed new state to set
* @return previous state of the service
*/","* Enter a state -thread safe.
   *
   * @param proposed proposed new state
   * @return the original state
   * @throws ServiceStateException if the transition is not permitted",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,instantiateService,org.apache.hadoop.service.launcher.ServiceLauncher:instantiateService(org.apache.hadoop.conf.Configuration),658,694,"/**
 * Initializes and returns a service instance.
 * @param conf configuration object
 * @return Service instance
 * @throws Exception if initialization fails
 */","* @return Instantiate the service defined in {@code serviceClassName}.
   *
   * Sets the {@code configuration} field
   * to the the value of {@code conf},
   * and the {@code service} field to the service created.
   *
   * @param conf configuration to use",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.Throwable,java.lang.String,java.lang.Object[])",1094,1098,"/**
* Constructs a KerberosDiagsFailure with a category, throwable cause, and formatted message.
* @param category failure category
* @param throwable underlying exception
* @param message error message template
* @param args arguments for message formatting
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceStop,org.apache.hadoop.service.CompositeService:serviceStop(),128,136,"/**
* Stops a specified number of services.
* Logs the action if logging is enabled.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,run,org.apache.hadoop.service.CompositeService$CompositeServiceShutdownHook:run(),184,187,"/**
 * Calls m1 on compositeService.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,progressable,org.apache.hadoop.io.MapFile$Writer:progressable(org.apache.hadoop.util.Progressable),308,310,"/**
* Creates an option for setting a progressable callback.
* @param value Progressable instance to track progress
* @return SequenceFile.Writer.Option configured with the progressable
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,valueClass,org.apache.hadoop.io.MapFile$Writer:valueClass(java.lang.Class),293,295,"/**
 * Creates an option for a SequenceFile writer.
 * @param value class type for the option
 * @return SequenceFile.Writer.Option instance
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,equals,org.apache.hadoop.io.BinaryComparable:equals(java.lang.Object),74,82,"/**
* Compares two objects for equality based on specific criteria.
* @param other the object to compare with
* @return true if objects are equal, false otherwise
*/",* Return true if bytes from {#getBytes()} match.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,hashCode,org.apache.hadoop.io.BytesWritable:hashCode(),207,210,"/**
 * Calls the superclass's m1 method.
 * @return result of superclass's m1 method
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,hashCode,org.apache.hadoop.io.Text:hashCode(),422,425,"/**
 * Calls the superclass implementation of m1.
 * @return result from superclass's m1 method
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,hashCode,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:hashCode(),95,98,"/**
 * Delegates to token's m1 method.
 * @return result of token.m1()
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,set,"org.apache.hadoop.io.BytesWritable:set(byte[],int,int)",178,182,"/**
* Masks data in byte array.
* @param newData source byte array to mask
* @param offset starting index in newData
* @param length number of bytes to mask
*/","* Set the value to a copy of the given byte range.
   *
   * @param newData the new values to copy in
   * @param offset the offset in newData to start at
   * @param length the number of bytes to copy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,readFields,org.apache.hadoop.io.BytesWritable:readFields(java.io.DataInput),184,189,"/**
 * Reads data from input and processes it.
 * @param in DataInput source to read from
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(org.apache.hadoop.io.BytesWritable),1690,1694,"/**
* Modifies and returns an integer value from BytesWritable.
* @param key input BytesWritable object
* @return modified integer value
*/","* Copy the key into BytesWritable. The input BytesWritable will be
         * automatically resized to the actual key size.
         * 
         * @param key
         *          BytesWritable to hold the key.
         * @throws IOException raised on errors performing I/O.
         * @return the key into BytesWritable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,list,org.apache.hadoop.fs.FileUtil:list(java.io.File),1619,1630,"/**
* Lists file names in a directory with permission check.
* @param dir directory to list files from
* @return array of file names
* @throws AccessDeniedException if access is denied
* @throws IOException for invalid directory or I/O errors
*/","* A wrapper for {@link File#list()}. This java.io API returns null
   * when a dir is not a directory or for any I/O error. Instead of having
   * null check everywhere File#list() is used, we will add utility API
   * to get around this problem. For the majority of cases where we prefer
   * an IOException to be thrown.
   * @param dir directory for which listing should be performed
   * @return list of file names or empty string list
   * @exception AccessDeniedException for unreadable directory
   * @exception IOException for invalid directory or for bad disk",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkAccessByFileMethods,org.apache.hadoop.util.DiskChecker:checkAccessByFileMethods(java.io.File),153,174,"/**
* Validates directory permissions.
* Throws DiskErrorException if any permission check fails.
*/","* Checks that the current running process can read, write, and execute the
   * given directory by using methods of the File object.
   * 
   * @param dir File to check
   * @throws DiskErrorException if dir is not readable, not writable, or not
   *   executable",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,mlock,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:mlock(java.nio.ByteBuffer,long)",470,477,"/**
 * Masks data in a direct ByteBuffer.
 * @param buffer direct ByteBuffer containing the data to mask
 * @param len length of the data to process
 * @throws IOException if ByteBuffer is not direct or other I/O error occurs
 */","* Locks the provided direct ByteBuffer into memory, preventing it from
     * swapping out. After a buffer is locked, future accesses will not incur
     * a page fault.
     * 
     * See the mlock(2) man page for more information.
     * 
     * @throws NativeIOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.java,create,"org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:create(java.lang.String,java.lang.String[])",74,100,"/**
 * Creates a SharedFileDescriptorFactory from given paths.
 * @param prefix common prefix for file operations
 * @param paths array of file paths to attempt factory creation
 * @return SharedFileDescriptorFactory instance if successful
 * @throws IOException if no valid factory can be created or an error occurs
 */","* Create a new SharedFileDescriptorFactory.
   *
   * @param prefix       The prefix to prepend to all the file names created
   *                       by this factory.
   * @param paths        An array of paths to use.  We will try each path in 
   *                       succession, and return a factory using the first 
   *                       usable path.
   * @return             The factory.
   * @throws IOException If a factory could not be created for any reason.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getMemlockLimit,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getMemlockLimit(),285,287,"/**
 * Calls native method m1.
 * @return result from native call
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,writeChecksumChunks,"org.apache.hadoop.fs.FSOutputSummer:writeChecksumChunks(byte[],int,int)",212,228,"/**
* Processes a byte array with checksum calculations.
* @param b the input byte array
* @param off starting offset in the array
* @param len number of bytes to process
* @throws IOException if an I/O error occurs during processing
*/","Generate checksums for the given data chunks and output chunks & checksums
   * to the underlying output stream.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,calculateChunkedSums,"org.apache.hadoop.util.DataChecksum:calculateChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer)",534,564,"/**
* Processes data and calculates checksums.
* @param data ByteBuffer containing input data
* @param checksums ByteBuffer to store calculated checksums
*/","* Calculate checksums for the given data.
   * 
   * The 'mark' of the ByteBuffer parameters may be modified by this function,
   * but the position is maintained.
   * 
   * @param data the DirectByteBuffer pointing to the data to checksum.
   * @param checksums the DirectByteBuffer into which checksums will be
   *                  stored. Enough space must be available in this
   *                  buffer to put the checksums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,freeBuffers,org.apache.hadoop.crypto.CryptoInputStream:freeBuffers(),809,813,"/**
* Applies cryptographic mask to input and output buffers.
* Uses CryptoStreamUtils methods for masking operations.
*/",Forcibly free the direct buffers.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,unbuffer,org.apache.hadoop.crypto.CryptoInputStream:unbuffer(),853,858,"/**
 * Calls methods m1 and m2, then invokes StreamCapabilitiesPolicy's m3 with input.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int),45,47,"/**
 * Constructs a bounded byte array output stream with specified capacity.
 * @param capacity maximum size of the buffer
 */","* Create a BoundedByteArrayOutputStream with the specified
   * capacity
   * @param capacity The capacity of the underlying byte array",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeFromUrlString,org.apache.hadoop.security.token.Token:decodeFromUrlString(java.lang.String),382,384,"/**
 * Masks a value by calling another function.
 * @param newValue new value to be masked
 * @throws IOException if an I/O error occurs
 */","* Decode the given url safe string into this token.
   * @param newValue the encoded string
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),89,107,"/**
* Filters ECBlocks based on a condition.
* @param blockGroup group of ECBlock objects
* @return filtered array of ECBlock objects
*/","* Which blocks were erased ?
   * @param blockGroup blockGroup.
   * @return output blocks to recover",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",36,49,"/**
* Initializes decoding state for erasure coding.
* @param decoder the raw erasure decoder to use
* @param inputs array of input ByteBuffers
* @param erasedIndexes indices of erased blocks
* @param outputs array of output ByteBuffers
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,byte[][],int[],byte[][])",37,52,"/**
 * Initializes decoding state for a raw erasure decoder.
 * @param decoder the erasure decoder to use
 * @param inputs array of input data buffers
 * @param erasedIndexes indices of erased blocks
 * @param outputs array of output buffers for decoded data
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,41,"/**
* Creates a native XOR raw erasure decoder.
* @param coderOptions configuration options for the decoder
* @return RawErasureDecoder instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,41,"/**
* Creates a raw erasure decoder using native RS algorithm.
* @param coderOptions configuration options for the erasure coder
* @return RawErasureDecoder instance initialized with given options
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,53,"/**
* Constructs an RSLegacyRawDecoder with given coder options.
* @param coderOptions configuration for erasure coding
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,53,"/**
* Initializes RSLegacyRawEncoder with given coder options.
* @param coderOptions configuration for erasure coding
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),33,36,"/**
* Creates a raw erasure encoder using XOR.
* @param coderOptions configuration options for the encoder
* @return instance of NativeXORRawEncoder
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),33,36,"/**
* Creates a raw erasure encoder using native RS.
* @param coderOptions configuration options for the encoder
* @return NativeRSRawEncoder instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),39,62,"/**
* Applies XOR operation on inputs excluding erased index.
* @param decodingState state containing inputs and outputs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),39,60,"/**
* Applies XOR mask to input buffers and writes to output.
* @param encodingState holds input and output buffers
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),167,217,"/**
* Processes decoding state to handle masked data.
* @param decodingState current decoding state with inputs and outputs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),55,88,"/**
 * Applies encoding mask to data buffers.
 * @param encodingState state containing input and output buffers
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),63,68,"/**
* Encodes data using specified state.
* @param encodingState contains input and output buffers
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),64,86,"/**
* Applies XOR mask to inputs, excluding erased index.
* @param decodingState state containing input/output buffers and lengths
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),62,85,"/**
 * Masks input data and writes to output.
 * @param encodingState state containing inputs, outputs, and offsets
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),111,165,"/**
 * Processes and adjusts input data based on erased indexes.
 * @param decodingState state containing input and output details
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),90,128,"/**
 * Masks data using a polynomial.
 * @param encodingState state containing input and output data
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),70,79,"/**
* Applies mask to encode data.
* @param encodingState holds encoding parameters and buffers
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
* Creates a raw erasure decoder.
* @param coderOptions configuration options for the decoder
* @return RawErasureDecoder instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
* Creates an encoder using RSEncoder.
* @param coderOptions configuration options for encoding
* @return RawErasureEncoder instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,processErasures,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:processErasures(int[]),117,140,"/**
* Initializes error correction matrices and flags for data erasure.
* @param erasedIndexes array of indexes indicating erased data units
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,skipToNextMarker,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextMarker(long,int)",217,257,"/**
 * Masks and checks for a marker in compressed data.
 * @param marker the marker value to search for
 * @param markerBitLength length of the marker in bits
 * @return true if marker is found, false otherwise
 * @throws IOException if an I/O error occurs
 * @throws IllegalArgumentException if markerBitLength exceeds 63
 */","* This method tries to find the marker (passed to it as the first parameter)
   * in the stream. It can find bit patterns of length &lt;= 63 bits.
   * Specifically this method is used in CBZip2InputStream to find the end of
   * block (EOB) delimiter in the stream, starting from the current position
   * of the stream. If marker is found, the stream position will be at the
   * byte containing the starting bit of the marker.
   * @param marker The bit pattern to be found in the stream
   * @param markerBitLength No of bits in the marker
   * @return true if the marker was found otherwise false
   * @throws IOException raised on errors performing I/O.
   * @throws IllegalArgumentException if marketBitLength is greater than 63",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetUByte,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetUByte(),660,662,"/**
 * Returns a masked character.
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetInt,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetInt(),664,666,"/**
* Reads and constructs a masked integer from byte input.
* @return masked integer value
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,getAndMoveToFrontDecode0,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode0(int),1006,1037,"/**
 * Decodes a symbol from the input stream using BWT.
 * @param groupNo index of the group to decode
 * @return decoded symbol
 * @throws IOException if end of stream is reached unexpectedly
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,recvDecodingTables,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:recvDecodingTables(),709,788,"/**
 * Masks data and sets up decoding tables.
 * @throws IOException if I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream,int)",627,643,"/**
 * Initializes a CBZip2OutputStream with the specified output stream and block size.
 * @param out the OutputStream to write compressed data to
 * @param blockSize the compression block size (1-9)
 * @throws IOException if an I/O error occurs
 */","* Constructs a new <tt>CBZip2OutputStream</tt> with specified blocksize.
  *
  * <p>
  * <b>Attention: </b>The caller is resonsible to write the two BZip2 magic
  * bytes <tt>""BZ""</tt> to the specified stream prior to calling this
  * constructor.
  * </p>
  *
  *
  * @param out
  *            the destination stream.
  * @param blockSize
  *            the blockSize as 100k units.
  *
  * @throws IOException
  *             if an I/O error occurs in the specified stream.
  * @throws IllegalArgumentException
  *             if {@code (blockSize < 1) || (blockSize > 9)}
  * @throws NullPointerException
  *             if {@code out == null}.
  *
  * @see #MIN_BLOCKSIZE
  * @see #MAX_BLOCKSIZE",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,moveToFrontCodeAndSend,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:moveToFrontCodeAndSend(),1391,1395,"/**
 * Masks function execution.
 * Calls three methods in sequence to perform masking operations.
 * @throws IOException if an I/O error occurs during operation
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,blockSort,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:blockSort(),1605,1629,"/**
* Initializes and processes work limits, checks conditions, and updates pointers.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",69,71,"/**
* Constructs a BlockCompressorStream with default block size and compression level.
* @param out output stream to compress data into
* @param compressor the compressor algorithm to use
*/","* Create a {@link BlockCompressorStream} with given output-stream and 
   * compressor.
   * Use default of 512 as bufferSize and compressionOverhead of 
   * (1% of bufferSize + 12 bytes) =  18 bytes (zlib algorithm).
   * 
   * @param out stream
   * @param compressor compressor to be used",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",74,77,"/**
* Constructs a DecompressorStream with default buffer size.
* @param in input stream to be decompressed
* @param decompressor decompressor instance
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",48,51,"/**
 * Constructs a BlockDecompressorStream.
 * @param in input stream to read compressed data from
 * @param decompressor decompressor to use for decompression
 * @param bufferSize size of the buffer used for decompression
 * @throws IOException if an I/O error occurs
 */","* Create a {@link BlockDecompressorStream}.
   * 
   * @param in input stream
   * @param decompressor decompressor to use
   * @param bufferSize size of buffer
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,createInputStream,"org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",138,142,"/**
* Creates a passthrough decompression stream.
* @param in input stream to be decompressed
* @param decompressor unused parameter
* @return passthrough decompression stream
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,read,"org.apache.hadoop.io.compress.DecompressorStream:read(byte[],int,int)",95,106,"/**
* Masks bytes in array using specified offset and length.
* @param b byte array to mask
* @param off starting offset in the array
* @param len number of bytes to mask
* @return result of masking operation or 0 if length is 0
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,write,"org.apache.hadoop.io.compress.BlockCompressorStream:write(byte[],int,int)",81,134,"/**
 * Masks data and writes it to the output stream.
 * @param b byte array containing data to be written
 * @param off starting offset in the byte array
 * @param len number of bytes to write
 * @throws IOException if an I/O error occurs or writing beyond end of stream
 */","* Write the data provided to the compression codec, compressing no more
   * than the buffer size less the compression overhead as specified during
   * construction for each block.
   *
   * Each block contains the uncompressed length for the block, followed by
   * one or more length-prefixed blocks of compressed data.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>(),70,72,"/**
* Initializes a new ZStandard decompressor with default stream size.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:<init>(int),298,300,"/**
 * Initializes a new instance of ZStandardDirectDecompressor.
 * @param directBufferSize size of the buffer for decompression
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int)",90,92,"/**
 * Constructs a new ZStandardCompressor with specified level and buffer size.
 * @param level compression level (0-21)
 * @param bufferSize size of the input/output buffer
 */","* Creates a new compressor with the default compression level.
   * Compressed data will be generated in ZStandard format.
   * @param level level.
   * @param bufferSize bufferSize.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getCompressor,org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec),167,169,"/**
 * Creates a compressor using the specified codec.
 * @param codec compression algorithm to use
 * @return Compressor instance configured with the codec
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getDecompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:getDecompressor(),319,343,"/**
* Obtains and configures a decompressor from CodecPool.
* @return Decompressor object or null if not available
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodec.java,createOutputStreamWithCodecPool,"org.apache.hadoop.io.compress.CompressionCodec$Util:createOutputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.OutputStream)",128,143,"/**
* Creates a CompressionOutputStream for the given codec and configuration.
* @param codec compression codec to use
* @param conf configuration settings
* @param out output stream to wrap
* @return CompressionOutputStream instance
* @throws IOException if an I/O error occurs
*/","* Create an output stream with a codec taken from the global CodecPool.
     *
     * @param codec       The codec to use to create the output stream.
     * @param conf        The configuration to use if we need to create a new codec.
     * @param out         The output stream to wrap.
     * @return            The new output stream
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionOutputStream.java,close,org.apache.hadoop.io.compress.CompressionOutputStream:close(),61,75,"/**
* Calls m3 and ensures resources are released.
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Writer:close(),1422,1443,"/**
* Resets serializers and releases resources.
* Throws IOException if an I/O error occurs during reset.
*/",Close the file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,returnCompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnCompressor(org.apache.hadoop.io.compress.Compressor),310,317,"/**
* Releases compressor back to pool.
* @param compressor the compressor to release
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodec.java,createInputStreamWithCodecPool,"org.apache.hadoop.io.compress.CompressionCodec$Util:createInputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.InputStream)",154,169,"/**
* Creates a CompressionInputStream for decoding.
* @param codec the compression codec to use
* @param conf configuration settings
* @param in input stream containing compressed data
* @return CompressionInputStream for decompression
* @throws IOException if an I/O error occurs
*/","* Create an input stream with a codec taken from the global CodecPool.
     *
     * @param codec       The codec to use to create the input stream.
     * @param conf        The configuration to use if we need to create a new codec.
     * @param in          The input stream to wrap.
     * @return            The new input stream
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionInputStream.java,close,org.apache.hadoop.io.compress.CompressionInputStream:close(),65,75,"/**
* Calls m2 on input and releases decompressor resource.
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Reader:close(),2169,2188,"/**
 * Releases decompressors and deserializers, closes input stream.
 * @throws IOException on I/O error during resource release
 */",Close the file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,returnDecompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnDecompressor(org.apache.hadoop.io.compress.Decompressor),345,352,"/**
 * Releases decompressor to pool if not null.
 * @param decompressor the decompressor to release
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createCompressor,org.apache.hadoop.io.compress.GzipCodec:createCompressor(),62,67,"/**
* Returns a Compressor based on configuration.
* @return GzipZlibCompressor if m1 is true, otherwise BuiltInGzipCompressor
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibCompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressor(org.apache.hadoop.conf.Configuration),109,113,"/**
* Returns a compressor based on configuration.
* @param conf configuration settings
* @return Compressor instance
*/","* Return the appropriate implementation of the zlib compressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib compressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDirectDecompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDirectDecompressor(org.apache.hadoop.conf.Configuration),144,147,"/**
* Returns a direct decompressor based on configuration.
* @param conf configuration settings
* @return DirectDecompressor instance or null if not applicable
*/","* Return the appropriate implementation of the zlib direct decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib decompressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.GzipCodec:createDirectDecompressor(),109,114,"/**
* Returns a direct decompressor for handling gzip/zlib.
* @return DirectDecompressor instance or null if not configured
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDecompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressor(org.apache.hadoop.conf.Configuration),133,136,"/**
* Returns a decompressor based on configuration.
* @param conf configuration object
* @return ZlibDecompressor if m1 is true, otherwise BuiltInZlibInflater
*/","* Return the appropriate implementation of the zlib decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib decompressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createDecompressor,org.apache.hadoop.io.compress.GzipCodec:createDecompressor(),95,100,"/**
 * Returns a decompressor based on configuration.
 * @return Decompressor instance, either GzipZlib or BuiltInGzip
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,decompress,"org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:decompress(byte[],int,int)",189,243,"/**
 * Decompresses data from a byte array.
 * @param b input byte array
 * @param off starting offset in the array
 * @param len number of bytes to decompress
 * @return number of available bytes after decompression
 * @throws IOException if an I/O error occurs or decompression fails
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,write,org.apache.hadoop.io.SequenceFile$Metadata:write(java.io.DataOutput),759,769,"/**
* Writes metadata to output stream.
* @param out DataOutput object for writing
* @throws IOException if I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,writeImpl,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:writeImpl(java.io.DataOutput),205,215,"/**
* Writes token data to output stream.
* @param out DataOutput stream for writing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,write,org.apache.hadoop.security.token.Token:write(java.io.DataOutput),323,331,"/**
* Writes object data to DataOutput.
* @param out the DataOutput stream to write to
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),333,345,"/**
 * Handles storing a delegation key using a DataOutputStream.
 * @param key DelegationKey object to be stored
 * @throws IOException if an I/O error occurs or SQL operation fails
 */","* Persists a DelegationKey into the SQL database. The delegation keyId
   * is expected to be unique and any duplicate key attempts will result
   * in an IOException.
   * @param key DelegationKey to persist into the SQL database.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),351,363,"/**
* Updates delegation key using SQL secret manager.
* @param key DelegationKey object to be updated
*/","* Updates an existing DelegationKey in the SQL database.
   * @param key Updated DelegationKey.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,addOrUpdateDelegationKey,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)",691,725,"/**
* Stores or updates a delegation key in ZooKeeper.
* @param key the DelegationKey to store
* @param isUpdate true if updating an existing key, false otherwise
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,readFields,org.apache.hadoop.io.VIntWritable:readFields(java.io.DataInput),49,52,"/**
 * Reads an integer from input and assigns it to value.
 * @param in DataInput stream to read from
 * @throws IOException if reading fails
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readStringSafely,"org.apache.hadoop.io.WritableUtils:readStringSafely(java.io.DataInput,int)",484,497,"/**
* Reads a masked string from input with a maximum length.
* @param in DataInput source for the string data
* @param maxLength maximum allowed length of the string
* @return decoded string or throws exception if invalid length
*/","* Read a string, but check it for sanity. The format consists of a vint
   * followed by the given number of bytes.
   * @param in the stream to read from
   * @param maxLength the largest acceptable length of the encoded string
   * @return the bytes as a string
   * @throws IOException if reading from the DataInput fails
   * @throws IllegalArgumentException if the encoded byte size for string 
             is negative or larger than maxSize. Only the vint is read.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readFields,org.apache.hadoop.io.Text:readFields(java.io.DataInput),346,350,"/**
* Reads data from input and processes it.
* @param in DataInput source
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readFields,"org.apache.hadoop.io.Text:readFields(java.io.DataInput,int)",352,362,"/**
* Deserializes data from input stream.
* @param in DataInput source
* @param maxLength Maximum allowed length for deserialization
* @throws IOException if deserialization fails or exceeds maxLength
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,skip,org.apache.hadoop.io.Text:skip(java.io.DataInput),369,372,"/**
* Masks data input by reading and processing its length.
* @param in DataInput stream to process
*/","* Skips over one Text in the input.
   * @param in input in.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readBuffer,"org.apache.hadoop.io.SequenceFile$Reader:readBuffer(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.compress.CompressionInputStream)",2274,2291,"/**
* Masks data from input buffer using compression filter.
* @param buffer input data buffer
* @param filter compression input stream
* @throws IOException if I/O error occurs
*/",Read a compressed buffer,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,readFields,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:readFields(java.io.DataInput),749,758,"/**
* Deserializes data from input stream.
* @param in DataInput source
* @throws IOException on I/O errors
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readString,"org.apache.hadoop.io.Text:readString(java.io.DataInput,int)",566,572,"/**
* Reads and masks data from input stream.
* @param in DataInput source
* @param maxLength maximum allowed length
* @return masked string representation of the data
* @throws IOException if reading fails
*/","* @return Read a UTF8 encoded string with a maximum size.
   * @param in input datainput.
   * @param maxLength input maxLength.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,readFields,org.apache.hadoop.security.token.delegation.DelegationKey:readFields(java.io.DataInput),108,119,"/**
* Reads and initializes object from DataInput.
* @param in source of input data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,writeString,"org.apache.hadoop.io.file.tfile.Utils:writeString(java.io.DataOutput,java.lang.String)",256,266,"/**
* Writes a string to DataOutput with length prefix.
* @param out DataOutput to write to
* @param s String to be written
*/","* Write a String as a VInt n, followed by n Bytes as in Text format.
   * 
   * @param out out.
   * @param s s.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,getCredentialEntry,org.apache.hadoop.security.alias.UserProvider:getCredentialEntry(java.lang.String),54,62,"/**
* Retrieves a credential entry by alias.
* @param alias unique identifier for the credential
* @return CredentialEntry object or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,createCredentialEntry,"org.apache.hadoop.security.alias.UserProvider:createCredentialEntry(java.lang.String,char[])",64,75,"/**
* Adds a new credential entry.
* @param name unique identifier for the credential
* @param credential character array representing the credential
* @return CredentialEntry object containing the added credential
* @throws IOException if credential already exists or an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,deleteCredentialEntry,org.apache.hadoop.security.alias.UserProvider:deleteCredentialEntry(java.lang.String),77,87,"/**
* Masks credential by name.
* @param name credential identifier
* @throws IOException if credential does not exist
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildTokenService,org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.InetSocketAddress),474,487,"/**
 * Masks IP address or hostname based on configuration.
 * @param addr InetSocketAddress object containing address details
 * @return Text object with masked address
 */","* Construct the service key for a token
   * @param addr InetSocketAddress of remote connection with a token
   * @return ""ip:port"" or ""host:port"" depending on the value of
   *          hadoop.security.token.service.use_ip",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.UserProvider:getKeyVersion(java.lang.String),58,66,"/**
* Fetches key version by name.
* @param versionName unique version identifier
* @return KeyVersion object or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getMetadata,org.apache.hadoop.crypto.key.UserProvider:getMetadata(java.lang.String),68,80,"/**
* Retrieves metadata by name with caching.
* @param name resource identifier
* @return Metadata object or null if not found
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,createKey,"org.apache.hadoop.crypto.key.UserProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",82,100,"/**
* Creates a new key version with specified options.
* @param name unique key identifier
* @param material raw key material
* @param options configuration options for the key
* @return KeyVersion object representing the created key version
* @throws IOException if key already exists or length is incorrect
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDtService,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDtService(java.net.URI),430,441,"/**
* Masks URI to extract and format service text.
* @param uri input URI object
* @return formatted service text as Text object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,tokenFromProto,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto),123,131,"/**
* Creates a Token from TokenProto.
* @param tokenProto protocol buffer containing token data
* @return Token object created from proto
*/","* Create a hadoop token from a protobuf token.
   * @param tokenProto token
   * @return a new token",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,find,org.apache.hadoop.io.Text:find(java.lang.String),171,173,"/**
 * Calls overloaded method with default offset.
 * @param what input string to process
 * @return result of processing with default offset
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeEnum,"org.apache.hadoop.io.WritableUtils:writeEnum(java.io.DataOutput,java.lang.Enum)",432,435,"/**
* Writes enum value to output stream.
* @param out DataOutput stream to write to
* @param enumVal Enum value to encode
* @throws IOException if an I/O error occurs
*/","* writes String value of enum to DataOutput. 
   * @param out Dataoutput stream
   * @param enumVal enum value
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,write,"org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput,java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",128,135,"/**
 * Writes user and group names with permissions to output.
 * @param out DataOutput stream for writing
 * @param username User name to write
 * @param groupname Group name to write
 * @param permission File system permissions to write
 * @throws IOException if an I/O error occurs
 */","* Serialize a {@link PermissionStatus} from its base components.
   * @param out out.
   * @param username username.
   * @param groupname groupname.
   * @param permission FsPermission.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(byte[]),112,114,"/**
 * Constructs a Text object from a UTF-8 byte array.
 * @param utf8 byte array containing UTF-8 encoded text
 */","* Construct from a byte array.
   *
   * @param utf8 input utf8.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(org.apache.hadoop.io.Text),103,105,"/**
 * Constructs a new Text instance from another Text.
 * @param utf8 source Text to copy
 */","* Construct from another text.
   * @param utf8 input utf8.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,"org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int,int)",180,187,"/**
 * Masks text based on delimiter presence.
 * @param str input text to process
 * @param maxLineLength maximum line length limit
 * @param maxBytesToConsume maximum bytes to consume
 * @return masked length or result of m1/m2 method
 * @throws IOException if an I/O error occurs
 */","* Read one line from the InputStream into the given Text.
   *
   * @param str the object to store the given line (without newline)
   * @param maxLineLength the maximum number of bytes to store into str;
   *  the rest of the line is silently discarded.
   * @param maxBytesToConsume the maximum number of bytes to consume
   *  in this call.  This is only a hint, because if the line cross
   *  this threshold, we allow it to happen.  It can overshoot
   *  potentially by as much as one buffer length.
   *
   * @return the number of bytes read including the (longest) newline
   * found.
   *
   * @throws IOException if the underlying stream throws",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,getTextLength,org.apache.hadoop.io.Text:getTextLength(),146,151,"/**
* Returns masked text length.
* Ensures textLength is non-negative by calling m1().m2().
* @return Masked text length
*/","* @return Returns the length of this text. The length is equal to the number of
   * Unicode code units in the text.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,toString,org.apache.hadoop.io.SequenceFile$Metadata:toString(),828,840,"/**
* Constructs a string representation of metadata.
* @return formatted metadata as a string
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setRenewer,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text),105,116,"/**
* Sets renewer text, handling null and IOException.
* @param renewer user or entity renewing the delegation token
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,getAliases,org.apache.hadoop.security.alias.UserProvider:getAliases(),111,119,"/**
* Masks credentials and returns a list of masked values.
* @return List of masked credential strings
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getCanonicalServiceName,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCanonicalServiceName(),1020,1023,"/**
 * Returns a masked string from the canonical service.
 * @return Masked string result from m1 method
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getCanonicalServiceName,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCanonicalServiceName(),246,249,"/**
* Returns masked function name.
* @return Masked string representation of the function
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,<init>,org.apache.hadoop.io.SortedMapWritable:<init>(org.apache.hadoop.io.SortedMapWritable),55,58,"/**
 * Constructs a new SortedMapWritable by copying another.
 * @param other the SortedMapWritable to copy from
 */","* Copy constructor.
   * 
   * @param other the map to copy from",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,<init>,org.apache.hadoop.io.MapWritable:<init>(org.apache.hadoop.io.MapWritable),53,56,"/**
 * Constructs a new MapWritable by copying another.
 * @param other the MapWritable to copy from
 */","* Copy constructor.
   * 
   * @param other the map to copy from",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,compression,org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType),297,300,"/**
 * Creates an option to set compression type for SequenceFile.
 * @param type desired compression type
 * @return Option object for configuration
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerializationComparator.java,<init>,org.apache.hadoop.io.serializer.JavaSerializationComparator:<init>(),42,45,"/**
 * Initializes a comparator using Java serialization.
 * @throws IOException if an I/O error occurs during initialization
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumTimeWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumTimeWithFixedSleep(long,long,java.util.concurrent.TimeUnit)",114,116,"/**
* Creates a retry policy with fixed sleep.
* @param maxTime maximum time for retries
* @param sleepTime time to sleep between retries
* @param timeUnit unit of time for maxTime and sleepTime
* @return RetryPolicy object
*/","* <p>
   * Keep trying for a maximum time, waiting a fixed time between attempts,
   * and then fail by re-throwing the exception.
   * </p>
   *
   * @param timeUnit timeUnit.
   * @param sleepTime sleepTime.
   * @param maxTime maxTime.
   * @return RetryPolicy.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,run,org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:run(),977,1053,"/**
 * Continuously renews Kerberos ticket until interrupted.
 * Logs current time and refresh status, handles exceptions gracefully.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int)",205,208,"/**
* Creates a retry policy with specified fallbacks and retries.
* @param fallbackPolicy default policy for handling failures
* @param maxFailovers maximum number of failover attempts
* @return RetryPolicy configured with given parameters
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,newCall,"org.apache.hadoop.io.retry.RetryInvocationHandler:newCall(java.lang.reflect.Method,java.lang.Object[],boolean,int)",349,356,"/**
* Creates a Call object or handles it asynchronously.
* @param method the Method to be called
* @param args arguments for the method
* @param isRpc flag indicating if it's an RPC call
* @param callId unique identifier for the call
* @return Call object representing the method invocation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)",50,57,"/**
* Constructs an InstrumentedWriteLock with specified parameters.
* @param name unique identifier for the lock
* @param logger for logging lock operations
* @param readWriteLock ReentrantReadWriteLock instance to wrap
* @param minLoggingGapMs minimum time gap between log entries
* @param lockWarningThresholdMs threshold for warning on long locks
* @param clock Timer for measuring lock durations
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long)",81,85,"/**
* Initializes an instrumented lock with a timer.
* @param name lock identifier
* @param logger for logging events
* @param lock underlying lock mechanism
* @param minLoggingGapMs minimum time between log entries
* @param lockWarningThresholdMs threshold for lock warning
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)",56,63,"/**
 * Constructs an InstrumentedReadLock.
 * @param name lock identifier
 * @param logger for logging lock operations
 * @param readWriteLock underlying read-write lock
 * @param minLoggingGapMs minimum gap between log entries
 * @param lockWarningThresholdMs threshold for lock warning
 * @param clock timer for tracking time intervals
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,tryLock,org.apache.hadoop.util.InstrumentedLock:tryLock(),117,124,"/**
* Executes method m2 if lock is acquired.
* @return true if execution successful, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",58,65,"/**
* Creates a proxied object with failover and retry capabilities.
* @param iface interface class to be proxied
* @param proxyProvider provider for the proxy instances
* @param retryPolicy policy for handling retries
* @return proxied object implementing the given interface
*/","* Create a proxy for an interface of implementations of that interface using
   * the given {@link FailoverProxyProvider} and the same retry policy for each
   * method in the interface.
   * 
   * @param iface the interface that the retry will implement
   * @param proxyProvider provides implementation instances whose methods should be retried
   * @param retryPolicy the policy for retrying or failing over method call failures
   * @param <T> T.
   * @return the retry proxy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:<init>(int,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",35,39,"/**
* Constructs a LossyRetryInvocationHandler.
* @param numToDrop number of invocations to drop before retrying
* @param proxyProvider provider for failover proxies
* @param retryPolicy policy defining retry behavior
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,java.util.Map)",79,85,"/**
* Creates a proxy with retry policies.
* @param iface interface class type
* @param implementation concrete implementation of the interface
* @param methodNameToPolicyMap mapping of method names to retry policies
* @return proxy object with specified retry behavior
*/","* Create a proxy for an interface of an implementation class
   * using the a set of retry policies specified by method name.
   * If no retry policy is defined for a method then a default of
   * {@link RetryPolicies#TRY_ONCE_THEN_FAIL} is used.
   * 
   * @param iface the interface that the retry will implement
   * @param <T> T.
   * @param implementation the instance whose methods should be retried
   * @param methodNameToPolicyMap a map of method names to retry policies
   * @return the retry proxy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,processRetryInfo,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processRetryInfo(),151,159,"/**
* Increments retries and handles failover if necessary.
* @param none
* @return void
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,handleException,"org.apache.hadoop.io.retry.RetryInvocationHandler:handleException(java.lang.reflect.Method,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,long,java.lang.Exception)",376,396,"/**
* Determines retry info for a method call.
* @param method the method being invoked
* @param callId unique identifier for the call
* @param policy retry policy to apply
* @param counters tracking counters for retries and failovers
* @param expectFailoverCount expected number of failovers
* @param e exception that occurred during the call
* @return RetryInfo object with retry details
* @throws Exception if not retryable or on failure
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,write,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(int),327,340,"/**
* Writes data to multiple output streams.
* @param d data to write
* @throws IOException if an I/O error occurs during writing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,write,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(byte[],int,int)",348,361,"/**
* Writes bytes to multiple output streams.
* @param bytes array of bytes to write
* @param offset starting index in the byte array
* @param len number of bytes to write
* @throws IOException if any I/O error occurs during writing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,flush,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:flush(),363,376,"/**
* Flushes all output streams and handles exceptions.
* @throws IOException if an I/O error occurs during flushing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProxyCombiner.java,close,org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:close(),133,149,"/**
* Executes m2 on each Closeable proxy, aggregating exceptions.
* @throws IOException if any operation fails and no proxies succeed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,java.lang.String,org.apache.hadoop.conf.Configuration)",285,297,"/**
* Initializes a Writer for compressed data output.
* @param fout file output stream
* @param compressionName name of the compression algorithm
* @param conf configuration settings
* @throws IOException if file offset is not zero or other I/O errors occur
*/","* Constructor
     * 
     * @param fout
     *          FS output stream.
     * @param compressionName
     *          Name of the compression algorithm, which will be used for all
     *          data blocks.
     * @throws IOException
     * @see Compression#getSupportedAlgorithms",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:<init>(java.io.DataInput),809,822,"/**
* Constructs a MetaIndexEntry from DataInput.
* @param in input stream containing meta data
* @throws IOException if data is corrupted or read fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.io.DataInput),2060,2068,"/**
* Constructs a TFileMeta object from a DataInput stream.
* @param in input stream containing metadata
* @throws IOException if I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.io.DataInput),864,875,"/**
* Constructs a DataIndex from an input stream.
* @param in DataInput stream containing index data
* @throws IOException if reading fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$TFileIndex:<init>(int,java.io.DataInput,org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator)",2145,2179,"/**
* Constructs a TFileIndex from input data.
* @param entryCount number of entries to index
* @param in DataInput stream containing the index data
* @param comparator BytesComparator for comparing byte arrays
* @throws IOException if an I/O error occurs
*/","* For reading from file.
     * 
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,checkEOF,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:checkEOF(),119,126,"/**
* Checks and processes data chunks.
* @return true if processing complete, false otherwise
*/","* Check whether we reach the end of the stream.
     * 
     * @return false if the chunk encoded stream has more data to read (in which
     *         case available() will be greater than 0); true otherwise.
     * @throws java.io.IOException
     *           on I/O errors.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,flushBuffer,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flushBuffer(),296,301,"/**
* Masks and resets buffer content.
* @throws IOException on I/O error during masking
*/","* Flush the internal buffer.
     * 
     * Is this the last call to flushBuffer?
     * 
     * @throws java.io.IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,close,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:close(),338,348,"/**
* Masks data in buffer and resets output stream.
* @throws IOException on I/O errors during masking
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[],int,int)",316,330,"/**
* Masks bytes in buffer.
* @param b byte array to mask
* @param off offset in byte array
* @param len length of bytes to mask
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:write(java.io.DataOutput),2272,2290,"/**
 * Writes data to output stream.
 * @param out DataOutput object for writing
 * @throws IOException if I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(org.apache.hadoop.io.file.tfile.RawComparable),1957,1961,"/**
* Applies mask function to key.
* @param key input key object
* @return masked integer value
*/","* Compare an entry with a RawComparable object. This is useful when
         * Entries are stored in a collection, and we want to compare a user
         * supplied key.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister:close(),432,473,"/**
* Processes a key block, updating indexes and checking order.
* @throws IOException if processing fails or keys are not in order
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8:<init>(java.lang.String),70,72,"/**
 * Constructs a new UTF8 object with the given string.
 * @param string the input string to be encoded in UTF-8
 */","* Construct from a given string.
   * @param string input string.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getBytes,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getBytes(),80,83,"/**
 * Generates a masked byte array representation.
 * @return byte array after applying mask function
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(java.lang.String),186,188,"/**
 * Computes MD5 hash of a given string using UTF-8 encoding.
 * @param string input text to hash
 * @return MD5Hash object representing the hash
 */","* Construct a hash value for a String.
   * @param string string.
   * @return MD5Hash.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:sync(),1638,1665,"/**
* Writes buffered records to output.
* @throws IOException if an I/O error occurs
*/",Compress and flush contents to dfs,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/TokenIdentifier.java,getTrackingId,org.apache.hadoop.security.token.TokenIdentifier:getTrackingId(),78,83,"/**
* Generates or returns a masked tracking ID.
* @return Masked tracking ID as a string
*/","* Returns a tracking identifier that can be used to associate usages of a
   * token across multiple client sessions.
   *
   * Currently, this function just returns an MD5 of {{@link #getBytes()}.
   *
   * @return tracking identifier",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,encodeToUrlString,org.apache.hadoop.security.token.Token:encodeToUrlString(),373,375,"/**
 * Masks function execution.
 * @throws IOException if an I/O error occurs during masking
 */","* Encode this token as a url safe string.
   * @return the encoded string
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,cloneWritableInto,"org.apache.hadoop.util.ReflectionUtils:cloneWritableInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",363,371,"/**
 * Masks data from source to destination.
 * @param dst Destination Writable object
 * @param src Source Writable object
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:hashCode(),162,166,"/**
 * Calls the superclass implementation of method m1.
 * @return result from superclass's m1 method
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,add,org.apache.hadoop.net.NetworkTopology:add(org.apache.hadoop.net.Node),133,169,"/**
 * Adds a new node to the network topology.
 * @param node Node to be added
 */","Add a leaf node
   * Update node counter &amp; rack counter if necessary
   * @param node node to be added; can be null
   * @exception IllegalArgumentException if add a node to a leave 
                                         or node to be added is not a leaf",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection)",497,554,"/**
* Selects a random node based on scope and exclusion criteria.
* @param scope the primary scope for node selection
* @param excludedScope optional scope to exclude nodes from
* @param excludedNodes collection of nodes to explicitly exclude
* @return selected Node or null if none available
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,remove,org.apache.hadoop.net.NetworkTopology:remove(org.apache.hadoop.net.Node),221,241,"/**
* Removes a node from the network topology.
* @param node the node to be removed
* Throws IllegalArgumentException if the node is an inner node.
*/","Remove a node
   * Update node counter and rack counter if necessary
   * @param node node to be removed; can be null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,decommissionNode,org.apache.hadoop.net.NetworkTopology:decommissionNode(org.apache.hadoop.net.Node),1061,1076,"/**
 * Masks a node, preventing it from being removed.
 * @param node the node to be masked
 */","* Update empty rack number when remove a node like decommission.
   * @param node node to be added; can be null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)",912,915,"/**
 * Calls overloaded method with additional parameter.
 * @param reader Node to read from
 * @param nodes Array of nodes to process
 * @param activeLen Length of active section in nodes array
 * @param secondarySort Consumer for secondary sorting logic
 */","* Sort nodes array by network distance to <i>reader</i> with secondary sort.
   * <p>
   * In a three-level topology, a node can be either local, on the same rack,
   * or on a different rack from the reader. Sorting the nodes based on network
   * distance from the reader reduces network traffic and improves
   * performance.
   * </p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param secondarySort a secondary sorting strategy which can inject into
   *     that point from outside to help sort the same distance.
   * @param <T> Generics Type T",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistanceUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)",953,956,"/**
* Masks and sorts nodes using a secondary sort consumer.
* @param reader node to read from
* @param nodes array of nodes to process
* @param activeLen length of active nodes
* @param secondarySort consumer for secondary sorting logic
*/","* Sort nodes array by network distance to <i>reader</i>.
   * <p> using network location. This is used when the reader
   * is not a datanode. Sorting the nodes based on network distance
   * from the reader reduces network traffic and improves
   * performance.
   * </p>
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param secondarySort a secondary sorting strategy which can inject into
   *     that point from outside to help sort the same distance.
   * @param <T> Generics Type T.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream:<init>(java.nio.channels.ReadableByteChannel,long)",72,76,"/**
 * Initializes a SocketInputStream with a readable byte channel and timeout.
 * @param channel the readable byte channel to read from
 * @param timeout the timeout value in milliseconds
 * @throws IOException if an I/O error occurs or channel is invalid
 */","* Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @param channel 
   *        Channel for reading, should also be a {@link SelectableChannel}.
   *        The channel will be configured to be non-blocking.
   * @param timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream:<init>(java.nio.channels.WritableByteChannel,long)",77,81,"/**
* Initializes a SocketOutputStream with a channel and timeout.
* @param channel WritableByteChannel to write data to
* @param timeout maximum time in milliseconds for I/O operations
* @throws IOException if an I/O error occurs
*/","* Create a new ouput stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @param channel 
   *        Channel for writing, should also be a {@link SelectableChannel}.  
   *        The channel will be configured to be non-blocking.
   * @param timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,createSocket,org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:createSocket(),184,196,"/**
 * Initializes a UDP socket and prepares a packet for communication.
 * @throws IOException if an I/O error occurs during socket creation or address resolution
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getRpcResponse,"org.apache.hadoop.ipc.Client:getRpcResponse(org.apache.hadoop.ipc.Client$Call,org.apache.hadoop.ipc.Client$Connection,long,java.util.concurrent.TimeUnit)",1566,1598,"/**
 * Executes a call with a timeout and handles exceptions.
 * @param call the call to execute
 * @param connection the connection to use
 * @param timeout the timeout duration
 * @param unit the time unit for the timeout
 * @return Writable result or null if timed out
 * @throws IOException if an I/O error occurs or call is interrupted
 */","@return the rpc response or, in case of timeout, null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(),87,89,"/**
* Constructs a new ScriptBasedMapping using a default RawScriptBasedMapping.
*/","* Create an instance with the default configuration.
   * <p>
   * Calling {@link #setConf(Configuration)} will trigger a
   * re-evaluation of the configuration settings and so be used to
   * set up the mapping script.
   *",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,<init>,org.apache.hadoop.net.ScriptBasedMappingWithDependency:<init>(),59,61,"/**
 * Constructs a new ScriptBasedMappingWithDependency.
 * Initializes with a RawScriptBasedMappingWithDependency instance.
 */","* Create an instance with the default configuration.
   * <p>
   * Calling {@link #setConf(Configuration)} will trigger a
   * re-evaluation of the configuration settings and so be used to
   * set up the mapping script.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,newInnerNode,org.apache.hadoop.net.InnerNodeImpl$Factory:newInnerNode(java.lang.String),32,35,"/**
 * Creates an InnerNodeImpl with the specified path.
 * @param path the node's path
 * @return a new InnerNodeImpl instance
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,<init>,org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:<init>(java.lang.String),306,308,"/**
 * Constructs an InnerNodeWithNodeGroup with a specified path.
 * @param path the path to initialize the node
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:<init>(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode),127,129,"/**
 * Constructs an MRNflyNode by copying properties from another NflyNode.
 * @param n source NflyNode to copy from
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,createParentNode,org.apache.hadoop.net.InnerNodeImpl:createParentNode(java.lang.String),184,187,"/**
* Creates and returns a new InnerNodeImpl.
* @param parentName name of the parent node
* @return new InnerNodeImpl instance configured with specified parameters
*/","* Creates a parent node to be added to the list of children.
   * Creates a node using the InnerNode four argument constructor specifying
   * the name, location, parent, and level of this node.
   *
   * <p>To be overridden in subclasses for specific InnerNode implementations,
   * as alternative to overriding the full {@link #add(Node)} method.
   *
   * @param parentName The name of the parent node
   * @return A new inner node
   * @see InnerNodeImpl(String, String, InnerNode, int)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,add,"org.apache.hadoop.net.unix.DomainSocketWatcher:add(org.apache.hadoop.net.unix.DomainSocket,org.apache.hadoop.net.unix.DomainSocketWatcher$Handler)",304,332,"/**
* Handles a domain socket with a handler, adding it to processing.
* @param sock the DomainSocket to process
* @param handler the Handler for socket operations
*/","* Add a socket.
   *
   * @param sock     The socket to add.  It is an error to re-add a socket that
   *                   we are already watching.
   * @param handler  The handler to associate with this socket.  This may be
   *                   called any time after this function is called.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,remove,org.apache.hadoop.net.unix.DomainSocketWatcher:remove(org.apache.hadoop.net.unix.DomainSocket),339,354,"/**
* Masks a socket by removing it from processing.
* @param sock the DomainSocket to be masked
*/","* Remove a socket.  Its handler will be called.
   *
   * @param sock     The socket to remove.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,<init>,"org.apache.hadoop.net.unix.DomainSocketWatcher:<init>(int,java.lang.String)",241,259,"/**
* Initializes a DomainSocketWatcher with specified parameters.
* @param interruptCheckPeriodMs period for checking interruptions in milliseconds
* @param src source identifier for the watcher
* @throws IOException if an I/O error occurs during socket creation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,select,"org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:select(java.nio.channels.SelectableChannel,int,long)",321,376,"/**
* Waits for I/O operations on a channel with a timeout.
* @param channel the SelectableChannel to wait on
* @param ops interest set of operations (e.g., OP_READ, OP_WRITE)
* @param timeout maximum time to wait in milliseconds
* @return result of the selection operation or 0 if timed out
* @throws IOException if an I/O error occurs
*/","* Waits on the channel with the given timeout using one of the 
     * cached selectors. It also removes any cached selectors that are
     * idle for a few seconds.
     * 
     * @param channel
     * @param ops
     * @param timeout
     * @return
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultIP,org.apache.hadoop.net.DNS:getDefaultIP(java.lang.String),224,228,"/**
* Retrieves the first IP address from network interface.
* @param strInterface name of the network interface
* @return first IP address as a string
* @throws UnknownHostException if interface is not found
*/","* Returns the first available IP address associated with the provided
   * network interface or the local host IP if ""default"" is given.
   *
   * @param strInterface
   *            The name of the network interface or subinterface to query
   *             (e.g. eth0 or eth0:0) or the string ""default""
   * @return The IP address in text form, the local host IP is returned
   *         if the interface name ""default"" is specified
   * @throws UnknownHostException
   *             If the given interface is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getHosts,org.apache.hadoop.net.DNS:getHosts(java.lang.String),340,343,"/**
 * Calls overloaded method with default parameters.
 * @param strInterface network interface name
 * @return array of IP addresses or empty if none found
 * @throws UnknownHostException if network interface is invalid
 */","* Returns all the host names associated by the default nameserver with the
   * address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @return The list of host names associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface
   *",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,"org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String,boolean)",360,374,"/**
 * Retrieves hostname using specified interface and nameserver.
 * @param strInterface network interface name or ""default"" for default
 * @param nameserver DNS server address or ""default"" for system default
 * @param tryfallbackResolution whether to attempt fallback resolution
 * @return resolved hostname or cached one if interface is ""default""
 * @throws UnknownHostException if hostname cannot be determined
 */","* Returns the default (first) host name associated by the provided
   * nameserver with the address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @param nameserver
   *            The DNS host name
   * @param tryfallbackResolution
   *            Input tryfallbackResolution.
   * @return The default host names associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream),134,136,"/**
 * Calls overloaded m1 with PrintStream and default USAGE.
 * @param pStr output stream to print messages
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkParameterValidity,"org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[],java.util.Map)",363,385,"/**
 * Validates command-line arguments for a masked function.
 * @param argv array of command-line arguments
 * @param helpEntries map of available commands and usage info
 * @return true if validation passes, false otherwise
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,help,"org.apache.hadoop.ha.HAAdmin:help(java.lang.String[],java.util.Map)",512,537,"/**
 * Handles command-line help for a given command.
 * @param argv array of command-line arguments
 * @param helpEntries map of command to its UsageInfo
 * @return 0 on success, -1 on error
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addContext,"org.apache.hadoop.http.HttpServer2:addContext(org.eclipse.jetty.servlet.ServletContextHandler,boolean)",997,1001,"/**
* Configures servlet context handler.
* @param ctxt ServletContextHandler to configure
* @param isFiltered indicates if filtering is applied
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getPlugin,org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin(java.lang.String),202,216,"/**
 * Creates and initializes a metrics plugin.
 * @param name plugin configuration name
 * @return initialized MetricsPlugin instance or null if class not found
 * @throws MetricsConfigException on creation error
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,loadFirst,"org.apache.hadoop.metrics2.impl.MetricsConfig:loadFirst(java.lang.String,java.lang.String[])",113,142,"/**
* Loads metrics configuration from files.
* @param prefix configuration key prefix
* @param fileNames list of property file names to load
* @return MetricsConfig object initialized with loaded properties or default if none found
*/","* Load configuration from a list of files until the first successful load
   * @param conf  the configuration object
   * @param files the list of filenames to try
   * @return  the configuration object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,toString,org.apache.hadoop.metrics2.impl.MetricsConfig:toString(),282,285,"/**
 * Calls overloaded method with this instance.
 * @return result of m1(this) call
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,putMetrics,org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),108,193,"/**
 * Processes and forwards metrics records for Ganglia.
 * @param record the MetricsRecord containing metric data
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,putMetrics,org.apache.hadoop.metrics2.sink.GraphiteSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),70,108,"/**
* Sends metrics record to Graphite.
* @param record the MetricsRecord containing metric data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/PrometheusServlet.java,doGet,"org.apache.hadoop.http.PrometheusServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",40,46,"/**
* Handles request and response processing.
* @param req HttpServletRequest object representing the client's request
* @param resp HttpServletResponse object for sending a response to the client
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,publishMetricsFromQueue,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:publishMetricsFromQueue(),128,166,"/**
* Handles retries with exponential backoff for a task.
* Adjusts delay based on exceptions and retry count.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,incrCacheClearedCounter,org.apache.hadoop.ipc.RetryCache:incrCacheClearedCounter(),221,223,"/**
 * Invokes metrics tracking for cache retries.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,requeueCall,org.apache.hadoop.ipc.Server$Handler:requeueCall(org.apache.hadoop.ipc.Server$Call),3232,3240,"/**
* Handles RPC call masking and metrics.
* @param call the RPC call object
* @throws IOException if an I/O error occurs
* @throws InterruptedException if interrupted while processing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,getRecords,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:getRecords(),57,66,"/**
* Aggregates metrics records from builders.
* @return List of aggregated MetricsRecordImpl objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,int)",62,65,"/**
* Masks metrics information with an integer value.
* @param info MetricsInfo object containing metric details
* @param value Integer value to mask the metrics with
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,long)",67,70,"/**
* Updates metrics with provided information and value.
* @param info MetricsInfo object containing metadata
* @param value Long value to update metrics with
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,float)",72,75,"/**
* Updates metrics with given information and value.
* @param info MetricsInfo object containing metadata
* @param value Float value to be recorded
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,double)",77,80,"/**
* Updates metrics with provided info and value.
* @param info MetricsInfo object containing metadata
* @param value Double value to be recorded
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,counter,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,int)",82,85,"/**
 * Applies mask to metrics information.
 * @param info MetricsInfo object containing data
 * @param value Integer value used for masking
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,counter,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,long)",87,90,"/**
 * Updates metric with given value.
 * @param info Metrics information object
 * @param value Long value to update metric with
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateInfoCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateInfoCache(java.lang.Iterable),243,248,"/**
* Updates the info cache with metrics records.
* @param lastRecs iterable of MetricsRecordImpl objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newMBeanName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newMBeanName(java.lang.String),108,111,"/**
 * Masks a given string name.
 * @param name input string to be masked
 * @return ObjectName instance after masking
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,sourceName,"org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:sourceName(java.lang.String,boolean)",123,126,"/**
* Masks function name.
* @param name original function name
* @param dupOK flag to allow duplicates
* @return masked function name
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,load,org.apache.hadoop.security.Groups$GroupCacheLoader:load(java.lang.String),342,370,"/**
* Fetches group list for a user.
* @param user the username
* @return set of group names or throws exception if none found
*/","* This method will block if a cache entry doesn't exist, and
     * any subsequent requests for the same user will wait on this
     * request to return. If a user already exists in the cache,
     * and when the key expires, the first call to reload the key
     * will block, but subsequent requests will return the old
     * value until the blocking thread returns.
     * If reloadGroupsInBackground is true, then the thread that
     * needs to refresh an expired key will not block either. Instead
     * it will return the old cache value and schedule a background
     * refresh
     * @param user key of cache
     * @return List of groups belonging to user
     * @throws IOException to prevent caching negative entries",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,shutdownSingleton,org.apache.hadoop.metrics2.source.JvmMetrics:shutdownSingleton(),139,141,"/**
 * Calls method m1 on Singleton instance.
 */","* Shutdown the JvmMetrics singleton. This is not necessary if the JVM itself
   * is shutdown, but may be necessary for scenarios where JvmMetrics instance
   * needs to be re-created while the JVM is still around. One such scenario
   * is unit-testing.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reattach,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:reattach(),151,153,"/**
* Updates metrics using UgiMetrics.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,stop,org.apache.hadoop.ipc.Server:stop(),3696,3719,"/**
* Stops the server and performs cleanup tasks.
*/",Stops the service.  No new calls will be handled after this is called.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,stopMBeans,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stopMBeans(),226,231,"/**
* Masks MBean by name and resets it.
* @param mbeanName the name of the MBean to mask
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,unregisterSource,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:unregisterSource(java.lang.String),896,902,"/**
* Registers metrics for a given namespace.
* @param namespace unique identifier for metric registration
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2Metrics.java,create,"org.apache.hadoop.http.HttpServer2Metrics:create(org.eclipse.jetty.server.handler.StatisticsHandler,int)",151,159,"/**
* Creates and registers HTTP server metrics.
* @param handler statistics handler for the server
* @param port server port number
* @return registered HttpServer2Metrics instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,stop,org.apache.hadoop.http.HttpServer2:stop(),1559,1609,"/**
* Stops monitoring and resources associated with a web app.
* @throws Exception if any step fails
*/","* stop the server.
   *
   * @throws Exception exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,getMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",569,581,"/**
* Collects and records metrics data.
* @param builder MetricsCollector instance to build metrics
* @param all flag indicating whether to include all details
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,<init>,"org.apache.hadoop.metrics2.lib.MutableQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",87,106,"/**
* Initializes mutable quantiles with specified parameters.
* @param name metric name
* @param description metric description
* @param sampleName sample name
* @param valueName value name
* @param interval time interval in seconds
*/","* Instantiates a new {@link MutableQuantiles} for a metric that rolls itself
   * over on the specified time interval.
   * 
   * @param name
   *          of the metric
   * @param description
   *          long-form textual description of the metric
   * @param sampleName
   *          type of items in the stream (e.g., ""Ops"")
   * @param valueName
   *          type of the values
   * @param interval
   *          rollover interval (in seconds) of the estimator",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:<init>(),993,1000,"/**
* Initializes metrics for delegation token secret manager.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,<init>,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:<init>(org.apache.hadoop.ipc.RetryCache),42,48,"/**
 * Initializes metrics for a given retry cache.
 * @param retryCache the cache to monitor
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Field)",129,131,"/**
 * Generates MetricsInfo using annotation and field.
 * @param annotation Metric annotation
 * @param field Field to process
 * @return MetricsInfo object
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Method)",137,139,"/**
 * Retrieves MetricsInfo using Metric annotation and method.
 * @param annotation Metric configuration annotation
 * @param method target method to analyze
 * @return MetricsInfo object containing metrics information
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newStat,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)",262,269,"/**
* Creates and registers a mutable statistic.
* @param name statistic identifier
* @param desc description of the statistic
* @param sampleName name of the sample
* @param valueName name of the value
* @param extended flag for extended statistics
* @return newly created MutableStat object
*/","* Create a mutable metric with stats
   * @param name  of the metric
   * @param desc  metric description
   * @param sampleName  of the metric (e.g., ""Ops"")
   * @param valueName   of the metric (e.g., ""Time"" or ""Latency"")
   * @param extended    produce extended stat (stdev, min/max etc.) if true.
   * @return a new mutable stat metric object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",94,97,"/**
* Constructs a new MutableStat with specified names and default visibility.
* @param name unique identifier for the stat
* @param description brief description of the stat
* @param sampleName name of the sample data
* @param valueName name of the value being tracked
*/","* Construct a snapshot stat metric with extended stat off by default
   * @param name        of the metric
   * @param description of the metric
   * @param sampleName  of the metric (e.g. ""Ops"")
   * @param valueName   of the metric (e.g. ""Time"", ""Latency"")",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRate.java,<init>,"org.apache.hadoop.metrics2.lib.MutableRate:<init>(java.lang.String,java.lang.String,boolean)",31,33,"/**
* Initializes a MutableRate with specified parameters.
* @param name unique identifier for the rate
* @param description detailed description of the rate
* @param extended flag indicating if extended features are enabled
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getGcUsage,org.apache.hadoop.metrics2.source.JvmMetrics:getGcUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder),180,207,"/**
* Records garbage collection metrics.
* @param rb MetricsRecordBuilder to store GC data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,setContext,org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:setContext(java.lang.String),147,150,"/**
 * Masks a value in metrics record.
 * @param value the value to mask
 * @return updated MetricsRecordBuilderImpl instance
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,setContext,org.apache.hadoop.metrics2.lib.MetricsRegistry:setContext(java.lang.String),380,382,"/**
 * Registers a metric with the given name.
 * @param name unique metric identifier
 * @return MetricsRegistry object
 */","* Set the metrics context tag
   * @param name of the context
   * @return the registry itself as a convenience",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String,boolean)",403,406,"/**
* Registers metrics with specified parameters.
* @param name metric name
* @param description metric description
* @param value metric value
* @param override flag to allow overriding existing metrics
* @return MetricsRegistry object
*/","* Add a tag to the metrics
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @param override  existing tag if true
   * @return the registry (for keep adding tags)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",422,424,"/**
 * Registers metrics with optional persistence.
 * @param info metrics configuration details
 * @param value metric value to register
 * @return MetricsRegistry instance
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,add,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:add(java.lang.String,long)",100,114,"/**
 * Updates metrics for a named sample with elapsed time.
 * @param name identifier for the sample
 * @param elapsed time taken for the sample in milliseconds
 */","* Add a rate sample for a rate metric.
   * @param name of the rate metric
   * @param elapsed time",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,publishMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,boolean)",435,449,"/**
 * Masks metrics buffer and publishes statistics.
 * @param buffer MetricsBuffer to be processed
 * @param immediate Flag indicating immediate processing
 */","* Publish a metrics snapshot to all the sinks
   * @param buffer  the metrics snapshot to publish
   * @param immediate  indicates that we should publish metrics immediately
   *                   instead of using a separate thread.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,copyTo,org.apache.hadoop.metrics2.util.SampleStat:copyTo(org.apache.hadoop.metrics2.util.SampleStat),59,61,"/**
 * Masks statistics from another SampleStat instance.
 * @param other SampleStat object to mask data into
 */","* Copy the values to other (saves object creation and gc.)
   * @param other the destination to hold our values",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,<init>,"org.apache.hadoop.metrics2.lib.MethodMetric:<init>(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.annotation.Metric$Type)",46,53,"/**
* Initializes a MethodMetric with specified object, method, info, and type.
* @param obj the target object for metric collection
* @param method the method to collect metrics from (must have no arguments)
* @param info additional information about the metric
* @param type the type of metric to collect
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,toString,org.apache.hadoop.metrics2.lib.MutableStat:toString(),187,190,"/**
 * Calls m2 on the result of m1().
 * @return String result from nested m2 call
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,logSlowRpcCalls,"org.apache.hadoop.ipc.Server:logSlowRpcCalls(java.lang.String,org.apache.hadoop.ipc.Server$Call,org.apache.hadoop.ipc.ProcessingDetails)",593,615,"/**
 * Logs slow RPC calls.
 * @param methodName name of the method being called
 * @param call Call object representing the RPC call
 * @param details ProcessingDetails for the call
 */","* Logs a Slow RPC Request.
   *
   * @param methodName - RPC Request method name
   * @param details - Processing Detail.
   *
   * If a request took significant more time than other requests,
   * and its processing time is at least `logSlowRPCThresholdMs` we consider that as a slow RPC.
   *
   * The definition rules for calculating whether the current request took too much time
   * compared to other requests are as follows:
   * 3 is a magic number that comes from 3 sigma deviation.
   * A very simple explanation can be found by searching for 68-95-99.7 rule.
   * We flag an RPC as slow RPC if and only if it falls above 99.7% of requests.
   * We start this logic only once we have enough sample size.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,toString,org.apache.hadoop.metrics2.util.SampleQuantiles:toString(),280,288,"/**
 * Generates a string representation of quantile data.
 * @return formatted string with quantiles or ""no samples"" if data is null
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addTopNCallerSummary,org.apache.hadoop.ipc.DecayRpcScheduler:addTopNCallerSummary(org.apache.hadoop.metrics2.MetricsRecordBuilder),1079,1096,"/**
* Masks metrics data for top users.
* @param rb MetricsRecordBuilder to append data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,cacheGroupsRefresh,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh(),78,83,"/**
* Fetches and processes netgroups.
* @throws IOException if an I/O error occurs
*/",* Refresh the netgroup cache,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupsSet,org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroupsSet(java.lang.String),1585,1592,"/**
 * Retrieves group mappings for a given user.
 * @param user the username to look up
 * @return set of group names or null if not found
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,endln,org.apache.hadoop.security.KDiag:endln(),875,878,"/**
 * Calls method m1 twice, once with default parameters and once with a string argument.
 */",* Print something at the end of a section,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,title,"org.apache.hadoop.security.KDiag:title(java.lang.String,java.lang.Object[])",886,891,"/**
 * Logs formatted message with mask.
 * @param format message format string
 * @param args arguments for the format string
 */","* Print a title entry.
   *
   * @param format format string
   * @param args any arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,fail,"org.apache.hadoop.security.KDiag:fail(java.lang.String,java.lang.String,java.lang.Object[])",942,946,"/**
 * Logs an error and throws a KerberosDiagsFailure exception.
 * @param category the error category
 * @param message the error message format
 * @param args arguments for the message format
 */","* Format and raise a failure.
   *
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @throws KerberosDiagsFailure containing the formatted text",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullMaps,org.apache.hadoop.security.ShellBasedIdMapping:loadFullMaps(),386,389,"/**
 * Executes two methods synchronously.
 * @throws IOException if an I/O error occurs during execution
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createRemoteUser,"org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",1451,1462,"/**
* Creates UserGroupInformation for a given user.
* @param user the username
* @param authMethod authentication method to use
* @return UserGroupInformation object
*/","* Create a user from a login name. It is intended to be used for remote
   * users in RPC, since it won't have any credentials.
   * @param user the full user principal name, must not be empty or null
   * @param authMethod authMethod.
   * @return the UserGroupInformation for the remote user.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getDefault,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getDefault(),1052,1067,"/**
* Returns a singleton SSL socket factory.
* @return SocketFactory instance for secure connections
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,retrievePassword,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),548,552,"/**
 * Applies mask to token.
 * @param identifier unique token identifier
 * @return masked token data as byte array
 * @throws InvalidToken if token is invalid
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,startThreads,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:startThreads(),169,177,"/**
* Starts the expired token removal process.
* Throws IOException if an I/O error occurs.
*/","* should be called before this object is used.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,rollMasterKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:rollMasterKey(),463,476,"/**
* Masks function with synchronization and key management.
* @throws IOException if an I/O error occurs
*/","* Update the current master key for generating delegation tokens 
   * It should be called only by tokenRemoverThread.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(),150,152,"/**
 * Constructs a new DelegationTokenAuthenticatedURL with default values.
 */","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   * <p>
   * An instance of the default {@link DelegationTokenAuthenticator} will be
   * used.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator),160,163,"/**
 * Constructs a new DelegationTokenAuthenticatedURL with a given authenticator.
 * @param authenticator the authenticator to use for delegation token management
 */","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   *
   * @param authenticator the {@link DelegationTokenAuthenticator} instance to
   * use, if <code>null</code> the default one will be used.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.authentication.client.ConnectionConfigurator),171,174,"/**
 * Constructs a new instance with default authentication.
 * @param connConfigurator configuration for connection settings
 */","* Creates an <code>DelegationTokenAuthenticatedURL</code> using the default
   * {@link DelegationTokenAuthenticator} class.
   *
   * @param connConfigurator a connection configurator.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(byte[]),225,228,"/**
 * Calls overloaded method with full byte array.
 * @param b byte array to process
 * @return result of processing
 * @throws IOException if an I/O error occurs
 */","* Reads up to <code>b.length</code> bytes of data from this input stream into
   * an array of bytes.
   * <p>
   * The <code>read</code> method of <code>InputStream</code> calls the
   * <code>read</code> method of three arguments with the arguments
   * <code>b</code>, <code>0</code>, and <code>b.length</code>.
   * 
   * @param b
   *          the buffer into which the data is read.
   * @return the total number of bytes read into the buffer, or <code>-1</code>
   *         is there is no more data because the end of the stream has been
   *         reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isFromKeytab,org.apache.hadoop.security.UserGroupInformation:isFromKeytab(),834,838,"/**
 * Checks conditions using three methods.
 * @return true if all conditions are met, false otherwise
 */","* Is this user logged in from a keytab file managed by the UGI?
   * @return true if the credentials are from a keytab file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isFromTicket,org.apache.hadoop.security.UserGroupInformation:isFromTicket(),844,846,"/**
 * Checks conditions using three methods.
 * @return true if all conditions are met, false otherwise
 */","*  Is this user logged in from a ticket (but no keytab) managed by the UGI?
   * @return true if the credentials are from a ticket cache.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,shouldRelogin,org.apache.hadoop.security.UserGroupInformation:shouldRelogin(),869,873,"/**
 * Checks conditions using two methods.
 * @return true if both m1() and m2() return true, otherwise false
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,toString,org.apache.hadoop.fs.FileSystem$Cache$Key:toString(),3915,3918,"/**
* Constructs a formatted string using user group info and URL components.
* @return formatted string combining user group info, scheme, and authority
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,toString,org.apache.hadoop.security.UserGroupInformation$RealUser:toString(),497,500,"/**
 * Delegates method call to realUser.
 * @return Result of calling m1 on realUser
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeIdle,org.apache.hadoop.ipc.Server$ConnectionManager:closeIdle(boolean),4130,4149,"/**
* Closes idle connections.
* @param scanAll whether to scan all connections
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeAll,org.apache.hadoop.ipc.Server$ConnectionManager:closeAll(),4151,4157,"/**
 * Iterates over connections and applies mask.
 * @param none
 * @return void
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeConnection,org.apache.hadoop.ipc.Server:closeConnection(org.apache.hadoop.ipc.Server$Connection),3493,3495,"/**
 * Masks a database connection.
 * @param connection the database connection to mask
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,javax.net.ssl.SSLSocket)",275,278,"/**
 * Calls overloaded method with single host.
 * @param host target server address
 * @param ssl SSL socket for secure connection
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,initializeDefaultFactory,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeDefaultFactory(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),103,108,"/**
* Initializes SSL socket factory with preferred mode.
* @param preferredMode SSL channel mode to use
*/","* Initialize a singleton SSL socket factory.
   *
   * @param preferredMode applicable only if the instance is not initialized.
   * @throws IOException if an error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,<init>,org.apache.hadoop.fs.shell.FsCommand:<init>(org.apache.hadoop.conf.Configuration),78,80,"/**
 * Constructs an FsCommand with the given configuration.
 * @param conf Configuration object containing settings
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,<init>,org.apache.hadoop.fs.shell.CommandFactory:<init>(),45,47,"/**
 * Constructs a new CommandFactory with no command.
 */",Factory constructor for commands,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,org.apache.hadoop.fs.HarFileSystem:<init>(),83,85,"/**
 * Constructs a new HarFileSystem instance.
 * Requires calling initialize() to set up the underlying file system.
 */",* public construction of harfilesystem,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,org.apache.hadoop.fs.HarFileSystem:<init>(org.apache.hadoop.fs.FileSystem),103,106,"/**
 * Constructs a new HarFileSystem.
 * @param fs underlying FileSystem object
 */","* Constructor to create a HarFileSystem with an
   * underlying filesystem.
   * @param fs underlying file system",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,<init>,org.apache.hadoop.fs.FilterFileSystem:<init>(),71,72,"/**
 * Constructs a new instance of FilterFileSystem.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,<init>,org.apache.hadoop.fs.FilterFileSystem:<init>(org.apache.hadoop.fs.FileSystem),74,77,"/**
 * Initializes a new FilterFileSystem with the given FileSystem.
 * @param fs underlying FileSystem to filter
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell:<init>(),66,68,"/**
* Constructs an FsShell instance with default settings.
* Initializes with null configuration by default.
*/","* Default ctor with no configuration.  Be sure to invoke
   * {@link #setConf(Configuration)} with a valid configuration prior
   * to running commands.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,<init>,org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration),43,45,"/**
 * Constructor initializes with configuration and default output stream.
 * @param conf Configuration object containing settings
 */","* Create an instance of this tool using the given configuration.
   * @param conf configuration.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,<init>,org.apache.hadoop.fs.shell.Command:<init>(),76,79,"/**
 * Constructs a Command instance.
 * Initializes output and error streams to System.out and System.err respectively.
 */",Constructor,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),39,43,"/**
* Initializes an ErasureEncoder with specified options.
* @param options configuration settings for erasure coding
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,42,"/**
* Initializes an ErasureDecoder with specified options.
* @param options configuration settings for erasure coding
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,<init>,"org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)",48,51,"/**
* Initializes deserializer with configuration and class.
* @param conf Configuration object
* @param c Writable class type
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,<init>,org.apache.hadoop.log.LogLevel$CLI:<init>(org.apache.hadoop.conf.Configuration),105,107,"/**
 * Initializes CLI with configuration settings.
 * @param conf Configuration object containing settings
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,org.apache.hadoop.security.KDiag:<init>(),186,187,"/**
 * Constructs a new instance of KDiag.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,<init>,org.apache.hadoop.ha.HAAdmin:<init>(),99,101,"/**
 * Default constructor for HAAdmin class.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByName,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByName(java.lang.String),647,685,"/**
* Resolves host to InetAddress.
* @param host hostname or IP address string
* @return resolved InetAddress object
* @throws UnknownHostException if resolution fails
*/","* Create an InetAddress with a fully qualified hostname of the given
     * hostname.  InetAddress does not qualify an incomplete hostname that
     * is resolved via the domain search list.
     * {@link InetAddress#getCanonicalHostName()} will fully qualify the
     * hostname, but it always return the A record whereas the given hostname
     * may be a CNAME.
     * 
     * @param host a hostname or ip address
     * @return InetAddress with the fully qualified hostname or ip
     * @throws UnknownHostException if host does not exist",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateStaticMapping,org.apache.hadoop.security.ShellBasedIdMapping:updateStaticMapping(),304,336,"/**
* Updates or initializes static UID/GID mapping from a file.
* @throws IOException if an I/O error occurs during file processing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,write,org.apache.hadoop.security.authorize.AccessControlList:write(java.io.DataOutput),317,321,"/**
* Writes ACL string to output.
* @param out DataOutput stream to write to
*/",* Serializes the AccessControlList object,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createZooKeeper,org.apache.hadoop.ha.ActiveStandbyElector:createZooKeeper(),752,762,"/**
* Initializes and returns a ZooKeeper instance.
* Configures SSL if truststore is provided.
* @return ZooKeeper instance
* @throws IOException on configuration or connection failure
*/","* Get a new zookeeper client instance. protected so that test class can
   * inherit and pass in a mock object for zookeeper
   *
   * @return new zookeeper client instance
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,"org.apache.hadoop.security.UserGroupInformation:relogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)",1334,1345,"/**
* Masks Hadoop login context.
* @param login HadoopLoginContext object
* @param ignoreLastLoginTime flag to ignore last login time
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],boolean)",34,40,"/**
 * Initializes a CryptoFSDataOutputStream for encryption.
 * @param out underlying FSDataOutputStream
 * @param codec encryption codec
 * @param bufferSize buffer size in bytes
 * @param key encryption key
 * @param iv initialization vector
 * @param closeOutputStream flag to close the underlying stream
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)",91,95,"/**
 * Constructs a CryptoOutputStream with specified parameters.
 * @param out underlying output stream
 * @param codec encryption codec to use
 * @param bufferSize size of the buffer
 * @param key encryption key
 * @param iv initialization vector
 * @param streamOffset initial offset in the stream
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite,java.lang.String)",124,128,"/**
* Initializes an OpenSSL CTR cipher.
* @param mode encryption/decryption mode
* @param suite cryptographic cipher suite
* @param engineId OpenSSL engine identifier
* @throws GeneralSecurityException if initialization fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,getInstance,org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String),111,114,"/**
 * Creates an OpenSSL cipher instance with specified transformation.
 * @param transformation encryption algorithm and mode/padding
 * @return OpensslCipher object
 * @throws NoSuchAlgorithmException if the algorithm is not available
 * @throws NoSuchPaddingException if the padding mechanism is not available
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,<init>,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:<init>(),39,48,"/**
* Initializes OpensslSm4CtrCryptoCodec, checks for loading failures and cipher support.
* Throws RuntimeException if initialization fails.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONEncKeyVersion,"org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersion(java.lang.String,java.util.Map)",157,183,"/**
* Creates an EncryptedKeyVersion from a key name and value map.
* @param keyName the name of the encryption key
* @param valueMap containing encryption details
* @return EncryptedKeyVersion object initialized with provided data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$CreateCommand:execute(),456,474,"/**
* Masks a key with specified options.
* @throws IOException if I/O error occurs
* @throws NoSuchAlgorithmException if algorithm is unavailable
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,createKey,"org.apache.hadoop.crypto.key.KeyProviderExtension:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",71,75,"/**
* Retrieves a key version by name with given options.
* @param name key identifier
* @param options configuration for key retrieval
* @return KeyVersion object
* @throws NoSuchAlgorithmException if algorithm is not found
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String),148,154,"/**
* Retrieves and processes a key version by name.
* @param name key identifier
* @return KeyVersion object
* @throws NoSuchAlgorithmException if algorithm is not found
* @throws IOException if I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,rollNewVersion,org.apache.hadoop.crypto.key.KeyProviderExtension:rollNewVersion(java.lang.String),77,81,"/**
 * Retrieves a key version by name.
 * @param name the name of the key
 * @return KeyVersion object
 * @throws NoSuchAlgorithmException if algorithm is not found
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$RollCommand:execute(),308,328,"/**
* Rolls a key version using a KeyProvider.
* @throws NoSuchAlgorithmException if algorithm is not found
* @throws IOException if I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getSize,org.apache.hadoop.crypto.key.kms.ValueQueue:getSize(java.lang.String),323,340,"/**
* Masks a function using the provided key name.
* @param keyName unique identifier for the function
* @return integer result of the masking operation or 0 if queue is null
*/","* Get size of the Queue for keyName. This is only used in unit tests.
   * @param keyName the key name
   * @return int queue size. Zero means the queue is empty or the key does not exist.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getAtMost,"org.apache.hadoop.crypto.key.kms.ValueQueue:getAtMost(java.lang.String,int)",354,399,"/**
* Retrieves a list of items from the queue.
* @param keyName identifier for the queue
* @param num number of items to retrieve
* @return list of retrieved items
*/","* This removes the ""num"" values currently at the head of the Queue for the
   * provided key. Will immediately fire the Queue filler function if key
   * does not exist
   * How many values are actually returned is governed by the
   * <code>SyncGenerationPolicy</code> specified by the user.
   * @param keyName String key name
   * @param num Minimum number of values to return.
   * @return {@literal List<E>} values returned
   * @throws IOException raised on errors performing I/O.
   * @throws ExecutionException execution exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,drain,org.apache.hadoop.crypto.key.kms.ValueQueue:drain(java.lang.String),302,316,"/**
* Processes tasks associated with a key.
* @param keyName unique identifier for the task queue
*/","* Drains the Queue for the provided key.
   *
   * @param keyName the key to drain the Queue for",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,refresh,"org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:refresh(java.lang.String,java.lang.String[])",57,68,"/**
* Refreshes resources using identifier and args.
* @param identifier unique resource identifier
* @param args additional arguments for refresh
* @return collection of RefreshResponse objects
* @throws IOException if network error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,newEntry,"org.apache.hadoop.ipc.RetryCache:newEntry(java.lang.Object,long,byte[],int)",341,345,"/**
* Creates a cache entry with an expiration time.
* @param payload data to be cached
* @param expirationTime duration until the cache entry expires
* @param clientId identifier of the client
* @param callId unique call identifier
* @return CacheEntryWithPayload object initialized with provided parameters
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long,boolean)",161,165,"/**
* Constructs a cache entry with a payload.
* @param clientId unique client identifier
* @param callId unique call identifier
* @param payload data associated with the call
* @param expirationTime timestamp when the entry expires
* @param success indicates if the operation was successful
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,put,org.apache.hadoop.ipc.CallQueueManager:put(java.lang.Object),289,299,"/**
* Handles event processing based on conditions.
* @param e the event to process
*/","* Insert e into the backing queue or block until we can.  If client
   * backoff is enabled this method behaves like add which throws if
   * the queue overflows.
   * If we block and the queue changes on us, we will insert while the
   * queue is drained.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,add,org.apache.hadoop.ipc.CallQueueManager:add(java.lang.Object),301,304,"/**
 * Masks an element by invoking a helper method.
 * @param e element to mask
 * @return result of masking operation
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,internalQueueCall,"org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call,boolean)",3111,3141,"/**
* Enqueues a call to the appropriate queue based on blocking flag.
* @param call the RPC call to enqueue
* @param blocking true if the call should block, false otherwise
* @throws IOException if an I/O error occurs
* @throws InterruptedException if the thread is interrupted
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,ensureInitialized,org.apache.hadoop.ipc.WritableRpcEngine:ensureInitialized(),71,75,"/**
 * Ensures initialization by calling m1 if not already initialized.
 * Synchronized to prevent concurrent access issues.
 */",* Initialize this class if it isn't already.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,get,org.apache.hadoop.util.LightWeightCache:get(java.lang.Object),186,199,"/**
* Retrieves and processes an entry by key.
* @param key unique identifier for the entry
* @return Entry object or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,close,org.apache.hadoop.util.StopWatch:close(),116,121,"/**
* Executes function M1 if started.
* @param isStarted flag indicating if process has begun
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,now,org.apache.hadoop.util.StopWatch:now(java.util.concurrent.TimeUnit),97,100,"/**
 * Converts a duration to nanoseconds.
 * @param timeUnit target time unit
 * @return duration in nanoseconds
 */","* now.
   *
   * @param timeUnit timeUnit.
   * @return current elapsed time in specified timeunit.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,toString,org.apache.hadoop.util.StopWatch:toString(),111,114,"/**
 * Masks a string using transformation methods.
 * @return masked string result
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Invocation:<init>(java.lang.reflect.Method,java.lang.Object[])",104,120,"/**
* Constructs an Invocation object for a given method and parameters.
* @param method the Method to be invoked
* @param parameters array of parameters for the method
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(int,long,java.lang.Class)",210,223,"/**
 * Generates a protocol signature based on client methods hash code and server version.
 * @param clientMethodsHashCode hash code of client methods
 * @param serverVersion version of the server
 * @param protocol class representing the protocol
 * @return ProtocolSignature object
 */","* Get a server protocol's signature
   * 
   * @param clientMethodsHashCode client protocol methods hashcode
   * @param serverVersion server protocol version
   * @param protocol protocol
   * @return the server's protocol signature",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(java.lang.String,long)",225,229,"/**
* Generates signature for a given protocol and version.
* @param protocolName name of the protocol class
* @param version version number of the protocol
* @return ProtocolSignature object
* @throws ClassNotFoundException if protocol class is not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:shouldRetry(java.lang.Exception,int,int,boolean)",697,750,"/**
 * Determines retry or failover action based on exception type and counts.
 * @param e the exception encountered
 * @param retries number of retry attempts
 * @param failovers number of failover attempts
 * @param isIdempotentOrAtMostOnce flag indicating if operation is idempotent
 * @return RetryAction with decision and delay
 * @throws Exception if unable to determine action
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,shouldRetry,"org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:shouldRetry(java.lang.Exception,int,int,boolean)",98,129,"/**
* Determines retry action based on exception type and policy.
* @param e the exception encountered
* @param retries number of retry attempts
* @param failovers number of failover attempts
* @param isMethodIdempotent flag indicating if method is idempotent
* @return RetryAction to be taken
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,doHealthChecks,org.apache.hadoop.ha.HealthMonitor:doHealthChecks(),195,227,"/**
 * Monitors service health and updates status.
 * @throws InterruptedException if thread is interrupted during sleep
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,org.apache.hadoop.ipc.Server$Call:<init>(),980,983,"/**
* Initializes a Call with default invalid parameters.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,addResponseTime,"org.apache.hadoop.ipc.CallQueueManager:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",256,258,"/**
 * Delegates scheduling task.
 * @param name task identifier
 * @param e schedulable event
 * @param details processing details
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doAccept,org.apache.hadoop.ipc.Server$Listener:doAccept(java.nio.channels.SelectionKey),1616,1639,"/**
* Accepts and processes incoming connections.
* @param key SelectionKey associated with the server socket
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,add,org.apache.hadoop.ipc.FairCallQueue:add(org.apache.hadoop.ipc.Schedulable),194,213,"/**
* Handles queue overflow by throwing appropriate exception.
* @param e element to process
* @return true if processing successful, otherwise throws exception
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,put,org.apache.hadoop.ipc.FairCallQueue:put(org.apache.hadoop.ipc.Schedulable),215,222,"/**
* Masks an element by adding it to a queue based on its priority.
* @param e the element to be masked
* @throws InterruptedException if thread is interrupted during execution
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$RpcCall:run(),1234,1272,"/**
 * Handles RPC request processing.
 * @throws Exception if an error occurs during processing
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setDeferredError,org.apache.hadoop.ipc.Server$RpcCall:setDeferredError(java.lang.Throwable),1367,1391,"/**
* Handles errors by setting up a deferred response.
* @param t the Throwable object representing the error, or null if none provided
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolVersions,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersions(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto)",44,68,"/**
 * Handles request to get protocol versions.
 * @param controller RPC controller
 * @param request GetProtocolVersionsRequestProto object
 * @return GetProtocolVersionsResponseProto object
 * @throws ServiceException if class not found or other issues occur
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolProxy.java,fetchServerMethods,org.apache.hadoop.ipc.ProtocolProxy:fetchServerMethods(java.lang.reflect.Method),57,78,"/**
* Masks a method to fetch and validate protocol versions.
* @param method the Method object to process
* @throws IOException if an I/O error occurs during processing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProtocolImpl,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:getProtocolImpl(org.apache.hadoop.ipc.RPC$Server,java.lang.String,long)",510,527,"/**
* Retrieves protocol implementation by name and version.
* @param server RPC server instance
* @param protoName protocol name
* @param clientVersion client's protocol version
* @return ProtoClassProtoImpl if found, throws exception otherwise
* @throws RpcServerException for server-related issues
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.lang.String)",1990,1992,"/**
* Constructs a FatalRpcServerException with an error code and message.
* @param errCode RPC error code
* @param message descriptive error message
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer:<init>(int),37,39,"/**
 * Initializes a ResponseBuffer with a specified capacity.
 * @param capacity maximum number of bytes that can be stored
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection$RpcRequestSender:run(),1115,1150,"/**
* Handles RPC requests and sends responses.
* Continues processing until connection should close.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,decayCurrentCosts,org.apache.hadoop.ipc.DecayRpcScheduler:decayCurrentCosts(),479,540,"/**
 * Decays current costs and updates totals.
 * Logs decay process and handles exceptions.
 */","* Decay the stored costs for each user and clean as necessary.
   * This method should be called periodically in order to keep
   * costs current.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.ipc.Schedulable),677,684,"/**
 * Applies a mask to an object's identity.
 * @param obj the Schedulable object to process
 * @return masked integer value
 */","* Compute the appropriate priority for a schedulable based on past requests.
   * @param obj the schedulable obj to query and remember
   * @return the level index which we recommend scheduling in",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),686,691,"/**
* Masks user group information.
* @param ugi UserGroupInformation object
* @return Masked integer value
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setPriorityLevel,"org.apache.hadoop.ipc.Server:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",732,735,"/**
 * Forwards user group info and priority to call queue.
 * @param ugi UserGroupInformation object
 * @param priority execution priority level
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invoke,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke(),161,163,"/**
 * Wraps the result of m1() in a CallReturn object.
 * @return CallReturn instance containing the result of m1()
 * @throws Throwable if m1() throws an exception
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,getValue,org.apache.hadoop.ipc.RpcWritable$Buffer:getValue(java.lang.Object),190,192,"/**
 * Masks and processes input value using RPC.
 * @param <T> generic type of the input value
 * @param value input value to be processed
 * @return processed value of type T
 * @throws IOException if an I/O error occurs during processing
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,setResponse,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:setResponse(com.google.protobuf.Message),405,410,"/**
* Handles message processing and logs performance.
* @param message the incoming message to process
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,setResponse,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:setResponse(org.apache.hadoop.thirdparty.protobuf.Message),437,442,"/**
* Processes a message and logs the execution time.
* @param message Message to be processed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseForWritable,"org.apache.hadoop.ipc.Server:setupResponseForWritable(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3564,3580,"/**
* Encodes RPC response header and data into a byte array.
* @param header RPC response header
* @param rv writable response data, may be null
* @return encoded byte array of the response
* @throws IOException if encoding fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,removeNextElement,org.apache.hadoop.ipc.FairCallQueue:removeNextElement(),165,178,"/**
* Retrieves an element from a queue based on priority.
* @return E type element from the appropriate queue
*/","* Returns an element first non-empty queue equal to the priority returned
   * by the multiplexer or scans from highest to lowest priority queue.
   *
   * Caller must always acquire a semaphore permit before invoking.
   *
   * @return the first non-empty queue with less priority, or null if
   * everything was empty",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client$Connection:close(),1266,1304,"/**
* Closes the IPC connection.
* Handles exceptions and logs closure status.
*/",Close the connection.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRunLoop,org.apache.hadoop.ipc.Server$Responder:doRunLoop(),1727,1796,"/**
* Processes and writes RPC calls.
* Handles exceptions and purges old calls periodically.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,sendResponse,org.apache.hadoop.ipc.Server$Connection:sendResponse(org.apache.hadoop.ipc.Server$RpcCall),3062,3064,"/**
 * Handles RPC call by delegating to responder.
 * @param call the RPC call to be processed
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",56,63,"/**
* Initializes a Globber with given filesystem, pattern, and filter.
* @param fs the FileSystem to operate on
* @param pathPattern the Path pattern to match files against
* @param filter the PathFilter to apply for file selection
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)",81,91,"/**
 * Initializes a new Globber with specified parameters.
 * @param fs FileSystem to operate on
 * @param pathPattern Path pattern to match files
 * @param filter Filter to apply to matched paths
 * @param resolveSymlinks Whether to resolve symbolic links
 */","* Filesystem constructor for use by {@link GlobBuilder}.
   * @param fs filesystem
   * @param pathPattern path pattern
   * @param filter optional filter
   * @param resolveSymlinks should symlinks be resolved.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,org.apache.hadoop.util.MachineList:<init>(java.lang.String),73,75,"/**
 * Constructs a MachineList with specified host entries.
 * @param hostEntries comma-separated list of host names or IP addresses
 */","* 
   * @param hostEntries comma separated ip/cidr/host addresses",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,<init>,org.apache.hadoop.util.FileBasedIPList:<init>(java.lang.String),52,65,"/**
* Initializes IP list from a file.
* @param fileName name of the file containing IP addresses
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfo.java,newInstance,org.apache.hadoop.util.SysInfo:newInstance(),36,44,"/**
* Returns system info for the current OS.
* @return SysInfo object specific to Linux or Windows
*/","* Return default OS instance.
   * @throws UnsupportedOperationException If cannot determine OS.
   * @return Default instance for the detected OS.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getPhysicalMemorySize,org.apache.hadoop.util.SysInfoLinux:getPhysicalMemorySize(),594,600,"/**
* Calculates available memory size.
* @return available memory in bytes
*/",{@inheritDoc},,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getAvailableVirtualMemorySize,org.apache.hadoop.util.SysInfoLinux:getAvailableVirtualMemorySize(),619,622,"/**
* Calculates total memory mask.
* @return sum of m1 and swap size in KB
*/",{@inheritDoc},,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,<init>,org.apache.hadoop.fs.FSDataInputStream:<init>(java.io.InputStream),58,64,"/**
 * Constructs a FSDataInputStream.
 * @param in input stream to be wrapped
 * @throws IllegalArgumentException if input stream does not implement Seekable and PositionedReadable
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,read,"org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)",196,212,"/**
* Reads data into a ByteBuffer from the input stream.
* @param bufferPool pool to allocate ByteBuffers from
* @param maxLength maximum length of data to read
* @param opts options for reading
* @return ByteBuffer containing the read data or null if none available
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,put,org.apache.hadoop.util.LightWeightResizableGSet:put(java.lang.Object),91,96,"/**
* Adds an element and performs additional processing.
* @param element the element to add
* @return the existing element or null if not present
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,org.apache.hadoop.util.LightWeightGSet$SetIterator:remove(),346,357,"/**
* Removes the current element from the set.
* Throws IllegalStateException if there's no current element.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,remove,org.apache.hadoop.util.LightWeightResizableGSet:remove(java.lang.Object),103,106,"/**
 * Retrieves an element by key.
 * @param key unique identifier for the element
 * @return the element associated with the key or null if not found
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evict,org.apache.hadoop.util.LightWeightCache:evict(),155,161,"/**
* Polls and removes an element from the queue.
* @return the polled element
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,transform,"org.apache.hadoop.util.XMLUtils:transform(java.io.InputStream,java.io.InputStream,java.io.Writer)",79,95,"/**
* Transforms XML using a stylesheet.
* @param styleSheet InputStream for the XSLT stylesheet
* @param xml InputStream for the XML document
* @param out Writer to output the transformation result
* @throws TransformerConfigurationException if configuration fails
* @throws TransformerException if transformation fails
*/","* Transform input xml given a stylesheet.
   * 
   * @param styleSheet the style-sheet
   * @param xml input xml data
   * @param out output
   * @throws TransformerConfigurationException synopsis signals a problem
   *         creating a transformer object.
   * @throws TransformerException this is used for throwing processor
   *          exceptions before the processing has started.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,boolean,java.util.List)",446,469,"/**
* Generates a summary string based on storage options.
* @param qOption quick summary option
* @param hOption human-readable format option
* @param tOption type-specific summary option
* @param xOption detailed difference summary option
* @param types list of storage types to include
* @return formatted summary string
*/","Return the string representation of the object in the output format.
   * if qOption is false, output directory count, file count, and content size;
   * if qOption is true, output quota and remaining quota as well.
   * if hOption is false, file sizes are returned in bytes
   * if hOption is true, file sizes are returned in human readable
   * if tOption is true, display the quota by storage types
   * if tOption is false, same logic with #toString(boolean,boolean)
   * if xOption is false, output includes the calculation from snapshots
   * if xOption is true, output excludes the calculation from snapshots
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output is to be used
   * @param tOption a flag indicating if display quota by storage types
   * @param xOption a flag indicating if calculation from snapshots is to be
   *                included in the output
   * @param types Storage types to display
   * @return the string representation of the object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toSnapshot,org.apache.hadoop.fs.ContentSummary:toSnapshot(boolean),498,503,"/**
 * Formats snapshot details into a string.
 * @param hOption flag to include human-readable format
 * @return formatted snapshot information as a string
 */","* Return the string representation of the snapshot counts in the output
   * format.
   * @param hOption flag indicating human readable or not
   * @return String representation of the snapshot counts",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,getQuotaUsage,org.apache.hadoop.fs.QuotaUsage:getQuotaUsage(boolean),329,346,"/**
* Generates a formatted string with quota and space usage information.
* @param hOption flag for human-readable formatting
* @return formatted quota and space usage details
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,getTypesQuotaUsage,"org.apache.hadoop.fs.QuotaUsage:getTypesQuotaUsage(boolean,java.util.List)",348,366,"/**
* Generates a summary of storage types with quotas.
* @param hOption human-readable option flag
* @param types list of StorageType objects
* @return formatted string summarizing storage quotas
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,computeCapacity,"org.apache.hadoop.util.LightWeightGSet:computeCapacity(double,java.lang.String)",374,377,"/**
* Calls another m3 method with runtime parameters.
* @param percentage decimal value representing percentage
* @param mapName name of the map
* @return result from nested m3 call
*/","* Let t = percentage of max memory.
   * Let e = round(log_2 t).
   * Then, we choose capacity = 2^e/(size of reference),
   * unless it is outside the close interval [1, 2^30].
   *
   * @param mapName mapName.
   * @param percentage percentage.
   * @return compute capacity.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,fill,org.apache.hadoop.fs.FSInputChecker:fill(),217,222,"/**
* Masks buffer content and updates count.
* @throws IOException if an I/O error occurs
*/","* Fills the buffer with a chunk data. 
   * No mark is supported.
   * This method assumes that all data in the buffer has already been read in,
   * hence pos > count.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,readAndDiscard,org.apache.hadoop.fs.FSInputChecker:readAndDiscard(int),231,245,"/**
 * Reads data from buffer until specified length is reached.
 * @param len desired number of bytes to read
 * @return actual number of bytes read
 * @throws IOException if an I/O error occurs
 */","* Like read(byte[], int, int), but does not provide a dest buffer,
   * so the read data is discarded.
   * @param      len maximum number of bytes to read.
   * @return     the number of bytes read.
   * @throws     IOException  if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,toString,org.apache.hadoop.io.UTF8:toString(),163,175,"/**
 * Builds a string from buffer operations.
 * @return resulting string after processing
 */",Convert to a String.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,toStringChecked,org.apache.hadoop.io.UTF8:toStringChecked(),183,190,"/**
* Masks data using specified buffer and methods.
* @return masked string representation of the data
*/","* Convert to a string, checking for valid UTF8.
   * @return the converted string
   * @throws UTFDataFormatException if the underlying bytes contain invalid
   * UTF8 data.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,fromBytes,org.apache.hadoop.io.UTF8:fromBytes(byte[]),257,263,"/**
* Masks byte array into a string.
* @param bytes input byte array to be masked
* @return masked string representation of the byte array
*/","* @return Convert a UTF-8 encoded byte array back into a string.
   *
   * @param bytes input bytes.
   * @throws IOException if the byte array is invalid UTF8",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,readString,org.apache.hadoop.io.UTF8:readString(java.io.DataInput),272,277,"/**
 * Reads and masks data from input.
 * @param in DataInput source
 * @return Masked string representation of the data
 */","* @return Read a UTF-8 encoded string.
   *
   * @see DataInput#readUTF()
   * @param in DataInput.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,checkResponse,org.apache.hadoop.ipc.Client:checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto),252,267,"/**
* Validates RPC response header.
* @param header RPC response header to validate
* @throws IOException if validation fails or header is null
*/",Check the rpc response header.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,toString,org.apache.hadoop.ipc.Client:toString(),1353,1357,"/**
 * Generates a masked string using client ID.
 * @return Masked string in ""m1-m2-clientId"" format
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,org.apache.hadoop.util.StringUtils:byteToHexString(byte),210,212,"/**
 * Converts a single byte to its hexadecimal string representation.
 * @param b the input byte
 * @return hexadecimal string of the byte
 */","* Convert a byte to a hex string.
   * @see #byteToHexString(byte[])
   * @see #byteToHexString(byte[], int, int)
   * @param b byte
   * @return byte's hex value as a String",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,toString,org.apache.hadoop.ha.ActiveStandbyElector:toString(),1274,1280,"/**
* Generates a masked string with elector ID, app data, and client info.
* @return Masked information as a formatted string
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,uncaughtException,"org.apache.hadoop.service.launcher.ServiceLauncher:uncaughtException(java.lang.Thread,java.lang.Throwable)",779,783,"/**
* Handles uncaught exceptions by logging and terminating the thread.
* @param thread the thread where the exception occurred
* @param exception the throwable exception
*/","* Handler for uncaught exceptions: terminate the service.
   * @param thread thread
   * @param exception exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exitWithUsageMessage,org.apache.hadoop.service.launcher.ServiceLauncher:exitWithUsageMessage(),1033,1035,"/**
 * Exits the program with usage information.
 */","* Exit with the usage exit code {@link #EXIT_USAGE}
   * and message {@link #USAGE_MESSAGE}.
   * @throws ExitUtil.ExitException if exceptions are disabled",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/HadoopUncaughtExceptionHandler.java,uncaughtException,"org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable)",83,127,"/**
* Handles exceptions thrown by threads during shutdown or normal operation.
* @param thread the thread that threw the exception
* @param exception the exception thrown by the thread
*/","* Uncaught exception handler.
   * If an error is raised: shutdown
   * The state of the system is unknown at this point -attempting
   * a clean shutdown is dangerous. Instead: exit
   * @param thread thread that failed
   * @param exception the raised exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exit,"org.apache.hadoop.service.launcher.ServiceLauncher:exit(int,java.lang.String)",856,858,"/**
 * Handles exit with code and message.
 * @param exitCode numeric exit code
 * @param message descriptive error or status message
 */","* Exit the JVM.
   *
   * This is method can be overridden for testing, throwing an 
   * exception instead. Any subclassed method MUST raise an 
   * {@code ExitException} instance/subclass.
   * The service launcher code assumes that after this method is invoked,
   * no other code in the same method is called.
   * @param exitCode code to exit
   * @param message input message.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,org.apache.hadoop.util.ExitUtil:terminate(int),368,370,"/**
 * Throws an ExitException with a default message.
 * @param status exit status code
 * @throws ExitException if status meets certain criteria
 */","* Like {@link #terminate(int, Throwable)} without a message.
   *
   * @param status exit code
   * @throws ExitException if {@link System#exit(int)} is disabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Classpath.java,terminate,"org.apache.hadoop.util.Classpath:terminate(int,java.lang.String)",121,124,"/**
* Logs error message and exits with given status.
* @param status exit status code
* @param msg error message to log
*/","* Prints a message to stderr and exits with a status code.
   *
   * @param status exit code
   * @param msg message",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,interrupted,org.apache.hadoop.service.launcher.InterruptEscalator:interrupted(org.apache.hadoop.service.launcher.IrqHandler$InterruptData),103,135,"/**
* Handles service interruption.
* @param interruptData data from the interrupt event
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,org.apache.hadoop.util.ExitUtil:halt(int),389,391,"/**
 * Throws a HaltException with a default message.
 * @param status error code for the exception
 */","* Forcibly terminates the currently running Java virtual machine.
   * @param status status code
   * @throws HaltException if {@link Runtime#halt(int)} is disabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sort,"org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)",58,61,"/**
* Calls overloaded method with default comparator.
* @param s IndexedSortable object to sort
* @param p starting index of the range to sort
* @param r ending index of the range to sort
*/","* Sort the given range of items using quick sort.
   * {@inheritDoc} If the recursion depth falls below {@link #getMaxDepth},
   * then switch to {@link HeapSort}.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,<init>,"org.apache.hadoop.fs.permission.AclStatus:<init>(java.lang.String,java.lang.String,boolean,java.lang.Iterable,org.apache.hadoop.fs.permission.FsPermission)",216,223,"/**
* Constructs an AclStatus object with specified details.
* @param owner the owner of the ACL
* @param group the group associated with the ACL
* @param stickyBit indicates whether the sticky bit is set
* @param entries iterable collection of ACL entries
* @param permission file system permissions
*/","* Private constructor.
   *
   * @param file Path file associated to this ACL
   * @param owner String file owner
   * @param group String file group
   * @param stickyBit the sticky bit
   * @param entries the ACL entries
   * @param permission permission of the path",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,parseACLs,org.apache.hadoop.util.ZKUtil:parseACLs(java.lang.String),95,122,"/**
* Parses ACL string into list of ACL objects.
* @param aclString comma-separated ACL components
* @return List of ACL objects or empty list if input is null
*/","* Parse comma separated list of ACL entries to secure generated nodes, e.g.
   * <code>sasl:hdfs/host1@MY.DOMAIN:cdrwa,sasl:hdfs/host2@MY.DOMAIN:cdrwa</code>
   *
   * @param aclString aclString.
   * @return ACL list
   * @throws BadAclFormatException if an ACL is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,parseAuth,org.apache.hadoop.util.ZKUtil:parseAuth(java.lang.String),133,154,"/**
 * Parses an authorization string into a list of authentication info.
 * @param authString comma-separated authentication components
 * @return List of ZKAuthInfo objects
 * @throws BadAuthFormatException if the format is incorrect
 */","* Parse a comma-separated list of authentication mechanisms. Each
   * such mechanism should be of the form 'scheme:auth' -- the same
   * syntax used for the 'addAuth' command in the ZK CLI.
   * 
   * @param authString the comma-separated auth mechanisms
   * @return a list of parsed authentications
   * @throws BadAuthFormatException if the auth format is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,preserveAttributes,"org.apache.hadoop.fs.shell.CommandWithDestination:preserveAttributes(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData,boolean)",445,490,"/**
 * Masks attributes from source to target path.
 * @param src source PathData object
 * @param target target PathData object
 * @param preserveRawXAttrs flag to preserve raw extended attributes
 */","* Preserve the attributes of the source to the target.
   * The method calls {@link #shouldPreserve(FileAttribute)} to check what
   * attribute to preserve.
   * @param src source to preserve
   * @param target where to preserve attributes
   * @param preserveRawXAttrs true if raw.* xattrs should be preserved
   * @throws IOException if fails to preserve attributes",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,add,org.apache.hadoop.util.ChunkedArrayList:add(java.lang.Object),132,146,"/**
* Adds an element to the list.
* Throws RuntimeException if the list is full.
* @param e element to add
* @return true if added successfully
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclUtil.java,getMinimalAcl,org.apache.hadoop.fs.permission.AclUtil:getMinimalAcl(org.apache.hadoop.fs.permission.FsPermission),99,116,"/**
 * Creates ACL entries for user, group, and other based on file permissions.
 * @param perm file system permissions object
 * @return list of AclEntry objects representing access control rules
 */","* Translates the given permission bits to the equivalent minimal ACL.
   *
   * @param perm FsPermission to translate
   * @return List&lt;AclEntry&gt; containing exactly 3 entries representing the
   *         owner, group and other permissions",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:trackDuration(java.lang.String,long)",461,468,"/**
 * Tracks duration for a given key and count.
 * @param key unique identifier for the tracker
 * @param count number of occurrences
 * @return DurationTracker object or default if not valid
 */","* If the store is tracking the given key, return the
   * duration tracker for it. If not tracked, return the
   * stub tracker.
   * @param key statistic key prefix
   * @param count  #of times to increment the matching counter in this
   * operation.
   * @return a tracker.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,<init>,"org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String)",58,62,"/**
* Constructs a StatisticDurationTracker with default count of 1.
* @param iostats storage for I/O statistics
* @param key identifier for the statistic entry
*/","* Constructor -increments the counter by 1.
   * @param iostats statistics to update
   * @param key prefix of values.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,<init>,"org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,java.lang.String,java.lang.Object[])",57,59,"/**
 * Constructs a DurationInfo with logging enabled.
 * @param log Logger instance for logging
 * @param format Message format string
 * @param args Arguments for the message format
 */","* Create the duration text from a {@code String.format()} code call;
   * log output at info level.
   * @param log log to write to
   * @param format format string
   * @param args list of arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.concurrent.CompletableFuture),116,126,"/**
* Waits for a CompletableFuture to complete.
* @param future the CompletableFuture to wait on
* @throws IOException if task is cancelled or an error occurs
*/","* Wait for a single of future to complete, extracting IOEs afterwards.
   *
   * @param <T> Generics Type T.
   * @param future future to wait for.
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletionIgnoringExceptions,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletionIgnoringExceptions(java.util.concurrent.CompletableFuture),133,143,"/**
* Masks a given CompletableFuture by waiting for its completion.
* @param future the CompletableFuture to mask
*/","* Wait for a single of future to complete, ignoring exceptions raised.
   * @param future future to wait for.
   * @param <T> Generics Type T.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,toString,org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:toString(),107,112,"/**
* Generates a duration message based on failure status.
* @return formatted duration string with key and suffix if failed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,toString,org.apache.hadoop.util.DurationInfo:toString(),89,92,"/**
 * Returns a string combining results from two methods.
 * @return concatenated string of m1 result and super.m2 result with duration info
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,newCrcComposer,"org.apache.hadoop.util.CrcComposer:newCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long)",60,64,"/**
 * Creates a CRC composer with specified parameters.
 * @param type checksum type to use
 * @param bytesPerCrcHint hint for bytes per CRC block
 * @return CrcComposer instance
 * @throws IOException if an I/O error occurs
 */","* Returns a CrcComposer which will collapse all ingested CRCs into a single
   * value.
   *
   * @param type type.
   * @param bytesPerCrcHint bytesPerCrcHint.
   * @throws IOException raised on errors performing I/O.
   * @return a CrcComposer which will collapse all ingested CRCs into a single value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(int,long)",167,192,"/**
* Updates composite CRC and checks position in stripe.
* @param crcB current CRC value
* @param bytesPerCrc number of bytes processed
* @throws IOException if position exceeds stripe length
*/","* Updates with a single additional CRC which corresponds to an underlying
   * data size of {@code bytesPerCrc}.
   *
   * @param crcB crcB.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,lock,org.apache.hadoop.util.InstrumentedLock:lock(),101,107,"/**
* Executes method sequence with timing and locking.
* @param waitStart starting timestamp for waiting period
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,lockInterruptibly,org.apache.hadoop.util.InstrumentedLock:lockInterruptibly(),109,115,"/**
* Executes method sequence with locking and timing.
* @throws InterruptedException if thread is interrupted
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,tryLock,"org.apache.hadoop.util.InstrumentedLock:tryLock(long,java.util.concurrent.TimeUnit)",126,136,"/**
* Attempts to acquire a lock with a timeout and performs actions based on success.
* @param time the maximum time to wait for the lock
* @param unit the time unit of the timeout parameter
* @return true if the lock was acquired, false otherwise
* @throws InterruptedException if the current thread is interrupted while waiting
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,unlock,org.apache.hadoop.util.InstrumentedLock:unlock(),138,144,"/**
* Releases and reacquires a lock, then calls m3.
* @param localLockAcquireTime timestamp when the lock was acquired
* @param localLockReleaseTime timestamp when the lock was released
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getFormattedTimeWithDiff,"org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(org.apache.commons.lang3.time.FastDateFormat,long,long)",375,379,"/**
* Formats and compares times.
* @param dateFormat formatter for time conversion
* @param finishTime end time in milliseconds
* @param startTime start time in milliseconds
* @return formatted time string or result of comparison
*/","* Formats time in ms and appends difference (finishTime - startTime)
   * as returned by formatTimeDiff().
   * If finish time is 0, empty string is returned, if start time is 0
   * then difference is not appended to return value.
   *
   * @param dateFormat date format to use
   * @param finishTime finish time
   * @param startTime  start time
   * @return formatted value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,org.apache.hadoop.util.StringUtils:escapeString(java.lang.String),665,667,"/**
 * Calls overloaded method with default escape and comma characters.
 * @param str input string to process
 * @return processed string result
 */","* Escape commas in the string using the default escape char
   * @param str a string
   * @return an escaped string",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String),723,725,"/**
 * Calls overloaded method with default escape and comma characters.
 * @param str input string to process
 * @return processed string result
 */","* Unescape commas in the string using the default escape char
   * @param str a string
   * @return an unescaped string",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,startupShutdownMessage,"org.apache.hadoop.service.launcher.ServiceLauncher:startupShutdownMessage(java.lang.String,java.util.List)",1010,1016,"/**
* Masks function with given class name and arguments.
* @param classname name of the class
* @param args list of argument strings
* @return masked string result
*/","* @return Build a log message for starting up and shutting down.
   * @param classname the class of the server
   * @param args arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceNext,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceNext(),489,499,"/**
* Masks function S.
* @throws IOException if an I/O error occurs
*/","* Get the next source value.
     * This calls {@link #sourceHasNext()} first to verify
     * that there is data.
     * @return the next value
     * @throws IOException failure
     * @throws NoSuchElementException no more data",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceHasNext,org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:sourceHasNext(),790,793,"/**
 * Checks conditions for work continuation.
 * @return true if both conditions are met, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,awaitFuture,org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future),65,69,"/**
* Deprecated: Retrieves result from Future.
* @param future task to retrieve result from
* @return result of the Future
* @throws InterruptedIOException if interrupted while waiting
* @throws IOException for other I/O errors
*/","* Given a future, evaluate it. Raised exceptions are
   * extracted and handled.
   * See {@link FutureIO#awaitFuture(Future, long, TimeUnit)}.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException future was interrupted
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitAllFutures,org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection),162,169,"/**
* Collects results from a collection of futures.
* @param collection futures to process
* @return list of results
* @throws InterruptedIOException if interrupted while waiting
* @throws IOException for IO errors
* @throws CancellationException if any future is cancelled
* @throws RuntimeException for other execution issues
*/","* Evaluates a collection of futures and returns their results as a list.
   * <p>
   * This method blocks until all futures in the collection have completed.
   * If any future throws an exception during its execution, this method
   * extracts and rethrows that exception.
   * </p>
   * @param collection collection of futures to be evaluated
   * @param <T> type of the result.
   * @return the list of future's result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,awaitFuture,"org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)",86,93,"/**
* Deprecated: Waits for a future to complete with a timeout.
* @param future the task to wait on
* @param timeout maximum time to wait
* @param unit time unit of the timeout
* @return result of the future
* @throws InterruptedIOException if interrupted during I/O operations
* @throws IOException if an I/O error occurs
* @throws RuntimeException for other exceptions
* @throws TimeoutException if the operation times out
*/","* Given a future, evaluate it. Raised exceptions are
   * extracted and handled.
   * See {@link FutureIO#awaitFuture(Future, long, TimeUnit)}.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @param timeout timeout.
   * @param unit unit.
   * @return the result, if all went well.
   * @throws InterruptedIOException future was interrupted
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitAllFutures,"org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection,java.time.Duration)",188,197,"/**
* Collects results from futures within a specified timeout.
* @param collection of Future tasks
* @param duration maximum time to wait for each future
* @return list of results from futures
* @throws InterruptedIOException if thread is interrupted during I/O operations
* @throws IOException on I/O errors
* @throws CancellationException if any future task is cancelled
* @throws RuntimeException for other runtime issues
* @throws TimeoutException if a future does not complete within the timeout
*/","* Evaluates a collection of futures and returns their results as a list,
   * but only waits up to the specified timeout for each future to complete.
   * <p>
   * This method blocks until all futures in the collection have completed or
   * the timeout expires, whichever happens first. If any future throws an
   * exception during its execution, this method extracts and rethrows that exception.
   * @param collection collection of futures to be evaluated
   * @param duration timeout duration
   * @param <T> type of the result.
   * @return the list of future's result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,cancelAllFuturesAndAwaitCompletion,"org.apache.hadoop.util.functional.FutureIO:cancelAllFuturesAndAwaitCompletion(java.util.Collection,boolean,java.time.Duration)",211,239,"/**
 * Waits for all futures in the collection to complete within a given duration.
 * @param collection of Future objects
 * @param interruptIfRunning true to interrupt running tasks, false otherwise
 * @param duration max time to wait for each future
 * @return list of results from completed futures
 */","* Cancels a collection of futures and awaits the specified duration for their completion.
   * <p>
   * This method blocks until all futures in the collection have completed or
   * the timeout expires, whichever happens first.
   * All exceptions thrown by the futures are ignored. as is any TimeoutException.
   * @param collection collection of futures to be evaluated
   * @param interruptIfRunning should the cancel interrupt any active futures?
   * @param duration total timeout duration
   * @param <T> type of the result.
   * @return all futures which completed successfully.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,newInstance,"org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.Class[],java.lang.Object[])",138,161,"/**
* Instantiates a class using provided arguments and configuration.
* @param theClass target class to instantiate
* @param conf configuration settings
* @param argTypes parameter types for constructor
* @param values argument values for constructor
* @return instance of T or throws RuntimeException on error
*/","Create an object for the given class and initialize it from conf
   *
   * @param theClass class of which an object is created
   * @param conf Configuration
   * @param argTypes the types of the arguments
   * @param values the values of the arguments
   * @param <T> Generics Type.
   * @return a new object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getKeyClass,org.apache.hadoop.io.SequenceFile$Reader:getKeyClass(),2196,2205,"/**
* Returns the key class, initializing it if necessary.
* @return Class<?> representing the key type
*/",@return Returns the class of keys in this file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getValueClass,org.apache.hadoop.io.SequenceFile$Reader:getValueClass(),2213,2222,"/**
* Returns the Class object, initializing it if necessary.
* @return initialized Class object or null on failure
*/",@return Returns the class of values in this file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,readFields,org.apache.hadoop.io.EnumSetWritable:readFields(java.io.DataInput),117,133,"/**
 * Deserializes an EnumSet from a DataInput stream.
 * @param in DataInput source containing serialized data
 * @throws IOException if reading from the input fails
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadClass,org.apache.hadoop.util.FindClass:loadClass(java.lang.String),244,259,"/**
* Loads and processes a class by name.
* @param name class name to be loaded
* @return status code indicating success or failure
*/","* Loads the class of the given name
   * @param name classname
   * @return outcome code",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,createClassInstance,org.apache.hadoop.util.FindClass:createClassInstance(java.lang.String),278,302,"/**
* Masks a function by creating an instance of a specified class.
* @param name class name to instantiate
* @return SUCCESS if successful, error code otherwise
*/","* Create an instance of a class
   * @param name classname
   * @return the outcome",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.Path),115,117,"/**
 * Initializes an AbstractFSBuilderImpl with a specified path.
 * @param path non-null file system path
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.PathHandle),119,121,"/**
 * Initializes an AbstractFSBuilderImpl with a specified path handle.
 * @param pathHandle the path handle to initialize with
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandler.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandler:<init>(),42,44,"/**
 * Initializes FsUrlStreamHandler with a default configuration.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,createConfiguration,org.apache.hadoop.service.launcher.ServiceLauncher:createConfiguration(),399,401,"/**
 * Creates and returns a new Configuration instance.
 * @return Configuration object initialized with default settings
 */","* Override point: create the base configuration for the service.
   *
   * Subclasses can override to create HDFS/YARN configurations etc.
   * @return the configuration to use as the service initializer.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,loadConf,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:loadConf(),437,449,"/**
* Returns the configuration, using a supplied one if available.
* @return Configuration object
*/","* Return the supplied configuration for testing or otherwise load a new
   * configuration.
   *
   * @return the configuration to use",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,<init>,org.apache.hadoop.util.FindClass:<init>(),123,125,"/**
 * Initializes a new instance of FindClass with a default configuration.
 */","* Empty constructor; passes a new Configuration
   * object instance to its superclass's constructor",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,<init>,org.apache.hadoop.conf.ReconfigurableBase:<init>(),75,77,"/**
* Initializes a new instance with default configuration.
*/",* Construct a ReconfigurableBase.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,<init>,org.apache.hadoop.conf.ReconfigurableBase:<init>(org.apache.hadoop.conf.Configuration),84,86,"/**
 * Constructs a ReconfigurableBase with the given configuration.
 * @param conf Configuration object; uses default if null
 */","* Construct a ReconfigurableBase with the {@link Configuration}
   * conf.
   * @param conf configuration.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,"org.apache.hadoop.util.DataChecksum:newDataChecksum(org.apache.hadoop.util.DataChecksum$Type,int)",135,150,"/**
* Creates a DataChecksum based on type and bytes per checksum.
* @param type the ChecksumType
* @param bytesPerChecksum number of bytes per checksum calculation
* @return DataChecksum object or null if invalid type or parameters
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBinPath,org.apache.hadoop.util.Shell:getQualifiedBinPath(java.lang.String),701,704,"/**
 * Masks an executable command.
 * @param executable the original command string
 * @return masked command string
 * @throws IOException if an I/O error occurs
 */","*  Fully qualify the path to a binary that should be in a known hadoop
   *  bin location. This is primarily useful for disambiguating call-outs
   *  to executable sub-components of Hadoop to avoid clashes with other
   *  executables that may be in the path.  Caveat:  this call doesn't
   *  just format the path to the bin directory.  It also checks for file
   *  existence of the composed path. The output of this call should be
   *  cached by callers.
   *
   * @param executable executable
   * @return executable file reference
   * @throws FileNotFoundException if the path does not exist
   * @throws IOException on path canonicalization failures",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,runCommand,org.apache.hadoop.util.Shell:runCommand(),967,1098,"/**
* Executes a shell command with optional timeout.
* @throws IOException if an I/O error occurs during execution
*/","* Run the command.
   *
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(java.lang.String),61,65,"/**
 * Updates progress with given status.
 * @param status current status to set
 * @return updated Progress object
 */","* Adds a named node to the tree.
   * @param status status.
   * @return Progress.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,createRootDirRecursively,org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String),361,363,"/**
 * Calls overloaded method with default null value.
 * @param path file path to process
 */","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   * @param path Path of the znode to create.
   * @throws Exception If it cannot create the file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,invoke,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invoke(java.lang.Object,java.lang.Object[])",77,82,"/**
* Masks a function with given arguments.
* @param target the object target (expected to be null)
* @param args variable number of arguments for the function
* @return result of the masked function cast to R
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,pathCapabilities_hasPathCapability,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:pathCapabilities_hasPathCapability(java.lang.Object,org.apache.hadoop.fs.Path,java.lang.String)",349,356,"/**
* Checks if a file system has a specific capability for a given path.
* @param fs the file system object
* @param path the path to check capability for
* @param capability the capability to verify
* @return true if capability exists, false otherwise
*/","* Does a path have a given capability?
   * Calls {@code PathCapabilities#hasPathCapability(Path, String)},
   * mapping IOExceptions to false.
   * @param fs filesystem
   * @param path path to query the capability of.
   * @param capability non-null, non-empty string to query the path for support.
   * @return true if the capability is supported
   * under that part of the FS
   * false if the method is not loaded or the path lacks the capability.
   * @throws IllegalArgumentException invalid arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,streamCapabilities_hasCapability,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:streamCapabilities_hasCapability(java.lang.Object,java.lang.String)",367,372,"/**
* Checks if an object has a specified capability.
* @param object the object to check capabilities for
* @param capability the capability to verify
* @return true if the object has the capability, false otherwise
*/","* Does an object implement {@code StreamCapabilities} and, if so,
   * what is the result of the probe for the capability?
   * Calls {@code StreamCapabilities#hasCapability(String)},
   * @param object object to probe
   * @param capability capability string
   * @return true iff the object implements StreamCapabilities and the capability is
   * declared available.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_counters,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_counters(java.io.Serializable),611,614,"/**
 * Masks and counts statistics from source.
 * @param source data source to process
 * @return map of masked statistics
 */","* Get the counters of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of counters.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_gauges,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_gauges(java.io.Serializable),621,625,"/**
* Masks and returns statistics from source.
* @param source input data to process
* @return Map of statistic key-value pairs
*/","* Get the gauges of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of gauges.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_minimums,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_minimums(java.io.Serializable),632,635,"/**
 * Calls m1 with null and given source.
 * @param source input data to process
 * @return map of statistics or null
 */","* Get the minimums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of minimums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_maximums,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_maximums(java.io.Serializable),642,645,"/**
* Processes serializable data to get statistics.
* @param source input data to process
* @return map of statistics or null if processing fails
*/","* Get the maximums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of maximums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_means,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_means(java.io.Serializable),654,657,"/**
* Applies a mask to the source data.
* @param source input data to be masked
* @return map of masked results
*/","* Get the means of an IOStatisticsSnapshot.
   * Each value in the map is the (sample, sum) tuple of the values;
   * the mean is then calculated by dividing sum/sample wherever sample is non-zero.
   * @param source source of statistics.
   * @return a map of mean key to (sample, sum) tuples.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invoke(java.lang.Object[]),219,221,"/**
 * Delegates call to method.m1 with null as first argument.
 * @param args variable number of arguments to pass
 * @return result of method.m1 execution
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeStatic,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeStatic(java.lang.Object[]),106,109,"/**
* Executes a masked function with variable arguments.
* @param args variable number of arguments for the function
* @return result of the function execution
*/","* Invoke a static method.
     * @param args arguments.
     * @return result.
     * @param <R> type of result.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invoke(java.lang.Object[]),202,204,"/**
 * Invokes method with variable arguments.
 * @param args variable number of arguments to pass
 * @return result of the method invocation
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.Class[])",308,311,"/**
* Configures builder with class name and argument classes.
* @param className name of the class to be configured
* @param argClasses variable number of argument classes
* @return Builder instance for method chaining
*/","* Checks for an implementation, first finding the given class by name.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param className name of a class
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.Class[])",411,414,"/**
* Initializes builder with class name and argument classes.
* @param className name of the class to be built
* @param argClasses array of argument classes for the constructor
* @return Builder instance
*/","* Checks for an implementation, first finding the given class by name.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param className name of a class
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,loadStaticMethod,"org.apache.hadoop.util.dynamic.BindingUtils:loadStaticMethod(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])",139,149,"/**
* Retrieves a dynamic unbound method.
* @param source class containing the method
* @param returnType expected return type of the method
* @param name method name
* @param parameterTypes types of the method parameters
* @return DynMethods.UnboundMethod instance or throws if invalid
*/","* Load a static method from the source class, which will be a noop() if
   * the class is null or the method isn't found.
   * If the class and method are not found, then an {@code IllegalStateException}
   * is raised on the basis that this means that the binding class is broken,
   * rather than missing/out of date.
   *
   * @param <T> return type
   * @param source source. If null, the method is a no-op.
   * @param returnType return type class (unused)
   * @param name method name
   * @param parameterTypes parameters
   *
   * @return the method or a no-op.
   * @throws IllegalStateException if the method is not static.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatisticsSource,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSource(java.lang.Object),394,397,"/**
* Checks conditions on an object.
* @param object the object to check
* @return true if both conditions are met, false otherwise
*/","* Probe for an object being an instance of {@code IOStatisticsSource}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatistics,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatistics(java.lang.Object),405,408,"/**
 * Checks conditions using two methods.
 * @param object input object to evaluate
 * @return true if both conditions are met, false otherwise
 */","* Probe for an object being an instance of {@code IOStatisticsSource}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatisticsSnapshot,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSnapshot(java.io.Serializable),416,419,"/**
* Checks conditions and IO statistics for an object.
* @param object the Serializable object to check
* @return true if both conditions are met, false otherwise
*/","* Probe for an object being an instance of {@code IOStatisticsSnapshot}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_enabled,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_enabled(),427,430,"/**
 * Checks conditions for mask function.
 * @return true if both conditions are met, false otherwise
 */","* Probe to check if the thread-level IO statistics enabled.
   * If the relevant classes and methods were not found, returns false
   * @return true if the IOStatisticsContext API was found
   * and is enabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,toString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:toString(),671,677,"/**
* Generates a string representation of DynamicWrappedStatistics.
* @return formatted string with IO statistics availability
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_pageSize,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",263,268,"/**
* Masks a file using specified filesystem and path.
* @param fileSystem the filesystem to use
* @param path the path of the file to mask
* @return result of masking operation
* @throws IOException if an I/O error occurs
*/","* Get the maximum number of objects/files to delete in a single request.
   * @param fileSystem filesystem
   * @param path path to delete under.
   * @return a number greater than or equal to zero.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException path not valid.
   * @throws IOException problems resolving paths
   * @throws RuntimeException invocation failure.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_delete,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)",293,299,"/**
 * Masks specified paths in a file system.
 * @param fs the FileSystem instance
 * @param base the base directory path
 * @param paths collection of paths to mask
 * @return list of masked path entries
 * @throws IOException if an I/O error occurs
 */","* Delete a list of files/objects.
   * <ul>
   *   <li>Files must be under the path provided in {@code base}.</li>
   *   <li>The size of the list must be equal to or less than the page size.</li>
   *   <li>Directories are not supported; the outcome of attempting to delete
   *       directories is undefined (ignored; undetected, listed as failures...).</li>
   *   <li>The operation is not atomic.</li>
   *   <li>The operation is treated as idempotent: network failures may
   *        trigger resubmission of the request -any new objects created under a
   *        path in the list may then be deleted.</li>
   *    <li>There is no guarantee that any parent directories exist after this call.
   *    </li>
   * </ul>
   * @param fs filesystem
   * @param base path to delete under.
   * @param paths list of paths which must be absolute and under the base path.
   * @return a list of all the paths which couldn't be deleted for a reason other than
   *          ""not found"" and any associated error message.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException if a path argument is invalid.
   * @throws IOException IO problems including networking, authentication and more.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,fileSystem_openFile,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)",323,335,"/**
 * Opens a file in the specified filesystem.
 * @param fs FileSystem instance
 * @param path Path to the file
 * @param policy File access policy
 * @param status Optional FileStatus object
 * @param length Optional file length
 * @param options Optional map of additional options
 * @return FSDataInputStream for reading the file
 * @throws IOException if an I/O error occurs
 */","* OpenFile assistant, easy reflection-based access to
   * {@code FileSystem#openFile(Path)} and blocks
   * awaiting the operation completion.
   * @param fs filesystem
   * @param path path
   * @param policy read policy
   * @param status optional file status
   * @param length optional file length
   * @param options nullable map of other options
   * @return stream of the opened file
   * @throws IOException if the operation was attempted and failed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_readFully,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)",412,419,"/**
 * Reads data from InputStream into ByteBuffer at specified position.
 * @param in source input stream
 * @param position starting position in the stream
 * @param buf destination buffer for read data
 * @throws IOException if an I/O error occurs
 */","* Delegate to {@code ByteBufferPositionedReadable#read(long, ByteBuffer)}.
   * @param in input stream
   * @param position position within file
   * @param buf the ByteBuffer to receive the results of the read operation.
   * @throws UnsupportedOperationException if the input doesn't implement
   * the interface or, if when invoked, it is raised.
   * Note: that is the default behaviour of {@code FSDataInputStream#readFully(long, ByteBuffer)}.
   * @throws IOException if the operation was attempted and failed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,checkIoStatisticsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsAvailable(),376,378,"/**
 * Masks the IO statistics snapshot creation.
 * Calls m1 with iostatisticsSnapshotCreateMethod.
 */","* Require a IOStatistics to be available.
   * @throws UnsupportedOperationException if the method was not found.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,checkIoStatisticsContextAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsContextAvailable(),384,386,"/**
 * Masks functionality by invoking another method.
 * @param iostatisticsContextEnabledMethod method to be invoked
 */","* Require IOStatisticsContext methods to be available.
   * @throws UnsupportedOperationException if the classes/methods were not found",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,<init>,org.apache.hadoop.util.ComparableVersion:<init>(java.lang.String),355,358,"/**
 * Constructs a ComparableVersion from a version string.
 * @param version the version string to be parsed
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,<init>,"org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long)",112,118,"/**
 * Constructs a LightWeightCache with specified parameters.
 * @param recommendedLength suggested initial capacity
 * @param sizeLimit maximum number of entries allowed
 * @param creationExpirationPeriod expiration time for entries since creation
 * @param accessExpirationPeriod expiration time for entries since last access
 */","* @param recommendedLength Recommended size of the internal array.
   * @param sizeLimit the limit of the size of the cache.
   *            The limit is disabled if it is &lt;= 0.
   * @param creationExpirationPeriod the time period C &gt; 0 in nanoseconds
   *            that the creation of an entry is expired if it is added to the
   *            cache longer than C.
   * @param accessExpirationPeriod the time period A &gt;= 0 in nanoseconds that
   *            the access of an entry is expired if it is not accessed
   *            longer than A.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,contains,org.apache.hadoop.util.LightWeightGSet$Values:contains(java.lang.Object),250,254,"/**
 * Checks if the set contains the specified element.
 * @param o element to check for presence in the set
 * @return true if the set contains the element, false otherwise
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToMap,"org.apache.hadoop.util.HostsFileReader:readFileToMap(java.lang.String,java.lang.String,java.util.Map)",123,128,"/**
 * Masks content in a file based on type and mapping.
 * @param type type of masking to apply
 * @param filename path to the input file
 * @param map key-value pairs for masking rules
 * @throws IOException if file operations fail
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,"org.apache.hadoop.util.HostsFileReader:refresh(java.io.InputStream,java.io.InputStream)",236,259,"/**
* Refreshes host include/exclude lists from input streams.
* @param inFileInputStream stream for include file updates
* @param exFileInputStream stream for exclude file updates
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Filter.java,<init>,"org.apache.hadoop.util.bloom.Filter:<init>(int,int,int)",102,107,"/**
* Initializes a Filter with specified parameters.
* @param vectorSize size of the filter vector
* @param nbHash number of hash functions
* @param hashType type of hash function to use
*/","* Constructor.
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash functions to consider.
   * @param hashType type of the hashing function (see {@link Hash}).",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Filter.java,readFields,org.apache.hadoop.util.bloom.Filter:readFields(java.io.DataInput),204,218,"/**
* Reads and processes hash configuration from input.
* @param in DataInput source containing hash settings
* @throws IOException if unsupported version or I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,delete,org.apache.hadoop.util.bloom.CountingBloomFilter:delete(org.apache.hadoop.util.bloom.Key),135,160,"/**
 * Decrements the count of a key in hash buckets.
 * @param key unique identifier for the item to decrement
 */","* Removes a specified key from <i>this</i> counting Bloom filter.
   * <p>
   * <b>Invariant</b>: nothing happens if the specified key does not belong to <i>this</i> counter Bloom filter.
   * @param key The key to remove.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.DynamicBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),175,188,"/**
* Checks if key is absent in the matrix.
* @param key the key to check
* @return true if key is not found, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.Collection),156,164,"/**
* Recursively processes a collection of keys.
* @param coll collection of Key objects to process
*/","* Adds a collection of false positive information to <i>this</i> retouched Bloom filter.
   * @param coll The collection of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.List),170,178,"/**
* Recursively processes a list of keys.
* @param keys list of Key objects to process
* @throws NullPointerException if the keys list is null
*/","* Adds a list of false positive information to <i>this</i> retouched Bloom filter.
   * @param keys The list of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key[]),184,192,"/**
* Recursively processes an array of keys.
* @param keys array of Key objects to process
* @throws NullPointerException if keys is null
*/","* Adds an array of false positive information to <i>this</i> retouched Bloom filter.
   * @param keys The array of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,clearBit,org.apache.hadoop.util.bloom.RetouchedBloomFilter:clearBit(int),313,344,"/**
* Masks keys and updates vectors at given index.
* @param index position in key and fp vectors
*/","* Clears a specified bit in the bit vector and keeps up-to-date the KeyList vectors.
   * @param index The position of the bit to clear.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,ratioRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:ratioRemove(int[]),294,307,"/**
* Finds index of minimum value in ratio array.
* @param h array of hash indices
* @return index with minimum ratio value
*/","* Chooses the bit position that minimizes the number of false negative generated while maximizing.
   * the number of false positive removed.
   * @param h The different bit positions.
   * @return The position that minimizes the number of false negative generated while maximizing.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,driver,org.apache.hadoop.util.ProgramDriver:driver(java.lang.String[]),155,159,"/**
* Executes function mask with provided arguments.
* @param argv array of string arguments
* @throws Throwable if an error occurs during execution
*/","* API compatible with Hadoop 1.x.
   *
   * @param argv argv.
   * @throws Throwable Anything thrown
   *                   by the example program's main",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String),130,132,"/**
 * Sets title with left justification and no bold.
 * @param title text to be set as title
 * @return Builder instance for further configuration
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification)",134,136,"/**
 * Initializes a builder with title and justification.
 * @param title content title
 * @param justification reason for content
 * @return Builder instance configured with title and justification
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,boolean)",138,140,"/**
 * Initializes a builder with title and justification.
 * @param title text to be displayed as title
 * @param wrap indicates if content should wrap
 * @return initialized Builder object
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getCredentialEntry,"org.apache.hadoop.conf.Configuration:getCredentialEntry(org.apache.hadoop.security.alias.CredentialProvider,java.lang.String)",2439,2469,"/**
* Retrieves a credential entry by name or alias.
* @param provider source of credential entries
* @param name credential name or alias
* @return CredentialEntry object or null if not found
* @throws IOException if an I/O error occurs
*/","* Get the credential entry by name from a credential provider.
   *
   * Handle key deprecation.
   *
   * @param provider a credential provider
   * @param name alias of the credential
   * @return the credential entry or null if not found",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadResource,"org.apache.hadoop.conf.Configuration:loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)",3102,3147,"/**
 * Processes a resource to extract configuration properties.
 * @param properties existing properties to update
 * @param wrapper resource wrapper object
 * @param quiet if true, suppresses exceptions
 * @return Resource object with updated properties or null if not applicable
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[],java.lang.String)",594,600,"/**
* Marks a function as deprecated with a custom message.
* @param key the original function name
* @param newKeys array of alternative function names
* @param customMessage custom deprecation message
*/","* Adds the deprecated key to the global deprecation map.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If a key is deprecated in favor of multiple keys, they are all treated as 
   * aliases of each other, and setting any one of them resets all the others 
   * to the new value.
   *
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   * 
   * @param key to be deprecated
   * @param newKeys list of keys that take up the values of deprecated key
   * @param customMessage depcrication message
   * @deprecated use {@link #addDeprecation(String key, String newKey,
      String customMessage)} instead",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,parseNext,org.apache.hadoop.conf.Configuration$Parser:parseNext(),3448,3466,"/**
 * Handles XML events and processes tokens.
 * @throws IOException if an I/O error occurs
 * @throws XMLStreamException if a parsing error occurs
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,openListeners,org.apache.hadoop.http.HttpServer2:openListeners(),1537,1552,"/**
* Opens listeners and processes them based on port conditions.
* @throws Exception if an error occurs during processing
*/","* Open the main listener for the server
   * @throws Exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,checkArgs,org.apache.hadoop.ha.SshFenceByTcpPort:checkArgs(java.lang.String),73,78,"/**
* Validates and processes configuration string.
* @param argStr configuration string to be validated
* @throws BadFencingConfigurationException if configuration is invalid
*/","* Verify that the argument, if given, in the conf is parseable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,parseOpts,"org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[])",505,507,"/**
* Creates and returns a CommandLine object.
* @param cmdName command name
* @param opts options configuration
* @param argv command line arguments
* @return CommandLine instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,clearParentZNode,org.apache.hadoop.ha.ActiveStandbyElector:clearParentZNode(),407,427,"/**
* Recursively deletes a directory from ZooKeeper.
* @throws IOException if deletion fails
* @throws InterruptedException if operation is interrupted
*/","* Clear all of the state held within the parent ZNode.
   * This recursively deletes everything within the znode as well as the
   * parent znode itself. It should only be used when it's certain that
   * no electors are currently participating in the election.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,fenceOldActive,org.apache.hadoop.ha.ActiveStandbyElector:fenceOldActive(),1016,1047,"/**
 * Checks for and processes any old active nodes that need fencing.
 * @return Stat object representing the status of the checked node, or null if no action is needed
 * @throws InterruptedException if thread is interrupted during operation
 * @throws KeeperException if there's a ZooKeeper error
 */","* If there is a breadcrumb node indicating that another node may need
   * fencing, try to fence that node.
   * @return the Stat of the breadcrumb node that was read, or null
   * if no breadcrumb node existed",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:createWithRetries(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode)",1082,1091,"/**
* Creates a new ZooKeeper node.
* @param path node path in the ZooKeeper namespace
* @param data initial data for the node
* @param acl access control list for the node
* @param mode creation mode (persistent, ephemeral, etc.)
* @return created node's path or null if failed
* @throws InterruptedException if operation is interrupted
* @throws KeeperException if a ZooKeeper exception occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,getDataWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:getDataWithRetries(java.lang.String,boolean,org.apache.zookeeper.data.Stat)",1093,1101,"/**
 * Masks a file path using ZooKeeper.
 * @param path the file path to mask
 * @param watch flag to enable watching for changes
 * @param stat object to capture file statistics
 * @return byte array representing masked data or null if operation fails
 * @throws InterruptedException if thread is interrupted during execution
 * @throws KeeperException if a ZooKeeper operation fails
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setDataWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:setDataWithRetries(java.lang.String,byte[],int)",1103,1111,"/**
* Updates data at a specified path with a given version.
* @param path the zookeeper node path
* @param data the data to update
* @param version the expected version of the node
* @return Stat object representing the updated node state
* @throws InterruptedException if operation is interrupted
* @throws KeeperException if a ZooKeeper error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,deleteWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:deleteWithRetries(java.lang.String,int)",1113,1122,"/**
* Masks a ZNode path with a given version.
* @param path the ZNode path to mask
* @param version the version of the ZNode
* @throws KeeperException if a ZooKeeper operation fails
* @throws InterruptedException if the operation is interrupted
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readRangeFrom,"org.apache.hadoop.fs.VectoredReadUtils:readRangeFrom(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.util.function.IntFunction)",117,142,"/**
* Reads data from a stream into a ByteBuffer.
* @param stream source of positioned readable data
* @param range file range to read
* @param allocate function to allocate ByteBuffer
* @return CompletableFuture containing the read ByteBuffer
* @throws EOFException if end of file is reached prematurely
*/","* Synchronously reads a range from the stream dealing with the combinations
   * of ByteBuffers buffers and PositionedReadable streams.
   * @param stream the stream to read from
   * @param range the range to read
   * @param allocate the function to allocate ByteBuffers
   * @return the CompletableFuture that contains the read data or an exception.
   * @throws IllegalArgumentException the range is invalid other than by offset or being null.
   * @throws EOFException the range offset is negative
   * @throws NullPointerException if the range is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,requestCaching,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestCaching(org.apache.hadoop.fs.impl.prefetch.BufferData),435,482,"/**
* Processes buffer data with caching logic.
* @param data BufferData object to process
*/","* Requests that the given block should be copied to the local cache.
   * The block must not be accessed by the caller after calling this method
   * because it will released asynchronously relative to the caller.
   *
   * @throws IllegalArgumentException if data is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setPrefetch,org.apache.hadoop.fs.impl.prefetch.BufferData:setPrefetch(java.util.concurrent.Future),181,186,"/**
* Sets and validates an action future.
* @param actionFuture future representing the action to be performed
*/","* Indicates that a prefetch operation is in progress.
   *
   * @param actionFuture the {@code Future} of a prefetch action.
   *
   * @throws IllegalArgumentException if actionFuture is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setReady,org.apache.hadoop.fs.impl.prefetch.BufferData:setReady(org.apache.hadoop.fs.impl.prefetch.BufferData$State[]),209,218,"/**
* Updates buffer and checksum, transitions to READY state.
* @param expectedCurrentState array of expected current states
*/","* Marks the completion of reading data into the buffer.
   * The buffer cannot be modified once in this state.
   *
   * @param expectedCurrentState the collection of states from which transition to READY is allowed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getSize,org.apache.hadoop.fs.impl.prefetch.BlockData:getSize(int),154,164,"/**
* Calculates mask value for a block.
* @param blockNumber the block number to calculate mask for
* @return mask value based on file size and block configuration
*/","* Gets the size of the given block.
   * @param blockNumber the id of the desired block.
   * @return the size of the given block.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getRelativeOffset,"org.apache.hadoop.fs.impl.prefetch.BlockData:getRelativeOffset(int,long)",194,198,"/**
 * Applies mask to calculate effective offset.
 * @param blockNumber identifier of the block
 * @param offset original byte offset
 * @return masked offset as integer
 */","* Gets the relative offset corresponding to the given block and the absolute offset.
   * @param blockNumber the id of the given block.
   * @param offset absolute offset in the file.
   * @return the relative offset corresponding to the given block and the absolute offset.
   * @throws IllegalArgumentException if either blockNumber or offset is invalid.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getStateString,org.apache.hadoop.fs.impl.prefetch.BlockData:getStateString(),225,241,"/**
 * Generates a masked string representation of blocks.
 * @return formatted string with block ranges and states
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BlockData:<init>(long,int)",75,95,"/**
* Initializes block data with file size and block size.
* @param fileSize total file size in bytes
* @param blockSize size of each block in bytes
*/","* Constructs an instance of {@link BlockData}.
   * @param fileSize the size of a file.
   * @param blockSize the file is divided into blocks of this size.
   * @throws IllegalArgumentException if fileSize is negative.
   * @throws IllegalArgumentException if blockSize is negative.
   * @throws IllegalArgumentException if blockSize is zero or negative.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,blockNumber,org.apache.hadoop.fs.impl.prefetch.FilePosition:blockNumber(),194,197,"/**
 * Masks data using buffer offset.
 * Calls m1(), then returns result of blockData.m2().
 * @return masked integer value
 */","* Gets the id of the current block.
   *
   * @return the id of the current block.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,run,org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer:run(),3834,3841,"/**
* Masks functionality with error handling.
* Calls m2(true) and logs exceptions.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(),3790,3792,"/**
 * Calls helper method with false flag.
 * @throws IOException if I/O error occurs in helper method
 */","* Close all FileSystems in the cache, whether they are marked for
     * automatic closing or not.
     * @throws IOException a problem arose closing one or more FileSystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAllForUGI,org.apache.hadoop.fs.FileSystem:closeAllForUGI(org.apache.hadoop.security.UserGroupInformation),653,657,"/**
* Closes all resources for a given UGI.
* @param ugi UserGroupInformation object representing the user
*/","* Close all cached FileSystem instances for a given UGI.
   * Be sure those filesystems are not used anymore.
   * @param ugi user group info to close
   * @throws IOException a problem arose closing one or more filesystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,readFully,"org.apache.hadoop.fs.FSInputStream:readFully(long,byte[])",135,139,"/**
* Writes bytes to a specific file position.
* @param position file position to start writing
* @param buffer data to write
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readFully,"org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[],int,int)",120,123,"/**
 * Reads bytes from a specific position into a buffer.
 * @param position starting position in the input stream
 * @param buffer destination byte array
 * @param offset starting index in the buffer
 * @param length number of bytes to read
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence)",2063,2066,"/**
* Calls overloaded m1 with UTF-8 charset.
* @param fileContext file context for operations
* @param path file path to operate on
* @param charseq character sequence to process
* @return FileContext result from the overloaded method
* @throws IOException if an I/O error occurs
*/","* Write a line of text to a file. Characters are encoded into bytes using
   * UTF-8. This utility method opens the file for writing, creating the file if
   * it does not exist, or overwrites an existing file.
   *
   * @param fileContext the files system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,createFile,org.apache.hadoop.fs.HarFileSystem:createFile(org.apache.hadoop.fs.Path),1305,1308,"/**
 * Returns an output stream builder for the specified file path.
 * @param path file path in the filesystem
 * @return FSDataOutputStreamBuilder for writing to the file
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[])",1851,1862,"/**
* Writes bytes to a file in the given filesystem.
* @param fs FileSystem instance
* @param path Path to the file
* @param bytes byte array to write
* @return the same FileSystem instance
* @throws IOException if an I/O error occurs
*/","* Writes bytes to a file. This utility method opens the file for writing,
   * creating the file if it does not exist, or overwrites an existing file. All
   * bytes in the byte array are written to the file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param bytes the byte array with the bytes to write
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)",1910,1928,"/**
 * Writes lines to a file in the specified charset.
 * @param fs FileSystem instance
 * @param path destination Path
 * @param lines Iterable of CharSequence lines to write
 * @param cs charset for encoding
 * @return the FileSystem instance
 * @throws IOException if an I/O error occurs
 */","* Write lines of text to a file. Each line is a char sequence and is written
   * to the file in sequence with each line terminated by the platform's line
   * separator, as defined by the system property {@code
   * line.separator}. Characters are encoded into bytes using the specified
   * charset. This utility method opens the file for writing, creating the file
   * if it does not exist, or overwrites an existing file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param lines a Collection to iterate over the char sequences
   * @param cs the charset to use for encoding
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)",1983,1997,"/**
 * Writes a CharSequence to a file using specified FileSystem and Charset.
 * @param fs the FileSystem to use
 * @param path the Path where the data will be written
 * @param charseq the CharSequence to write
 * @param cs the Charset to encode the CharSequence
 * @return the FileSystem instance used for writing
 * @throws IOException if an I/O error occurs
 */","* Write a line of text to a file. Characters are encoded into bytes using the
   * specified charset. This utility method opens the file for writing, creating
   * the file if it does not exist, or overwrites an existing file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   * @param cs the charset to use for encoding
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createFile,org.apache.hadoop.fs.FilterFileSystem:createFile(org.apache.hadoop.fs.Path),699,702,"/**
 * Returns an output stream builder for the specified file path.
 * @param path the file path to write data to
 * @return FSDataOutputStreamBuilder for writing data
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,appendFile,org.apache.hadoop.fs.HarFileSystem:appendFile(org.apache.hadoop.fs.Path),1310,1313,"/**
 * Returns an output stream builder for the specified file path.
 * @param path the file path
 * @return FSDataOutputStreamBuilder for writing to the file
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,appendFile,org.apache.hadoop.fs.FilterFileSystem:appendFile(org.apache.hadoop.fs.Path),704,707,"/**
 * Returns an output stream builder for the specified file path.
 * @param path file path to write data to
 * @return FSDataOutputStreamBuilder for writing data
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)",150,153,"/**
 * Constructs a BlockLocation with specified parameters.
 * @param names array of block replica hostnames
 * @param hosts array of network locations for each replica
 * @param topologyPaths array of network paths for each replica (nullable)
 * @param offset starting byte offset in the file
 * @param length total bytes in the block
 * @param corrupt flag indicating if the block is corrupted
 */","* Constructor with host, name, network topology, offset, length 
   * and corrupt flag.
   * @param names names.
   * @param hosts hosts.
   * @param topologyPaths topologyPaths.
   * @param offset offset.
   * @param length length.
   * @param corrupt corrupt.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults(),60,64,"/**
 * Deprecated method to get server defaults.
 * @return FsServerDefaults object
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults(org.apache.hadoop.fs.Path),66,69,"/**
 * Retrieves default FTP server settings.
 * @param f file path (not used in this implementation)
 * @return FsServerDefaults object with default configurations
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,getServerDefaults,org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults(org.apache.hadoop.fs.Path),66,70,"/**
* Retrieves default server settings from local configuration.
* @param f file path (unused)
* @return FsServerDefaults object with default configurations
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,getServerDefaults,org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults(),72,76,"/**
 * Retrieves server defaults.
 * @deprecated Use alternative method instead.
 * @return FsServerDefaults object
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults(),1135,1139,"/**
 * Retrieves server defaults from local configuration.
 * @deprecated Use alternative method instead.
 * @return FsServerDefaults object
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path),1141,1144,"/**
 * Retrieves server defaults from configuration.
 * @param f file path (not used in this implementation)
 * @return FsServerDefaults object
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(),292,296,"/**
 * Retrieves server defaults from local configuration.
 * @deprecated Use alternative method instead.
 * @throws IOException if an I/O error occurs
 * @return FsServerDefaults object containing server settings
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(org.apache.hadoop.fs.Path),298,307,"/**
* Resolves and returns server defaults for a given file path.
* @param f file path to resolve
* @return FsServerDefaults object
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,int)",93,95,"/**
 * Creates a mutable counter with interned name and description.
 * @param name unique identifier for the counter
 * @param desc description of the counter
 * @param iVal initial value of the counter
 * @return MutableCounterInt instance
 */","* Create a mutable integer counter
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new counter object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,long)",117,119,"/**
* Creates a mutable counter with given name and description.
* @param name unique identifier for the counter
* @param desc description of the counter
* @param iVal initial value of the counter
* @return MutableCounterLong instance
*/","* Create a mutable long integer counter
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new counter object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,long)",166,168,"/**
* Creates a mutable gauge with specified name and description.
* @param name unique identifier for the gauge
* @param desc description of the gauge
* @param iVal initial value for the gauge
* @return MutableGaugeLong object
*/","* Create a mutable long integer gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,float)",190,192,"/**
* Creates a mutable gauge with a float value.
* @param name metric name
* @param desc description of the metric
* @param iVal initial float value
* @return MutableGaugeFloat instance
*/","* Create a mutable float gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,int)",142,144,"/**
* Creates a mutable gauge with integer value.
* @param name metric name
* @param desc metric description
* @param iVal initial integer value
* @return MutableGaugeInt instance
*/","* Create a mutable integer gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,checkCalls,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:checkCalls(),126,143,"/**
 * Determines the minimum wait time for asynchronous calls.
 * @return Minimum wait time in milliseconds, or MAX_WAIT_PERIOD if no pending calls.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedStringArray,"org.apache.hadoop.io.WritableUtils:writeCompressedStringArray(java.io.DataOutput,java.lang.String[])",149,158,"/**
* Writes array of strings to DataOutput.
* @param out DataOutput stream to write to
* @param s array of strings to be written
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,copy,org.apache.hadoop.fs.statistics.MeanStatistic:copy(),281,283,"/**
 * Creates and returns a new MeanStatistic instance.
 * @return MeanStatistic object initialized with current context
 */","* Create a copy of this instance.
   * @return copy.
   *",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString:toString(),297,302,"/**
 * Masks the source string.
 * @return masked source or NULL_SOURCE if source is null
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtDebug,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(org.slf4j.Logger,java.lang.String,java.lang.Object)",227,238,"/**
 * Logs a masked message if logging is enabled.
 * @param log Logger instance for logging
 * @param message Message to be logged
 * @param source Source object for generating stats
 */","* Extract any statistics from the source and log at debug, if
   * the log is set to log at debug.
   * No-op if logging is not at debug or the source is null/of
   * the wrong type/doesn't provide statistics.
   * @param log log to log to
   * @param message message for log -this must contain ""{}"" for the
   * statistics report to actually get logged.
   * @param source source object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,toString,org.apache.hadoop.fs.FSInputStream:toString(),148,158,"/**
* Overrides m1 to append statistics.
* @return String with appended statistics or original value if not applicable
*/","* toString method returns the superclass toString, but if the subclass
   * implements {@link IOStatisticsSource} then those statistics are
   * extracted and included in the output.
   * That is: statistics of subclasses are automatically reported.
   * @return a string value.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_toPrettyString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_toPrettyString(java.lang.Object),324,328,"/**
 * Masks statistics by converting to string.
 * @param statistics object containing statistics data
 * @return masked string representation or empty if null
 */","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * @param statistics A statistics instance; may be null
   * @return string value or the empty string if null",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,measureDurationOfInvocation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:measureDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)",484,507,"/**
* Tracks duration of an operation and handles exceptions.
* @param factory DurationTrackerFactory to create a tracker
* @param statistic name of the statistic to track
* @param input InvocationRaisingIOE object representing the operation
* @return Duration of the operation
* @throws IOException if an I/O error occurs during execution
*/","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic,
   * returning the measured duration.
   *
   * {@link #trackDurationOfInvocation(DurationTrackerFactory, String, InvocationRaisingIOE)}
   * with the duration returned for logging etc.; added as a new
   * method to avoid linking problems with any code calling the existing
   * method.
   *
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @return the duration of the operation, as measured by the duration tracker.
   * @throws IOException IO failure.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDurationOfSupplier,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfSupplier(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.function.Supplier)",642,663,"/**
* Executes a supplier function with duration tracking.
* @param factory optional DurationTrackerFactory for creating trackers
* @param statistic name of the statistic to track
* @param input supplier function to execute
* @return result of the supplier function
*/","* Given a Java supplier, evaluate it while
   * tracking the duration of the operation and success/failure.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @param <B> return type.
   * @return the output of the supplier.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,addToLinkedListAndEvictIfRequired,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListAndEvictIfRequired(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),421,439,"/**
* Masks an entry, updating internal state and purging if necessary.
* @param entry the Entry to be masked
*/","* Add the given entry to the head of the linked list and if the LRU cache size
   * exceeds the max limit, evict tail of the LRU linked list.
   *
   * @param entry Block entry to add.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,<init>,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:<init>(java.util.List,java.util.List,java.util.List,java.util.List,java.util.List)",92,140,"/**
* Initializes IO statistics with various metrics.
* @param counters list of counter keys
* @param gauges list of gauge keys
* @param minimums list of minimum keys
* @param maximums list of maximum keys
* @param meanStatistics list of mean statistic keys
*/","* Constructor invoked via the builder.
   * @param counters keys to use for the counter statistics.
   * @param gauges names of gauges
   * @param minimums names of minimums
   * @param maximums names of maximums
   * @param meanStatistics names of mean statistics.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(byte[],int,int)",162,220,"/**
 * Reads bytes from input into buffer.
 * @param b destination byte array
 * @param off offset in the array to start storing bytes
 * @param len maximum number of bytes to read
 * @return number of bytes actually read or -1 if end of stream
 * @throws IOException if an I/O error occurs
 */","* Decryption is buffer based.
   * If there is data in {@link #outBuffer}, then read it out of this buffer.
   * If there is no data in {@link #outBuffer}, then read more from the 
   * underlying stream and do the decryption.
   * @param b the buffer into which the decrypted data is read.
   * @param off the buffer offset.
   * @param len the maximum number of decrypted data bytes to read.
   * @return int the total number of decrypted data bytes read into the buffer.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,byte[],int,int)",395,425,"/**
 * Decrypts data from buffer into another buffer.
 * @param position starting position in the data stream
 * @param buffer input/output byte array for decryption
 * @param offset start index in the buffer
 * @param length number of bytes to decrypt
 * @throws IOException if an I/O error occurs during decryption
 */","* Decrypt length bytes in buffer starting at offset. Output is also put 
   * into buffer starting at offset. It is thread-safe.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,java.nio.ByteBuffer,int,int)",456,499,"/**
 * Decrypts data from a ByteBuffer into another.
 * @param filePosition starting position in the file
 * @param buf input ByteBuffer containing encrypted data
 * @param length total number of bytes to decrypt
 * @param start offset in the output buffer to start decryption
 * @throws IOException if an I/O error occurs during decryption
 */","* Decrypts the given {@link ByteBuffer} in place. {@code length} bytes are
   * decrypted from {@code buf} starting at {@code start}.
   * {@code buf.position()} and {@code buf.limit()} are unchanged after this
   * method returns. This method is thread-safe.
   *
   * <p>
   *   This method decrypts the input buf chunk-by-chunk and writes the
   *   decrypted output back into the input buf. It uses two local buffers
   *   taken from the {@link #bufferPool} to assist in this process: one is
   *   designated as the input buffer and it stores a single chunk of the
   *   given buf, the other is designated as the output buffer, which stores
   *   the output of decrypting the input buffer. Both buffers are of size
   *   {@link #bufferSize}.
   * </p>
   *
   * <p>
   *   Decryption is done by using a {@link Decryptor} and the
   *   {@link #decrypt(Decryptor, ByteBuffer, ByteBuffer, byte)} method. Once
   *   the decrypted data is written into the output buffer, is is copied back
   *   into buf. Both buffers are returned back into the pool once the entire
   *   buf is decrypted.
   * </p>
   *
   * @param filePosition the current position of the file being read
   * @param buf the {@link ByteBuffer} to decrypt
   * @param length the number of bytes in {@code buf} to decrypt
   * @param start the position in {@code buf} to start decrypting data from",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(java.nio.ByteBuffer,int,int)",649,670,"/**
 * Decrypts data from ByteBuffer.
 * @param buf input buffer containing encrypted data
 * @param length total bytes to decrypt
 * @param start starting index for decryption
 * @throws IOException if an I/O error occurs during decryption
 */","* Decrypts the given {@link ByteBuffer} in place. {@code length} bytes are
   * decrypted from {@code buf} starting at {@code start}.
   * {@code buf.position()} and {@code buf.limit()} are unchanged after this
   * method returns.
   *
   * @see #decrypt(long, ByteBuffer, int, int)",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)",124,140,"/**
* Initializes a CryptoInputStream for decryption.
* @param in input stream to be decrypted
* @param codec cryptographic codec to use
* @param bufferSize size of the buffer
* @param key encryption key
* @param iv initialization vector
* @param streamOffset offset for the stream
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,seek,org.apache.hadoop.crypto.CryptoInputStream:seek(long),523,546,"/**
* Seeks to a specified position in the stream.
* @param pos target position to seek
* @throws IOException if an I/O error occurs or seeking is unsupported
*/",Seek to a position.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,skip,org.apache.hadoop.crypto.CryptoInputStream:skip(long),549,577,"/**
* Skips bytes in the input stream.
* @param n number of bytes to skip
* @return actual number of bytes skipped
* @throws IOException if an I/O error occurs
*/",Skip n bytes,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,seekToNewSource,org.apache.hadoop.crypto.CryptoInputStream:seekToNewSource(long),693,705,"/**
 * Seeks to a target position in the input stream.
 * @param targetPos position to seek to, must be non-negative
 * @return true if seek is successful, false otherwise
 * @throws IOException if an I/O error occurs or seek is not supported
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,write,org.apache.hadoop.crypto.CryptoOutputStream:write(int),271,275,"/**
* Writes a single byte to the output stream.
* @param b byte value to write
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,close,org.apache.hadoop.crypto.CryptoOutputStream:close(),238,256,"/**
* Executes method logic, ensuring resources are closed properly.
* @throws IOException if an I/O error occurs during execution
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hflush,org.apache.hadoop.crypto.CryptoOutputStream:hflush(),294,300,"/**
* Calls m1 and invokes m2 on out if it's Syncable.
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hsync,org.apache.hadoop.crypto.CryptoOutputStream:hsync(),302,308,"/**
* Calls m1 and invokes m2 on out if it's Syncable.
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/GlobFilter.java,compile,org.apache.hadoop.metrics2.filter.GlobFilter:compile(java.lang.String),36,39,"/**
 * Compiles a glob pattern into a regular expression.
 * @param s glob pattern as a string
 * @return compiled Pattern object
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,<init>,org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String),49,51,"/**
 * Initializes a new GlobFilter with a given file pattern.
 * @param filePattern pattern to match files against
 */","* Creates a glob filter with the specified file pattern.
   *
   * @param filePattern the file pattern.
   * @throws IOException thrown if the file pattern is incorrect.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,<init>,"org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String,org.apache.hadoop.fs.PathFilter)",60,62,"/**
* Initializes a GlobFilter with a pattern and a path filter.
* @param filePattern glob pattern for matching files
* @param filter additional path filter criteria
* @throws IOException if initialization fails
*/","* Creates a glob filter with the specified file pattern and an user filter.
   *
   * @param filePattern the file pattern.
   * @param filter user filter in addition to the glob pattern.
   * @throws IOException thrown if the file pattern is incorrect.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTar,"org.apache.hadoop.fs.FileUtil:unTar(java.io.InputStream,java.io.File,boolean)",985,1003,"/**
 * Unpacks an input stream into a directory.
 * @param inputStream the input stream to unpack
 * @param untarDir the target directory for extraction
 * @param gzipped indicates if the input is gzip compressed
 */","* Given a Tar File as input it will untar the file in a the untar directory
   * passed as the second parameter
   *
   * This utility will untar "".tar"" files and "".tar.gz"",""tgz"" files.
   *
   * @param inputStream The tar file as input.
   * @param untarDir The untar directory where to untar the tar file.
   * @param gzipped The input stream is gzipped
   *                TODO Use magic number and PusbackInputStream to identify
   * @throws IOException an exception occurred
   * @throws InterruptedException command interrupted
   * @throws ExecutionException task submit failed",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAllStatistics,org.apache.hadoop.fs.FileContext:getAllStatistics(),2420,2422,"/**
 * Retrieves file system statistics.
 * @return Map of URIs to their corresponding statistics
 */","* @return Map of uri and statistics for each filesystem instantiated. The uri
   *         consists of scheme and authority for the filesystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,clearStatistics,org.apache.hadoop.fs.FileContext:clearStatistics(),2404,2406,"/**
 * Calls m1 method on AbstractFileSystem.
 */","* Clears all the statistics stored in AbstractFileSystem, for all the file
   * systems.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,primitiveCreate,"org.apache.hadoop.fs.FilterFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",551,559,"/**
 * Creates a file output stream for writing data.
 * @param f file path
 * @param absolutePermission file permissions
 * @param flag creation flags
 * @param bufferSize buffer size in bytes
 * @param replication number of replicas
 * @param blockSize block size in bytes
 * @param progress progress reporter
 * @param checksumOpt checksum options
 * @return FSDataOutputStream for writing data
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.fs.viewfs.InodeTree,org.apache.hadoop.conf.Configuration)",977,988,"/**
* Constructs an InternalDirOfViewFs instance.
* @param dir directory inode in the filesystem tree
* @param cTime creation time of the directory
* @param ugi user and group information
* @param uri URI associated with the directory
* @param fsState state of the filesystem
* @param conf configuration settings
* @throws URISyntaxException if URI is invalid
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,<init>,org.apache.hadoop.fs.FilterFs:<init>(org.apache.hadoop.fs.AbstractFileSystem),63,66,"/**
* Initializes a new FilterFs instance.
* @param fs underlying AbstractFileSystem to filter
* @throws URISyntaxException if URI is invalid
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,getPiggyBacksFromInput,"org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBacksFromInput(java.nio.ByteBuffer[],int[],int,int,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",64,120,"/**
 * Encodes input buffers using erasure coding.
 * @param inputs array of ByteBuffer inputs
 * @param piggyBackIndex indices for piggybacking data
 * @param numParityUnits number of parity units to generate
 * @param pgIndex index of the primary group
 * @param encoder RawErasureEncoder instance for encoding
 * @return array of encoded ByteBuffer outputs
 * @throws IOException if an I/O error occurs during encoding
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",146,150,"/**
* Recursively processes input and output chunks.
* @param inputs array of ECChunk objects for processing
* @param outputs array of ECChunk objects for results
* @throws IOException if an I/O error occurs
*/","* Encode with inputs and generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param outputs output buffers to put the encoded data into, read to read
   *                after the call
   * @throws IOException if the encoder is closed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>(java.lang.Object),163,165,"/**
 * Constructs an internal object with a given value.
 * @param value the value to be stored internally
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,acquireHelper,"org.apache.hadoop.fs.impl.prefetch.BufferPool:acquireHelper(int,boolean)",161,187,"/**
* Retrieves or allocates buffer data for a block.
* @param blockNumber the block identifier
* @param canBlock flag to allow blocking if no buffer is available
* @return BufferData object or null if allocation fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.BufferPool:numAvailable(),300,303,"/**
 * Calls m1 and returns result from pool's m2.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,newInstance,"org.apache.hadoop.util.BlockingThreadPoolExecutorService:newInstance(int,int,long,java.util.concurrent.TimeUnit,java.lang.String)",122,148,"/**
* Creates a blocking thread pool with specified parameters.
* @param activeTasks number of threads in the pool
* @param waitingTasks max tasks that can wait in queue
* @param keepAliveTime time to keep idle threads alive
* @param unit time unit for keepAliveTime
* @param prefixName prefix for thread names
* @return BlockingThreadPoolExecutorService instance
*/","* A thread pool that that blocks clients submitting additional tasks if
   * there are already {@code activeTasks} running threads and {@code
   * waitingTasks} tasks waiting in its queue.
   *
   * @param activeTasks maximum number of active tasks
   * @param waitingTasks maximum number of waiting tasks
   * @param keepAliveTime time until threads are cleaned up in {@code unit}
   * @param unit time unit
   * @param prefixName prefix of name for threads
   * @return BlockingThreadPoolExecutorService.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,setData,"org.apache.hadoop.fs.impl.prefetch.FilePosition:setData(org.apache.hadoop.fs.impl.prefetch.BufferData,long,long)",109,128,"/**
* Masks buffer data starting from specified offsets.
* @param bufferData data to be masked
* @param startOffset starting offset for masking
* @param readOffset read offset for masking process
*/","* Associates a buffer with this file.
   *
   * @param bufferData the buffer associated with this file.
   * @param startOffset Start offset of the buffer relative to the start of a file.
   * @param readOffset Offset where reading starts relative to the start of a file.
   *
   * @throws IllegalArgumentException if bufferData is null.
   * @throws IllegalArgumentException if startOffset is negative.
   * @throws IllegalArgumentException if readOffset is negative.
   * @throws IllegalArgumentException if readOffset is outside the range [startOffset, buffer end].",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_getCurrent,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_getCurrent(),261,263,"/**
 * Calls and returns the result of m1().
 * @return Object returned by m1()
 */","* Get the context's {@link IOStatisticsContext} which
   * implements {@link IOStatisticsSource}.
   * This is either a thread-local value or a global empty context.
   * @return instance of {@link IOStatisticsContext}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_reset,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_reset(),287,289,"/**
* Calls method m2 on the result of m1.
*/","* Reset the context's IOStatistics.
   * {@link IOStatisticsContext#reset()}",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_snapshot,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_snapshot(),296,298,"/**
 * Returns a serializable object by calling nested methods.
 * @return Serializable object from nested method calls
 */","* Take a snapshot of the context IOStatistics.
   * {@link IOStatisticsContext#snapshot()}
   * @return an instance of {@link IOStatisticsSnapshot}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_aggregate,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_aggregate(java.lang.Object),308,316,"/**
 * Checks and processes statistics from a source object.
 * @param source the object to process
 * @return true if processing is successful, false otherwise
 */","* Aggregate into the IOStatistics context the statistics passed in via
   * IOStatistics/source parameter.
   * <p>
   * Returns false if the source is null or does not contain any statistics.
   * @param source implementation of {@link IOStatisticsSource} or {@link IOStatistics}
   * @return true if the the source object was aggregated.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,runParallel,org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task),385,519,"/**
 * Executes tasks concurrently, handling failures and reverting changes.
 * @param task the task to execute on each item
 * @return true if all tasks succeeded, false otherwise
 * @throws E if a task throws an exception
 * @throws IOException if there's an issue iterating items
 */","* Parallel execution.
     * All tasks run within the same IOStatisticsContext as the
     * thread calling this method.
     * @param task task to execute
     * @param <E> exception which may be raised in execution.
     * @return true if the operation executed successfully
     * @throws E any exception raised.
     * @throws IOException IOExceptions raised by remote iterator or in execution.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_setThreadIOStatisticsContext,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object),270,273,"/**
 * Masks the given statistics context.
 * @param statisticsContext the context to be masked; can be null
 */","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,setStatisticsContext,org.apache.hadoop.util.functional.TaskPool$Builder:setStatisticsContext(),524,528,"/**
* Calls m1 on ioStatisticsContext if it's not null.
*/",* Set the statistics context for this thread.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,resetStatisticsContext,org.apache.hadoop.util.functional.TaskPool$Builder:resetStatisticsContext(),535,539,"/**
 * Calls m1 on ioStatisticsContext if it is not null.
 */","* Reset the statistics context if it was set earlier.
     * This unbinds the current thread from any statistics
     * context.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processPath,org.apache.hadoop.fs.shell.find.Find:processPath(org.apache.hadoop.fs.shell.PathData),394,401,"/**
 * Masks an item if certain conditions are met.
 * @param item PathData object to be masked
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,postProcessPath,org.apache.hadoop.fs.shell.find.Find:postProcessPath(org.apache.hadoop.fs.shell.PathData),403,410,"/**
 * Masks an item if certain conditions are met.
 * @param item PathData object to be masked
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rmr:processOptions(java.util.LinkedList),174,178,"/**
* Modifies argument list and calls parent method.
* @param args linked list of arguments
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processOptions,org.apache.hadoop.fs.shell.Ls$Lsr:processOptions(java.util.LinkedList),411,416,"/**
 * Modifies argument list and calls superclass method.
 * @param args linked list of command-line arguments
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Dus:processOptions(java.util.LinkedList),236,240,"/**
* Modifies arguments and calls superclass method.
* @param args list of string arguments to modify
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayError,org.apache.hadoop.fs.shell.Command:displayError(java.lang.Exception),474,493,"/**
* Handles exceptions by logging and rethrowing specific ones.
* @param e the exception to handle
*/","* Display an exception prefaced with the command name.  Also increments
   * the error count for the command which will result in a non-zero exit
   * code.
   * @param e exception to display",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getPathHandle,"org.apache.hadoop.fs.FileSystem:getPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",1050,1057,"/**
* Applies mask to file status with optional handle options.
* @param stat file status object
* @param opt optional handle options
* @return PathHandle result
*/","* Create a durable, serializable handle to the referent of the given
   * entity.
   * @param stat Referent in the target FileSystem
   * @param opt If absent, assume {@link HandleOpt#path()}.
   * @throws IllegalArgumentException If the FileStatus does not belong to
   *         this FileSystem
   * @throws UnsupportedOperationException If {@link #createPathHandle}
   *         not overridden by subclass.
   * @throws UnsupportedOperationException If this FileSystem cannot enforce
   *         the specified constraints.
   * @return path handle.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long)",1227,1230,"/**
* Constructs a ShellCommandExecutor with specified parameters.
* @param execString command to execute
* @param dir working directory for the command
* @param env environment variables for the command
* @param timeout maximum time to wait for command execution
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,init,org.apache.hadoop.fs.CachingGetSpaceUsed:init(),90,102,"/**
* Masks data based on usage status.
* Sets mask to 0 if not already set and refreshes accordingly.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addToken,"org.apache.hadoop.security.Credentials:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)",121,137,"/**
* Masks a token for a given alias.
* @param alias the text alias for the token
* @param t the token to be masked or null if ignored
*/","* Add a token in the storage (in memory).
   * @param alias the alias for the key
   * @param t the token object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,encrypt,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",148,152,"/**
 * Masks input buffer and writes to output buffer.
 * @param inBuffer source ByteBuffer containing data to mask
 * @param outBuffer destination ByteBuffer for masked data
 * @throws IOException if an I/O error occurs
 */","* AES-CTR will consume all of the input data. It requires enough space in
     * the destination buffer to encrypt entire input buffer.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,decrypt,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",158,162,"/**
 * Masks input buffer data and writes to output buffer.
 * @param inBuffer source ByteBuffer containing data to be masked
 * @param outBuffer destination ByteBuffer for the masked data
 * @throws IOException if an I/O error occurs during processing
 */","*  AES-CTR will consume all of the input data. It requires enough space in
     * the destination buffer to decrypt entire input buffer.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,initialize,org.apache.hadoop.fs.viewfs.RegexMountPoint:initialize(),86,96,"/**
 * Initializes source pattern and destination path map.
 * @throws IOException if there's an issue with the source path regex or initialization fails.
 */","* Initialize regex mount point.
   *
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",149,164,"/**
 * Constructs a Path by resolving a child path against a parent path.
 * @param parent the parent Path object
 * @param child the child Path object to resolve
 */","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,org.apache.hadoop.fs.Path:<init>(java.lang.String),184,223,"/**
* Constructs a new Path object from a given string.
* @param pathString the input path string
* @throws IllegalArgumentException if the input is invalid
*/","* Construct a path from a String.  Path strings are URIs, but with
   * unescaped elements and some additional normalization.
   *
   * @param pathString the path string",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String,java.lang.String)",241,256,"/**
 * Constructs a Path object with specified components.
 * @param scheme URI scheme
 * @param authority authority component of the URI
 * @param path file or directory path
 */","* Construct a Path from components.
   *
   * @param scheme the scheme
   * @param authority the authority
   * @param path the path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,toString,org.apache.hadoop.fs.shell.PathData:toString(),461,464,"/**
 * Masks the URI using an inferred scheme.
 * @return masked URI as a string
 */","* Returns the printable version of the path that is either the path
   * as given on the commandline, or the full path
   * @return String of the path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,checkNotRelative,org.apache.hadoop.fs.Path:checkNotRelative(),92,96,"/**
 * Validates path; throws exception if path is relative.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getUriPath,org.apache.hadoop.fs.AbstractFileSystem:getUriPath(org.apache.hadoop.fs.Path),423,431,"/**
* Masks and validates path component.
* @param p file path to process
* @return masked string if valid, throws exception otherwise
*/","* Get the path-part of a pathname. Checks that URI matches this file system
   * and that the path-part is a valid name.
   * 
   * @param p path
   * 
   * @return path-part of the Path p",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,resolvePath,org.apache.hadoop.fs.AbstractFileSystem:resolvePath(org.apache.hadoop.fs.Path),506,510,"/**
 * Processes and returns a masked path.
 * @param p input file path
 * @return processed Path object
 * @throws FileNotFoundException if file not found
 * @throws UnresolvedLinkException if symbolic link is unresolved
 * @throws AccessControlException if access is denied
 * @throws IOException for other I/O errors
 */","* Return the fully-qualified path of path f resolving the path
   * through any internal symlinks or mount point
   * @param p path to be resolved
   * @return fully qualified path 
   * @throws FileNotFoundException when file not find throw.
   * @throws AccessControlException when accees control error throw.
   * @throws IOException raised on errors performing I/O.
   * @throws UnresolvedLinkException if symbolic link on path cannot be
   * resolved internally",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,create,"org.apache.hadoop.fs.AbstractFileSystem:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",530,640,"/**
 * Creates an output stream for writing to a file with specified options.
 * @param f the path of the file to be created
 * @param createFlag flags specifying how to handle the file creation
 * @param opts optional parameters for file creation
 * @return FSDataOutputStream for writing to the file
 * @throws IOException if an I/O error occurs
 */","* The specification of this method matches that of
   * {@link FileContext#create(Path, EnumSet, Options.CreateOpts...)} except
   * that the Path f must be fully qualified and the permission is absolute
   * (i.e. umask has been applied).
   *
   * @param f the path.
   * @param createFlag create_flag.
   * @param opts create ops.
   * @throws AccessControlException access controll exception.
   * @throws FileAlreadyExistsException file already exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not dir exception.
   * @throws UnsupportedFileSystemException unsupported file system exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.
   * @return output stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,checkPath,org.apache.hadoop.fs.FilterFs:checkPath(org.apache.hadoop.fs.Path),184,187,"/**
 * Delegates file operation to underlying filesystem.
 * @param path file path to operate on
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path),1505,1510,"/**
* Checks file access with default options.
* @param f file path to check
* @return true if accessible, false otherwise
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,applyUMask,"org.apache.hadoop.fs.permission.FsCreateModes:applyUMask(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",43,49,"/**
* Applies umask to file permission.
* @param mode original file permission
* @param umask user mask for permissions
* @return adjusted file permission
*/","* Create from unmasked mode and umask.
   *
   * @param mode mode.
   * @param umask umask.
   * @return If the mode is already
   * an FsCreateModes object, return it.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfoByNativeIO,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNativeIO(),1063,1085,"/**
* Masks file permissions by setting owner, group, and mode.
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,mkdirs,org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path),2495,2497,"/**
 * Checks file permission using default settings.
 * @param f file path to check
 * @return true if permissions match, false otherwise
 * @throws IOException if an I/O error occurs
 */","* Call {@link #mkdirs(Path, FsPermission)} with default permission.
   * @param f path
   * @return true if the directory was created
   * @throws IOException IO failure",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set)",160,190,"/**
* Constructs a FileStatus object with specified attributes.
* @param length file size in bytes
* @param isdir true if the path is a directory
* @param block_replication replication factor for blocks
* @param blocksize block size in bytes
* @param modification_time time of last modification
* @param access_time time of last access
* @param permission file permissions
* @param owner file owner
* @param group file group
* @param symlink symbolic link path, if any
* @param path file path
* @param attr set of attribute flags
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,setPermission,org.apache.hadoop.fs.FileStatus:setPermission(org.apache.hadoop.fs.permission.FsPermission),367,370,"/**
* Sets file system permission.
* @param permission new permission to set or null to use default
*/","* Sets permission.
   * @param permission if permission is null, default value is set",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,getPermission,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getPermission(),111,116,"/**
* Returns file system permissions mask.
* Initializes if not already set.
* @return FsPermission object
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1432,1438,"/**
 * Creates a file output stream with specified parameters.
 * @param f path to the file
 * @param overwrite whether to overwrite existing file
 * @param bufferSize size of buffer in bytes
 * @param replication desired block replication factor
 * @param blockSize desired block size in bytes
 * @param progress callback for reporting progress
 * @return FSDataOutputStream for writing to the file
 * @throws IOException if an I/O error occurs
 */","* Opens an FSDataOutputStream at the indicated Path with write-progress
   * reporting. Same as create(), except fails if parent directory doesn't
   * already exist.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   * the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getPermission,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getPermission(),146,151,"/**
 * Returns file system permissions.
 * @return FsPermission object
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,createImmutable,org.apache.hadoop.fs.permission.FsPermission:createImmutable(short),64,66,"/**
 * Converts short permission to FsPermission.
 * @param permission numeric permission value
 * @return FsPermission object representing the permission
 */","* Create an immutable {@link FsPermission} object.
   * @param permission permission.
   * @return FsPermission.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processPath,org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData),77,104,"/**
* Writes file metadata to output.
* @param item PathData object representing the file
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,createPermissions,org.apache.hadoop.security.alias.KeyStoreProvider:createPermissions(java.lang.String),68,71,"/**
 * Sets file system permissions.
 * @param perms permission string in octal format
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$Writer:append(java.lang.Object,java.lang.Object)",1469,1502,"/**
* Masks and writes key-value pair to output stream.
* @param key key object to be masked
* @param val value object to be masked
* @throws IOException if there's an I/O error during masking or writing
*/","* Append a key/value pair.
     * @param key input Object key.
     * @param val input Object val.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendRaw,"org.apache.hadoop.io.SequenceFile$Writer:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)",1504,1517,"/**
 * Masks a value with a given key.
 * @param keyData byte array containing the key data
 * @param keyOffset offset in the key data array
 * @param keyLength length of the key to use
 * @param val ValueBytes object to be masked
 * @throws IOException if invalid key length or I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:getCompressedSize(),242,244,"/**
 * Delegates to wBlkState's m1 method.
 * @return result of wBlkState.m1()
 * @throws IOException if an I/O error occurs
 */","* Get the compressed size of the block in progress.
       * 
       * @return the number of compressed bytes written to the underlying FS
       *         file. The size may be smaller than actual need to compress the
       *         all data written due to internal buffering inside the
       *         compressor.
       * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,skip,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:skip(long),528,536,"/**
* Moves cursor in file by specified offset.
* @param n offset to move
* @return new position after move
* @throws IOException if I/O error occurs
*/","* Skips over and discards <code>n</code> bytes of data from the
     * input stream.
     *
     *The <code>skip</code> method skips over some smaller number of bytes
     * when reaching end of file before <code>n</code> bytes have been skipped.
     * The actual number of bytes skipped is returned.  If <code>n</code> is
     * negative, no bytes are skipped.
     *
     * @param      n   the number of bytes to be skipped.
     * @return     the actual number of bytes skipped.
     * @exception  IOException  if an I/O error occurs.
     *             ChecksumException if the chunk to skip to is corrupted",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,seek,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:seek(long),550,556,"/**
 * Moves file pointer to specified position.
 * @param pos target position in the file
 * @throws IOException if an I/O error occurs or position is out of bounds
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPaths,"org.apache.hadoop.fs.shell.Ls:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])",270,283,"/**
* Processes path data items under a parent.
* @param parent parent PathData object
* @param items variable number of child PathData objects
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getUsed,org.apache.hadoop.fs.HarFileSystem:getUsed(org.apache.hadoop.fs.Path),1277,1280,"/**
 * Delegates file size calculation to underlying filesystem.
 * @param path file path to calculate size
 * @return size of the file in bytes
 * @throws IOException if an I/O error occurs
 */",Return the total size of all files from a specified path.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getUsed,org.apache.hadoop.fs.FilterFileSystem:getUsed(org.apache.hadoop.fs.Path),421,424,"/**
 * Delegates to file system to get size of file at given path.
 * @param path file path
 * @return size of file in bytes
 * @throws IOException if an I/O error occurs
 */",Return the total size of all files from a specified path.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,main,org.apache.hadoop.util.JvmPauseMonitor:main(java.lang.String[]),221,231,"/**
 * Initializes JVM pause monitoring and processes a list of strings.
 * @param args command line arguments (unused)
 */","* Simple 'main' to facilitate manual testing of the pause monitor.
   * 
   * This main function just leaks memory into a list. Running this class
   * with a 1GB heap will very quickly go into ""GC hell"" and result in
   * log messages about the GC pauses.
   *
   * @param args args.
   * @throws Exception Exception.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,start,org.apache.hadoop.service.AbstractService:start(),185,208,"/**
* Starts the service if not already started.
* Handles exceptions and logs errors.
*/","* {@inheritDoc}
   * @throws ServiceStateException if the current service state does not permit
   * this action",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,enterState,org.apache.hadoop.service.AbstractService:enterState(org.apache.hadoop.service.Service$STATE),440,449,"/**
 * Updates the current state.
 * @param newState new state to transition to
 * @return previous state before update
 */","* Enter a state; record this via {@link #recordLifecycleEvent}
   * and log at the info level.
   * @param newState the proposed new state
   * @return the original state
   * it wasn't already in that state, and the state model permits state re-entrancy.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printDefaultRealm,org.apache.hadoop.security.KDiag:printDefaultRealm(),488,512,"/**
* Retrieves and logs the default Kerberos realm.
* Handles exceptions related to class invocation.
*/","* Get the default realm.
   * <p>
   * Not having a default realm may be harmless, so is noted at info.
   * All other invocation failures are downgraded to warn, as
   * follow-on actions may still work.
   * Failure to invoke the method via introspection is considered a failure,
   * as it's a sign of JVM compatibility issues that may have other 
   * consequences",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,equals,org.apache.hadoop.io.BytesWritable:equals(java.lang.Object),200,205,"/**
* Checks if object is instance of BytesWritable and delegates to superclass.
* @param right_obj object to check
* @return true if object is BytesWritable, otherwise false
*/",* Are the two byte sequences equal?,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,equals,org.apache.hadoop.io.Text:equals(java.lang.Object),415,420,"/**
* Checks if object is instance of Text and calls superclass method.
* @param o object to check
* @return true if object is Text, otherwise false
*/","* Returns true iff <code>o</code> is a Text with the same length and same
   * contents.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,hashCode,org.apache.hadoop.security.token.Token$PrivateToken:hashCode(),299,304,"/**
* Overrides and extends base method functionality.
* @return Combined hash code using superclass and service results
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,set,org.apache.hadoop.io.BytesWritable:set(org.apache.hadoop.io.BytesWritable),167,169,"/**
 * Delegates to another method with byte array details.
 * @param newData input data as BytesWritable
 */","* Set the BytesWritable to the contents of the given newData.
   *
   * @param newData the value to set this BytesWritable to.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirInternal,org.apache.hadoop.util.DiskChecker:checkDirInternal(java.io.File),94,101,"/**
 * Masks a directory by creating it and setting its permissions.
 * @param dir the directory to be masked
 * @throws DiskErrorException if directory creation fails
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,mlock,"org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:mlock(java.lang.String,java.nio.ByteBuffer,long)",280,283,"/**
 * Calls POSIX.m1 with given parameters.
 * @param identifier unique identifier (not used)
 * @param buffer data buffer to process
 * @param len length of data in buffer
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flushBuffer,"org.apache.hadoop.fs.FSOutputSummer:flushBuffer(boolean,boolean)",159,176,"/**
* Flushes buffer data based on flags.
* @param keep retain partial data in buffer if true
* @param flushPartial flush only partial data if true
* @return remaining bytes in buffer after flushing
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,close,org.apache.hadoop.crypto.CryptoInputStream:close(),319,329,"/**
* Executes methods in sequence and marks object as closed.
* @throws IOException if an I/O error occurs during execution
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",85,116,"/**
 * Processes input buffers to decode and reconstruct data.
 * @param inputs array of ByteBuffer containing input data
 * @param erasedIndexes indices of erased or incomplete data
 * @param outputs array of ByteBuffer for decoded output
 * @throws IOException if an I/O error occurs during processing
 */","* Decode with inputs and erasedIndexes, generates outputs.
   * How to prepare for inputs:
   * 1. Create an array containing data units + parity units. Please note the
   *    data units should be first or before the parity units.
   * 2. Set null in the array locations specified via erasedIndexes to indicate
   *    they're erased and no data are to read from;
   * 3. Set null in the array locations for extra redundant items, as they're
   *    not necessary to read when decoding. For example in RS-6-3, if only 1
   *    unit is really erased, then we have 2 extra items as redundant. They can
   *    be set as null to indicate no data will be used from them.
   *
   * For an example using RS (6, 3), assuming sources (d0, d1, d2, d3, d4, d5)
   * and parities (p0, p1, p2), d2 being erased. We can and may want to use only
   * 6 units like (d1, d3, d4, d5, p0, p2) to recover d2. We will have:
   *     inputs = [null(d0), d1, null(d2), d3, d4, d5, p0, null(p1), p2]
   *     erasedIndexes = [2] // index of d2 into inputs array
   *     outputs = [a-writable-buffer]
   *
   * Note, for both inputs and outputs, no mixing of on-heap buffers and direct
   * buffers are allowed.
   *
   * If the coder option ALLOW_CHANGE_INPUTS is set true (false by default), the
   * content of input buffers may change after the call, subject to concrete
   * implementation.
   *
   * @param inputs input buffers to read data from. The buffers' remaining will
   *               be 0 after decoding
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(byte[][],int[],byte[][])",135,145,"/**
* Masks input data based on erased indexes.
* @param inputs byte arrays containing data to be masked
* @param erasedIndexes indices of elements to be erased
* @param outputs resulting byte arrays after masking
*/","* Decode with inputs and erasedIndexes, generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException if the decoder is closed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
* Creates an instance of RSLegacyRawDecoder.
* @param coderOptions configuration options for erasure coding
* @return RawErasureDecoder implementation
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
* Creates a raw erasure encoder with mask functionality.
* @param coderOptions configuration options for the erasure coder
* @return RawErasureEncoder instance configured with mask settings
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,prepareDecoding,"org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:prepareDecoding(java.lang.Object[],int[])",103,115,"/**
* Masks elements in input array at specified indexes.
* @param inputs array of generic type T to be masked
* @param erasedIndexes array of indexes to mask
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,skipToNextBlockMarker,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextBlockMarker(),328,332,"/**
* Checks block delimiter using BZip2 format.
* @return true if block delimiter is valid, false otherwise
*/","* Skips bytes in the stream until the start marker of a block is reached
   * or end of stream is reached. Used for testing purposes to identify the
   * start offsets of blocks.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,complete,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:complete(),591,599,"/**
 * Masks data and checks CRC.
 * Throws IOException on error.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,getAndMoveToFrontDecode,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode(),821,1004,"/**
 * Decodes a block of data using Burrows-Wheeler transform.
 * Throws IOException if unexpected end of stream or block overrun occurs.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,<init>,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream),598,600,"/**
 * Creates a new CBZip2OutputStream with default block size.
 * @param out output stream to compress data into
 * @throws IOException if an I/O error occurs
 */","* Constructs a new <tt>CBZip2OutputStream</tt> with a blocksize of 900k.
  *
  * <p>
  * <b>Attention: </b>The caller is resonsible to write the two BZip2 magic
  * bytes <tt>""BZ""</tt> to the specified stream prior to calling this
  * constructor.
  * </p>
  *
  * @param out *
  *            the destination stream.
  *
  * @throws IOException
  *             if an I/O error occurs in the specified stream.
  * @throws NullPointerException
  *             if <code>out == null</code>.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,endBlock,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endBlock(),788,831,"/**
 * Updates CRC and processes block data.
 * @throws IOException if I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",60,62,"/**
 * Initializes a new BlockDecompressorStream.
 * @param in InputStream containing compressed data
 * @param decompressor Decompressor to use for decompression
 * @throws IOException if an I/O error occurs
 */","* Create a {@link BlockDecompressorStream}.
   * 
   * @param in input stream
   * @param decompressor decompressor to use
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,createInputStream,org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream),132,136,"/**
 * Wraps input stream with compression.
 * @param in input stream to compress
 * @return CompressionInputStream for compressed data
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,read,org.apache.hadoop.io.compress.DecompressorStream:read(),89,93,"/**
* Calls m1 and processes oneByte array.
* @throws IOException if an I/O error occurs
* @return processed byte value or -1 if error
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,skip,org.apache.hadoop.io.compress.DecompressorStream:skip(long),193,213,"/**
* Skips specified number of bytes.
* @param n number of bytes to skip
* @return actual number of bytes skipped
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(),78,82,"/**
* Constructs a new ZStandardCompressor with default settings.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:getCompressor(),285,308,"/**
 * Retrieves and initializes a compressor from CodecPool.
 * @return Compressor object or null if none available
 * @throws IOException if an I/O error occurs during retrieval
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createOutputStream,org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream),53,58,"/**
 * Creates a compression output stream.
 * @param out underlying output stream
 * @return CompressionOutputStream for writing compressed data
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createOutputStream,org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream),66,71,"/**
 * Creates a compression output stream.
 * @param out underlying output stream
 * @return CompressionOutputStream for writing compressed data
 * @throws IOException if an I/O error occurs
 */","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createOutputStream,org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream),105,110,"/**
 * Creates a compression output stream.
 * @param out underlying output stream
 * @return CompressionOutputStream for writing compressed data
 * @throws IOException if an I/O error occurs
 */","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out        the location for the final output stream
   * @return a stream the user can write uncompressed data to, to have it 
   *         compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createOutputStream,org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream),121,126,"/**
 * Creates a compression output stream.
 * @param out underlying output stream
 * @return CompressionOutputStream instance
 * @throws IOException if an I/O error occurs
 */","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createOutputStream,org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream),44,49,"/**
 * Creates a compression output stream.
 * @param out underlying output stream
 * @return CompressionOutputStream for writing compressed data
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createOutputStream,org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream),66,71,"/**
 * Creates a compression output stream.
 * @param out underlying output stream
 * @return CompressionOutputStream configured with codec settings
 * @throws IOException if an I/O error occurs
 */","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,close,org.apache.hadoop.io.compress.CompressorStream:close(),102,112,"/**
* Ensures method m1 is called only once and marks it as closed.
* @throws IOException if an I/O error occurs during execution
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Writer:close(),385,389,"/**
 * Synchronizes and executes operations on data and index.
 */",Close the map.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,finish,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:finish(),178,188,"/**
* Masks and compresses data.
* Resets output stream and compressor after processing.
*/",* Finishing up the current block.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createInputStream,org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream),79,84,"/**
 * Creates a compression input stream.
 * @param in input stream to compress
 * @return CompressionInputStream object
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createInputStream,org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream),130,135,"/**
* Creates a compression input stream.
* @param in input stream to compress
* @return CompressionInputStream for reading compressed data
*/","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream),160,165,"/**
 * Creates a compression input stream.
 * @param in input stream to compress
 * @return CompressionInputStream instance
 * @throws IOException if an I/O error occurs
 */","* Create a {@link CompressionInputStream} that will read from the given
   * input stream and return a stream for uncompressed data.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createInputStream,org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream),178,183,"/**
 * Compresses input stream using specified configuration.
 * @param in input stream to compress
 * @return compressed input stream
 * @throws IOException if an I/O error occurs
 */","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createInputStream,org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream),76,81,"/**
 * Wraps input stream with compression.
 * @param in input stream to compress
 * @return CompressionInputStream for reading compressed data
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createInputStream,org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream),127,132,"/**
 * Compresses input stream using specified codec.
 * @param in input stream to compress
 * @return CompressionInputStream with compressed data
 * @throws IOException if an I/O error occurs
 */","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,close,org.apache.hadoop.io.compress.DecompressorStream:close(),221,230,"/**
* Ensures m1 is called only once, marking as closed afterward.
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,close,org.apache.hadoop.fs.shell.Display$TextRecordInputStream:close(),259,263,"/**
* Calls method m1 on both r and superclass.
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:close(),3181,3191,"/**
* Recursively calls m1 on input, output, and indexOutput streams.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Reader:close(),881,887,"/**
 * Executes methods m1 on index and data.
 * Skips index execution if already closed.
 * @throws IOException if an I/O error occurs
 */","* Close the map.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:close(),3877,3880,"/**
 * Calls m1 on input stream and closes it.
 * @throws IOException if an I/O error occurs
 */",closes the underlying reader,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,finish,org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:finish(),532,539,"/**
* Calls m2 on input and ensures decompressor is closed.
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createCompressor,org.apache.hadoop.io.compress.DefaultCodec:createCompressor(),74,77,"/**
* Returns a compressor using ZlibFactory.
* @return Compressor instance configured with 'conf'
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.DefaultCodec:createDirectDecompressor(),108,111,"/**
 * Returns a zlib decompressor configured with the given settings.
 * @return DirectDecompressor instance
 */",* {@inheritDoc},,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createDecompressor,org.apache.hadoop.io.compress.DefaultCodec:createDecompressor(),100,103,"/**
 * Returns a decompressor instance.
 * @return Decompressor configured with specified settings
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeFileHeader,org.apache.hadoop.io.SequenceFile$Writer:writeFileHeader(),1274,1289,"/**
* Writes metadata and configuration to output stream.
* @throws IOException if I/O error occurs
*/",Write and flush the file header.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,write,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:write(java.io.DataOutput),217,229,"/**
* Serializes the object to a DataOutput stream.
* Throws IOException if any string exceeds Text.DEFAULT_MAX_LEN.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,write,org.apache.hadoop.security.Credentials:write(java.io.DataOutput),354,371,"/**
 * Writes token and secret key maps to DataOutput.
 * @param out the DataOutput stream to write to
 */","* Stores all the keys to DataOutput.
   * @param out DataOutput.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),681,684,"/**
 * Handles delegation key processing.
 * @param key DelegationKey object to process
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),686,689,"/**
 * Masks functionality using delegation key.
 * @param key unique delegation identifier
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readFields,org.apache.hadoop.io.SequenceFile$Metadata:readFields(java.io.DataInput),771,783,"/**
* Reads metadata from input stream.
* @param in DataInput stream containing metadata
* @throws IOException if invalid size or I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,readFields,org.apache.hadoop.security.token.Token:readFields(java.io.DataInput),307,321,"/**
 * Reads data from input stream into identifier, password, kind, and service.
 * @param in DataInput stream to read from
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,readFields,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:readFields(java.io.DataInput),189,203,"/**
* Reads delegation token data from input.
* @param in DataInput source
* @throws IOException if version mismatch or read fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readBlock,org.apache.hadoop.io.SequenceFile$Reader:readBlock(),2294,2330,"/**
* Processes input data for decompression and validation.
* @throws IOException if file corruption is detected or I/O error occurs
*/",Read the next 'compressed' block,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,seekToCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:seekToCurrentValue(),2336,2369,"/**
 * Masks values based on decompression settings.
 * @throws IOException if seeking fails
 */","* Position valLenIn/valIn to the 'value' 
     * corresponding to the 'current' key",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,createTokenInfo,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenInfo(byte[]),262,271,"/**
* Parses delegation token information from byte array.
* @param tokenInfoBytes serialized token data
* @return DelegationTokenInformation object
* @throws IOException if parsing fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,readFields,org.apache.hadoop.fs.permission.PermissionStatus:readFields(java.io.DataInput),96,101,"/**
* Reads and sets user, group names, and permissions from input.
* @param in DataInput stream to read from
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readString,org.apache.hadoop.io.Text:readString(java.io.DataInput),556,558,"/**
 * Reads data from input stream up to specified length.
 * @param in DataInput source
 * @return String representation of read data
 */","* @return Read a UTF8 encoded string from in.
   * @param in input in.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getDelegationKey(int),385,412,"/**
 * Retrieves a DelegationKey by ID, creating it if not found.
 * @param keyId unique identifier for the key
 * @return DelegationKey object or null if retrieval fails
 */","* Obtains the DelegationKey from the SQL database.
   * @param keyId KeyId of the DelegationKey to obtain.
   * @return DelegationKey that matches the given keyId or null
   *         if it doesn't exist in the database.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processKeyAddOrUpdate,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processKeyAddOrUpdate(byte[]),391,397,"/**
* Masks data using a delegation key.
* @param data input byte array to be masked
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getKeyFromZK,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getKeyFromZK(int),579,598,"/**
* Retrieves delegation key by ID.
* @param keyId unique key identifier
* @return DelegationKey object or null if not found
* @throws IOException on data retrieval errors
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:write(java.io.DataOutput),843,848,"/**
* Writes data to output stream.
* @param out DataOutput object for writing
* @throws IOException if I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:write(java.io.DataOutput),2098,2102,"/**
 * Writes data to output stream.
 * @param out DataOutput stream to write to
 * @throws IOException if I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:write(java.io.DataOutput),897,905,"/**
* Writes data to output stream using specified compression and regions.
* @param out DataOutput stream to write to
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,selectDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:selectDelegationToken(java.net.URL,org.apache.hadoop.security.Credentials)",344,354,"/**
* Retrieves a delegation token for a given URL and credentials.
* @param url the target URL
* @param creds user credentials containing tokens
* @return the delegation token or null if not found
*/","* Select a delegation token from all tokens in credentials, based on url.
   *
   * @param url url.
   * @param creds credentials.
   * @return token.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getServerToken,org.apache.hadoop.security.SaslRpcClient:getServerToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),279,293,"/**
* Generates a security token for authentication.
* @param authType type of SASL authentication
* @return Token object or null if token info is unavailable
* @throws IOException if instantiation fails
*/","* Try to locate the required token for the server.
   * 
   * @param authType of the SASL client
   * @return Token for server, or null if no token available
   * @throws IOException - token selector cannot be instantiated",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setTokenService,"org.apache.hadoop.security.SecurityUtil:setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress)",456,466,"/**
* Masks a token with a service name from address.
* @param token the token to mask
* @param addr the InetSocketAddress of the service
*/","* Set the given token's service to the format expected by the RPC client 
   * @param token a delegation token
   * @param addr the socket for the rpc connection",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,deleteKey,org.apache.hadoop.crypto.key.UserProvider:deleteKey(java.lang.String),102,113,"/**
 * Masks a credential by name.
 * @param name unique identifier for the credential
 * @throws IOException if the key does not exist or an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.UserProvider:rollNewVersion(java.lang.String,byte[])",115,131,"/**
* Masks a key with the given name and material.
* @param name key identifier
* @param material key material bytes
* @return KeyVersion object representing the masked key
* @throws IOException if key not found or length mismatch
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.UserProvider:getKeyVersions(java.lang.String),167,181,"/**
 * Retrieves all key versions for a given name.
 * @param name the key identifier
 * @return list of KeyVersion objects
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,tokenFromProto,org.apache.hadoop.ipc.ProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto),114,117,"/**
* Converts TokenProto to Token.
* @param tokenProto Protocol buffer representation of the token
* @return Token object or null if conversion fails
*/","* Get a token from a TokenProto payload.
   * @param tokenProto marshalled token
   * @return the token.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeys,org.apache.hadoop.crypto.key.UserProvider:getKeys(),155,165,"/**
* Masks and returns a list of strings from credentials.
* @return List of masked strings
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,write,org.apache.hadoop.fs.FsServerDefaults:write(java.io.DataOutput),163,173,"/**
* Writes block metadata to DataOutput.
* @param out destination for serialized data
* @throws IOException if I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,write,org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput),103,106,"/**
 * Writes data to output stream.
 * @param out DataOutput stream to write to
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text),82,89,"/**
 * Masks a text key to a byte string.
 * @param key the input text key
 * @return masked byte string representation of the key
 */","* Get the ByteString for frequently used fixed and small set strings.
   * @param key Hadoop Writable Text string
   * @return the ByteString for frequently used fixed and small set strings.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,org.apache.hadoop.security.token.Token:<init>(org.apache.hadoop.security.token.Token),107,112,"/**
 * Clones an existing token.
 * @param other the token to clone
 */","* Clone a token.
   * @param other the token to clone",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,"org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int)",380,382,"/**
* Calls overloaded method with default maxLines.
* @param str text input
* @param maxLineLength maximum length of a line
* @return result of the overloaded method
*/","* Read from the InputStream into the given Text.
   * @param str the object to store the given line
   * @param maxLineLength the maximum number of bytes to store into str.
   * @return the number of bytes read including the newline
   * @throws IOException if the underlying stream throws",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text),390,392,"/**
 * Calls overloaded method with max integer values as limits.
 * @param str input text
 * @return result of processing text
 * @throws IOException if an I/O error occurs
 */","* Read from the InputStream into the given Text.
   * @param str the object to store the given line
   * @return the number of bytes read including the newline
   * @throws IOException if the underlying stream throws",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)",166,188,"/**
* Creates a delegation token for a user.
* @param ugi UserGroupInformation object representing the user
* @param renewer User who can renew the token
* @param service Service for which the token is issued
* @return Token object with specified parameters
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,<init>,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",55,61,"/**
 * Constructs a new delegation token identifier.
 * @param owner the owner of the token
 * @param renewer the user allowed to renew the token
 * @param realUser the real user on behalf of whom the token is issued
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(int),201,203,"/**
* Creates a retry policy with specified max failovers.
* @param maxFailovers maximum number of retries allowed
* @return configured RetryPolicy object
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)",43,48,"/**
 * Initializes an InstrumentedWriteLock with a custom timer.
 * @param name unique identifier for the lock
 * @param logger used for logging lock operations
 * @param readWriteLock underlying lock mechanism
 * @param minLoggingGapMs minimum time gap between log entries
 * @param lockWarningThresholdMs threshold for warning about long locks
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,long,long)",75,79,"/**
* Constructs an instrumented lock with a specified name and logger.
* @param name unique identifier for the lock
* @param logger used for logging lock events
* @param minLoggingGapMs minimum time gap between log entries (in milliseconds)
* @param lockWarningThresholdMs threshold for warning on long lock holds (in milliseconds)
*/","* Create a instrumented lock instance which logs a warning message
   * when lock held time is above given threshold.
   *
   * @param name the identifier of the lock object
   * @param logger this class does not have its own logger, will log to the
   *               given logger instead
   * @param minLoggingGapMs  the minimum time gap between two log messages,
   *                         this is to avoid spamming to many logs
   * @param lockWarningThresholdMs the time threshold to view lock held
   *                               time as being ""too long""",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)",49,54,"/**
* Initializes a new InstrumentedReadLock with specified parameters.
* @param name unique identifier for the lock
* @param logger Logger instance for logging events
* @param readWriteLock ReentrantReadWriteLock to instrument
* @param minLoggingGapMs minimum time gap between log entries in milliseconds
* @param lockWarningThresholdMs threshold for lock warning in milliseconds
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,org.apache.hadoop.io.retry.RetryPolicy)",40,45,"/**
 * Creates a retry proxy for an interface.
 * @param iface interface class
 * @param implementation concrete implementation of the interface
 * @param retryPolicy policy defining retry behavior
 * @return proxy object with retry capabilities
 */","* <p>
   * Create a proxy for an interface of an implementation class
   * using the same retry policy for each method in the interface. 
   * </p>
   * @param iface the interface that the retry will implement
   * @param implementation the instance whose methods should be retried
   * @param retryPolicy the policy for retrying method call failures
   * @param <T> T.
   * @return the retry proxy",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,processWaitTimeAndRetryInfo,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processWaitTimeAndRetryInfo(),129,149,"/**
* Handles retry logic with a delay.
* @return CallReturn.RETRY indicating retry action
* @throws InterruptedIOException if interrupted during wait
*/","* It first processes the wait time, if there is any,
     * and then invokes {@link #processRetryInfo()}.
     *
     * If the wait time is positive, it either sleeps for synchronous calls
     * or immediately returns for asynchronous calls.
     *
     * @return {@link CallReturn#RETRY} if the retryInfo is processed;
     *         otherwise, return {@link CallReturn#WAIT_RETRY}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,int,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)",281,292,"/**
* Initializes a Writer for TFile with given parameters.
* @param fsdos output stream to write data
* @param minBlockSize minimum block size
* @param compressName compression algorithm name
* @param comparator key comparator
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/","* Constructor
     * 
     * @param fsdos
     *          output stream for writing. Must be at position 0.
     * @param minBlockSize
     *          Minimum compressed block size in bytes. A compression block will
     *          not be closed until it reaches this size except for the last
     *          block.
     * @param compressName
     *          Name of the compression algorithm. Must be one of the strings
     *          returned by {@link TFile#getSupportedCompressionAlgorithms()}.
     * @param comparator
     *          Leave comparator as null or empty string if TFile is not sorted.
     *          Otherwise, provide the string name for the comparison algorithm
     *          for keys. Two kinds of comparators are supported.
     *          <ul>
     *          <li>Algorithmic comparator: binary comparators that is language
     *          independent. Currently, only ""memcmp"" is supported.
     *          <li>Language-specific comparator: binary comparators that can
     *          only be constructed in specific language. For Java, the syntax
     *          is ""jclass:"", followed by the class name of the RawComparator.
     *          Currently, we only support RawComparators that can be
     *          constructed through the default constructor (with no
     *          parameters). Parameterized RawComparators such as
     *          {@link WritableComparator} or
     *          {@link JavaSerializationComparator} may not be directly used.
     *          One should write a wrapper class that inherits from such classes
     *          and use its default constructor to perform proper
     *          initialization.
     *          </ul>
     * @param conf
     *          The configuration object.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:<init>(java.io.DataInput),772,780,"/**
* Constructs a MetaIndex from a DataInput.
* @param in input stream containing meta data
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,isLastChunk,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:isLastChunk(),80,83,"/**
 * Masks data using function m1 and returns last chunk status.
 * @return true if masking successful, false otherwise
 * @throws IOException if an I/O error occurs during masking
 */","* Have we reached the last chunk.
     * 
     * @return true if we have reached the last chunk.
     * @throws java.io.IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,getRemain,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:getRemain(),91,94,"/**
 * Calls m1 and returns the remain value.
 * @throws IOException if an I/O error occurs during m1 execution
 */","* How many bytes remain in the current chunk?
     * 
     * @return remaining bytes left in the current chunk.
     * @throws java.io.IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(),137,144,"/**
* Calls m1, returns -1 if true. Otherwise, reads from 'in' and checks for corruption.
* @return integer value read or -1 if condition met
* @throws IOException if corrupted data is detected
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,"org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[],int,int)",151,165,"/**
 * Reads bytes from the input stream into a byte array.
 * @param b destination buffer
 * @param off offset in the buffer to start writing
 * @param len number of bytes to read
 * @return number of bytes actually read or -1 if no bytes are available
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,skip,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:skip(long),167,175,"/**
 * Calls m3 on input stream if m1 returns false.
 * @param n input value for calculation
 * @return result of m3 or 0 if m1 is true
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(int),303,309,"/**
* Writes a byte to the buffer.
* @param b byte to be written
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,flush,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flush(),332,336,"/**
 * Calls m1 and then invokes out.m2.
 * @throws IOException if an I/O error occurs during execution
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[]),311,314,"/**
 * Calls overloaded method with full byte array.
 * @param b byte array to process
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[],int,int)",1948,1950,"/**
 * Calls m1 with a ByteArray wrapper.
 * @param buf byte array containing data
 * @param offset starting index in the buffer
 * @param length number of bytes to process
 * @return result from m1 method call
 */","* Compare the entry key to another key. Synonymous to compareTo(new
         * ByteArray(buf, offset, length)
         * 
         * @param buf
         *          The key buffer
         * @param offset
         *          offset into the key buffer.
         * @param length
         *          the length of the key.
         * @return comparison result between the entry key with the input key.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,<init>,org.apache.hadoop.io.ArrayWritable:<init>(java.lang.String[]),62,67,"/**
* Constructs an ArrayWritable from a string array.
* @param strings input string array
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:close(),1668,1674,"/**
* Calls m1 and then superclass's m2 if out is not null.
* @throws IOException if an I/O error occurs
*/",Close the file.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:append(java.lang.Object,java.lang.Object)",1677,1707,"/**
 * Masks a key-value pair for serialization.
 * @param key the key object to serialize
 * @param val the value object to serialize
 * @throws IOException if key or value classes are incorrect, or key length is negative
 */",Append a key/value pair.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendRaw,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)",1710,1733,"/**
* Masks a value with a given key.
* @param keyData byte array containing the key
* @param keyOffset offset of the key in the array
* @param keyLength length of the key
* @param val ValueBytes object to be masked
* @throws IOException if negative key length or I/O error occurs
*/",Append a key/value pair.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,delegationTokenToJSON,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:delegationTokenToJSON(org.apache.hadoop.security.token.Token),357,365,"/**
* Generates a delegation token map.
* @param token authentication token object
* @return map containing delegation token information
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,doDelegationTokenOperation,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:doDelegationTokenOperation(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation,java.lang.String,org.apache.hadoop.security.token.Token,boolean,java.lang.String)",288,356,"/**
 * Executes a delegation token operation.
 * @param url target URL for the operation
 * @param token authentication token
 * @param operation type of operation to perform
 * @param renewer user allowed to renew the token
 * @param dToken delegation token
 * @param hasResponse indicates if a response is expected
 * @param doAsUser user to act as
 * @return map containing response data or null
 * @throws IOException if an I/O error occurs
 * @throws AuthenticationException if authentication fails
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,cloneInto,"org.apache.hadoop.io.WritableUtils:cloneInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",236,239,"/**
 * Masks destination writable with source writable.
 * @param dst destination Writable object
 * @param src source Writable object
 * @throws IOException if an I/O error occurs
 */","* Make a copy of the writable object using serialization to a buffer.
   * @param dst the object to copy from
   * @param src the object to copy into, which is destroyed
   * @throws IOException raised on errors performing I/O.
   * @deprecated use ReflectionUtils.cloneInto instead.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.util.Collection)",483,495,"/**
* Recursively searches for a node within a given scope.
* @param scope search scope identifier
* @param excludedNodes nodes to exclude from the search
* @return Node object or null if not found
*/","* Randomly choose one node from <i>scope</i>.
   *
   * If scope starts with ~, choose one from the all nodes except for the
   * ones in <i>scope</i>; otherwise, choose one from <i>scope</i>.
   * If excludedNodes is given, choose a node that's not in excludedNodes.
   *
   * @param scope range of nodes from which a node will be chosen
   * @param excludedNodes nodes to be excluded from
   * @return the chosen node",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",886,892,"/**
 * Calls overloaded method with default null value.
 * @param reader Node to read from
 * @param nodes Array of Nodes
 * @param activeLen Length of active nodes
 */","* Sort nodes array by network distance to <i>reader</i>.
   * <p>
   * In a three-level topology, a node can be either local, on the same rack,
   * or on a different rack from the reader. Sorting the nodes based on network
   * distance from the reader reduces network traffic and improves
   * performance.
   * <p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistanceUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",929,936,"/**
 * Calls overloaded m1 with null as default value.
 * @param reader current node being processed
 * @param nodes array of nodes to process
 * @param activeLen length of active portion in nodes
 */","* Sort nodes array by network distance to <i>reader</i> with secondary sort.
   * <p> using network location. This is used when the reader
   * is not a datanode. Sorting the nodes based on network distance
   * from the reader reduces network traffic and improves
   * performance.
   * </p>
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket,long)",91,94,"/**
 * Constructs a SocketInputStream with a specified timeout.
 * @param socket the socket to read from
 * @param timeout the read timeout in milliseconds
 * @throws IOException if an I/O error occurs
 */","* Same as SocketInputStream(socket.getChannel(), timeout): <br><br>
   * 
   * Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @param timeout timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket),108,110,"/**
 * Constructs a SocketInputStream from a given socket.
 * @param socket the underlying socket to read data from
 * @throws IOException if an I/O error occurs during initialization
 */","* Same as SocketInputStream(socket.getChannel(), socket.getSoTimeout())
   * :<br><br>
   * 
   * Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream:<init>(java.net.Socket,long)",96,99,"/**
 * Constructs a SocketOutputStream with a specified socket and timeout.
 * @param socket The socket to be used for output.
 * @param timeout Timeout in milliseconds for blocking operations.
 * @throws IOException if an I/O error occurs.
 */","* Same as SocketOutputStream(socket.getChannel(), timeout):<br><br>
   * 
   * Create a new ouput stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @see SocketOutputStream#SocketOutputStream(WritableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @param timeout timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,write,org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:write(java.lang.String),198,205,"/**
 * Sends a message as a metric.
 * @param msg the message to send
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,<init>,org.apache.hadoop.net.NetworkTopology:<init>(),122,125,"/**
* Initializes network topology with root node.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getNodeForNetworkLocation,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeForNetworkLocation(org.apache.hadoop.net.Node),42,55,"/**
* Masks a node and retrieves its associated node group.
* @param node the input Node to be masked
* @return the Node representing the node group
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,add,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:add(org.apache.hadoop.net.Node),176,222,"/**
* Processes a node addition, ensuring it's not an inner node and updating network topology.
* @param node the node to be processed
*/","Add a leaf node
   * Update node counter &amp; rack counter if necessary
   * @param node node to be added; can be null
   * @exception IllegalArgumentException if add a node to a leave 
   *                                     or node to be added is not a leaf",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,add,org.apache.hadoop.net.InnerNodeImpl:add(org.apache.hadoop.net.Node),129,170,"/**
 * Adds a node to the tree, ensuring it's a valid descendant.
 * @param n Node to be added
 * @return true if added as a new leaf, false if replaced an existing child
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,doIO,"org.apache.hadoop.net.SocketIOWithTimeout:doIO(java.nio.ByteBuffer,int)",124,170,"/**
* Processes buffer with given operations.
* @param buf ByteBuffer to process
* @param ops operation set for processing
* @return result of processing or -1 if closed
* @throws IOException on I/O errors
*/","* Performs one IO and returns number of bytes read or written.
   * It waits up to the specified timeout. If the channel is 
   * not read before the timeout, SocketTimeoutException is thrown.
   * 
   * @param buf buffer for IO
   * @param ops Selection Ops used for waiting. Suggested values: 
   *        SelectionKey.OP_READ while reading and SelectionKey.OP_WRITE while
   *        writing. 
   *        
   * @return number of bytes read or written. negative implies end of stream.
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,connect,"org.apache.hadoop.net.SocketIOWithTimeout:connect(java.nio.channels.SocketChannel,java.net.SocketAddress,int)",182,228,"/**
 * Connects a socket channel to an endpoint with a specified timeout.
 * @param channel the socket channel to connect
 * @param endpoint the target address to connect to
 * @param timeout connection timeout in milliseconds
 * @throws IOException if connection fails or is interrupted
 */","* The contract is similar to {@link SocketChannel#connect(SocketAddress)} 
   * with a timeout.
   * 
   * @see SocketChannel#connect(SocketAddress)
   * 
   * @param channel - this should be a {@link SelectableChannel}
   * @param endpoint
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,waitForIO,org.apache.hadoop.net.SocketIOWithTimeout:waitForIO(int),242,248,"/**
* Applies mask operations to channel.
* @param ops bitwise OR of selected operations
* @throws IOException if operation times out or fails
*/","* This is similar to {@link #doIO(ByteBuffer, int)} except that it
   * does not perform any I/O. It just waits for the channel to be ready
   * for I/O as specified in ops.
   * 
   * @param ops Selection Ops used for waiting
   * 
   * @throws SocketTimeoutException 
   *         if select on the channel times out.
   * @throws IOException
   *         if any other I/O error occurs.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String),388,391,"/**
 * Calls overloaded method with default parameters.
 * @param strInterface interface string or null
 * @return result of the overloaded method
 * @throws UnknownHostException if network error occurs
 */","* Returns the default (first) host name associated by the default
   * nameserver with the address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0).
   *            Must not be null.
   * @return The default host name associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,"org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String)",404,408,"/**
 * Calls m1 with default secure flag.
 * @param strInterface network interface name
 * @param nameserver DNS server address
 * @return result of m1 call
 * @throws UnknownHostException if host cannot be determined
 */","* @return Returns the default (first) host name associated by the provided
   * nameserver with the address bound to the specified network interface.
   *
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @param nameserver
   *            The DNS host name
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkParameterValidity,org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[]),387,389,"/**
 * Calls m1 with arguments and usage string.
 * @param argv command line arguments array
 * @return result of m1 call
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,help,org.apache.hadoop.ha.HAAdmin:help(java.lang.String[]),508,510,"/**
 * Calls m1 with arguments and default usage message.
 * @param argv command-line arguments
 * @return result of m1 invocation
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getFilter,org.apache.hadoop.metrics2.impl.MetricsConfig:getFilter(java.lang.String),266,280,"/**
* Creates a metrics filter based on configuration.
* @param prefix configuration key prefix
* @return MetricsFilter object or null if configuration is disabled
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,create,org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String),98,101,"/**
* Creates a MetricsConfig with specified prefix.
* @param prefix configuration prefix
* @return MetricsConfig object configured with properties file
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,create,"org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String,java.lang.String[])",103,105,"/**
 * Configures metrics with specified prefix and file names.
 * @param prefix common prefix for metric keys
 * @param fileNames array of configuration file names
 * @return MetricsConfig object configured with given parameters
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,clear,org.apache.hadoop.ipc.RetryCache:clear(org.apache.hadoop.ipc.RetryCache),398,403,"/**
* Updates and processes cache entries.
* @param cache RetryCache instance to operate on
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getMetrics,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMetrics(org.apache.hadoop.metrics2.impl.MetricsCollectorImpl,boolean)",196,210,"/**
* Collects and processes metrics.
* @param builder MetricsCollectorImpl instance to collect metrics into
* @param all flag to include all metrics or only filtered ones
* @return Iterable of processed MetricsRecordImpl objects
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,getMBeanName,"org.apache.hadoop.metrics2.util.MBeans:getMBeanName(java.lang.String,java.lang.String,java.util.Map)",151,169,"/**
* Creates an ObjectName for a metrics system.
* @param serviceName name of the service
* @param nameName name component
* @param additionalParameters map with additional key-value pairs
* @return ObjectName or null if creation fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reattachMetrics,org.apache.hadoop.security.UserGroupInformation:reattachMetrics(),259,261,"/**
 * Calls metrics function m1 from UgiMetrics.
 */",* Reattach the class's metrics to a new metric system.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopMetricsMBeans,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopMetricsMBeans(),341,346,"/**
* Iterates over MetricsSourceAdapters and calls their m1 method.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,stop,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stop(),212,214,"/**
 * Synchronizes execution of m1().
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,stop,org.apache.hadoop.ipc.DecayRpcScheduler:stop(),1156,1161,"/**
* Records metrics for namespace operations.
* Proxies calls to multiple metric systems.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,<init>,"org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",61,64,"/**
* Constructs a MutableInverseQuantiles instance.
* @param name descriptive name of the metric
* @param description detailed description of the metric
* @param sampleName name of the sample
* @param valueName name of the value
* @param intervalSecs time interval in seconds
*/","* Instantiates a new {@link MutableInverseQuantiles} for a metric that rolls itself
   * over on the specified time interval.
   *
   * @param name          of the metric
   * @param description   long-form textual description of the metric
   * @param sampleName    type of items in the stream (e.g., ""Ops"")
   * @param valueName     type of the values
   * @param intervalSecs  rollover interval (in seconds) of the estimator",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newQuantiles,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",217,228,"/**
 * Creates a mutable quantiles metric.
 * @param name unique metric identifier
 * @param desc metric description
 * @param sampleName name of the sample
 * @param valueName name of the value
 * @param interval sampling interval
 * @return MutableQuantiles object
 * @throws MetricsException if interval is not positive
 */","* Create a mutable metric that estimates quantiles of a stream of values
   * @param name of the metric
   * @param desc metric description
   * @param sampleName of the metric (e.g., ""Ops"")
   * @param valueName of the metric (e.g., ""Time"" or ""Latency"")
   * @param interval rollover interval of estimator in seconds
   * @return a new quantile estimator object
   * @throws MetricsException if interval is not a positive integer",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,create,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:create(),989,991,"/**
* Returns an instance of DelegationTokenSecretManagerMetrics.
* @return Metrics instance for delegation token secret manager
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,create,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:create(org.apache.hadoop.ipc.RetryCache),52,55,"/**
* Creates metrics for a retry cache.
* @param cache the RetryCache instance to monitor
* @return RetryCacheMetrics object representing the cache's metrics
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newStat,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",279,282,"/**
 * Creates a mutable statistic with default enabled flag.
 * @param name statistic name
 * @param desc description of the statistic
 * @param sampleName name of the sample
 * @param valueName name of the value
 * @return MutableStat object
 */","* Create a mutable metric with stats
   * @param name  of the metric
   * @param desc  metric description
   * @param sampleName  of the metric (e.g., ""Ops"")
   * @param valueName   of the metric (e.g., ""Time"" or ""Latency"")
   * @return a new mutable metric object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,addMetricIfNotExists,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:addMetricIfNotExists(java.lang.String),163,172,"/**
* Retrieves or creates a MutableRate metric.
* @param name metric identifier
* @return MutableRate object associated with the name
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean,boolean)",314,329,"/**
* Retrieves or creates a mutable rate metric.
* @param name metric name
* @param desc metric description
* @param extended flag for extended stats
* @param returnExisting if true, returns existing metric if present
* @return MutableRate object
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getMetrics,"org.apache.hadoop.metrics2.source.JvmMetrics:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",143,155,"/**
* Collects and records JVM metrics.
* @param collector MetricsCollector instance for recording
* @param all flag to include all metrics
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,initRegistry,org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:initRegistry(java.lang.Object),98,127,"/**
* Retrieves or creates a MetricsRegistry for the given source.
* @param source object to inspect for MetricsRegistry
* @return MetricsRegistry instance associated with the source
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String)",391,393,"/**
 * Creates a metrics registry entry with default visibility.
 * @param name unique metric name
 * @param description brief description of the metric
 * @param value initial metric value
 * @return MetricsRegistry object
 */","* Add a tag to the metrics
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @return the registry (for keep adding tags)",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,add,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:add(java.lang.String,long)",212,214,"/**
 * Delegates metrics recording to innerMetrics.
 * @param name metric name
 * @param value metric value
 */","* @param name
   *          name of metric
   * @param value
   *          value of metric",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,addQueueTime,"org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addQueueTime(int,long)",88,90,"/**
* Updates RPC queue rate with priority and time.
* @param priority queue priority level
* @param queueTime time spent in the queue
*/","* Instrument a Call queue time based on its priority.
   *
   * @param priority of the RPC call
   * @param queueTime of the RPC call in the queue of the priority",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,addProcessingTime,"org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addProcessingTime(int,long)",98,100,"/**
* Records RPC processing time by priority.
* @param priority level of the operation
* @param processingTime time taken for processing in milliseconds
*/","* Instrument a Call processing time based on its priority.
   *
   * @param priority of the RPC call
   * @param processingTime of the RPC call in the queue of the priority",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addProcessingTime(java.lang.String,long)",87,89,"/**
 * Records RPC call metrics.
 * @param rpcCallName name of the RPC call
 * @param processingTime time taken to process the call in milliseconds
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addDeferredProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addDeferredProcessingTime(java.lang.String,long)",91,93,"/**
 * Records RPC rate metrics.
 * @param name name of the RPC
 * @param processingTime time taken to process the RPC in milliseconds
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addOverallProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addOverallProcessingTime(java.lang.String,long)",100,102,"/**
* Records RPC call processing time.
* @param rpcCallName name of the RPC call
* @param overallProcessingTime time taken for the call in milliseconds
*/","* Add an overall RPC processing time sample.
   * @param rpcCallName of the RPC call
   * @param overallProcessingTime  the overall RPC processing time",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableStat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",138,163,"/**
 * Updates metrics record with sample data.
 * @param builder MetricsRecordBuilder to update
 * @param all Whether to include all metrics
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,newForMethod,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)",94,105,"/**
* Creates or retrieves a mutable metric for a method with an annotation.
* @param source the source object of the metric
* @param method the method associated with the metric
* @param annotation the metric annotation
* @param registry the metrics registry to register the metric
* @return the created or retrieved MutableMetric instance
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getMetrics,"org.apache.hadoop.ipc.DecayRpcScheduler:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",1009,1027,"/**
* Collects metrics using the provided collector.
* @param collector MetricsCollector instance for collecting metrics
* @param all Flag indicating whether to collect all metrics
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroups,org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroups(java.lang.String),1580,1583,"/**
 * Masks sensitive information in user data.
 * @param user input string containing user data
 * @return list of masked strings
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKrb5File,org.apache.hadoop.security.KDiag:validateKrb5File(),561,588,"/**
 * Configures Kerberos settings based on system properties and environment variables.
 * Throws IOException if an error occurs during configuration.
 */","* Locate the {@code krb5.conf} file and dump it.
   *
   * No-op on windows.
   * @throws IOException problems reading the file.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verify,"org.apache.hadoop.security.KDiag:verify(boolean,java.lang.String,java.lang.String,java.lang.Object[])",963,981,"/**
* Evaluates condition; logs failure if false.
* @param condition boolean to evaluate
* @param category log category
* @param message log message template
* @param args message arguments
* @return true if condition is met, otherwise false and logs error
*/","* Assert that a condition must hold.
   *
   * If not, an exception is raised, or, if {@link #nofail} is set,
   * an error will be logged and the method return false.
   *
   * @param condition condition which must hold
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @return true if the verification succeeded, false if it failed but
   * an exception was not raised.
   * @throws KerberosDiagsFailure containing the formatted text
   *         if the condition was met",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,failif,"org.apache.hadoop.security.KDiag:failif(boolean,java.lang.String,java.lang.String,java.lang.Object[])",1034,1042,"/**
* Logs a diagnostic message if condition is true.
* @param condition boolean to check if logging should occur
* @param category message category
* @param message log message format
* @param args arguments for the message format
*/","* Conditional failure with string formatted arguments.
   * There is no chek for the {@link #nofail} value.
   * @param condition failure condition
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @throws KerberosDiagsFailure containing the formatted text
   *         if the condition was met",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createRemoteUser,org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String),1438,1442,"/**
* Creates UserGroupInformation for a given user.
* @param user username for which to create UserGroupInformation
* @return UserGroupInformation object with SIMPLE authentication method
*/","* Create a user from a login name. It is intended to be used for remote
   * users in RPC, since it won't have any credentials.
   * @param user the full user principal name, must not be empty or null
   * @return the UserGroupInformation for the remote user.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuthorizedUgi,org.apache.hadoop.ipc.Server$Connection:getAuthorizedUgi(java.lang.String),2161,2176,"/**
 * Retrieves user group information based on authorization ID.
 * @param authorizedId the authorization identifier string
 * @return UserGroupInformation object
 * @throws InvalidToken if token is invalid
 * @throws AccessControlException if access control fails
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,verifyToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])",575,582,"/**
* Validates token with provided password.
* @param identifier unique token identifier
* @param password user-provided password
* @throws InvalidToken if password does not match stored password
*/","* Verifies that the given identifier and password are valid and match.
   * @param identifier Token identifier.
   * @param password Password in the token.
   * @throws InvalidToken InvalidToken.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:init(),143,152,"/**
* Starts managed secret manager if enabled.
* Throws runtime exception on failure to start.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,startThreads,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:startThreads(),243,348,"/**
 * Initializes ZK clients and sets up caches for sequence numbers, key IDs, and tokens.
 * Throws IOException on failure.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,run,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover:run(),829,859,"/**
 * Runs a thread to remove expired delegation tokens and update master keys.
 * Logs start, exceptions, and updates intervals. Exits on unexpected errors.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(java.nio.ByteBuffer),367,384,"/**
* Reads data into a ByteBuffer.
* @param dst destination ByteBuffer
* @return number of bytes read or -1 on error
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,spawnAutoRenewalThreadForKeytab,org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForKeytab(),905,918,"/**
* Handles Kerberos ticket renewal process.
* Returns early if conditions in m1() or m2() fail.
*/","* Spawn a thread to do periodic renewals of kerberos credentials from a
   * keytab file.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,<init>,org.apache.hadoop.fs.shell.Ls:<init>(org.apache.hadoop.conf.Configuration),122,124,"/**
 * Initializes Ls with given configuration.
 * @param conf Configuration object to be used
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,<init>,"org.apache.hadoop.fs.shell.Count:<init>(java.lang.String[],int,org.apache.hadoop.conf.Configuration)",118,122,"/**
* Deprecated constructor initializes command arguments.
* @param cmd array of command line arguments
* @param pos starting position in the array
* @param conf configuration settings
*/","Constructor
   * @deprecated invoke via {@link FsShell}
   * @param cmd the count command
   * @param pos the starting index of the arguments
   * @param conf configuration",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,<init>,org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:<init>(org.apache.hadoop.fs.FileSystem),495,497,"/**
 * Initializes a new instance of TargetFileSystem.
 * @param fs underlying file system to be used
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,org.apache.hadoop.fs.ChecksumFileSystem:<init>(org.apache.hadoop.fs.FileSystem),79,81,"/**
 * Initializes a ChecksumFileSystem with the given FileSystem.
 * @param fs the underlying file system to use
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,newShellInstance,org.apache.hadoop.fs.FsShell:newShellInstance(),398,400,"/**
 * Creates and returns a new instance of FsShell.
 * @return A new FsShell object
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,<init>,org.apache.hadoop.fs.shell.FsCommand:<init>(),76,76,"/**
 * Default constructor for FsCommand class.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,39,"/**
 * Initializes an erasure encoder with given options.
 * @param options configuration settings for the encoder
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),44,46,"/**
 * Constructs an HHXORErasureEncoder with specified options.
 * @param options configuration settings for the encoder
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Initializes a new instance of DummyErasureEncoder.
 * @param options configuration settings for the encoder
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,38,"/**
 * Constructs a new XORErasureEncoder with specified options.
 * @param options configuration settings for erasure coding
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Constructs a DummyErasureDecoder with specified options.
 * @param options configuration settings for erasure coding
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,38,"/**
 * Initializes an XOR erasure decoder with given options.
 * @param options configuration settings for the decoder
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,39,"/**
 * Constructs an RSErasureDecoder with specified options.
 * @param options configuration settings for erasure coding
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),45,47,"/**
* Initializes an HHXORErasureDecoder with given options.
* @param options configuration settings for erasure decoding
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,getDeserializer,org.apache.hadoop.io.serializer.WritableSerialization:getDeserializer(java.lang.Class),120,124,"/**
* Returns a deserializer for the given Writable class.
* @param c Writable class type
* @return Deserializer instance for the specified class
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMaps,org.apache.hadoop.security.ShellBasedIdMapping:updateMaps(),343,357,"/**
* Masks data based on conditions.
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapIncr,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(java.lang.String,boolean)",465,504,"/**
* Masks user or group name based on OS and updates mapping.
* @param name the username or groupname to mask
* @param isGrp true if masking a group, false for user
* @throws IOException if an I/O error occurs during processing
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapIncr,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(int,boolean)",506,540,"/**
 * Updates user or group mapping based on OS.
 * @param id unique identifier for user/group
 * @param isGrp true if updating a group, false for user
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,connectToZooKeeper,org.apache.hadoop.ha.ActiveStandbyElector:connectToZooKeeper(),723,743,"/**
* Initializes and configures a ZooKeeper instance.
* @return Configured ZooKeeper object
* @throws IOException if connection fails
* @throws KeeperException for ZooKeeper errors
*/","* Get a new zookeeper client instance. protected so that test class can
   * inherit and mock out the zookeeper instance
   * 
   * @return new zookeeper client instance
   * @throws IOException raised on errors performing I/O.
   * @throws KeeperException zookeeper connectionloss exception",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,"org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean,boolean)",1272,1289,"/**
* Performs Kerberos authentication and ticket renewal.
* @param checkTGT flag to check for TGT validity
* @param ignoreLastLoginTime flag to bypass last login time check
* @throws IOException if an I/O error occurs during authentication
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache(boolean),1322,1332,"/**
* Masks user login based on conditions.
* @param ignoreLastLoginTime flag to ignore last login time
* @throws IOException if an error occurs during processing
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",29,32,"/**
* Constructs a CryptoFSDataOutputStream.
* @param out underlying FSDataOutputStream
* @param codec encryption codec
* @param bufferSize buffer size for encryption
* @param key encryption key
* @param iv initialization vector
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",86,89,"/**
* Constructs a CryptoOutputStream with specified parameters.
* @param out underlying output stream
* @param codec cryptographic codec for encryption
* @param bufferSize size of the buffer
* @param key encryption key
* @param iv initialization vector
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createEncryptor(),72,76,"/**
* Creates an encryptor using OpenSSL CTR mode.
* @throws GeneralSecurityException if encryption setup fails
* @return Encryptor instance configured for encryption
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createDecryptor(),78,82,"/**
* Creates a decryptor using OpenSSL CTR mode.
* @return Decryptor instance for decryption operations
* @throws GeneralSecurityException if initialization fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite)",130,134,"/**
* Initializes an OpenSSL CTR cipher.
* @param mode encryption/decryption mode
* @param suite cryptographic cipher suite
* @throws GeneralSecurityException if cipher initialization fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONEncKeyVersions,"org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersions(java.lang.String,java.util.List)",143,155,"/**
* Masks key values in a list.
* @param keyName name of the key to mask
* @param valueList list of values containing key-value pairs
* @return list of EncryptedKeyVersion objects
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getEncKeyQueueSize,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getEncKeyQueueSize(java.lang.String),966,969,"/**
 * Masks a function with the given key name.
 * @param keyName the name of the key to be used for masking
 * @return an integer result from masking operation
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getNext,org.apache.hadoop.crypto.key.kms.ValueQueue:getNext(java.lang.String),292,295,"/**
* Retrieves and processes data using specified key.
* @param keyName unique identifier for data retrieval
* @return processed result of type E
*/","* This removes the value currently at the head of the Queue for the
   * provided key. Will immediately fire the Queue filler function if key
   * does not exist.
   * If Queue exists but all values are drained, It will ask the generator
   * function to add 1 value to Queue and then drain it.
   * @param keyName String key name
   * @return E the next value in the Queue
   * @throws IOException raised on errors performing I/O.
   * @throws ExecutionException executionException.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,drain,org.apache.hadoop.crypto.key.kms.KMSClientProvider:drain(java.lang.String),961,964,"/**
 * Delegates to encKeyVersionQueue to process key.
 * @param keyName name of the encryption key
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,internalQueueCall,org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call),3106,3109,"/**
 * Calls another overloaded method with default retry flag.
 * @param call the Call object to process
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByName,org.apache.hadoop.security.SecurityUtil:getByName(java.lang.String),569,588,"/**
* Resolves hostname to InetAddress, logging slow lookups.
* @param hostname the hostname to resolve
* @return resolved InetAddress
* @throws UnknownHostException if resolution fails
*/","* Resolves a host subject to the security requirements determined by
   * hadoop.security.token.service.use_ip. Optionally logs slow resolutions.
   * 
   * @param hostname host or ip to resolve
   * @return a resolved host
   * @throws UnknownHostException if the host doesn't exist",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,run,org.apache.hadoop.util.JvmPauseMonitor$Monitor:run(),181,208,"/**
* Monitors JVM pause times and logs warnings/info based on thresholds.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(org.apache.hadoop.ipc.VersionedProtocol,java.lang.String,long,int)",241,254,"/**
* Generates protocol signature for a given versioned protocol.
* @param server VersionedProtocol instance
* @param protocol protocol name
* @param clientVersion client's version of the protocol
* @param clientMethodsHash hash of client methods
* @return ProtocolSignature object
* @throws IOException if class loading fails
*/","* Get a server protocol's signature
   *
   * @param server server implementation
   * @param protocol server protocol
   * @param clientVersion client's version
   * @param clientMethodsHash client's protocol's hash code
   * @return the server protocol's signature
   * @throws IOException if any error occurs",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)",74,86,"/**
* Validates and returns protocol signature.
* @param protocol requested protocol name
* @param clientVersion client version number
* @param clientMethodsHash hash of client methods
* @return ProtocolSignature object or throws IOException if protocol is unknown
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)",186,198,"/**
* Validates and returns protocol signature.
* @param protocol requested protocol name
* @param clientVersion client version number
* @param clientMethodsHash hash of client methods
* @return ProtocolSignature object or throws IOException if invalid
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolSignature(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto)",70,104,"/**
 * Handles protocol signature requests.
 * @param controller RPC controller
 * @param request protocol signature request
 * @return response with protocol signatures or empty if none found
 * @throws ServiceException on class not found
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ExternalCall.java,<init>,org.apache.hadoop.ipc.ExternalCall:<init>(java.security.PrivilegedExceptionAction),36,38,"/**
 * Initializes an ExternalCall with a given action.
 * @param action the PrivilegedExceptionAction to be executed
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Listener:run(),1551,1600,"/**
* Handles server operations, including accepting connections and processing I/O events.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,add,org.apache.hadoop.ipc.FairCallQueue:add(java.lang.Object),194,213,"/**
* Handles element processing with overflow checks.
* @param e element to process
* @return true if processed, throws exception on overflow
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,put,org.apache.hadoop.ipc.FairCallQueue:put(java.lang.Object),215,222,"/**
* Masks an element by adding it to a queue.
* @param e the element to be masked
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolProxy.java,isMethodSupported,"org.apache.hadoop.ipc.ProtocolProxy:isMethodSupported(java.lang.String,java.lang.Class[])",95,117,"/**
* Checks if a method is supported.
* @param methodName name of the method to check
* @param parameterTypes types of the method parameters
* @return true if method is supported, false otherwise
*/","* Check if a method is supported by the server or not.
   * 
   * @param methodName a method's name in String format
   * @param parameterTypes a method's parameter types
   * @return true if the method is supported by the server
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,checkRpcHeaders,org.apache.hadoop.ipc.Server$Connection:checkRpcHeaders(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto),2830,2852,"/**
 * Validates RPC request header.
 * @param header RPC request header to validate
 * @throws RpcServerException if validation fails
 */","* Verify RPC header is valid
     * @param header - RPC request header
     * @throws RpcServerException - header contains invalid values",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer:<init>(),33,35,"/**
 * Initializes a new ResponseBuffer with default capacity.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,forceDecay,org.apache.hadoop.ipc.DecayRpcScheduler:forceDecay(),822,825,"/**
 * Invokes method m1 for testing purposes.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,getPriorityLevel,org.apache.hadoop.ipc.CallQueueManager:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),265,270,"/**
 * Delegates to DecayRpcScheduler if applicable.
 * @param user the UserGroupInformation object
 * @return result of m1 call or 0 if not a DecayRpcScheduler
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getReturnMessage,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)",301,323,"/**
 * Masks a function call and processes the response.
 * @param method the Method object to be invoked
 * @param buf buffer containing data for processing
 * @return Message object representing the processed response
 * @throws ServiceException if an error occurs during processing
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,newInstance,"org.apache.hadoop.ipc.RpcWritable$Buffer:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",175,188,"/**
* Creates and configures an instance of the specified class.
* @param valueClass Class type to instantiate
* @param conf Configuration settings
* @return Configured instance of T
* @throws IOException if instantiation fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getMessage,"org.apache.hadoop.ipc.Server$Connection:getMessage(org.apache.hadoop.thirdparty.protobuf.Message,org.apache.hadoop.ipc.RpcWritable$Buffer)",3046,3057,"/**
* Masks a message using a buffer.
* @param message the input message to be masked
* @param buffer the buffer used for masking
* @return the masked message as type T
* @throws RpcServerException if an error occurs during masking
*/","* Decode the a protobuf from the given input stream 
     * @return Message - decoded protobuf
     * @throws RpcServerException - deserialization failed",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getReturnMessage,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)",311,333,"/**
* Processes a method call and returns a response message.
* @param method the method to be processed
* @param buf buffer containing request data
* @return the response message
* @throws ServiceException if processing fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponse,"org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3549,3562,"/**
* Handles RPC call response.
* @param call the RPC call object
* @param header the response header
* @param rv the response value
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,take,org.apache.hadoop.ipc.FairCallQueue:take(),292,296,"/**
 * Executes masked function with synchronization.
 * @throws InterruptedException if thread is interrupted during execution
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,poll,"org.apache.hadoop.ipc.FairCallQueue:poll(long,java.util.concurrent.TimeUnit)",298,301,"/**
* Attempts to acquire semaphore with a timeout.
* @param timeout time to wait before giving up
* @param unit unit of timeout time
* @return result of m2 if successful, otherwise null
* @throws InterruptedException if interrupted while waiting
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,poll,org.apache.hadoop.ipc.FairCallQueue:poll(),307,310,"/**
 * Returns an element if condition is met; otherwise, returns null.
 * @return element of type E or null
 */","* poll() provides no strict consistency: it is possible for poll to return
   * null even though an element is in the queue.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Responder:run(),1711,1725,"/**
* Starts and stops a server function, handling exceptions.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doSaslReply,org.apache.hadoop.ipc.Server$Connection:doSaslReply(org.apache.hadoop.thirdparty.protobuf.Message),2420,2426,"/**
 * Processes a message using SASL authentication.
 * @param message the input message to process
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doSaslReply,org.apache.hadoop.ipc.Server$Connection:doSaslReply(java.lang.Exception),2428,2433,"/**
* Handles authorization failure by logging and rethrowing an exception.
* @param ioe the IOException to be handled
* @throws IOException if an error occurs during processing
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupBadVersionResponse,org.apache.hadoop.ipc.Server$Connection:setupBadVersionResponse(int),2636,2665,"/**
* Handles version mismatch with client.
* @param clientVersion client's IPC version
* @throws IOException if communication fails
*/","* Try to set up the response to indicate that the client version
     * is incompatible with the server. This can contain special-case
     * code to speak enough of past IPC protocols to pass back
     * an exception to the caller.
     * @param clientVersion the version the caller is using 
     * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupHttpRequestOnIpcPortResponse,org.apache.hadoop.ipc.Server$Connection:setupHttpRequestOnIpcPortResponse(),2667,2672,"/**
* Processes an HTTP request response.
* @throws IOException if an I/O error occurs during processing
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CombinedIPWhiteList.java,<init>,"org.apache.hadoop.util.CombinedIPWhiteList:<init>(java.lang.String,java.lang.String,long)",31,43,"/**
 * Initializes CombinedIPWhiteList with fixed and optional variable white lists.
 * @param fixedWhiteListFile path to the fixed white list file
 * @param variableWhiteListFile path to the variable white list file (optional)
 * @param cacheExpiryInSeconds expiry time for caching variable list
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CombinedIPList.java,<init>,"org.apache.hadoop.util.CombinedIPList:<init>(java.lang.String,java.lang.String,long)",33,44,"/**
* Initializes CombinedIPList with fixed and optional variable blacklists.
* @param fixedBlackListFile path to the fixed blacklist file
* @param variableBlackListFile path to the variable blacklist file (can be null)
* @param cacheExpiryInSeconds expiry time for variable blacklist cache in seconds
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,reload,org.apache.hadoop.util.FileBasedIPList:reload(),67,69,"/**
 * Creates and returns a new FileBasedIPList instance.
 * @return FileBasedIPList object initialized with fileName
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getVirtualMemorySize,org.apache.hadoop.util.SysInfoLinux:getVirtualMemorySize(),603,606,"/**
* Returns masked value combining base and swap size.
* @return long representing combined mask value
*/",{@inheritDoc},,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.InputStream)",495,499,"/**
 * Constructs an FSDataBoundedInputStream.
 * @param fs FileSystem instance
 * @param file Path to the file
 * @param in Underlying input stream
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
* Opens an input stream for the specified file path.
* @param path file path to open
* @param bufferSize buffer size for reading
* @return FSDataInputStream for the file
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)",1109,1112,"/**
* Creates a HarFSDataInputStream for reading from a specified range in a file.
* @param fs FileSystem instance
* @param p Path to the file
* @param start Starting position in the file
* @param length Length of the data to read
* @param bufsize Buffer size
* @throws IOException if an I/O error occurs
*/","* constructors for har input stream.
     * @param fs the underlying filesystem
     * @param p The path in the underlying filesystem
     * @param start the start position in the part file
     * @param length the length of valid data in the part file
     * @param bufsize the buffer size
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,read,"org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int)",217,220,"/**
* Reads data into a ByteBuffer from the pool.
* @param bufferPool source of ByteBuffers
* @param maxLength maximum length to read
* @return ByteBuffer containing read data
* @throws IOException if I/O error occurs
* @throws UnsupportedOperationException if operation is not supported
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evictExpiredEntries,org.apache.hadoop.util.LightWeightCache:evictExpiredEntries(),164,175,"/**
* Masks entries in the queue based on eviction criteria.
*/",Evict expired entries.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evictEntries,org.apache.hadoop.util.LightWeightCache:evictEntries(),178,184,"/**
 * Masks elements by removing excess beyond size limit.
 * @param sizeLimit maximum allowed size
 */",Evict entries in order to enforce the size limit of the cache.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean)",408,410,"/**
* Calls overloaded method with default values.
* @param qOption first option flag
* @param hOption second option flag
* @param xOption third option flag
* @return result of the overloaded method call
*/","Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output is to be used
   * @param xOption a flag indicating if calculation from snapshots is to be
   *                included in the output
   * @return the string representation of the object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,java.util.List)",423,426,"/**
 * Calls overloaded method with default sort option.
 * @param qOption query option flag
 * @param hOption history option flag
 * @param tOption test option flag
 * @param types list of storage types
 * @return result string from overloaded method
 */","* Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output if to be used
   * @param tOption a flag indicating if display quota by storage types
   * @param types Storage types to display
   * @return the string representation of the object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,"org.apache.hadoop.fs.QuotaUsage:toString(boolean,boolean,java.util.List)",321,327,"/**
* Masks storage types based on options.
* @param hOption horizontal processing option
* @param tOption type-specific processing option
* @param types list of storage types to process
* @return masked string result
*/","* Return the string representation of the object in the output format.
   * if hOption is false file sizes are returned in bytes
   * if hOption is true file sizes are returned in human readable
   *
   * @param hOption a flag indicating if human readable output if to be used
   * @param tOption type option.
   * @param types storage types.
   * @return the string representation of the object.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read,org.apache.hadoop.fs.FSInputChecker:read(),150,159,"/**
* Reads and returns the next byte from the buffer.
* @return next byte as an integer, or -1 if end of stream is reached
* @throws IOException if I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read1,"org.apache.hadoop.fs.FSInputChecker:read1(byte[],int,int)",251,275,"/**
 * Reads bytes from buffer into array.
 * @param b destination byte array
 * @param off offset in the array to start storing bytes
 * @param len number of bytes to read
 * @return number of bytes read or -1 if end of stream
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readFields,org.apache.hadoop.io.ObjectWritable$NullInstance:readFields(java.io.DataInput),114,125,"/**
* Reads class name from input and sets declaredClass.
* @param in DataInput stream containing class name
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,readFields,org.apache.hadoop.io.ArrayPrimitiveWritable:readFields(java.io.DataInput),205,251,"/**
 * Reads and initializes an array from input.
 * @param in DataInput source
 * @throws IOException if reading fails or unsupported type encountered
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.List),98,106,"/**
* Waits for all futures to complete or throws an exception.
* @param futures list of CompletableFuture objects
* @throws IOException if any future completes exceptionally
*/","* Wait for a list of futures to complete. If the list is empty,
   * return immediately.
   *
   * @param futures list of futures.
   * @param <T> Generics Type T.
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,maybeAwaitCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:maybeAwaitCompletion(java.util.concurrent.CompletableFuture),152,157,"/**
* Masks a given CompletableFuture by invoking a helper method.
* @param future the CompletableFuture to be masked, can be null
* @throws IOException if an I/O error occurs during masking
*/","* Block awaiting completion for any non-null future passed in;
   * No-op if a null arg was supplied.
   * @param future future
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(byte[],int,int,long)",123,138,"/**
 * Updates CRC from byte array.
 * @param crcBuffer source byte array
 * @param offset starting index in the array
 * @param length number of bytes to process
 * @param bytesPerCrc bytes per CRC calculation
 * @throws IOException if length is not a multiple of CRC_SIZE_BYTES
 */","* Composes length / CRC_SIZE_IN_BYTES more CRCs from crcBuffer, with
   * each CRC expected to correspond to exactly {@code bytesPerCrc} underlying
   * data bytes.
   *
   * @param crcBuffer crcBuffer.
   * @param offset offset.
   * @param length must be a multiple of the expected byte-size of a CRC.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(java.io.DataInputStream,long,long)",150,157,"/**
 * Reads and processes checksum values from input stream.
 * @param checksumIn input stream for reading checksums
 * @param numChecksumsToRead number of checksums to read
 * @param bytesPerCrc number of bytes per CRC
 * @throws IOException if an I/O error occurs
 */","* Composes {@code numChecksumsToRead} additional CRCs into the current digest
   * out of {@code checksumIn}, with each CRC expected to correspond to exactly
   * {@code bytesPerCrc} underlying data bytes.
   *
   * @param checksumIn checksumIn.
   * @param numChecksumsToRead numChecksumsToRead.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,hasNext,org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:hasNext(),780,783,"/**
 * Overrides default function mask.
 * @throws IOException if an I/O error occurs
 * @return result of m1 method call
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,newInstance,"org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",124,127,"/**
* Instantiates an object of type T using configuration.
* @param theClass class type to instantiate
* @param conf configuration settings
* @return instance of T or null if instantiation fails
*/","Create an object for the given class and initialize it from conf
   * 
   * @param theClass class of which an object is created
   * @param conf Configuration
   * @param <T> Generics Type T.
   * @return a new object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getKeyClass,org.apache.hadoop.io.MapFile$Reader:getKeyClass(),465,465,"/**
 * Returns the class of the underlying data.
 * @return Class object representing the type of data
 */","* Returns the class of keys in this file.
     *
     * @return keyClass.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getValueClass,org.apache.hadoop.io.MapFile$Reader:getValueClass(),472,472,"/**
* Returns the class type of the data object.
* @return Class object representing the type of data
*/","* Returns the class of values in this file.
     *
     * @return Value Class.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,run,org.apache.hadoop.util.FindClass:run(java.lang.String[]),310,335,"/**
 * Handles actions based on input arguments.
 * @param args array containing action and name
 * @return result of the operation
 */","* Run the class/resource find or load operation
   * @param args command specific arguments.
   * @return the outcome
   * @throws Exception if something went very wrong",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",78,84,"/**
* Initializes a FutureDataInputStreamBuilderImpl with file context and path.
* @param fc FileContext object for file operations
* @param path Path to the file
* @throws IOException if an I/O error occurs
*/","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param path path.
   * @throws IOException failure",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,"org.apache.hadoop.util.DataChecksum:newDataChecksum(byte[],int)",160,181,"/**
* Creates a DataChecksum from a byte array.
* @param bytes source data array
* @param offset starting index in the byte array
* @return DataChecksum object or throws InvalidChecksumSizeException if creation fails
*/","* Creates a DataChecksum from HEADER_LEN bytes from arr[offset].
   *
   * @param bytes bytes.
   * @param offset offset.
   * @return DataChecksum of the type in the array or null in case of an error.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,org.apache.hadoop.util.DataChecksum:newDataChecksum(java.io.DataInputStream),192,202,"/**
 * Creates a DataChecksum from input stream.
 * @param in DataInputStream to read checksum data
 * @return DataChecksum object or throws exception if creation fails
 */","* This constructs a DataChecksum by reading HEADER_LEN bytes from input
   * stream <i>in</i>.
   *
   * @param in data input stream.
   * @throws IOException raised on errors performing I/O.
   * @return DataChecksum by reading HEADER_LEN
   *         bytes from input stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell:run(),951,960,"/**
* Executes function with rate limiting and system configuration.
* @throws IOException on process execution failure
*/","* Check to see if a command needs to be executed and execute if needed.
   *
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>(java.lang.String),148,210,"/**
 * Initializes a DynamicWrappedIO instance by loading methods from a specified class.
 * @param classname name of the class to wrap
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>(java.lang.String),228,346,"/**
* Initializes DynamicWrappedStatistics by loading class and methods.
* @param classname name of the class to wrap
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_aggregate,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)",502,507,"/**
* Aggregates I/O statistics from a snapshot.
* @param snapshot Serializable object containing data
* @param statistics nullable Object for additional stats
* @return true if aggregation is successful
* @throws UnsupportedOperationException if operation not supported
*/","* Aggregate an existing {@code IOStatisticsSnapshot} with
   * the supplied statistics.
   * @param snapshot snapshot to update
   * @param statistics IOStatistics to add
   * @return true if the snapshot was updated.
   * @throws IllegalArgumentException if the {@code statistics} argument is not
   * null but not an instance of IOStatistics, or if  {@code snapshot} is invalid.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create(),514,518,"/**
* Throws UnsupportedOperationException.
* @return Nothing as it always throws an exception
*/","* Create a new {@code IOStatisticsSnapshot} instance.
   * @return an empty IOStatisticsSnapshot.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create(java.lang.Object),527,532,"/**
* Applies mask to source object.
* @param source input object to be masked
* @return Serializable result of masking operation
* @throws UnsupportedOperationException if operation is not supported
* @throws ClassCastException if casting fails
*/","* Create a new {@code IOStatisticsSnapshot} instance.
   * @param source optional source statistics
   * @return an IOStatisticsSnapshot.
   * @throws ClassCastException if the {@code source} is not valid.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_toJsonString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable),541,545,"/**
* Converts snapshot to JSON string.
* @param snapshot data to be converted
* @return JSON representation of the snapshot
* @throws UncheckedIOException if I/O error occurs
* @throws UnsupportedOperationException if operation is not supported
*/","* Save IOStatisticsSnapshot to a JSON string.
   * @param snapshot statistics; may be null or of an incompatible type
   * @return JSON string value or null if source is not an IOStatisticsSnapshot
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_fromJsonString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String),554,558,"/**
* Processes JSON string to create a serializable object.
* @param json input JSON data as string
* @return Serializable object or null if processing fails
*/","* Load IOStatisticsSnapshot from a JSON string.
   * @param json JSON string value.
   * @return deserialized snapshot.
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_load,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",568,573,"/**
* Applies mask to file system path.
* @param fs target file system
* @param path file path to mask
* @return Serializable result of masking operation
* @throws UncheckedIOException if an I/O error occurs
* @throws UnsupportedOperationException if operation is not supported
*/","* Load IOStatisticsSnapshot from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return the loaded snapshot
   * @throws UncheckedIOException Any IO exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_retrieve,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object),581,585,"/**
* Applies mask to source object.
* @param source object to be masked
* @return Serializable result of masking operation
*/","* Extract the IOStatistics from an object in a serializable form.
   * @param source source object, may be null/not a statistics source/instance
   * @return {@code IOStatisticsSnapshot} or null if the object is null/doesn't have statistics
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_save,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",596,604,"/**
 * Saves a snapshot to the file system.
 * @param snapshot data to save, can be null
 * @param fs target file system
 * @param path destination path
 * @param overwrite flag to overwrite existing files
 */","* Save IOStatisticsSnapshot to a Hadoop filesystem as a JSON file.
   * @param snapshot statistics
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten?
   * @throws UncheckedIOException Any IO exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_toPrettyString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_toPrettyString(java.lang.Object),666,669,"/**
 * Converts statistics to a pretty string.
 * @param statistics object containing statistical data
 * @return formatted string representation of statistics
 */","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_getCurrent,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_getCurrent(),439,443,"/**
 * Calls m1 and returns result of m2 with null.
 * @throws UnsupportedOperationException if operation not supported
 */","* Get the context's {@code IOStatisticsContext} which
   * implements {@code IOStatisticsSource}.
   * This is either a thread-local value or a global empty context.
   * @return instance of {@code IOStatisticsContext}.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_setThreadIOStatisticsContext,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object),451,455,"/**
 * Masks and sets thread context with given statistics.
 * @param statisticsContext optional statistics context object
 * @throws UnsupportedOperationException if operation is not supported
 */","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_reset,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_reset(),462,466,"/**
 * Calls two methods without parameters.
 * Throws UnsupportedOperationException.
 */","* Reset the context's IOStatistics.
   * {@code IOStatisticsContext#reset()}
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_snapshot,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_snapshot(),474,478,"/**
 * Throws UnsupportedOperationException as this method is not implemented.
 */","* Take a snapshot of the context IOStatistics.
   * {@code IOStatisticsContext#snapshot()}
   * @return an instance of {@code IOStatisticsSnapshot}.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_aggregate,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_aggregate(java.lang.Object),487,490,"/**
 * Masks an object using context aggregation.
 * @param source object to be masked
 * @return true if masking is successful, false otherwise
 */","* Aggregate into the IOStatistics context the statistics passed in via
   * IOStatistics/source parameter.
   * <p>
   * Returns false if the source is null or does not contain any statistics.
   * @param source implementation of {@link IOStatisticsSource} or {@link IOStatistics}
   * @return true if the the source object was aggregated.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionUtil.java,compareVersions,"org.apache.hadoop.util.VersionUtil:compareVersions(java.lang.String,java.lang.String)",39,43,"/**
* Compares two version strings.
* @param version1 first version string
* @param version2 second version string
* @return comparison result as integer
*/","* Compares two version name strings using maven's ComparableVersion class.
   *
   * @param version1
   *          the first version to compare
   * @param version2
   *          the second version to compare
   * @return a negative integer if version1 precedes version2, a positive
   *         integer if version2 precedes version1, and 0 if and only if the two
   *         versions are equal.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refreshInternal,"org.apache.hadoop.util.HostsFileReader:refreshInternal(java.lang.String,java.lang.String,boolean)",200,225,"/**
 * Refreshes host details from include/exclude files.
 * @param includesFile path to the includes file
 * @param excludesFile path to the excludes file
 * @param lazy true for lazy refresh, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,<init>,"org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.io.InputStream,java.lang.String,java.io.InputStream)",67,75,"/**
 * Initializes HostsFileReader with include and exclude files.
 * @param includesFile path to the includes file
 * @param inFileInputStream input stream for the includes file
 * @param excludesFile path to the excludes file
 * @param exFileInputStream input stream for the excludes file
 * @throws IOException if an I/O error occurs during initialization
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.CountingBloomFilter:<init>(int,int,int)",93,96,"/**
* Initializes a Counting Bloom Filter.
* @param vectorSize size of the bit vector
* @param nbHash number of hash functions to use
* @param hashType type of hash function to use
*/","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,<init>,"org.apache.hadoop.util.bloom.BloomFilter:<init>(int,int,int)",110,114,"/**
* Initializes a Bloom Filter with specified parameters.
* @param vectorSize size of the bit vector
* @param nbHash number of hash functions
* @param hashType type of hash function to use
*/","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,readFields,org.apache.hadoop.util.bloom.CountingBloomFilter:readFields(java.io.DataInput),301,309,"/**
 * Reads data from input, initializes buckets array.
 * @param in DataInput source
 * @throws IOException if reading fails
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,readFields,org.apache.hadoop.util.bloom.BloomFilter:readFields(java.io.DataInput),218,233,"/**
 * Reads data from input stream and initializes BitSet.
 * @param in DataInput stream to read from
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,probablyHasKey,org.apache.hadoop.io.BloomMapFile$Reader:probablyHasKey(org.apache.hadoop.io.WritableComparable),264,272,"/**
* Checks if a key is present in the Bloom filter.
* @param key the key to check
* @return true if key may be present, false otherwise
*/","* Checks if this MapFile has the indicated key. The membership test is
     * performed using a Bloom filter, so the result has always non-zero
     * probability of false positives.
     * @param key key to check
     * @return  false iff key doesn't exist, true if key probably exists.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,selectiveClearing,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:selectiveClearing(org.apache.hadoop.util.bloom.Key,short)",199,235,"/**
* Applies a mask to the key based on the specified scheme.
* @param k the key to be masked
* @param scheme the masking scheme to apply
*/","* Performs the selective clearing for a given key.
   * @param k The false positive key to remove from <i>this</i> retouched Bloom filter.
   * @param scheme The selective clearing scheme to apply.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,createOptionTableListing,org.apache.hadoop.fs.FsShell:createOptionTableListing(),292,295,"/**
* Creates a table listing with specific configurations.
* @return TableListing object configured with methods m1, m2, and m3
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadResources,"org.apache.hadoop.conf.Configuration:loadResources(java.util.Properties,java.util.ArrayList,int,boolean,boolean)",3082,3100,"/**
 * Processes resource list with optional defaults loading.
 * @param properties configuration settings
 * @param resources list of resources to process
 * @param startIdx starting index in the resources list
 * @param fullReload flag for full reload operation
 * @param quiet suppresses output if true
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String,java.lang.String)",616,619,"/**
* Calls overloaded method with single-element array.
* @param key original key string
* @param newKey replacement key string
* @param customMessage message to display
*/","* Adds the deprecated key to the global deprecation map.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key to be deprecated
   * @param newKey key that take up the values of deprecated key
   * @param customMessage deprecation message",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[])",640,643,"/**
 * Deprecated method. Use m1 with all parameters.
 * @param key primary key string
 * @param newKeys array of new keys
 */","* Adds the deprecated key to the global deprecation map when no custom
   * message is provided.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If a key is deprecated in favor of multiple keys, they are all treated as 
   * aliases of each other, and setting any one of them resets all the others 
   * to the new value.
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key Key that is to be deprecated
   * @param newKeys list of keys that take up the values of deprecated key
   * @deprecated use {@link #addDeprecation(String key, String newKey)} instead",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String)",659,661,"/**
 * Calls overloaded method with single new key.
 * @param key original key
 * @param newKey replacement key
 */","* Adds the deprecated key to the global deprecation map when no custom
   * message is provided.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key Key that is to be deprecated
   * @param newKey key that takes up the value of deprecated key",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,start,org.apache.hadoop.http.HttpServer2:start(),1382,1434,"/**
* Starts the HTTP server and initializes metrics.
* @throws IOException if there is an issue starting the server or initializing components
*/","* Start the server. Does not wait for the server to start.
   *
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,writeBreadCrumbNode,org.apache.hadoop.ha.ActiveStandbyElector:writeBreadCrumbNode(org.apache.zookeeper.data.Stat),963,977,"/**
* Updates the znode to mark the local node as active.
* @param oldBreadcrumbStat existing znode stats or null if not found
* @throws KeeperException if a ZooKeeper operation fails
* @throws InterruptedException if the thread is interrupted
*/","* Write the ""ActiveBreadCrumb"" node, indicating that this node may need
   * to be fenced on failover.
   * @param oldBreadcrumbStat",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,tryDeleteOwnBreadCrumbNode,org.apache.hadoop.ha.ActiveStandbyElector:tryDeleteOwnBreadCrumbNode(),985,1008,"/**
 * Deletes the breadcrumb for an active node.
 * Asserts that the state is ACTIVE.
 * Throws IllegalStateException if data mismatch occurs.
 * Logs exceptions during deletion.
 */","* Try to delete the ""ActiveBreadCrumb"" node when gracefully giving up
   * active status.
   * If this fails, it will simply warn, since the graceful release behavior
   * is only an optimization.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readVectored,"org.apache.hadoop.fs.VectoredReadUtils:readVectored(org.apache.hadoop.fs.PositionedReadable,java.util.List,java.util.function.IntFunction)",98,104,"/**
 * Masks data in the stream within specified ranges.
 * @param stream input stream with positioned reading capability
 * @param ranges list of file ranges to mask
 * @param allocate function to allocate ByteBuffer instances
 * @throws EOFException if end of file is reached unexpectedly
 */","* This is the default implementation which iterates through the ranges
   * to read each synchronously, but the intent is that subclasses
   * can make more efficient readers.
   * The data or exceptions are pushed into {@link FileRange#getData()}.
   * @param stream the stream to read the data from
   * @param ranges the byte ranges to read
   * @param allocate the byte buffer allocation
   * @throws IllegalArgumentException if there are overlapping ranges or a range is invalid
   * @throws EOFException the range offset is negative",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,cancelPrefetches,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:cancelPrefetches(),294,306,"/**
* Processes buffer pool data for prefetching and readiness.
*/",* Requests cancellation of any previously issued prefetch requests.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.BlockManager:get(int),77,86,"/**
* Reads data from a block into a BufferData object.
* @param blockNumber the block identifier to read
* @return BufferData containing the block's data
* @throws IOException if an I/O error occurs during reading
*/","* Gets the block having the given {@code blockNumber}.
   *
   * The entire block is read into memory and returned as a {@code BufferData}.
   * The blocks are treated as a limited resource and must be released when
   * one is done reading them.
   *
   * @param blockNumber the number of the block to be read and returned.
   * @return {@code BufferData} having data from the given block.
   *
   * @throws IOException if there an error reading the given block.
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,readBlock,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:readBlock(org.apache.hadoop.fs.impl.prefetch.BufferData,boolean,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",331,395,"/**
 * Processes buffer data, handling caching and I/O operations.
 * @param data BufferData instance to process
 * @param isPrefetch true if operation is a prefetch
 * @param expectedState variable number of expected states for the buffer
 * @throws IOException on I/O errors during processing
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,<init>,"org.apache.hadoop.fs.impl.prefetch.FilePosition:<init>(long,int)",83,95,"/**
* Initializes FilePosition with file size and block size.
* @param fileSize total size of the file in bytes
* @param blockSize size of each block in bytes
*/","* Constructs an instance of {@link FilePosition}.
   *
   * @param fileSize size of the associated file.
   * @param blockSize size of each block within the file.
   *
   * @throws IllegalArgumentException if fileSize is negative.
   * @throws IllegalArgumentException if blockSize is zero or negative.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,isLastBlock,org.apache.hadoop.fs.impl.prefetch.FilePosition:isLastBlock(),204,206,"/**
 * Checks a condition using block data.
 * @return true if condition met, false otherwise
 */","* Determines whether the current block is the last block in this file.
   *
   * @return true if the current block is the last block in this file, false otherwise.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,toString,org.apache.hadoop.fs.impl.prefetch.FilePosition:toString(),275,296,"/**
* Constructs a status string for the buffer.
* @return Status string indicating buffer state or null if buffer is not set
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem:closeAll(),642,645,"/**
 * Calls m1 with specific parameters and executes CACHE.m2().
 * @throws IOException if an I/O error occurs during execution
 */","* Close all cached FileSystem instances. After this operation, they
   * may not be used in any operations.
   *
   * @throws IOException a problem arose closing one or more filesystem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readFully,"org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[])",125,128,"/**
* Reads bytes from a specified position into a buffer.
* @param position file position to start reading from
* @param buffer destination buffer for the read data
* @throws IOException if an I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence)",2044,2047,"/**
 * Writes CharSequence to file using UTF-8 encoding.
 * @param fs FileSystem instance
 * @param path destination file path
 * @param charseq data to write
 * @return the same FileSystem instance
 * @throws IOException if I/O error occurs
 */","* Write a line of text to a file. Characters are encoded into bytes using
   * UTF-8. This utility method opens the file for writing, creating the file if
   * it does not exist, or overwrites an existing file.
   *
   * @param fs the files system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long,boolean)",122,125,"/**
 * Constructs a BlockLocation with specified names, hosts, and metadata.
 * @param names array of block storage names
 * @param hosts array of hostnames where the block is stored
 * @param offset starting byte position in the file
 * @param length size of the block in bytes
 * @param corrupt indicates if the block is corrupted
 */","* Constructor with host, name, offset, length and corrupt flag.
   * @param names names.
   * @param hosts hosts.
   * @param offset offset.
   * @param length length.
   * @param corrupt corrupt.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long)",135,138,"/**
* Constructs a BlockLocation with specified parameters.
* @param names array of block replica hostnames
* @param hosts array of block replica host addresses
* @param topologyPaths network topology paths for each replica
* @param offset starting byte offset in the file
* @param length length of the block in bytes
*/","* Constructor with host, name, network topology, offset and length.
   * @param names names.
   * @param hosts hosts.
   * @param topologyPaths topologyPaths.
   * @param offset offset.
   * @param length length.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,clone,org.apache.hadoop.fs.statistics.MeanStatistic:clone(),271,274,"/**
 * Returns mean statistic using mask function.
 * @return MeanStatistic object
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,<init>,org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:<init>(),58,59,"/**
 * Initializes dynamic I/O statistics tracking.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,aggregateMeanStatistics,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMeanStatistics(org.apache.hadoop.fs.statistics.MeanStatistic,org.apache.hadoop.fs.statistics.MeanStatistic)",312,317,"/**
* Applies mask operation on two MeanStatistics.
* @param l left MeanStatistic operand
* @param r right MeanStatistic operand
* @return result of mask operation as a new MeanStatistic
*/","* Aggregate the mean statistics.
   * This returns a new instance.
   * @param l left value
   * @param r right value
   * @return aggregate value",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,snapshot,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:snapshot(org.apache.hadoop.fs.statistics.IOStatistics),160,168,"/**
* Updates statistics from IO source.
* @param source IOStatistics object containing data
*/","* Take a snapshot.
   *
   * This completely overwrites the map data with the statistics
   * from the source.
   * @param source statistics source.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtDebug,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(java.lang.String,java.lang.Object)",250,254,"/**
 * Logs a message with a source.
 * @param message log message
 * @param source object where the log originates from
 */","* Extract any statistics from the source and log to
   * this class's log at debug, if
   * the log is set to log at debug.
   * No-op if logging is not at debug or the source is null/of
   * the wrong type/doesn't provide statistics.
   * @param message message for log -this must contain ""{}"" for the
   * statistics report to actually get logged.
   * @param source source object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtLevel,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtLevel(org.slf4j.Logger,java.lang.String,java.lang.Object)",263,281,"/**
* Logs IO statistics based on the specified level.
* @param log Logger instance for logging
* @param level Logging level as a string
* @param source Source object for statistics
*/","* A method to log IOStatistics from a source at different levels.
   *
   * @param log    Logger for logging.
   * @param level  LOG level.
   * @param source Source to LOG.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,cleanupRemoteIterator,org.apache.hadoop.util.functional.RemoteIterators:cleanupRemoteIterator(org.apache.hadoop.fs.RemoteIterator),297,304,"/**
* Logs and closes a RemoteIterator.
* @param source the RemoteIterator to process
*/","* Clean up after an iteration.
   * If the log is at debug, calculate and log the IOStatistics.
   * If the iterator is closeable, cast and then cleanup the iterator
   * @param source iterator source
   * @param <T> type of source",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDurationOfInvocation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)",460,466,"/**
* Masks function execution.
* @param factory DurationTrackerFactory instance
* @param statistic name of the statistic
* @param input InvocationRaisingIOE object
* @throws IOException if an I/O error occurs
*/","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @throws IOException IO failure.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,build,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:build(),107,111,"/**
* Returns an instance of IOStatisticsStore.
* @return IOStatisticsStoreImpl object initialized with provided metrics
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,org.apache.hadoop.crypto.CryptoInputStream:read(),779,782,"/**
* Reads one byte from input and returns its value.
* @return byte value or -1 if end of stream
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(long,byte[],int,int)",332,348,"/**
* Reads data from a specified position into a buffer.
* @param position starting position for reading
* @param buffer target byte array to store read data
* @param offset starting offset in the buffer
* @param length number of bytes to read
* @return number of bytes actually read, or -1 if end of stream
* @throws IOException if an I/O error occurs
*/",Positioned read. It is thread-safe,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[],int,int)",502,515,"/**
 * Reads bytes from a specified position into a buffer.
 * @param position the starting position to read from
 * @param buffer the byte array to store the read data
 * @param offset the offset in the buffer to start writing
 * @param length the number of bytes to read
 * @throws IOException if an I/O error occurs
 */",Positioned read fully. It is thread-safe,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(long,java.nio.ByteBuffer)",353,369,"/**
* Reads bytes into buffer from specified position.
* @param position starting read position
* @param buf destination buffer
* @return number of bytes read or -1 if end reached
* @throws IOException on I/O error
*/",* Positioned read using {@link ByteBuffer}s. This method is thread-safe.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,java.nio.ByteBuffer)",374,389,"/**
 * Reads bytes from a specified position into a ByteBuffer.
 * @param position the starting position in the input stream
 * @param buf the ByteBuffer to read data into
 * @throws IOException if an I/O error occurs
 */",* Positioned readFully using {@link ByteBuffer}s. This method is thread-safe.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,org.apache.hadoop.crypto.CryptoInputStream:read(java.nio.ByteBuffer),588,639,"/**
* Reads data from a buffer into another buffer.
* @param buf target buffer to read into
* @return number of bytes read or -1 if end of stream
*/",ByteBuffer read.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)",707,736,"/**
 * Reads data into a ByteBuffer from the input source.
 * @param bufferPool pool to allocate ByteBuffers
 * @param maxLength maximum number of bytes to read
 * @param opts options for reading
 * @return ByteBuffer containing read data or null if no data is available
 * @throws IOException if an I/O error occurs
 * @throws UnsupportedOperationException if the input source does not support required operations
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",118,122,"/**
* Initializes a CryptoInputStream with specified parameters.
* @param in input stream to encrypt/decrypt
* @param codec encryption/decryption codec
* @param bufferSize buffer size for processing data
* @param key encryption key
* @param iv initialization vector
* @throws IOException if an I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,doEncode,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:doEncode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][])",101,118,"/**
* Encodes input data using Reed-Solomon and XOR encoding.
* @param inputs 2D array of ByteBuffers containing input data
* @param outputs 2D array of ByteBuffers for encoded output
* @throws IOException if an I/O error occurs during processing
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",50,54,"/**
* Encodes input chunks into output chunks.
* @param inputChunks array of input data chunks
* @param outputChunks array to store encoded data chunks
* @throws IOException if an I/O error occurs during encoding
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,writeObject,"org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)",165,234,"/**
 * Writes an object to a DataOutput stream.
 * @param out output stream for writing data
 * @param instance object to be written
 * @param declaredClass class of the object
 * @param conf configuration settings
 * @param allowCompactArrays flag to enable compact array handling
 * @throws IOException if an I/O error occurs
 */","* Write a {@link Writable}, {@link String}, primitive type, or an array of
     * the preceding.  
     * 
     * @param allowCompactArrays - set true for RPC and internal or intra-cluster
     * usages.  Set false for inter-cluster, File, and other persisted output 
     * usages, to preserve the ability to interchange files with other clusters 
     * that may not be running the same version of software.  Sometime in ~2013 
     * we can consider removing this parameter and always using the compact format.
     *
     * @param conf configuration.
     * @param out dataoutput.
     * @param declaredClass declaredClass.
     * @param instance instance.
     * @throws IOException raised on errors performing I/O.
     *",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,tryAcquire,org.apache.hadoop.fs.impl.prefetch.BufferPool:tryAcquire(int),157,159,"/**
 * Retrieves buffered data for a specific block.
 * @param blockNumber identifier of the block to fetch
 * @return BufferData object containing requested data
 */","* Acquires a buffer if one is immediately available. Otherwise returns null.
   * @param blockNumber the id of the block to try acquire.
   * @return the acquired block's {@code BufferData} or null.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numAvailable(),600,602,"/**
 * Delegates to buffer pool's m1 method.
 * @return result of bufferPool.m1()
 */","* Number of ByteBuffers available to be acquired.
   *
   * @return the number of available buffers.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,run,org.apache.hadoop.util.functional.TaskPool$Builder:run(org.apache.hadoop.util.functional.TaskPool$Task),268,282,"/**
* Executes a task with error handling.
* @param task the task to execute
* @return true if task is skipped, false otherwise
* @throws E if task execution fails
* @throws IOException if I/O error occurs
*/","* Execute the task across the data.
     * @param task task to execute
     * @param <E> exception which may be raised in execution.
     * @return true if the operation executed successfully
     * @throws E any exception raised.
     * @throws IOException IOExceptions raised by remote iterator or in execution.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPaths,"org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])",342,351,"/**
 * Masks multiple path data items under a parent.
 * @param parent the parent PathData object
 * @param items variable number of PathData objects to mask
 * @throws IOException if an I/O error occurs during masking
 */","*  Iterates over the given expanded paths and invokes
   *  {@link #processPath(PathData)} on each element.  If ""recursive"" is true,
   *  will do a post-visit DFS on directories.
   *  @param parent if called via a recurse, will be the parent dir, else null
   *  @param items a list of {@link PathData} objects to process
   *  @throws IOException if anything goes wrong...",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,getPathHandle,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:getPathHandle(org.apache.hadoop.fs.Path),163,166,"/**
 * Retrieves file handle from given path.
 * @param filePath path to the file
 * @return PathHandle object representing the file
 * @throws IOException if an I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,resolve,"org.apache.hadoop.fs.Options$HandleOpt:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Options$HandleOpt[])",359,362,"/**
* Creates a function to get PathHandle from FileStatus.
* @param fs FileSystem instance
* @param opt optional HandleOpt arguments
* @return Function mapping FileStatus to PathHandle
*/","* Utility function for mapping {@link FileSystem#getPathHandle} to a
     * fixed set of handle options.
     * @param fs Target filesystem
     * @param opt Options to bind in partially evaluated function
     * @return Function reference with options fixed",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createPathHandle,"org.apache.hadoop.fs.FilterFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",177,180,"/**
 * Applies mask operation to file status.
 * @param stat file status object
 * @param opts optional handle options
 * @return PathHandle result of the operation
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,createGroupExecutor,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupExecutor(java.lang.String),132,135,"/**
 * Executes a shell command with user name masked.
 * @param userName the original user name to be masked
 * @return ShellCommandExecutor instance for executing the masked command
 */","* Create a ShellCommandExecutor object using the user's name.
   *
   * @param userName user's name
   * @return a ShellCommandExecutor object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,createGroupIDExecutor,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupIDExecutor(java.lang.String),153,156,"/**
* Executes a shell command with user name mask.
* @param userName user's name to be masked in command
* @return ShellCommandExecutor instance for execution
*/","* Create a ShellCommandExecutor object for fetch a user's group id list.
   *
   * @param userName the user's name
   * @return a ShellCommandExecutor object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map)",1222,1225,"/**
* Constructs a new ShellCommandExecutor with specified command, directory, and environment.
* @param execString array of strings representing the shell command
* @param dir working directory for the command
* @param env environment variables for the command execution
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,"org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[],long)",1373,1379,"/**
* Executes a shell command with given environment and timeout.
* @param env environment variables for the command
* @param cmd command to be executed
* @param timeout maximum time to wait for command execution
* @return output of the executed command or null on error
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readProto,org.apache.hadoop.security.Credentials:readProto(java.io.DataInput),403,413,"/**
* Masks credentials from input stream.
* @param in DataInput stream containing credentials
*/","* Populates keys/values from proto buffer storage.
   * @param in - stream ready to read a serialized proto buffer message",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addAll,"org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials,boolean)",463,476,"/**
 * Merges secrets and tokens from another Credentials object.
 * @param other the source Credentials object
 * @param overwrite flag to determine if existing entries should be overwritten
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DelegationTokenIssuer.java,collectDelegationTokens,"org.apache.hadoop.security.token.DelegationTokenIssuer:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)",98,138,"/**
* Recursively retrieves and adds delegation tokens to credentials.
* @param issuer token issuer
* @param renewer user allowed to renew the token
* @param credentials where tokens are stored
* @param tokens list to store retrieved tokens
* @throws IOException if an I/O error occurs
*/","* NEVER call this method directly.
   *
   * @param issuer issuer.
   * @param renewer renewer.
   * @param credentials cache in which to add new delegation tokens.
   * @param tokens list of new delegation tokens.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addToken,"org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)",1712,1717,"/**
* Processes text with given token.
* @param alias text to process
* @param token processing token
* @return always true after processing
*/","* Add a named token to this UGI
   * 
   * @param alias Name of the token
   * @param token Token to be added
   * @return true on successful add of new token",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addRegexMountEntry,org.apache.hadoop.fs.viewfs.InodeTree:addRegexMountEntry(org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry),807,816,"/**
* Adds a regex mount point to the system.
* @param le LinkEntry containing configuration details
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,makeAbsolute,"org.apache.hadoop.fs.sftp.SFTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",176,181,"/**
 * Masks the given path if it doesn't meet condition m1.
 * @param workDir working directory path
 * @param path original file path
 * @return masked path or original if condition is met
 */","* Resolve against given working directory.
   *
   * @param workDir
   * @param path
   * @return absolute path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,makeAbsolute,"org.apache.hadoop.fs.ftp.FTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",269,274,"/**
 * Masks a file path within a working directory.
 * @param workDir base directory for masking
 * @param path original file path to mask
 * @return masked file path or original if not modified
 */","* Resolve against given working directory. *
   * 
   * @param workDir
   * @param path
   * @return",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,makeAbsolute,org.apache.hadoop.fs.RawLocalFileSystem:makeAbsolute(org.apache.hadoop.fs.Path),105,111,"/**
 * Masks file path to working directory.
 * @param f original file path
 * @return masked file path within working directory
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,pathToFile,org.apache.hadoop.fs.RawLocalFileSystem:pathToFile(org.apache.hadoop.fs.Path),119,125,"/**
* Masks and processes a file path.
* @param path the original file path
* @return File object representing the processed path
*/","* Convert a path to a File.
   *
   * @param path the path.
   * @return file.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,fixRelativePart,org.apache.hadoop.fs.FileSystem:fixRelativePart(org.apache.hadoop.fs.Path),2884,2890,"/**
 * Masks path by prepending directory if condition not met.
 * @param p input path
 * @return masked path
 */","* See {@link FileContext#fixRelativePart}.
   * @param p the path.
   * @return relative part.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,makeAbsolute,org.apache.hadoop.fs.viewfs.ViewFileSystem:makeAbsolute(org.apache.hadoop.fs.Path),269,271,"/**
 * Masks file path if condition is met.
 * @param f input file path
 * @return modified file path or original if condition not met
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),188,191,"/**
* Updates working directory.
* @param new_dir new directory path
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,makeQualified,"org.apache.hadoop.fs.Path:makeQualified(java.net.URI,org.apache.hadoop.fs.Path)",562,598,"/**
* Constructs a masked path with default scheme and authority.
* @param defaultUri default URI for scheme and authority
* @param workingDir working directory for relative paths
* @return new Path with updated URI components
*/","* Returns a qualified path object.
   *
   * @param defaultUri if this path is missing the scheme or authority
   * components, borrow them from this URI
   * @param workingDir if this path isn't absolute, treat it as relative to this
   * working directory
   * @return this path if it contains a scheme and authority and is absolute, or
   * a new path that includes a path and authority and is fully qualified",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,fixRelativePart,org.apache.hadoop.fs.FileContext:fixRelativePart(org.apache.hadoop.fs.Path),284,291,"/**
 * Ensures path is absolute.
 * @param p input file path
 * @return absolute path
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.HarFileSystem:getWorkingDirectory(),273,276,"/**
 * Returns a path using URI method m1.
 * @return Path object representing the URI
 */",* return the top level archive.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.HarFileSystem:getHomeDirectory(),809,812,"/**
 * Returns a new Path using the result of uri.m1().
 * @return Path object created from URI method m1()
 */",* return the top level archive path.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory(com.jcraft.jsch.ChannelSftp),680,686,"/**
* Retrieves file path from SFTP channel.
* @param channel SFTP channel object
* @return Path object or null if an exception occurs
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,org.apache.hadoop.fs.ChecksumFs:<init>(org.apache.hadoop.fs.AbstractFileSystem),58,63,"/**
* Constructs a ChecksumFs instance.
* @param theFs underlying file system
* @throws IOException if an I/O error occurs
* @throws URISyntaxException if URI syntax is invalid
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.RawLocalFileSystem:getHomeDirectory(),842,845,"/**
 * Returns a masked path based on user home directory.
 * @return Path object representing the masked directory
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.RawLocalFileSystem:getInitialWorkingDirectory(),861,864,"/**
 * Returns the current working directory path.
 * @return Path object representing the user's current directory
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,abort,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:abort(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path)",245,261,"/**
* Processes file upload by handle.
* @param uploadId unique upload identifier
* @param filePath path to the file to be processed
* @return CompletableFuture indicating completion of processing
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,lookupStat,"org.apache.hadoop.fs.shell.PathData:lookupStat(org.apache.hadoop.fs.FileSystem,java.lang.String,boolean)",175,186,"/**
 * Retrieves file status from the filesystem.
 * @param fs FileSystem instance to operate on
 * @param pathString path of the file to check
 * @param ignoreFNF flag to ignore FileNotFoundException
 * @return FileStatus object or null if not found and ignored
 * @throws IOException if an I/O error occurs
 */","* Get the FileStatus info
   * @param ignoreFNF if true, stat will be null if the path doesn't exist
   * @return FileStatus for the given path
   * @throws IOException if anything goes wrong",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getPath,org.apache.hadoop.fs.PathIOException:getPath(),107,107,"/**
 * Returns a new Path instance using the stored path.
 * @return Path object initialized with the internal path
 */",@return Path that generated the exception,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getTargetPath,org.apache.hadoop.fs.PathIOException:getTargetPath(),110,112,"/**
 * Returns a Path object based on targetPath.
 * @return Path object if targetPath is not null, otherwise null
 */","@return Path if the operation involved copying or moving, else null",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUsed,org.apache.hadoop.fs.FileSystem:getUsed(),2719,2722,"/**
 * Calls m1 with root directory path.
 * @return result of m1 with root path
 * @throws IOException if an I/O error occurs
 */","* Return the total size of all files in the filesystem.
   * @throws IOException IO failure
   * @return the number of path used.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ViewFileSystem:getHomeDirectory(),422,434,"/**
* Returns the user's home directory path.
* Initializes if not already set using base directory and user info.
* @return Path object representing the home directory
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getMountPoints,org.apache.hadoop.fs.viewfs.ViewFileSystem:getMountPoints(),1060,1070,"/**
* Retrieves and converts filesystem mount points.
* @return Array of MountPoint objects representing mounted filesystems
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(org.apache.hadoop.fs.FileSystem,java.net.URI)",105,116,"/**
 * Constructs a ChRootedFileSystem with a specified FileSystem and URI.
 * @param fs the underlying FileSystem
 * @param uri the root URI for the new filesystem
 * @throws IOException if an I/O error occurs
 */","* Constructor
   * @param fs base file system
   * @param uri base uri
   * @throws IOException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getResolvedQualifiedPath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getResolvedQualifiedPath(org.apache.hadoop.fs.Path),177,181,"/**
* Masks and returns a modified file path.
* @param f original file path
* @return masked file path
* @throws FileNotFoundException if the file is not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ViewFs:getHomeDirectory(),314,326,"/**
 * Returns the user's home directory path.
 * Initializes if not already set using base directory and user info.
 * @return Path object representing the home directory
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getMountPoints,org.apache.hadoop.fs.viewfs.ViewFs:getMountPoints(),720,730,"/**
* Retrieves and converts filesystem mount points.
* @return Array of MountPoint objects representing mounted filesystems
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,getRemainingPathStr,"org.apache.hadoop.fs.viewfs.RegexMountPoint:getRemainingPathStr(java.lang.String,java.lang.String)",207,215,"/**
 * Masks a source path using a resolved path.
 * @param srcPath the original source path
 * @param resolvedPathStr the resolved path string
 * @return masked Path object
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,createLink,"org.apache.hadoop.fs.viewfs.InodeTree:createLink(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$LinkType,java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration)",423,505,"/**
 * Creates a symbolic link from source to target.
 * @param src source path
 * @param target target path
 * @param linkType type of link to create
 * @param settings configuration settings
 * @param aUgi user group information
 * @param config system configuration
 * @throws URISyntaxException if URI syntax is incorrect
 * @throws IOException if an I/O error occurs
 * @throws FileAlreadyExistsException if file already exists
 * @throws UnsupportedFileSystemException if file system is unsupported
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRemainingPath,"org.apache.hadoop.fs.viewfs.InodeTree:getRemainingPath(java.lang.String[],int)",996,1008,"/**
 * Constructs a Path from the given array starting at the specified index.
 * @param path array of path components
 * @param startIndex index to start constructing the path from
 * @return constructed Path object
 */","* Return remaining path from specified index to the end of the path array.
   * @param path An array of path components split by slash
   * @param startIndex the specified start index of the path array
   * @return remaining path.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getTargetLink,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetLink(),370,377,"/**
* Constructs a path from directory links.
* @return Path object representing the combined directories
*/","* Get the target of the link. If a merge link then it returned
     * as "","" separated URI list.
     *
     * @return the path.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)",119,121,"/**
 * Constructs a path from a parent and child string.
 * @param parent the parent directory as a string
 * @param child the child file or directory name as a string
 */","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)",129,131,"/**
 * Constructs a path by combining a parent and a child.
 * @param parent the parent path
 * @param child the child path component as a string
 */","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,org.apache.hadoop.fs.Path)",139,141,"/**
 * Constructs a Path by combining a parent path and a child path.
 * @param parent the parent directory as a String
 * @param child the child path as a Path object
 */","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,rename,"org.apache.hadoop.io.MapFile:rename(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.String)",898,905,"/**
* Renames a directory in the given file system.
* @param fs FileSystem object
* @param oldName current name of the directory
* @param newName new name for the directory
* @throws IOException if renaming fails
*/","* Renames an existing map directory.
   * @param fs fs.
   * @param oldName oldName.
   * @param newName newName.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,insecureCreateForWrite,"org.apache.hadoop.io.SecureIOUtils:insecureCreateForWrite(java.io.File,int)",243,262,"/**
 * Creates a FileOutputStream with specified permissions.
 * @param f File to be created
 * @param permissions file access permissions
 * @return FileOutputStream object
 * @throws IOException on I/O error or if file already exists
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,fileToPath,org.apache.hadoop.security.token.DtFileOperations:fileToPath(java.io.File),93,95,"/**
 * Creates a Path from File using nested methods.
 * @param f input File object
 * @return Path object created from file's metadata
 */",Add the service prefix for a local filesystem.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,unnestUri,org.apache.hadoop.security.ProviderUtils:unnestUri(java.net.URI),80,101,"/**
 * Constructs a Path from a URI, excluding the authority.
 * @param nestedUri input URI to process
 * @return Path object representing the URI without authority
 */","* Convert a nested URI to decode the underlying path. The translation takes
   * the authority and parses it into the underlying scheme and authority.
   * For example, ""myscheme://hdfs@nn/my/path"" is converted to
   * ""hdfs://nn/my/path"".
   * @param nestedUri the URI from the nested URI
   * @return the unnested path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,constructNewPath,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructNewPath(org.apache.hadoop.fs.Path),300,302,"/**
 * Appends ""_NEW"" to the path's first component.
 * @param path input path object
 * @return modified path with appended string
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,constructOldPath,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructOldPath(org.apache.hadoop.fs.Path),304,306,"/**
 * Renames a file by appending ""_OLD"" to its name.
 * @param path original file path
 * @return new Path with modified filename
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,stringToPath,org.apache.hadoop.util.StringUtils:stringToPath(java.lang.String[]),273,282,"/**
* Converts array of strings to Paths.
* @param str array of string paths
* @return array of Path objects or null if input is null
*/","* stringToPath.
   * @param str str.
   * @return path array.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,makeQualified,org.apache.hadoop.fs.HarFileSystem:makeQualified(org.apache.hadoop.fs.Path),400,412,"/**
 * Masks a given file path.
 * @param path original file path
 * @return masked file path with authentication
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getPathWithoutSchemeAndAuthority,org.apache.hadoop.fs.Path:getPathWithoutSchemeAndAuthority(org.apache.hadoop.fs.Path),104,111,"/**
 * Masks a path based on its mode.
 * @param path input path to be masked
 * @return masked path or original if not applicable
 */","* Return a version of the given Path without the scheme information.
   *
   * @param path the source Path
   * @return a copy of this Path without the scheme information",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,mergePaths,"org.apache.hadoop.fs.Path:mergePaths(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",277,286,"/**
 * Combines two paths with a transformation on the second path.
 * @param path1 first path object
 * @param path2 second path object to be transformed
 * @return new Path object combining elements of both input paths
 */","* Merge 2 paths such that the second path is appended relative to the first.
   * The returned path has the scheme and authority of the first path.  On
   * Windows, the drive specification in the second path is discarded.
   * 
   * @param path1 the first path
   * @param path2 the second path, to be appended relative to path1
   * @return the merged path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getParentUtil,org.apache.hadoop.fs.Path:getParentUtil(),444,459,"/**
 * Constructs a masked path from URI components.
 * @return Path object or null if conditions are not met
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,apply,"org.apache.hadoop.fs.shell.find.Print:apply(org.apache.hadoop.fs.shell.PathData,int)",59,63,"/**
 * Processes an item with a given depth.
 * @param item PathData object to process
 * @param depth current processing depth
 * @return Result indicating the outcome of processing
 * @throws IOException if an I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processPath,org.apache.hadoop.fs.shell.Display$Checksum:processPath(org.apache.hadoop.fs.shell.PathData),197,214,"/**
 * Processes a file path, checks for directories, and outputs checksum.
 * @param item PathData object representing the file or directory
 * @throws IOException if an I/O error occurs during processing
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,checkIfExists,org.apache.hadoop.fs.shell.PathData:checkIfExists(org.apache.hadoop.fs.shell.PathData$FileTypeRequirement),220,233,"/**
* Validates file type based on requirement.
* @param typeRequirement specifies the required file type
* @throws PathIOException if validation fails
*/","* Ensure that the file exists and if it is or is not a directory
   * @param typeRequirement Set it to the desired requirement.
   * @throws PathIOException if file doesn't exist or the type does not match
   * what was specified in typeRequirement.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getStringForChildPath,org.apache.hadoop.fs.shell.PathData:getStringForChildPath(org.apache.hadoop.fs.Path),319,328,"/**
* Masks a file path.
* @param childPath the path to mask
* @return masked path as a string
*/","* Given a child of this directory, use the directory's path and the child's
   * basename to construct the string to the child.  This preserves relative
   * paths since Path will fully qualify.
   * @param childPath a path contained within this directory
   * @return String of the path relative to this directory",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPath,org.apache.hadoop.fs.shell.Ls:processPath(org.apache.hadoop.fs.shell.PathData),285,321,"/**
 * Masks and formats file information for output.
 * @param item PathData object containing file details
 * @throws IOException if an I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,rename,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:rename(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",546,563,"/**
 * Renames a file from source to target.
 * @param src source file data
 * @param target target file data
 * @throws IOException if rename operation fails
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processPath,org.apache.hadoop.fs.shell.Delete$Rmdir:processPath(org.apache.hadoop.fs.shell.PathData),205,217,"/**
* Checks and processes directory item.
* @param item PathData object representing the directory
* @throws IOException if directory is not empty or cannot be processed
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processPath,org.apache.hadoop.fs.shell.Mkdir:processPath(org.apache.hadoop.fs.shell.PathData),58,67,"/**
* Handles path masking logic.
* @param item PathData object to process
* @throws IOException if path exists and createParents is false, or path is not a directory
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),143,148,"/**
* Checks if path is a directory.
* @param item PathData object representing the file path
* @throws IOException if path is not a directory or other I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processNonexistentPath,org.apache.hadoop.fs.shell.Command:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),330,332,"/**
 * Throws PathNotFoundException with item's m1 value.
 * @param item PathData object to process
 * @throws IOException if an I/O error occurs
 */","*  Provides a hook for handling paths that don't exist.  By default it
   *  will throw an exception.  Primarily overriden by commands that create
   *  paths such as mkdir or touch.
   *  @param item the {@link PathData} that doesn't exist
   *  @throws FileNotFoundException if arg is a path and it doesn't exist
   *  @throws IOException if anything else goes wrong...",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),57,62,"/**
 * Checks if the given path is a directory.
 * @param item the path data to check
 * @throws IOException if an I/O error occurs
 * @throws PathIsNotDirectoryException if the path is not a directory
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processPath,"org.apache.hadoop.fs.shell.MoveCommands$Rename:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",111,128,"/**
 * Masks a source path to a target path.
 * @param src source path data
 * @param target target path data
 * @throws IOException if masking fails due to filesystem issues or path constraints
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),102,107,"/**
* Checks if the given path is a directory.
* @param item PathData object representing the file or directory
* @throws IOException if an I/O error occurs
* @throws PathIsNotDirectoryException if the path is not a directory
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processPath,org.apache.hadoop.fs.shell.Truncate:processPath(org.apache.hadoop.fs.shell.PathData),75,97,"/**
 * Truncates a file to a specified length.
 * @param item the PathData object representing the file
 * @throws IOException if an I/O error occurs
 * @throws IllegalArgumentException if newLength is larger than current file size
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processPath,org.apache.hadoop.fs.shell.SetReplication:processPath(org.apache.hadoop.fs.shell.PathData),81,103,"/**
* Processes path data, checks for symlinks and sets replication.
* @param item PathData object to process
* @throws IOException if symlink is found or setting replication fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processPath,"org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",61,68,"/**
 * Copies path data from source to target.
 * Throws exception if target already exists and is a directory.
 * @param src source path data
 * @param target target path data
 * @throws IOException if an I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,postProcessPath,org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:postProcessPath(org.apache.hadoop.fs.shell.PathData),70,78,"/**
* Checks and removes file if it exists.
* @param src PathData object containing file details
* @throws IOException if file removal fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFSofPath,org.apache.hadoop.fs.FileContext:getFSofPath(org.apache.hadoop.fs.Path),325,337,"/**
 * Masks a file system path.
 * @param absOrFqPath absolute or fully qualified path
 * @return AbstractFileSystem instance
 * @throws UnsupportedFileSystemException if unsupported file system
 * @throws IOException if I/O error occurs
 */","* Get the file system of supplied path.
   * 
   * @param absOrFqPath - absolute or fully qualified path
   * @return the file system of the path
   * 
   * @throws UnsupportedFileSystemException If the file system for
   *           <code>absOrFqPath</code> is not supported.
   * @throws IOException If the file system for <code>absOrFqPath</code> could
   *         not be instantiated.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFs:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)",102,122,"/**
 * Initializes a ChRootedFs with a given file system and root path.
 * @param fs the underlying AbstractFileSystem
 * @param theRoot the root directory for this ChRootedFs
 * @throws URISyntaxException if URI operations fail
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getUriPath,org.apache.hadoop.fs.FilterFs:getUriPath(org.apache.hadoop.fs.Path),189,192,"/**
 * Delegates to myFs's m1 method.
 * @param p file path
 * @return result of myFs.m1
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,resolvePath,org.apache.hadoop.fs.viewfs.ViewFs:resolvePath(org.apache.hadoop.fs.Path),328,338,"/**
* Resolves a file path in the filesystem.
* @param f input file path
* @return resolved Path object
* @throws FileNotFoundException if file not found
* @throws AccessControlException if access is denied
* @throws UnresolvedLinkException if link cannot be resolved
* @throws IOException for other I/O errors
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,resolvePath,org.apache.hadoop.fs.FilterFs:resolvePath(org.apache.hadoop.fs.Path),168,172,"/**
 * Delegates file system operation to underlying implementation.
 * @param p path to operate on
 * @return result of the file system operation
 * @throws FileNotFoundException if file is not found
 * @throws UnresolvedLinkException if symbolic link cannot be resolved
 * @throws AccessControlException if access is denied
 * @throws IOException for other I/O errors
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createInternal,"org.apache.hadoop.fs.FilterFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",88,97,"/**
* Creates a file with specified options.
* @param f file path
* @param flag creation flags
* @param absolutePermission file permissions
* @param bufferSize buffer size for I/O operations
* @param replication number of replications
* @param blockSize block size in bytes
* @param progress progress monitor
* @param checksumOpt checksum options
* @param createParent create parent directories if true
* @return FSDataOutputStream for writing to the file
* @throws IOException on I/O errors
* @throws UnresolvedLinkException if a symlink cannot be resolved
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,delete,"org.apache.hadoop.fs.FilterFs:delete(org.apache.hadoop.fs.Path,boolean)",99,104,"/**
* Recursively checks file existence.
* @param f file path to check
* @param recursive whether to check recursively
* @return true if file exists, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileBlockLocations,"org.apache.hadoop.fs.FilterFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",106,111,"/**
 * Retrieves block locations for a file.
 * @param f file path
 * @param start starting offset
 * @param len length of the range
 * @return array of BlockLocation objects
 * @throws IOException if an I/O error occurs
 * @throws UnresolvedLinkException if a symlink cannot be resolved
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileChecksum,org.apache.hadoop.fs.FilterFs:getFileChecksum(org.apache.hadoop.fs.Path),113,118,"/**
* Computes checksum for a file.
* @param f file path
* @return FileChecksum object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileStatus,org.apache.hadoop.fs.FilterFs:getFileStatus(org.apache.hadoop.fs.Path),120,125,"/**
* Checks file status.
* @param f file path to check
* @return FileStatus object
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symlink cannot be resolved
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileLinkStatus,org.apache.hadoop.fs.FilterFs:getFileLinkStatus(org.apache.hadoop.fs.Path),139,144,"/**
 * Checks file status.
 * @param f file path to check
 * @return FileStatus object
 * @throws IOException if I/O error occurs
 * @throws UnresolvedLinkException if symbolic link cannot be resolved
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listStatus,org.apache.hadoop.fs.FilterFs:listStatus(org.apache.hadoop.fs.Path),194,199,"/**
* Checks file status.
* @param f file path to check
* @return array of FileStatus objects
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link cannot be resolved
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listLocatedStatus,org.apache.hadoop.fs.FilterFs:listLocatedStatus(org.apache.hadoop.fs.Path),201,207,"/**
* Retrieves file status iterator.
* @param f path to the file or directory
* @return RemoteIterator of LocatedFileStatus objects
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,mkdir,"org.apache.hadoop.fs.FilterFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",215,221,"/**
* Sets directory permissions and optionally creates parent directories.
* @param dir target directory path
* @param permission desired file system permissions
* @param createParent flag to create parent directories if missing
* @throws IOException on I/O errors
* @throws UnresolvedLinkException if a symlink cannot be resolved
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,open,org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path),223,228,"/**
* Reads file data stream.
* @param f file path
* @return FSDataInputStream of the file
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws UnresolvedLinkException if a symlink could not be resolved
* @throws IOException for other I/O errors
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,open,"org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path,int)",230,235,"/**
* Opens an input stream for reading file data.
* @param f path to the file
* @param bufferSize size of the buffer
* @return FSDataInputStream for reading the file
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link cannot be resolved
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,truncate,"org.apache.hadoop.fs.FilterFs:truncate(org.apache.hadoop.fs.Path,long)",237,243,"/**
* Sets file length.
* @param f file path
* @param newLength new file length in bytes
* @return true if operation successful
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setOwner,"org.apache.hadoop.fs.FilterFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",261,267,"/**
* Calls m1 and myFs.m2 with file path and user/group names.
* @param f file path
* @param username user name
* @param groupname group name
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setPermission,"org.apache.hadoop.fs.FilterFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",269,274,"/**
* Sets file permissions.
* @param f file path
* @param permission new file permissions
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link cannot be resolved
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setReplication,"org.apache.hadoop.fs.FilterFs:setReplication(org.apache.hadoop.fs.Path,short)",276,281,"/**
* Sets file replication factor.
* @param f file path
* @param replication desired replication factor
* @return true if successful, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setTimes,"org.apache.hadoop.fs.FilterFs:setTimes(org.apache.hadoop.fs.Path,long,long)",283,288,"/**
* Updates file metadata.
* @param f file path
* @param mtime last modified time in milliseconds
* @param atime last accessed time in milliseconds
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link cannot be resolved
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,mkdirs,"org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",764,771,"/**
* Checks and sets directory permissions.
* @param fs FileSystem instance
* @param dir directory path
* @param permission desired file system permissions
* @return true if initial check passes, otherwise false
*/","* Create a directory with the provided permission.
   * The permission of the directory is set to be the provided permission as in
   * setPermission, not permission{@literal &~}umask
   *
   * @see #create(FileSystem, Path, FsPermission)
   *
   * @param fs FileSystem handle
   * @param dir the name of the directory to be created
   * @param permission the permission of the directory
   * @return true if the directory creation succeeds; false otherwise
   * @throws IOException A problem creating the directories.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,mkdirs,org.apache.hadoop.fs.ChecksumFileSystem:mkdirs(org.apache.hadoop.fs.Path),986,989,"/**
 * Checks if a file exists at the specified path.
 * @param f file path to check
 * @return true if the file exists, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,mkdirs,org.apache.hadoop.fs.FilterFileSystem:mkdirs(org.apache.hadoop.fs.Path),339,342,"/**
 * Checks if a file exists.
 * @param f file path to check
 * @return true if file exists, false otherwise
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FileStatusProto),49,106,"/**
 * Converts a FileStatusProto to a FileStatus object.
 * @param proto Protocol buffer containing file status information
 * @return FileStatus object with details from proto
 * @throws IOException if block replication doesn't fit in 16 bits
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean)",151,158,"/**
* Constructs a FileStatus object with specified attributes.
* @param length file size in bytes
* @param isdir true if the path is a directory
* @param block_replication replication factor for blocks
* @param blocksize block size in bytes
* @param modification_time last modification time in milliseconds since epoch
* @param access_time last access time in milliseconds since epoch
* @param permission file permissions
* @param owner file owner
* @param group file group
* @param symlink symbolic link target path (if any)
* @param path file path
* @param hasAcl true if the file has an ACL
* @param isEncrypted true if the file is encrypted
* @param isErasureCoded true if the file uses erasure coding
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set,org.apache.hadoop.fs.BlockLocation[])",142,149,"/**
* Constructs a LocatedFileStatus object.
* @param length file size in bytes
* @param isdir true if the path is a directory
* @param block_replication replication factor for blocks
* @param blocksize block size in bytes
* @param modification_time last modified time in milliseconds since epoch
* @param access_time last accessed time in milliseconds since epoch
* @param permission file permissions
* @param owner file owner
* @param group file group
* @param symlink symbolic link target path, or null if not a symlink
* @param path file path
* @param attr set of attribute flags
* @param locations array of block locations
*/","* Constructor.
   *
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param attr Attribute flags (See {@link FileStatus.AttrFlags}).
   * @param locations a file's block locations",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getPermission,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getPermission(),59,62,"/**
 * Returns file system permissions.
 * Overrides parent method to provide specific implementation.
 * @return FsPermission object representing permissions
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$Writer:append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",1458,1461,"/**
 * Invokes overloaded m1 method with Object parameters.
 * @param key writable key object
 * @param val writable value object
 * @throws IOException if an I/O error occurs
 */","* Append a key/value pair.
     * @param key input Writable key.
     * @param val input Writable val.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeFile,"org.apache.hadoop.io.SequenceFile$Sorter:writeFile(org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator,org.apache.hadoop.io.SequenceFile$Writer)",3431,3438,"/**
* Masks and writes key-value pairs from records.
* @param records iterator for raw key-value pairs
* @param writer destination for processed data
*/","* Writes records from RawKeyValueIterator into a file represented by the 
     * passed writer.
     * @param records the RawKeyValueIterator
     * @param writer the Writer created earlier 
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,init,org.apache.hadoop.service.AbstractService:init(org.apache.hadoop.conf.Configuration),152,178,"/**
* Initializes service with configuration.
* @param conf Configuration object for the service
*/","* {@inheritDoc}
   * This invokes {@link #serviceInit}
   * @param conf the configuration of the service. This must not be null
   * @throws ServiceStateException if the configuration was null,
   * the state change not permitted, or something else went wrong",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,stop,org.apache.hadoop.service.AbstractService:stop(),213,240,"/**
* Handles stopping the service.
* Ensures no re-entry and handles exceptions gracefully.
*/",* {@inheritDoc},,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Metadata:equals(org.apache.hadoop.io.SequenceFile$Metadata),797,820,"/**
* Compares metadata for equality.
* @param other Metadata object to compare with
* @return true if metadata matches, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,handleKind,org.apache.hadoop.security.token.Token$TrivialRenewer:handleKind(org.apache.hadoop.io.Text),528,531,"/**
* Checks if text matches specified criteria.
* @param kind Text object to check
* @return true if criteria match, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSelector.java,selectToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)",46,60,"/**
 * Filters and returns a token matching the service and kind.
 * @param service the service to match against
 * @param tokens collection of tokens to filter
 * @return Token object or null if no match found
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,isPrivateCloneOf,org.apache.hadoop.security.token.Token$PrivateToken:isPrivateCloneOf(org.apache.hadoop.io.Text),279,282,"/**
 * Masks a text using a public service.
 * @param thePublicService the text to be masked
 * @return true if masking is successful, false otherwise
 */","* Whether this is a private clone of a public token.
     * @param thePublicService the public service name
     * @return true when the public service is the same as specified",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,equals,org.apache.hadoop.security.token.Token:equals(java.lang.Object),386,400,"/**
* Compares this token with another for equality.
* @param right the object to compare with
* @return true if equal, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,matchAlias,"org.apache.hadoop.security.token.DtFileOperations:matchAlias(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",73,75,"/**
 * Checks if token matches alias or if alias is null.
 * @param token the Token object to check
 * @param alias the Text alias to match against
 * @return true if token matches alias or alias is null, false otherwise
 */",Match token service field to alias text.  True if alias is null.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,matchService,"org.apache.hadoop.security.token.DtFileOperations:matchService(org.apache.hadoop.security.token.DtFetcher,org.apache.hadoop.io.Text,java.lang.String)",78,83,"/**
* Checks if URL matches service or is protocol-based.
* @param fetcher object for fetching data
* @param service optional service name to match against
* @param url URL to check
* @return true if matches, false otherwise
*/",Match fetcher's service name to the service text and/or url prefix.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,selectDelegationToken,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text)",1008,1018,"/**
* Retrieves a token for the given credentials and service.
* @param creds user credentials
* @param service target service
* @return Token object or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,handleKind,org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:handleKind(org.apache.hadoop.io.Text),179,182,"/**
 * Checks if text kind matches TOKEN_KIND.
 * @param kind Text object to check
 * @return true if kind matches TOKEN_KIND, false otherwise
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDir,org.apache.hadoop.util.DiskChecker:checkDir(java.io.File),76,78,"/**
 * Masks files in the specified directory.
 * @param dir directory containing files to mask
 * @throws DiskErrorException if an error occurs during masking
 */","* Create the directory if it doesn't exist and check that dir is readable,
   * writable and executable
   *  
   * @param dir dir.
   * @throws DiskErrorException disk problem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirWithDiskIo,org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(java.io.File),88,92,"/**
 * Masks files in a directory.
 * @param dir directory containing files to mask
 * @throws DiskErrorException if disk error occurs
 */","* Create the directory if it doesn't exist and check that dir is
   * readable, writable and executable. Perform some disk IO to
   * ensure that the disk is usable for writes.
   *
   * @param dir dir.
   * @throws DiskErrorException disk problem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flushBuffer,org.apache.hadoop.fs.FSOutputSummer:flushBuffer(),145,147,"/**
 * Calls m1 with default parameters.
 * @throws IOException if an I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flush,org.apache.hadoop.fs.FSOutputSummer:flush(),183,185,"/**
 * Calls m1 with both parameters set to false.
 * @throws IOException if an I/O error occurs in m1
 */","* Checksums all complete data chunks and flushes them to the underlying
   * stream. If there is a trailing partial chunk, it is not flushed and is
   * maintained in the buffer.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,doDecodeSingle,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeSingle(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,boolean)",123,222,"/**
 * Processes input buffers to reconstruct data and parity units.
 * @param inputs 2D array of input ByteBuffers
 * @param outputs 2D array of output ByteBuffers
 * @param erasedLocationToFix index of erased location to fix
 * @param bufSize buffer size
 * @param isDirect indicates if direct memory access is used
 * @throws IOException if I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,doDecodeMultiAndParity,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeMultiAndParity(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int[],int)",265,351,"/**
 * Processes input buffers to fix erased locations using Reed-Solomon decoding.
 * @param inputs array of input ByteBuffers
 * @param outputs array for storing fixed ByteBuffers
 * @param erasedLocationToFix indices of erased locations
 * @param bufSize buffer size for processing
 * @throws IOException if an I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingValidator.java,validate,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",73,128,"/**
* Processes input buffers, replaces erased ones with outputs, and decodes.
* @param inputs array of ByteBuffer inputs
* @param erasedIndexes indexes of erased buffers
* @param outputs array of output ByteBuffers
* @throws IOException if decoding fails
*/","* Validate outputs decoded from inputs, by decoding an input back from
   * the outputs and comparing it with the original one.
   *
   * For instance, in RS (6, 3), let (d0, d1, d2, d3, d4, d5) be sources
   * and (p0, p1, p2) be parities, and assume
   *  inputs = [d0, null (d1), d2, d3, d4, d5, null (p0), p1, null (p2)];
   *  erasedIndexes = [1, 6];
   *  outputs = [d1, p0].
   * Then
   *  1. Create new inputs, erasedIndexes and outputs for validation so that
   *     the inputs could contain the decoded outputs, and decode them:
   *      newInputs = [d1, d2, d3, d4, d5, p0]
   *      newErasedIndexes = [0]
   *      newOutputs = [d0']
   *  2. Compare d0 and d0'. The comparison will fail with high probability
   *     when the initial outputs are wrong.
   *
   * Note that the input buffers' positions must be the ones where data are
   * read: If the input buffers have been processed by a decoder, the buffers'
   * positions must be reset before being passed into this method.
   *
   * This method does not change outputs and erasedIndexes.
   *
   * @param inputs input buffers used for decoding. The buffers' position
   *               are moved to the end after this method.
   * @param erasedIndexes indexes of erased units used for decoding
   * @param outputs decoded output buffers, which are ready to be read after
   *                the call
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])",168,173,"/**
 * Processes input and output chunks for error correction.
 * @param inputs array of ECChunk objects representing input data
 * @param erasedIndexes indices of erased chunks
 * @param outputs array of ECChunk objects for corrected output data
 */","* Decode with inputs and erasedIndexes, generates outputs. More see above.
   *
   * Note, for both input and output ECChunks, no mixing of on-heap buffers and
   * direct buffers are allowed.
   *
   * @param inputs input buffers to read data from
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException if the decoder is closed",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",55,68,"/**
* Processes input buffers, updates erased indexes, and produces output buffers.
* @param inputs original input ByteBuffers
* @param erasedIndexes indices of erased elements
* @param outputs resulting output ByteBuffers
* @throws IOException if an I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(byte[][],int[],byte[][])",70,83,"/**
 * Processes inputs, erases specified indexes, and outputs results.
 * @param inputs original input byte arrays
 * @param erasedIndexes indices to be erased in inputs
 * @param outputs original output byte arrays
 * @throws IOException if an I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),73,84,"/**
* Masks and decodes input data.
* @param decodingState state containing inputs and outputs for decoding
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),86,101,"/**
 * Processes decoding state for masked data.
 * @param decodingState current state of decoding process
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,initBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:initBlock(),511,570,"/**
 * Handles block reading and validation.
 * @throws IOException if bad block header is detected
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,internalReset,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:internalReset(),274,280,"/**
* Resets and initializes compression output stream.
* @throws IOException if an I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,writeRun,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:writeRun(),654,706,"/**
 * Handles character processing and block updates.
 * Throws IOException if an I/O error occurs.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Merger:close(),1148,1157,"/**
* Closes input readers and output writer.
* @throws IOException if an I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:close(),256,271,"/**
* Handles block processing and error tracking.
* Increments error count, processes blocks, decrements error count.
* Marks the operation as complete and closed.
*/","* Signaling the end of write to the block. The block register will be
       * called for registering the finished block.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cleanup,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:cleanup(),3887,3892,"/**
* Masks input by calling helper methods.
* @throws IOException if file operations fail
*/","* The default cleanup. Subclasses can override this with a custom
       * cleanup.
       * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:close(),557,569,"/**
* Masks data by processing a block.
* Closes the resource after processing.
*/",* Finishing reading the block. Release all resources.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeWritableOutputStream,org.apache.hadoop.security.Credentials:writeWritableOutputStream(java.io.DataOutputStream),321,326,"/**
* Writes mask data to output stream.
* @param os DataOutputStream for writing
* @throws IOException if I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readFields,org.apache.hadoop.security.Credentials:readFields(java.io.DataInput),420,443,"/**
 * Reads and processes token and secret key data from input.
 * @param in DataInput source for reading data
 * @throws IOException if an I/O error occurs
 */","* Loads all the keys.
   * @param in DataInput.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,createTokenIdent,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenIdent(byte[]),253,260,"/**
 * Masks and processes a token identifier.
 * @param tokenIdentBytes byte array representing the token identifier
 * @return processed TokenIdent object
 * @throws IOException if an I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processTokenAddOrUpdate,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenAddOrUpdate(byte[]),411,427,"/**
* Processes input data to create and store a TokenIdent.
* @param data byte array containing token information
* @return TokenIdent object or null if processing fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processTokenRemoved,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenRemoved(org.apache.curator.framework.recipes.cache.ChildData),429,435,"/**
* Masks child data by applying token identification.
* @param data ChildData object containing input stream
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(java.lang.String,boolean)",652,679,"/**
* Retrieves delegation token information from ZooKeeper.
* @param nodePath path to the node containing token data
* @param quiet suppresses logging if true
* @return DelegationTokenInformation object or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawKey,org.apache.hadoop.io.SequenceFile$Reader:nextRawKey(org.apache.hadoop.io.DataOutputBuffer),2662,2697,"/**
* Reads and processes a key from the input stream.
* @param key DataOutputBuffer to store the read key
* @return Length of the key or -1 if no more keys are available
*/","* Read 'raw' keys.
     * @param key - The buffer into which the key is read
     * @return Returns the key length or -1 for end of file
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(org.apache.hadoop.io.Writable),2376,2408,"/**
* Masks a writable value, applying configuration and validation.
* @param val the Writable object to be masked
* @throws IOException if an I/O error occurs during processing
*/","* Get the 'value' corresponding to the last read 'key'.
     * @param val : The 'value' to be read.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(java.lang.Object),2415,2448,"/**
* Masks and processes the input value.
* @param val the input object to be processed
* @return processed object
* @throws IOException if processing fails
*/","* @return Get the 'value' corresponding to the last read 'key'.
     * @param val : The 'value' to be read.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRaw,"org.apache.hadoop.io.SequenceFile$Reader:nextRaw(org.apache.hadoop.io.DataOutputBuffer,org.apache.hadoop.io.SequenceFile$ValueBytes)",2603,2654,"/**
* Reads key-value pair from input stream.
* @param key buffer to store key data
* @param val object to store value data
* @return total length of key and value, or -1 if end reached
* @throws IOException on I/O error
*/","* Read 'raw' records.
     * @param key - The buffer into which the key is read
     * @param val - The 'raw' value
     * @return Returns the total record length or -1 for end of file
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawValue,org.apache.hadoop.io.SequenceFile$Reader:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes),2765,2790,"/**
* Processes and masks value bytes.
* @param val ValueBytes object to process
* @return Length of processed value
* @throws IOException if I/O error occurs
*/","* Read 'raw' values.
     * @param val - The 'raw' value
     * @return Returns the value length
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getTokenInfoFromSQL,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getTokenInfoFromSQL(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),238,251,"/**
* Retrieves delegation token information.
* @param ident token identifier
* @return DelegationTokenInformation object
* @throws NoSuchElementException if token not found
* @throws RuntimeException for SQL or IO errors
*/","* Obtains the DelegationTokenInformation associated with the given
   * TokenIdentifier in the SQL database.
   * @param ident Existing TokenIdentifier in the SQL database.
   * @return DelegationTokenInformation that matches the given TokenIdentifier or
   *         null if it doesn't exist in the database.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,read,org.apache.hadoop.fs.permission.PermissionStatus:read(java.io.DataInput),114,118,"/**
* Reads and returns permission status from input.
* @param in DataInput source
* @return PermissionStatus object
* @throws IOException if reading fails
*/","* Create and initialize a {@link PermissionStatus} from {@link DataInput}.
   * @param in data input.
   * @throws IOException raised on errors performing I/O.
   * @return PermissionStatus.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readEnum,"org.apache.hadoop.io.WritableUtils:readEnum(java.io.DataInput,java.lang.Class)",422,425,"/**
* Reads an integer from input and returns corresponding enum value.
* @param in DataInput to read from
* @param enumType Enum class type
* @return Enum value or null if not found
* @throws IOException if reading fails
*/","* Read an Enum value from DataInput, Enums are read and written 
   * using String values. 
   * @param <T> Enum type
   * @param in DataInput to read from 
   * @param enumType Class type of Enum
   * @return Enum represented by String read from DataInput
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,readFields,org.apache.hadoop.security.authorize.AccessControlList:readFields(java.io.DataInput),326,330,"/**
* Reads ACL string from input and processes it.
* @param in DataInput stream containing the ACL data
*/",* Deserializes the AccessControlList object,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getDelegationKey(int),561,577,"/**
 * Retrieves a delegation key by ID.
 * @param keyId unique key identifier
 * @return DelegationKey object or null if not found
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:write(java.io.DataOutput),790,796,"/**
 * Writes data to output stream.
 * @param out DataOutput object to write to
 * @throws IOException if an I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text),85,87,"/**
 * Converts Text to ByteString.
 * @param key input text to be converted
 * @return ByteString representation of the input text
 */","* Get the ByteString for frequently used fixed and small set strings.
   * @param key string
   * @return the ByteString for frequently used fixed and small set strings.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,protoFromToken,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token),142,149,"/**
* Masks token details and returns a proto representation.
* @param tok input Token object
* @return TokenProto with masked information
*/","* Create a {@code TokenProto} instance
   * from a hadoop token.
   * This builds and caches the fields
   * (identifier, password, kind, service) but not
   * renewer or any payload.
   * @param tok token
   * @return a marshallable protobuf class.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,copyToken,org.apache.hadoop.security.token.Token:copyToken(),114,116,"/**
 * Creates and returns a new token linked to this instance.
 * @return Token object associated with current instance
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",160,164,"/**
* Generates a token for user group information.
* @param ugi UserGroupInformation object
* @param renewer entity that can renew the token
* @return Token object associated with the user and renewer
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenIdentifier.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",49,53,"/**
 * Initializes a new delegation token identifier.
 * @param kind type of the delegation token
 * @param owner user who owns the token
 * @param renewer authorized to renew the token
 * @param realUser actual user on behalf of whom the token is issued
 */","* Create a new delegation token identifier
   *
   * @param kind token kind
   * @param owner the effective username of the token owner
   * @param renewer the username of the renewer
   * @param realUser the real username of the token owner",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>(),51,53,"/**
 * Constructs an AbstractDelegationTokenIdentifier with default values.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadWriteLock:<init>(boolean,java.lang.String,org.slf4j.Logger,long,long)",40,47,"/**
* Initializes an instrumented read-write lock with logging.
* @param fair true for fair lock ordering
* @param name lock identifier for logging
* @param logger used for logging lock events
* @param minLoggingGapMs minimum time between log entries
* @param lockWarningThresholdMs threshold for warning on long-held locks
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeOnce,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce(),89,117,"/**
 * Handles method invocation with retry logic.
 * @return CallReturn object containing result or exception
 */",Invoke the call once without retrying.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,checkKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:checkKey(),1596,1611,"/**
* Reads key-value pair from block reader.
* Throws EOFException if no data to read.
*/","* check whether we have already successfully obtained the key. It also
       * initializes the valueInputStream.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(org.apache.hadoop.io.BytesWritable),1706,1720,"/**
* Masks a BytesWritable value.
* @param value input BytesWritable object to mask
* @return masked long value
* @throws IOException if an I/O error occurs
*/","* Copy the value into BytesWritable. The input BytesWritable will be
         * automatically resized to the actual value size. The implementation
         * directly uses the buffer inside BytesWritable for storing the value.
         * The call does not require the value length to be known.
         * 
         * @param value value.
         * @throws IOException raised on errors performing I/O.
         * @return long value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,writeValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:writeValue(java.io.OutputStream),1747,1763,"/**
* Transfers data from an input stream to an output stream with size constraints.
* @param out destination OutputStream for writing data
* @return total bytes transferred
* @throws IOException if I/O error occurs during transfer
*/","* Writing the value to the output stream. This method avoids copying
         * value data from Scanner into user buffer, then writing to the output
         * stream. It does not require the value length to be known.
         * 
         * @param out
         *          The output stream
         * @return the length of the value
         * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[]),146,149,"/**
 * Calls overloaded method with full array range.
 * @param b byte array to process
 * @return result of processing
 * @throws IOException if an I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,close,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:close(),186,197,"/**
* Continuously processes data until a condition is met, then closes the resource.
* @throws IOException if an I/O error occurs during processing
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[]),1932,1934,"/**
 * Calls overloaded method with full buffer range.
 * @param buf byte array to process
 * @return result of processing the entire buffer
 */","* Compare the entry key to another key. Synonymous to compareTo(key, 0,
         * key.length).
         * 
         * @param buf
         *          The key buffer.
         * @return comparison result between the entry key with the input key.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,equals,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:equals(java.lang.Object),1966,1971,"/**
* Compares this entry with another for equality.
* @param other object to compare with
* @return true if entries are equal, false otherwise
*/",* Compare whether this and other points to the same key value.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String,java.lang.String)",188,203,"/**
* Retrieves delegation token from URL.
* @param url target URL for fetching the token
* @param token authentication token
* @param renewer user allowed to renew the token
* @param doAsUser user on behalf of whom the operation is performed
* @return Token object containing delegation token details
* @throws IOException if an I/O error occurs
* @throws AuthenticationException if authentication fails
*/","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return abstract delegation token identifier.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)",237,245,"/**
 * Renews a delegation token.
 * @param url URL to the service
 * @param token authentication token
 * @param dToken delegation token to renew
 * @param doAsUser user on behalf of whom to perform the operation
 * @return expiration time of the renewed token in milliseconds
 * @throws IOException if an I/O error occurs
 * @throws AuthenticationException if authentication fails
 */","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param doAsUser the user to do as, which will be the token owner.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)",275,286,"/**
* Cancels a delegation token.
* @param url target URL
* @param token authentication token
* @param dToken delegation token to cancel
* @param doAsUser user on behalf of whom the operation is performed
* @throws IOException if an I/O error occurs or authentication fails
*/","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param dToken abstract delegation token identifier.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String),468,470,"/**
 * Calls overloaded m1 with default value.
 * @param scope operation scope
 * @return Node object
 */","* Randomly choose a node.
   *
   * @param scope range of nodes from which a node will be chosen
   * @return the chosen node
   *
   * @see #chooseRandom(String, Collection)",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,sortByDistance,"org.apache.hadoop.net.NetworkTopologyWithNodeGroup:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",285,300,"/**
 * Overrides method to process nodes with specific conditions.
 * @param reader current node being processed
 * @param nodes array of nodes to be processed
 * @param activeLen length of active nodes
 */","* Sort nodes array by their distances to <i>reader</i>.
   * <p>
   * This is the same as {@link NetworkTopology#sortByDistance(Node, Node[],
   * int)} except with a four-level network topology which contains the
   * additional network distance of a ""node group"" which is between local and
   * same rack.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getInputStream,"org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket,long)",496,503,"/**
 * Wraps a socket input stream with a timeout.
 * @param socket the underlying socket
 * @param timeout max time in milliseconds to wait for data
 * @return SocketInputWrapper instance
 * @throws IOException if an I/O error occurs
 */","* Return a {@link SocketInputWrapper} for the socket and set the given
   * timeout. If the socket does not have an associated channel, then its socket
   * timeout will be set to the specified value. Otherwise, a
   * {@link SocketInputStream} will be created which reads with the configured
   * timeout.
   * 
   * Any socket created using socket factories returned by {@link #NetUtils},
   * must use this interface instead of {@link Socket#getInputStream()}.
   * 
   * In general, this should be called only once on each socket: see the note
   * in {@link SocketInputWrapper#setTimeout(long)} for more information.
   *
   * @see Socket#getChannel()
   * 
   * @param socket socket.
   * @param timeout timeout in milliseconds. zero for waiting as
   *                long as necessary.
   * @return SocketInputWrapper for reading from the socket.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getOutputStream,"org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket,long)",550,554,"/**
* Returns an OutputStream for the socket.
* @param socket the target socket
* @param timeout connection timeout in milliseconds
* @return OutputStream or null if no output stream is available
*/","* Returns OutputStream for the socket. If the socket has an associated
   * SocketChannel then it returns a 
   * {@link SocketOutputStream} with the given timeout. If the socket does not
   * have a channel, {@link Socket#getOutputStream()} is returned. In the later
   * case, the timeout argument is ignored and the write will wait until 
   * data is available.<br><br>
   * 
   * Any socket created using socket factories returned by {@link NetUtils},
   * must use this interface instead of {@link Socket#getOutputStream()}.
   * 
   * @see Socket#getChannel()
   * 
   * @param socket socket.
   * @param timeout timeout in milliseconds. This may not always apply. zero
   *        for waiting as long as necessary.
   * @return OutputStream for writing to the socket.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,writeMetric,org.apache.hadoop.metrics2.sink.StatsDSink:writeMetric(java.lang.String),150,157,"/**
* Sends a metric line to StatsD.
* @param line the metric data to send
* @throws MetricsException if an I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,<init>,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:<init>(),38,40,"/**
 * Initializes a new network topology with a root node.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,connect,"org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,java.net.SocketAddress,int)",590,636,"/**
 * Connects a socket to an endpoint with optional local address and timeout.
 * @param socket the socket to connect
 * @param endpoint the remote endpoint to connect to
 * @param localAddr the local address to bind, or null for default
 * @param timeout the connection timeout in milliseconds
 * @throws IOException if an I/O error occurs during connection
 */","* Like {@link NetUtils#connect(Socket, SocketAddress, int)} but
   * also takes a local address and port to bind the socket to. 
   * 
   * @param socket socket.
   * @param endpoint the remote address
   * @param localAddr the local address to bind the socket to
   * @param timeout timeout in milliseconds
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,long,org.apache.hadoop.metrics2.impl.MetricsConfig)",90,98,"/**
 * Constructs a MetricsSourceAdapter with specified configuration.
 * @param prefix metric name prefix
 * @param name source name
 * @param description source description
 * @param source metrics data source
 * @param injectedTags additional tags for metrics
 * @param period update period in milliseconds
 * @param conf configuration settings for the adapter
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,snapshotMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:snapshotMetrics(org.apache.hadoop.metrics2.impl.MetricsSourceAdapter,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder)",420,427,"/**
* Records metrics from source adapter.
* @param sa MetricsSourceAdapter instance
* @param bufferBuilder MetricsBufferBuilder to collect data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateJmxCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateJmxCache(),160,194,"/**
* Updates JMX cache with metrics records.
* Checks cache TTL and fetches all metrics if expired.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,register,"org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.util.Map,java.lang.Object)",89,114,"/**
* Registers an MBean with the specified properties.
* @param serviceName service name for the MBean
* @param nameName name of the MBean
* @param properties MBean properties
* @param theMbean the MBean object to register
* @return ObjectName of registered MBean or null if registration fails
*/","* Register the MBean using our standard MBeanName format
   * ""hadoop:service={@literal <serviceName>,name=<nameName>}""
   * Where the {@literal <serviceName> and <nameName>} are the supplied
   * parameters.
   *
   * @param serviceName serviceName.
   * @param nameName nameName.
   * @param properties - Key value pairs to define additional JMX ObjectName
   *                     properties.
   * @param theMbean    - the MBean to register
   * @return the named used to register the MBean",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,unregisterSource,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:unregisterSource(java.lang.String),245,258,"/**
* Processes a name by checking and invoking methods on various sources.
* @param name the identifier to process
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopSources,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSources(),460,469,"/**
* Stops all metrics sources and system source.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newInverseQuantiles,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newInverseQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",240,251,"/**
* Creates and registers a MutableQuantiles metric.
* @param name unique metric identifier
* @param desc description of the metric
* @param sampleName name for samples
* @param valueName name for values
* @param interval quantile calculation interval
* @return created MutableQuantiles object
*/","* Create a mutable inverse metric that estimates inverse quantiles of a stream of values
   * @param name of the metric
   * @param desc metric description
   * @param sampleName of the metric (e.g., ""Ops"")
   * @param valueName of the metric (e.g., ""Rate"")
   * @param interval rollover interval of estimator in seconds
   * @return a new inverse quantile estimator object
   * @throws MetricsException if interval is not a positive integer",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,<init>,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:<init>(),53,71,"/**
* Initializes metrics for disk read and write operations.
* Creates quantile metrics for file read and write latencies.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache:<init>(java.lang.String,double,long)",196,204,"/**
 * Initializes a RetryCache with specified parameters.
 * @param cacheName name of the cache
 * @param percentage capacity utilization percentage
 * @param expirationTime time in milliseconds for entry expiration
 */","* Constructor
   * @param cacheName name to identify the cache by
   * @param percentage percentage of total java heap space used by this cache
   * @param expirationTime time for an entry to expire in nanoseconds",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class),71,81,"/**
* Processes protocol methods, logs and caches them.
* @param protocol the protocol class to process
*/","* Initialize the registry with all the methods in a protocol
   * so they all show up in the first snapshot.
   * Convenient for JMX implementations.
   * @param protocol the protocol class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.String[]),89,93,"/**
 * Masks each name in the provided array.
 * @param names array of names to be masked
 */","* Initialize the registry with all rate names passed in.
   * This is an alternative to the above init function since this metric
   * can be used more than just for rpc name.
   * @param names the array of all rate names",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,aggregateLocalStatesToGlobalMetrics,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:aggregateLocalStatesToGlobalMetrics(java.util.concurrent.ConcurrentMap),149,157,"/**
 * Updates global metrics with local stats.
 * @param localStats map of sample statistics to update
 */","* Aggregates the thread's local samples into the global metrics. The caller
   * should ensure its thread safety.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean)",310,312,"/**
 * Creates a mutable rate with optional extended features.
 * @param name     name of the rate
 * @param desc     description of the rate
 * @param extended whether to enable extended features
 * @return MutableRate object
 */","* Create a mutable rate metric (for throughput measurement).
   * @param name  of the metric
   * @param desc  description
   * @param extended  produce extended stat (stdev/min/max etc.) if true
   * @return a new mutable rate metric object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,init,org.apache.hadoop.metrics2.lib.MutableRates:init(java.lang.Class),58,69,"/**
* Masks protocol by caching and registering methods.
* @param protocol the protocol class to be masked
*/","* Initialize the registry with all the methods in a protocol
   * so they all show up in the first snapshot.
   * Convenient for JMX implementations.
   * @param protocol the protocol class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,<init>,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:<init>(java.lang.String),53,58,"/**
* Initializes detailed metrics for RPC scheduler.
* @param ns namespace identifier
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,<init>,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:<init>(int),57,62,"/**
* Initializes RPC detailed metrics for a specified port.
* @param port the port number to monitor
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addResponseTime,"org.apache.hadoop.ipc.DecayRpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",747,770,"/**
 * Records and logs response time metrics for a scheduled task.
 * @param callName name of the function call
 * @param schedulable task to be processed
 * @param details processing details including timing information
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,updateDeferredMetrics,"org.apache.hadoop.ipc.Server:updateDeferredMetrics(java.lang.String,long)",670,673,"/**
* Records RPC metrics.
* @param name operation name
* @param processingTime time taken in milliseconds
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,updateMetrics,"org.apache.hadoop.ipc.Server:updateMetrics(org.apache.hadoop.ipc.Server$Call,long,boolean)",617,668,"/**
 * Updates metrics and processes call details.
 * @param call the call object to process
 * @param processingStartTimeNanos start time of processing in nanoseconds
 * @param connDropped flag indicating if connection was dropped
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,add,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Method)",162,170,"/**
* Processes method annotations to handle metrics.
* @param source object containing the method
* @param method method being processed
*/",Add {@link MutableMetric} for a method annotated with {@link Metric},,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getMetrics,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",963,969,"/**
 * Schedules metrics collection using a decay RPC scheduler.
 * @param collector MetricsCollector instance to collect metrics
 * @param all boolean flag to indicate if all metrics should be collected
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKeyLength,org.apache.hadoop.security.KDiag:validateKeyLength(),442,450,"/**
* Checks AES encryption support and logs key length details.
* @throws NoSuchAlgorithmException if AES algorithm is unsupported
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateUGI,"org.apache.hadoop.security.KDiag:validateUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",697,706,"/**
* Logs Kerberos authentication issues for a user.
* @param messagePrefix prefix for log messages
* @param user UserGroupInformation object representing the user
*/","* Validate the UGI: verify it is kerberized.
   * @param messagePrefix message in exceptions
   * @param user user to validate",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verifyFileIsValid,"org.apache.hadoop.security.KDiag:verifyFileIsValid(java.io.File,java.lang.String,java.lang.String)",795,805,"/**
* Validates file existence, type, content, and readability.
* @param file File object to validate
* @param category Category for validation messages
* @param text Description of the file being validated
* @return true if all checks pass, false otherwise
*/","* Verify that a file is valid: it is a file, non-empty and readable.
   * @param file file
   * @param category category for exceptions
   * @param text text message
   * @return true if the validation held; false if it did not <i>and</i>
   * {@link #nofail} has disabled raising exceptions.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateShortName,org.apache.hadoop.security.KDiag:validateShortName(),459,476,"/**
* Validates and processes Kerberos principal.
* @param principal the Kerberos principal to process
*/","* Verify whether auth_to_local rules transform a principal name
   * <p>
   * Having a local user name ""bar@foo.com"" may be harmless, so it is noted at
   * info. However if what was intended is a transformation to ""bar""
   * it can be difficult to debug, hence this check.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,getUser,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getUser(),71,87,"/**
 * Retrieves user group information based on owner and real user conditions.
 * @return UserGroupInformation object or null if conditions are not met
 */","* Get the username encoded in the token identifier
   * 
   * @return the username or owner",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,getUgi,org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto),133,150,"/**
* Creates UserGroupInformation from proto.
* @param userInfo user information in protobuf format
* @return UserGroupInformation object or null if effective user is missing
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,<init>,org.apache.hadoop.fs.LocalFileSystem:<init>(org.apache.hadoop.fs.FileSystem),70,72,"/**
 * Constructs a LocalFileSystem instance.
 * @param rawLocalFileSystem underlying file system to wrap
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,<init>,org.apache.hadoop.fs.shell.find.Find:<init>(),162,164,"/**
 * Constructs a new Find instance with recursion enabled.
 */",Default constructor for the Find command.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,<init>,org.apache.hadoop.fs.shell.Ls:<init>(),120,120,"/**
* Default constructor for class initialization.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,<init>,org.apache.hadoop.fs.shell.Count:<init>(),110,110,"/**
 * Default constructor for Count class.
 */",Constructor,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createEncoder(),38,41,"/**
 * Returns an erasure encoder with masking enabled.
 * @return ErasureEncoder configured for masking
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createEncoder(),38,41,"/**
 * Creates an ErasureEncoder using HHXORErasureEncoder.
 * @return ErasureEncoder instance configured with m1()
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createEncoder(),36,39,"/**
 * Returns an encoder with masking functionality.
 * @return ErasureEncoder configured with masking
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createEncoder(),39,42,"/**
 * Returns an XOR erasure encoder.
 * @return ErasureEncoder instance using XOR encoding
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createDecoder(),41,44,"/**
 * Creates an instance of DummyErasureDecoder with specific parameters.
 * @return ErasureDecoder object initialized with m1()'s result
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createDecoder(),44,47,"/**
 * Returns an XOR erasure decoder.
 * @return ErasureDecoder instance configured with m1 settings
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createDecoder(),43,46,"/**
 * Creates an erasure decoder using RSErasureDecoder and m1 configuration.
 * @return ErasureDecoder instance configured with m1 settings
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createDecoder(),43,46,"/**
 * Creates an erasure decoder using HHXOR algorithm.
 * @return ErasureDecoder instance configured with m1 parameters
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,checkAndUpdateMaps,org.apache.hadoop.security.ShellBasedIdMapping:checkAndUpdateMaps(),166,176,"/**
* Updates cache if condition is met.
* Logs and handles exceptions during update.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createConnection,org.apache.hadoop.ha.ActiveStandbyElector:createConnection(),894,909,"/**
* Resets and re-establishes the ZooKeeper client connection.
* @throws IOException if an I/O error occurs during connection reset
* @throws KeeperException if a KeeperException is encountered
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,forceReloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:forceReloginFromKeytab(),1262,1266,"/**
* Calls method m1 with specific parameters.
* @throws IOException if an I/O error occurs
*/","* Force re-Login a user in from a keytab file irrespective of the last login
   * time. Loads a user identity from a keytab file and logs them in. They
   * become the currently logged-in user. This method assumes that
   * {@link #loginUserFromKeytab(String, String)} had happened already. The
   * Subject field of this UserGroupInformation object is updated to have the
   * new credentials.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean),1268,1270,"/**
 * Calls overloaded method with default value for forceRefresh.
 * @param checkTGT flag to check TGT status
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,forceReloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:forceReloginFromTicketCache(),1302,1306,"/**
* Masks data by invoking helper method.
* @throws IOException if an I/O error occurs
*/","* Force re-Login a user in from the ticket cache irrespective of the last
   * login time. This method assumes that login had happened already. The
   * Subject field of this UserGroupInformation object is updated to have the
   * new credentials.
   *
   * @throws IOException
   *           raised on errors performing I/O.
   * @throws KerberosAuthException
   *           on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache(),1316,1320,"/**
 * Calls m1 with default false flag.
 * @throws IOException if an I/O error occurs
 */","* Re-Login a user in from the ticket cache.  This
   * method assumes that login had happened already.
   * The Subject field of this UserGroupInformation object is updated to have
   * the new credentials.
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createEncryptor(),58,62,"/**
* Creates an encryptor using OpenSSL CTR mode.
* @return Encryptor instance configured for encryption
* @throws GeneralSecurityException if there's a security issue
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createDecryptor(),64,68,"/**
* Creates a decryptor using OpenSSL CTR mode.
* @return Decryptor instance configured for decryption
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,fillQueueForKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$EncryptedQueueRefiller:fillQueueForKey(java.lang.String,java.util.Queue,int)",145,161,"/**
* Generates encrypted key versions and adds them to the queue.
* @param keyName name of the encryption key
* @param keyQueue queue to store generated EncryptedKeyVersions
* @param numEKVs number of EncryptedKeyVersions to generate
* @throws IOException if an I/O error occurs during the process
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,generateEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateEncryptedKey(java.lang.String),788,799,"/**
* Retrieves an encrypted key version by name.
* @param encryptionKeyName the name of the encryption key
* @return EncryptedKeyVersion object
* @throws IOException if an I/O error occurs
* @throws GeneralSecurityException if a security issue arises
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,drain,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:drain(java.lang.String),311,316,"/**
 * Delegates key operation to all registered providers.
 * @param keyName name of the key to operate on
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,queueCall,org.apache.hadoop.ipc.Server:queueCall(org.apache.hadoop.ipc.Server$Call),3097,3104,"/**
 * Handles call processing and exception handling.
 * @param call the incoming call to process
 * @throws IOException if an I/O error occurs
 * @throws InterruptedException if the thread is interrupted
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddrForHost,"org.apache.hadoop.net.NetUtils:createSocketAddrForHost(java.lang.String,int)",308,325,"/**
* Creates an InetSocketAddress for a given host and port.
* @param host the hostname or IP address
* @param port the port number
* @return InetSocketAddress object with resolved address
*/","* Create a socket address with the given host and port.  The hostname
   * might be replaced with another host that was set via
   * {@link #addStaticResolution(String, String)}.  The value of
   * hadoop.security.token.service.use_ip will determine whether the
   * standard java host resolver is used, or if the fully qualified resolver
   * is used.
   * @param host the hostname or IP use to instantiate the object
   * @param port the port number
   * @return InetSocketAddress",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,canonicalizeHost,org.apache.hadoop.net.NetUtils:canonicalizeHost(java.lang.String),362,377,"/**
* Masks the given host using cache or security utility.
* @param host original hostname
* @return masked hostname or original if masking fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getLocalInetAddress,org.apache.hadoop.net.NetUtils:getLocalInetAddress(java.lang.String),798,811,"/**
 * Retrieves the InetAddress for a given host, ensuring it has a valid network interface.
 * @param host hostname or IP address as a string
 * @return InetAddress object if valid, otherwise null
 */","* Checks if {@code host} is a local host name and return {@link InetAddress}
   * corresponding to that address.
   * 
   * @param host the specified host
   * @return a valid local {@link InetAddress} or null
   * @throws SocketException if an I/O error occurs",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,sendSaslMessage,"org.apache.hadoop.security.SaslRpcClient:sendSaslMessage(java.io.OutputStream,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)",457,469,"/**
 * Sends a SASL message to the output stream.
 * @param out OutputStream to send the message
 * @param message RpcSaslProto message to be sent
 * @throws IOException if an I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,writeConnectionContext,"org.apache.hadoop.ipc.Client$Connection:writeConnectionContext(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",1007,1028,"/**
 * Sends authentication request to remote server.
 * @param remoteId identifier for the remote connection
 * @param authMethod method used for authentication
 * @throws IOException if communication fails
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,sendRpcRequest,org.apache.hadoop.ipc.Client$Connection:sendRpcRequest(org.apache.hadoop.ipc.Client$Call),1159,1191,"/**
 * Masks and enqueues an RPC call for processing.
 * @param call the RPC call to be processed
 * @throws InterruptedException if interrupted while waiting
 * @throws IOException if I/O error occurs during processing
 */","Initiates a rpc call by sending the rpc request to the remote server.
     * Note: this is not called from the current thread, but by another
     * thread, so that if the current thread is interrupted that the socket
     * state isn't corrupted with a partially written message.
     * @param call - the rpc request",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),727,730,"/**
 * Delegates to callQueue's m1 method.
 * @param ugi UserGroupInformation object
 * @return Result from callQueue.m1
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,receiveRpcResponse,org.apache.hadoop.ipc.Client$Connection:receiveRpcResponse(),1196,1249,"/**
 * Handles RPC response processing.
 * Closes connection if necessary, processes header and value, throws exceptions on error.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processRpcRequest,"org.apache.hadoop.ipc.Server$Connection:processRpcRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)",2869,2970,"/**
 * Processes an RPC request.
 * @param header request header containing metadata
 * @param buffer buffer containing the request data
 * @throws RpcServerException if there's a server error processing the request
 * @throws InterruptedException if the thread is interrupted while processing
 */","* Process an RPC Request 
     *   - the connection headers and context must have been already read.
     *   - Based on the rpcKind, decode the rpcRequest.
     *   - A successfully decoded RpcCall will be deposited in RPC-Q and
     *     its response will be sent later when the request is processed.
     * @param header - RPC request header
     * @param buffer - stream to request payload
     * @throws RpcServerException - generally due to fatal rpc layer issues
     *   such as invalid header or deserialization error.  The call queue
     *   may also throw a fatal or non-fatal exception on overflow.
     * @throws IOException - fatal internal error that should/could not
     *   be sent to client.
     * @throws InterruptedException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponse,"org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)",3507,3547,"/**
 * Handles RPC call response.
 * @param call RPC call object
 * @param status RPC status
 * @param erCode error code proto
 * @param rv writable return value
 * @param errorClass class name of the error
 * @param error error message
 * @throws IOException if an I/O error occurs
 */","* Setup response for the IPC Call.
   * 
   * @param call {@link Call} to which we are setting up the response
   * @param status of the IPC call
   * @param rv return value for the IPC Call, if the call was successful
   * @param errorClass error class, if the the call failed
   * @param error error message, if the call failed
   * @throws IOException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,wrapWithSasl,org.apache.hadoop.ipc.Server:wrapWithSasl(org.apache.hadoop.ipc.Server$RpcCall),3641,3661,"/**
* Handles SASL token wrapping for an RPC call.
* @param call the RPC call to process
* @throws IOException if I/O error occurs during processing
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,initializeAuthContext,org.apache.hadoop.ipc.Server$Connection:initializeAuthContext(int),2567,2593,"/**
* Determines the appropriate AuthProtocol based on type.
* @param authType authentication type identifier
* @return AuthProtocol object or throws IOException if invalid
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,reset,org.apache.hadoop.util.CacheableIPList:reset(),42,45,"/**
 * Masks IP addresses in the list and performs additional processing.
 */",* Reloads the ip list,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,main,org.apache.hadoop.util.SysInfoLinux:main(java.lang.String[]),705,734,"/**
* Prints system information for a Linux environment.
* @param args command line arguments (not used)
*/","* Test the {@link SysInfoLinux}.
   *
   * @param args - arguments to this calculator test",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.HttpsFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
* Opens an input stream for the specified file path.
* @param path file path to access
* @param bufferSize size of buffer for reading data
* @return FSDataInputStream for reading file data
* @throws IOException if an I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.HttpFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
* Opens a stream for reading from a specified path.
* @param path the file path to read from
* @param bufferSize size of the buffer used for reading
* @return FSDataInputStream for reading data
* @throws IOException if an I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,remove,org.apache.hadoop.util.LightWeightCache:remove(java.lang.Object),223,232,"/**
 * Removes and returns element by key.
 * @param key unique identifier for the element
 * @return removed element or null if not found
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,put,org.apache.hadoop.util.LightWeightCache:put(java.lang.Object),201,221,"/**
* Processes and adds an entry to the queue.
* @param entry object to be processed
* @return existing entry or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean)",394,396,"/**
 * Calls overloaded method with default values.
 * @param qOption query option flag
 * @param hOption helper option flag
 * @return result of the overloaded method
 */","Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   * 
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output if to be used
   * @return the string representation of the object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,processPath,org.apache.hadoop.fs.shell.Count:processPath(org.apache.hadoop.fs.shell.PathData),195,217,"/**
* Generates and outputs file system information based on specified flags.
* @param src PathData object containing source path details
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,org.apache.hadoop.fs.QuotaUsage:toString(boolean),307,309,"/**
 * Calls overloaded method with default values.
 * @param hOption primary option flag
 * @return result from overloaded method
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read,"org.apache.hadoop.fs.FSInputChecker:read(byte[],int,int)",191,209,"/**
* Masks bytes in the array.
* @param b byte array to mask
* @param off starting offset
* @param len length of bytes to mask
* @return number of bytes masked or -1 if no bytes read
* @throws IOException on I/O error
*/","* Read checksum verified bytes from this byte-input stream into 
   * the specified byte array, starting at the given offset.
   *
   * <p> This method implements the general contract of the corresponding
   * <code>{@link InputStream#read(byte[], int, int) read}</code> method of
   * the <code>{@link InputStream}</code> class.  As an additional
   * convenience, it attempts to read as many bytes as possible by repeatedly
   * invoking the <code>read</code> method of the underlying stream.  This
   * iterated <code>read</code> continues until one of the following
   * conditions becomes true: <ul>
   *
   *   <li> The specified number of bytes have been read,
   *
   *   <li> The <code>read</code> method of the underlying stream returns
   *   <code>-1</code>, indicating end-of-file.
   *
   * </ul> If the first <code>read</code> on the underlying stream returns
   * <code>-1</code> to indicate end-of-file then this method returns
   * <code>-1</code>.  Otherwise this method returns the number of bytes
   * actually read.
   *
   * @param      b     destination buffer.
   * @param      off   offset at which to start storing bytes.
   * @param      len   maximum number of bytes to read.
   * @return     the number of bytes read, or <code>-1</code> if the end of
   *             the stream has been reached.
   * @exception  IOException  if an I/O error occurs.
   *             ChecksumException if any checksum error occurs",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,createExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.Class,org.apache.hadoop.conf.Configuration)",127,134,"/**
* Creates an Expression instance using reflection.
* @param expressionClass the Class of the Expression to instantiate
* @param conf configuration object for instantiation
* @return Expression instance or null if expressionClass is null
*/","* Creates an instance of the requested {@link Expression} class.
   *
   * @param expressionClass
   *          {@link Expression} class to be instantiated
   * @param conf
   *          the Hadoop configuration
   * @return a new instance of the requested {@link Expression} class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,getInstance,"org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String,org.apache.hadoop.conf.Configuration)",118,131,"/**
 * Retrieves and initializes a Command instance.
 * @param cmdName name of the command
 * @param conf configuration object
 * @return initialized Command or null if not found
 */","* Get an instance of the requested command
   * @param cmdName name of the command to lookup
   * @param conf the hadoop configuration
   * @return the {@link Command} or null if the command is unknown",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,newKey,org.apache.hadoop.io.WritableComparator:newKey(),166,168,"/**
 * Creates an instance of WritableComparable using reflection.
 * @return WritableComparable object
 */","* Construct a new {@link WritableComparable} instance.
   * @return WritableComparable.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,readFields,org.apache.hadoop.io.SortedMapWritable:readFields(java.io.DataInput),156,180,"/**
 * Reads data from input and populates the instance with entries.
 * @param in DataInput to read from
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/GenericWritable.java,readFields,org.apache.hadoop.io.GenericWritable:readFields(java.io.DataInput),124,135,"/**
* Reads and initializes a writable instance from input.
* @param in DataInput source for reading
* @throws IOException if initialization fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,add,"org.apache.hadoop.io.serializer.SerializationFactory:add(org.apache.hadoop.conf.Configuration,java.lang.String)",69,79,"/**
* Loads and registers a serialization class by name.
* @param conf Configuration object containing serialization settings
* @param serializationName name of the serialization class to load
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,deserialize,org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:deserialize(org.apache.hadoop.io.Writable),62,73,"/**
* Creates or updates a Writable object.
* @param w existing Writable object or null for new one
* @return processed Writable object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,readFields,org.apache.hadoop.io.MapWritable:readFields(java.io.DataInput),165,191,"/**
* Reads data from input and processes entries.
* @param in DataInput source
* @throws IOException if an I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableFactories.java,newInstance,"org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",63,74,"/**
 * Creates a writable instance using a factory or reflection.
 * @param c Class type of the writable
 * @param conf Configuration for the writable
 * @return Writable instance configured with given settings
 */","* Create a new instance of a class with a defined factory.
   *
   * @param c input c.
   * @param conf input configuration.
   * @return a new instance of a class with a defined factory.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getSocketFactoryFromProperty,"org.apache.hadoop.net.NetUtils:getSocketFactoryFromProperty(org.apache.hadoop.conf.Configuration,java.lang.String)",142,152,"/**
* Creates a SocketFactory instance using configuration.
* @param conf Configuration object containing factory settings
* @param propValue Property value to determine the factory class
* @return SocketFactory instance
* @throws RuntimeException if class is not found
*/","* Get the socket factory corresponding to the given proxy URI. If the
   * given proxy URI corresponds to an absence of configuration parameter,
   * returns null. If the URI is malformed raises an exception.
   *
   * @param conf configuration.
   * @param propValue the property which is the class name of the
   *        SocketFactory to instantiate; assumed non null and non empty.
   * @return a socket factory as defined in the property value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeIdentifier,org.apache.hadoop.security.token.Token:decodeIdentifier(),164,176,"/**
 * Decodes a token identifier from byte array.
 * @return decoded TokenIdentifier object or null if class is not found
 */","* Get the token identifier object, or null if it could not be constructed
   * (because the class could not be loaded, for example).
   * @return the token identifier, or null if there was no class found for it
   * @throws IOException failure to unmarshall the data
   * @throws RuntimeException if the token class could not be instantiated.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskValidatorFactory.java,getInstance,org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.Class),45,62,"/**
 * Retrieves or creates a DiskValidator instance.
 * @param clazz class type of the DiskValidator to be instantiated
 * @return DiskValidator instance of the specified class
 */","* Returns a {@link DiskValidator} instance corresponding to the passed clazz.
   * @param clazz a class extends {@link DiskValidator}
   * @return disk validator.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,createFenceMethod,"org.apache.hadoop.ha.NodeFencer:createFenceMethod(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",167,195,"/**
* Creates a FenceMethodWithArg instance.
* @param conf configuration object
* @param clazzName class name of the fencing method
* @param arg argument for the fencing method
* @return FenceMethodWithArg instance configured with given parameters
* @throws BadFencingConfigurationException if class not found or does not implement FenceMethod
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>(),144,146,"/**
* Constructs a new DynamicWrappedIO instance with default class name.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>(),224,226,"/**
 * Constructs a new DynamicWrappedStatistics instance.
 * @param className name of the statistics class to wrap
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,"org.apache.hadoop.util.HostsFileReader:refresh(java.lang.String,java.lang.String)",190,193,"/**
 * Masks file content based on include and exclude patterns.
 * @param includesFile path to file containing include patterns
 * @param excludesFile path to file containing exclude patterns
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,lazyRefresh,"org.apache.hadoop.util.HostsFileReader:lazyRefresh(java.lang.String,java.lang.String)",195,198,"/**
 * Masks content based on include and exclude files.
 * @param includesFile file containing patterns to include
 * @param excludesFile file containing patterns to exclude
 * @throws IOException if an I/O error occurs
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>(int,int,int,int)",126,134,"/**
* Initializes a DynamicBloomFilter with specified parameters.
* @param vectorSize size of the bit vector
* @param nbHash number of hash functions
* @param hashType type of hash function to use
* @param nr initial number of records
*/","* Constructor.
   * <p>
   * Builds an empty Dynamic Bloom filter.
   * @param vectorSize The number of bits in the vector.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).
   * @param nr The threshold for the maximum number of keys to record in a
   * dynamic Bloom filter row.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,addRow,org.apache.hadoop.util.bloom.DynamicBloomFilter:addRow(),275,285,"/**
* Adds a new BloomFilter to the matrix.
* @param vectorSize size of the Bloom filter bit vector
* @param nbHash number of hash functions
* @param hashType type of hash function used
*/",* Adds a new row to <i>this</i> dynamic Bloom filter.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>(int,int,int)",111,116,"/**
* Initializes a new RetouchedBloomFilter.
* @param vectorSize size of the bit vector
* @param nbHash number of hash functions
* @param hashType type of hash function to use
*/","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,readFields,org.apache.hadoop.util.bloom.DynamicBloomFilter:readFields(java.io.DataInput),259,270,"/**
* Reads data from input and initializes a Bloom filter array.
* @param in DataInput source
* @throws IOException if an I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,readFields,org.apache.hadoop.util.bloom.RetouchedBloomFilter:readFields(java.io.DataInput),429,454,"/**
* Reads data from input stream and populates vectors.
* @param in DataInput stream to read from
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInstanceHelp,"org.apache.hadoop.fs.FsShell:printInstanceHelp(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)",253,288,"/**
 * Processes and prints command details with table listings.
 * @param out PrintStream to output the result
 * @param instance Command object containing data to process
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadProps,"org.apache.hadoop.conf.Configuration:loadProps(java.util.Properties,int,boolean)",2961,2981,"/**
 * Updates resource properties with optional overlay and backup.
 * @param props properties to update
 * @param startIdx starting index for update
 * @param fullReload flag for full reload
 */","* Loads the resource at a given index into the properties.
   * @param props the object containing the loaded properties.
   * @param startIdx the index where the new resource has been added.
   * @param fullReload flag whether we do complete reload of the conf instead
   *                   of just loading the new resource.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,becomeActive,org.apache.hadoop.ha.ActiveStandbyElector:becomeActive(),936,956,"/**
* Attempts to activate the component.
* @return true if activation successful, otherwise false
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,quitElection,org.apache.hadoop.ha.ActiveStandbyElector:quitElection(boolean),443,452,"/**
* Handles yielding from an election process.
* @param needFence indicates whether a fence is required
*/","* Any service instance can drop out of the election by calling quitElection. 
   * <br>
   * This will lose any leader status, if held, and stop monitoring of the lock
   * node. <br>
   * If the instance wants to participate in election again, then it needs to
   * call joinElection(). <br>
   * This allows service instances to take themselves out of rotation for known
   * impending unavailable states (e.g. long GC pause or software upgrade).
   * 
   * @param needFence true if the underlying daemon may need to be fenced
   * if a failover occurs due to dropping out of the election.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PositionedReadable.java,readVectored,"org.apache.hadoop.fs.PositionedReadable:readVectored(java.util.List,java.util.function.IntFunction)",132,135,"/**
* Reads file ranges using vectored I/O.
* @param ranges list of file ranges to read
* @param allocate function to allocate ByteBuffer
* @throws IOException if an I/O error occurs
*/","* Read fully a list of file ranges asynchronously from this file.
   * The default iterates through the ranges to read each synchronously, but
   * the intent is that FSDataInputStream subclasses can make more efficient
   * readers.
   * As a result of the call, each range will have FileRange.setData(CompletableFuture)
   * called with a future that when complete will have a ByteBuffer with the
   * data from the file's range.
   * <p>
   *   The position returned by getPos() after readVectored() is undefined.
   * </p>
   * <p>
   *   If a file is changed while the readVectored() operation is in progress, the output is
   *   undefined. Some ranges may have old data, some may have new and some may have both.
   * </p>
   * <p>
   *   While a readVectored() operation is in progress, normal read api calls may block.
   * </p>
   * @param ranges the byte ranges to read
   * @param allocate the function to allocate ByteBuffer
   * @throws IOException any IOE.
   * @throws IllegalArgumentException if the any of ranges are invalid, or they overlap.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,close,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:close(),228,248,"/**
* Closes the resource and performs cleanup operations.
* @param none
* @return void
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,read,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:read(org.apache.hadoop.fs.impl.prefetch.BufferData),308,317,"/**
* Masks buffer data by synchronizing and processing it.
* @param data the BufferData object to process
* @throws IOException if an I/O error occurs during processing
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,prefetch,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:prefetch(org.apache.hadoop.fs.impl.prefetch.BufferData,java.time.Instant)",319,329,"/**
* Updates data state and records prefetching statistics.
* @param data the buffer data to update
* @param taskQueuedStartTime time when the task was queued
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long)",109,112,"/**
 * Constructs a block location with specified details.
 * @param names array of hostnames storing the block
 * @param hosts array of IP addresses or hostnames
 * @param offset starting byte offset in the file
 * @param length length of the block in bytes
 */","* Constructor with host, name, offset and length.
   * @param names names array.
   * @param hosts host array.
   * @param offset offset.
   * @param length length.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,<init>,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:<init>(java.lang.String,boolean,long,long,long,org.apache.hadoop.fs.statistics.MeanStatistic)",71,83,"/**
* Constructs a DurationStatisticSummary.
* @param key unique identifier for the statistic
* @param success indicates if the operation was successful
* @param count number of occurrences
* @param max maximum duration value
* @param min minimum duration value
* @param mean MeanStatistic object representing average duration, cloned if not null
*/","* Constructor.
   * @param key Statistic key.
   * @param success Are these success or failure statistics.
   * @param count Count of operation invocations.
   * @param max Max duration; -1 if unknown.
   * @param min Min duration; -1 if unknown.
   * @param mean Mean duration -may be null. (will be cloned)",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,aggregate,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:aggregate(org.apache.hadoop.fs.statistics.IOStatistics),178,199,"/**
 * Aggregates statistics from the given source.
 * @param source the source of IO statistics to aggregate
 * @return true if aggregation is successful, false if source is null
 */","* Aggregate the current statistics with the
   * source reference passed in.
   *
   * The operation is synchronized.
   * @param source source; may be null
   * @return true if a merge took place.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,<init>,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>(org.apache.hadoop.fs.statistics.IOStatistics),123,129,"/**
* Constructs an IOStatisticsSnapshot.
* @param source IOStatistics object to snapshot; if null, initializes empty maps
*/","* Construct, taking a snapshot of the source statistics data
   * if the source is non-null.
   * If the source is null, the empty maps are created
   * @param source statistics source. Nullable.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,foreach,"org.apache.hadoop.util.functional.RemoteIterators:foreach(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.ConsumerRaisingIOE)",273,288,"/**
 * Iterates over a remote iterator, consuming elements and counting them.
 * @param source RemoteIterator providing the data
 * @param consumer ConsumerRaisingIOE to process each element
 * @return Count of processed elements
 * @throws IOException if an I/O error occurs during processing
 */","* Apply an operation to all values of a RemoteIterator.
   *
   * If the iterator is an IOStatisticsSource returning a non-null
   * set of statistics, <i>and</i> this classes log is set to DEBUG,
   * then the statistics of the operation are evaluated and logged at
   * debug.
   * <p>
   * The number of entries processed is returned, as it is useful to
   * know this, especially during tests or when reporting values
   * to users.
   * </p>
   * This does not close the iterator afterwards.
   * @param source iterator source
   * @param consumer consumer of the values.
   * @return the number of elements processed
   * @param <T> type of source
   * @throws IOException if the source RemoteIterator or the consumer raise one.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackInvocation,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackInvocation(org.apache.hadoop.util.functional.InvocationRaisingIOE,java.lang.String,org.apache.hadoop.metrics2.lib.MutableRate)",1014,1024,"/**
* Records I/O operation statistics and updates metrics.
* @param invocation the I/O operation to record
* @param statistic the name of the statistic to update
* @param metric the mutable rate metric to update
* @throws IOException if an I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[])",517,520,"/**
* Writes bytes to a specified position.
* @param position file position to write to
* @param buffer data to write
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataInputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",28,31,"/**
* Initializes a new CryptoFSDataInputStream.
* @param in the underlying FSDataInputStream to wrap
* @param codec the encryption codec to use
* @param bufferSize size of the buffer for decryption
* @param key the encryption key
* @param iv the initialization vector
* @throws IOException if an I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",67,99,"/**
* Processes input and output ByteBuffers for Reed-Solomon encoding.
* @param inputs array of ByteBuffer containing data units
* @param outputs array of ByteBuffer for storing parity units
* @throws IOException if an I/O error occurs during processing
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,writeObject,"org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration)",142,146,"/**
* Writes object to DataOutput.
* @param out destination for writing data
* @param instance object to write
* @param declaredClass class of the object
* @param conf configuration settings
*/","* Write a {@link Writable}, {@link String}, primitive type, or an array of
   * the preceding.
   *
   * @param out DataOutput.
   * @param instance instance.
   * @param conf Configuration.
   * @param declaredClass declaredClass.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,write,org.apache.hadoop.ipc.WritableRpcEngine$Invocation:write(java.io.DataOutput),166,179,"/**
 * Serializes RPC method data to output stream.
 * @param out DataOutput stream to serialize to
 * @throws IOException if I/O error occurs
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestPrefetch(int),256,289,"/**
* Processes block by number.
* @param blockNumber identifier of the block to process
*/","* Requests optional prefetching of the given block.
   * The block is prefetched only if we can acquire a free buffer.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,getData,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getData(int),631,633,"/**
 * Retrieves data from buffer pool using block number.
 * @param blockNumber index of the block in the buffer pool
 * @return BufferData object containing the requested data
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,acquire,org.apache.hadoop.fs.impl.prefetch.BufferPool:acquire(int),125,150,"/**
 * Acquires and returns buffer data for a given block number.
 * @param blockNumber the block identifier to acquire
 * @return BufferData object if successful, otherwise throws an exception
 */","* Acquires a {@code ByteBuffer}; blocking if necessary until one becomes available.
   * @param blockNumber the id of the block to acquire.
   * @return the acquired block's {@code BufferData}.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPathArgument,org.apache.hadoop.fs.shell.Command:processPathArgument(org.apache.hadoop.fs.shell.PathData),315,320,"/**
 * Masks an item by processing its path data.
 * @param item PathData object to be masked
 * @throws IOException if I/O error occurs during masking
 */","*  This is the last chance to modify an argument before going into the
   *  (possibly) recursive {@link #processPaths(PathData, PathData...)}
   *  {@literal ->} {@link #processPath(PathData)} loop.  Ex.  ls and du use
   *  this to expand out directories.
   *  @param item a {@link PathData} representing a path which exists
   *  @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPaths,"org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)",361,380,"/**
 * Recursively processes directory items.
 * @param parent parent directory data
 * @param itemsIterator iterator for child items
 * @throws IOException if an I/O error occurs
 */","* Iterates over the given expanded paths and invokes
   * {@link #processPath(PathData)} on each element. If ""recursive"" is true,
   * will do a post-visit DFS on directories.
   * @param parent if called via a recurse, will be the parent dir, else null
   * @param itemsIterator a iterator of {@link PathData} objects to process
   * @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,resolvePartialGroupNames,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:resolvePartialGroupNames(java.lang.String,java.lang.String,java.lang.String)",280,319,"/**
* Resolves group names for a user.
* @param userName unique user identifier
* @param errMessage error message to append
* @param groupNames comma-separated group names
* @return set of resolved group names
* @throws PartialGroupNameException if resolution fails
*/","* Attempt to partially resolve group names.
   *
   * @param userName the user's name
   * @param errMessage error message from the shell command
   * @param groupNames the incomplete list of group names
   * @return a set of resolved group names
   * @throws PartialGroupNameException if the resolution fails or times out",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File)",1218,1220,"/**
 * Constructs a ShellCommandExecutor with command and directory.
 * @param execString array of command parts to execute
 * @param dir working directory for the command
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,org.apache.hadoop.util.Shell:execCommand(java.lang.String[]),1358,1360,"/**
 * Executes a command with no input and default timeout.
 * @param cmd command to execute as varargs
 * @return output of the command execution
 * @throws IOException if an I/O error occurs
 */","* Static method to execute a shell command.
   * Covers most of the simple cases without requiring the user to implement
   * the <code>Shell</code> interface.
   * @param cmd shell command to execute.
   * @return the output of the executed command.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,"org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[])",1390,1393,"/**
 * Executes command with environment variables.
 * @param env map of environment variables
 * @param cmd command and its arguments
 * @return output of the executed command
 * @throws IOException if an I/O error occurs
 */","* Static method to execute a shell command.
   * Covers most of the simple cases without requiring the user to implement
   * the <code>Shell</code> interface.
   * @param env the map of environment key=value
   * @param cmd shell command to execute.
   * @return the output of the executed command.
   * @throws IOException on any problem.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addAll,org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials),450,452,"/**
 * Calls overloaded m1 method with default flag set to true.
 * @param other Credentials object containing user credentials
 */","* Copy all of the credentials from one credential object into another.
   * Existing secrets and tokens are overwritten.
   * @param other the credentials to copy",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,mergeAll,org.apache.hadoop.security.Credentials:mergeAll(org.apache.hadoop.security.Credentials),459,461,"/**
 * Masks credentials by calling another method.
 * @param other Credentials to be masked
 */","* Copy all of the credentials from one credential object into another.
   * Existing secrets and tokens are not overwritten.
   * @param other the credentials to copy",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DelegationTokenIssuer.java,addDelegationTokens,"org.apache.hadoop.security.token.DelegationTokenIssuer:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)",79,87,"/**
* Generates tokens for a given user.
* @param renewer user identifier for token renewal
* @param credentials user credentials, default if null
* @return array of generated tokens
* @throws IOException if an I/O error occurs
*/","* Given a renewer, add delegation tokens for issuer and it's child issuers
   * to the <code>Credentials</code> object if it is not already present.
   *<p>
   * Note: This method is not intended to be overridden.  Issuers should
   * implement getCanonicalService and getDelegationToken to ensure
   * consistent token acquisition behavior.
   *
   * @param renewer the user allowed to renew the delegation tokens
   * @param credentials cache in which to add new delegation tokens
   * @return list of new delegation tokens
   * @throws IOException thrown if IOException if an IO error occurs.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addToken,org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.security.token.Token),1701,1703,"/**
 * Recursively checks if token is valid.
 * @param token the token to validate
 * @return true if token is valid, false otherwise
 */","* Add a token to this UGI
   * 
   * @param token Token to be added
   * @return true on successful add of new token",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.RawLocalFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),850,854,"/**
 * Sets a new working directory and processes it.
 * @param newDir path to the new directory
 */",* Set the working directory to the given directory.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,exists,org.apache.hadoop.fs.RawLocalFileSystem:exists(org.apache.hadoop.fs.Path),768,771,"/**
* Checks file status using another method.
* @param f file path to check
* @return true if condition met, false otherwise
* @throws IOException if an I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getStatus,org.apache.hadoop.fs.RawLocalFileSystem:getStatus(org.apache.hadoop.fs.Path),866,874,"/**
* Retrieves file system status for a given path.
* @param p the path to check, defaults to root if null
* @return FsStatus object containing size, free space, and used space
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setTimes,"org.apache.hadoop.fs.RawLocalFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",1129,1140,"/**
 * Sets modification and access times for a file.
 * @param p path to the file
 * @param mtime new modification time in milliseconds, or -1 if unchanged
 * @param atime new access time in milliseconds, or -1 if unchanged
 * @throws IOException if an I/O error occurs
 */","* Sets the {@link Path}'s last modified time and last access time to
   * the given valid times.
   *
   * @param mtime the modification time to set (only if no less than zero).
   * @param atime the access time to set (only if no less than zero).
   * @throws IOException if setting the times fails.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,pathToFile,org.apache.hadoop.fs.LocalFileSystem:pathToFile(org.apache.hadoop.fs.Path),79,81,"/**
 * Retrieves a file from the specified path.
 * @param path the path to the file
 * @return the File object representing the path
 */","* Convert a path to a File.
   * @param path the path.
   * @return file.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getUriPath,org.apache.hadoop.fs.viewfs.ViewFileSystem:getUriPath(org.apache.hadoop.fs.Path),264,267,"/**
 * Masks file path.
 * @param p file path to be masked
 * @return masked string representation of the path
 */","* Make the path Absolute and get the path-part of a pathname.
   * Checks that URI matches this file system
   * and that the path-part is a valid name.
   *
   * @param p path
   * @return path-part of the Path p",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),854,859,"/**
* Recursively calls m1 on each node's file system.
* @param newDir directory path to be processed
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,<init>,"org.apache.hadoop.fs.Stat:<init>(org.apache.hadoop.fs.Path,long,boolean,org.apache.hadoop.fs.FileSystem)",50,68,"/**
* Initializes a Stat object with given path and filesystem settings.
* @param path the file path to analyze
* @param blockSize block size for the file system
* @param deref whether to follow symbolic links
* @param fs the FileSystem instance
* @throws IOException if an I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,makeQualified,org.apache.hadoop.fs.Path:makeQualified(org.apache.hadoop.fs.FileSystem),547,550,"/**
 * Deprecated method to obtain a path using file system operations.
 * @param fs FileSystem instance
 * @return Path object
 */","* Returns a qualified path object for the {@link FileSystem}'s working
   * directory.
   *  
   * @param fs the target FileSystem
   * @return a qualified path object for the FileSystem's working directory
   * @deprecated use {@link #makeQualified(URI, Path)}",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,makeQualified,org.apache.hadoop.fs.FileContext:makeQualified(org.apache.hadoop.fs.Path),629,631,"/**
 * Applies transformation to path using default file system and another operation.
 * @param path input file path
 * @return transformed file path
 */","* Make the path fully qualified if it is isn't. 
   * A Fully-qualified path has scheme and authority specified and an absolute
   * path.
   * Use the default file system and working dir in this FileContext to qualify.
   * @param path the path.
   * @return qualified path",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,makeQualified,org.apache.hadoop.fs.AbstractFileSystem:makeQualified(org.apache.hadoop.fs.Path),438,441,"/**
 * Modifies and returns the given file path.
 * @param path the original file path
 * @return modified file path after applying transformations
 */","* Make the path fully qualified to this file system
   * @param path the path.
   * @return the qualified path",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path),1926,1937,"/**
 * Resolves and fetches file status for a given path.
 * @param f the input path
 * @return array of FileStatus objects
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if file is not found
 * @throws UnsupportedFileSystemException if file system is unsupported
 * @throws IOException for other I/O errors
 */","* List the statuses of the files/directories in the given path 
     * if the path is a directory.
     * 
     * @param f is the path
     *
     * @return an array that contains statuses of the files/directories 
     *         in the given path
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for <code>f</code> is
     *           not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,fixRelativePart,org.apache.hadoop.fs.Globber:fixRelativePart(org.apache.hadoop.fs.Path),138,144,"/**
 * Delegates file processing to appropriate handler.
 * @param path file path to process
 * @return processed path or result of operation
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,delete,"org.apache.hadoop.fs.FileContext:delete(org.apache.hadoop.fs.Path,boolean)",842,853,"/**
 * Checks file access recursively or not.
 * @param f the file path to check
 * @param recursive whether to check recursively
 * @return true if accessible, false otherwise
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if file is not found
 * @throws UnsupportedFileSystemException if file system is unsupported
 * @throws IOException for other I/O errors
 */","* Delete a file.
   * @param f the path to delete.
   * @param recursive if path is a directory and set to 
   * true, the directory is deleted else throws an exception. In
   * case of a file the recursive can be set to either true or false.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is invalid
   *
   * @return if delete success true, not false.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,open,org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path),873,883,"/**
 * Opens a file for reading.
 * @param f file path
 * @return FSDataInputStream for the file
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if file does not exist
 * @throws UnsupportedFileSystemException if file system is unsupported
 * @throws IOException if an I/O error occurs
 */","* Opens an FSDataInputStream at the indicated Path using
   * default buffersize.
   * @param f the file name to open
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * @return input stream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,open,"org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path,int)",904,915,"/**
* Opens an input stream for a file with specified buffer size.
* @param f file path
* @param bufferSize size of the buffer
* @return FSDataInputStream for reading the file
* @throws IOException if an I/O error occurs
*/","* Opens an FSDataInputStream at the indicated Path.
   * 
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * @return output stream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,truncate,"org.apache.hadoop.fs.FileContext:truncate(org.apache.hadoop.fs.Path,long)",947,958,"/**
 * Resolves and sets the length of a file.
 * @param f file path
 * @param newLength new length to set for the file
 * @return true if operation is successful, false otherwise
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if file does not exist
 * @throws UnsupportedFileSystemException if file system is unsupported
 * @throws IOException if an I/O error occurs
 */","* Truncate the file in the indicated path to the indicated size.
   * <ul>
   * <li>Fails if path is a directory.
   * <li>Fails if path does not exist.
   * <li>Fails if path is not closed.
   * <li>Fails if new size is greater than current size.
   * </ul>
   * @param f The path to the file to be truncated
   * @param newLength The size the file is to be truncated to
   *
   * @return <code>true</code> if the file has been truncated to the desired
   * <code>newLength</code> and is immediately available to be reused for
   * write operations such as <code>append</code>, or
   * <code>false</code> if a background process of adjusting the length of
   * the last block has been started, and clients should wait for it to
   * complete before proceeding with further file updates.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   *
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setReplication,"org.apache.hadoop.fs.FileContext:setReplication(org.apache.hadoop.fs.Path,short)",978,989,"/**
* Sets file replication factor.
* @param f file path
* @param replication desired replication level
* @return true if operation successful
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file not found
* @throws IOException for other I/O errors
*/","* Set replication for an existing file.
   * 
   * @param f file name
   * @param replication new replication
   *
   * @return true if successful
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setPermission,"org.apache.hadoop.fs.FileContext:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1079,1091,"/**
* Applies file permissions to a path.
* @param f the path to modify
* @param permission new file permissions
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws UnsupportedFileSystemException if file system is unsupported
* @throws IOException for other I/O errors
*/","* Set permission of a path.
   * @param f the path.
   * @param permission - the new absolute permission (umask is not applied)
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setOwner,"org.apache.hadoop.fs.FileContext:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1117,1134,"/**
 * Sets file owner and group.
 * @param f file path
 * @param username new owner's name
 * @param groupname new group's name
 * @throws AccessControlException if permission denied
 * @throws UnsupportedFileSystemException if unsupported file system
 * @throws FileNotFoundException if file not found
 * @throws IOException for other I/O errors
 */","* Set owner of a path (i.e. a file or a directory). The parameters username
   * and groupname cannot both be null.
   * 
   * @param f The path
   * @param username If it is null, the original username remains unchanged.
   * @param groupname If it is null, the original groupname remains unchanged.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws HadoopIllegalArgumentException If <code>username</code> or
   *           <code>groupname</code> is invalid.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setTimes,"org.apache.hadoop.fs.FileContext:setTimes(org.apache.hadoop.fs.Path,long,long)",1158,1170,"/**
 * Updates file timestamps and resolves links.
 * @param f file path to update
 * @param mtime modification time in milliseconds
 * @param atime access time in milliseconds
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if the file does not exist
 * @throws UnsupportedFileSystemException if the file system is unsupported
 * @throws IOException for other I/O errors
 */","* Set access time of a file.
   * @param f The path
   * @param mtime Set the modification time of this file.
   *        The number of milliseconds since epoch (Jan 1, 1970). 
   *        A value of -1 means that this call should not set modification time.
   * @param atime Set the access time of this file.
   *        The number of milliseconds since Jan 1, 1970. 
   *        A value of -1 means that this call should not set access time.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileChecksum,org.apache.hadoop.fs.FileContext:getFileChecksum(org.apache.hadoop.fs.Path),1191,1202,"/**
* Computes checksum for a file.
* @param f file path
* @return FileChecksum object
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws IOException on I/O errors
*/","* Get the checksum of a file.
   *
   * @param f file path
   *
   * @return The file checksum.  The default return value is null,
   *  which indicates that no checksum algorithm is implemented
   *  in the corresponding FileSystem.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileStatus,org.apache.hadoop.fs.FileContext:getFileStatus(org.apache.hadoop.fs.Path),1248,1258,"/**
 * Resolves and returns file status for a given path.
 * @param f file path to check
 * @return FileStatus object representing the file's status
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if file does not exist
 * @throws UnsupportedFileSystemException if file system is unsupported
 * @throws IOException for other I/O errors
 */","* Return a file status object that represents the path.
   * @param f The path we want information from
   *
   * @return a FileStatus object
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,access,"org.apache.hadoop.fs.FileContext:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",1305,1318,"/**
* Applies access control to a file path.
* @param path the file path to apply controls to
* @param mode the file system action mode
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if the file does not exist
* @throws UnsupportedFileSystemException if the file system is unsupported
* @throws IOException for other I/O errors
*/","* Checks if the user can access a path.  The mode specifies which access
   * checks to perform.  If the requested permissions are granted, then the
   * method returns normally.  If access is denied, then the method throws an
   * {@link AccessControlException}.
   * <p>
   * The default implementation of this method calls {@link #getFileStatus(Path)}
   * and checks the returned permissions against the requested permissions.
   * Note that the getFileStatus call will be subject to authorization checks.
   * Typically, this requires search (execute) permissions on each directory in
   * the path's prefix, but this is implementation-defined.  Any file system
   * that provides a richer authorization model (such as ACLs) may override the
   * default implementation so that it checks against that model instead.
   * <p>
   * In general, applications should avoid using this method, due to the risk of
   * time-of-check/time-of-use race conditions.  The permissions on a file may
   * change immediately after the access call returns.  Most applications should
   * prefer running specific file system actions as the desired user represented
   * by a {@link UserGroupInformation}.
   *
   * @param path Path to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws FileNotFoundException if the path does not exist
   * @throws UnsupportedFileSystemException if file system for <code>path</code>
   *   is not supported
   * @throws IOException see specific implementation
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileLinkStatus,org.apache.hadoop.fs.FileContext:getFileLinkStatus(org.apache.hadoop.fs.Path),1334,1350,"/**
* Resolves file status for a given path.
* @param f file path to resolve
* @return FileStatus object representing the file's status
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws UnsupportedFileSystemException if file system is unsupported
* @throws IOException on I/O errors
*/","* Return a file status object that represents the path. If the path 
   * refers to a symlink then the FileStatus of the symlink is returned.
   * The behavior is equivalent to #getFileStatus() if the underlying
   * file system does not support symbolic links.
   * @param  f The path we want information from.
   * @return A FileStatus object
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLinkTarget,org.apache.hadoop.fs.FileContext:getLinkTarget(org.apache.hadoop.fs.Path),1367,1378,"/**
* Resolves a file path with access control checks.
* @param f input file path
* @return resolved Path object
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file is not found
* @throws UnsupportedFileSystemException if file system is unsupported
* @throws IOException for other I/O errors
*/","* Returns the target of the given symbolic link as it was specified
   * when the link was created.  Links in the path leading up to the
   * final path component are resolved transparently.
   *
   * @param f the path to return the target of
   * @return The un-interpreted target of the symbolic link.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If path <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If the given path does not refer to a symlink
   *           or an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileBlockLocations,"org.apache.hadoop.fs.FileContext:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",1437,1450,"/**
 * Retrieves block locations for a file within a specified range.
 * @param f the file path
 * @param start the starting offset of the range
 * @param len the length of the range
 * @return array of BlockLocation objects
 * @throws IOException if an I/O error occurs
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if the file does not exist
 * @throws UnsupportedFileSystemException if the file system is unsupported
 */","* Return blockLocation of the given file for the given offset and len.
   *  For a nonexistent file or regions, null will be returned.
   *
   * This call is most helpful with DFS, where it returns 
   * hostnames of machines that contain the given file.
   *
   * In HDFS, if file is three-replicated, the returned array contains
   * elements like:
   * <pre>
   * BlockLocation(offset: 0, length: BLOCK_SIZE,
   *   hosts: {""host1:9866"", ""host2:9866, host3:9866""})
   * BlockLocation(offset: BLOCK_SIZE, length: BLOCK_SIZE,
   *   hosts: {""host2:9866"", ""host3:9866, host4:9866""})
   * </pre>
   *
   * And if a file is erasure-coded, the returned BlockLocation are logical
   * block groups.
   *
   * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).
   * 1. If the file size is less than one stripe size, say 2 * CELL_SIZE, then
   * there will be one BlockLocation returned, with 0 offset, actual file size
   * and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks.
   * 3. If the file size is less than one group size but greater than one
   * stripe size, then there will be one BlockLocation returned, with 0 offset,
   * actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting
   * the actual blocks.
   * 4. If the file size is greater than one group size, 3 * BLOCK_SIZE + 123
   * for example, then the result will be like:
   * <pre>
   * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {""host1:9866"",
   *   ""host2:9866"",""host3:9866"",""host4:9866"",""host5:9866""})
   * BlockLocation(offset: 3 * BLOCK_SIZE, length: 123, hosts: {""host1:9866"",
   *   ""host4:9866"", ""host5:9866""})
   * </pre>
   *
   * @param f - get blocklocations of this file
   * @param start position (byte offset)
   * @param len (in bytes)
   *
   * @return block locations for given file at specified offset of len
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is invalid",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFsStatus,org.apache.hadoop.fs.FileContext:getFsStatus(org.apache.hadoop.fs.Path),1476,1489,"/**
* Resolves file status by path.
* @param f file path to resolve
* @return FsStatus of the file or default if null
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file is not found
* @throws UnsupportedFileSystemException if filesystem is unsupported
* @throws IOException for other I/O errors
*/","* Returns a status object describing the use and capacity of the
   * file system denoted by the Parh argument p.
   * If the file system has multiple partitions, the
   * use and capacity of the partition pointed to by the specified
   * path is reflected.
   * 
   * @param f Path for which status should be obtained. null means the
   * root partition of the default file system. 
   *
   * @return a FsStatus object
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSymlink,"org.apache.hadoop.fs.FileContext:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1570,1588,"/**
* Creates a symbolic link.
* @param target the path to which the link should point
* @param link the path of the new symbolic link
* @param createParent if true, creates parent directories as needed
* @throws various exceptions based on file system operations
*/","* Creates a symbolic link to an existing file. An exception is thrown if 
   * the symlink exits, the user does not have permission to create symlink,
   * or the underlying file system does not support symlinks.
   * 
   * Symlink permissions are ignored, access to a symlink is determined by
   * the permissions of the symlink target.
   * 
   * Symlinks in paths leading up to the final path component are resolved 
   * transparently. If the final path component refers to a symlink some 
   * functions operate on the symlink itself, these are:
   * - delete(f) and deleteOnExit(f) - Deletes the symlink.
   * - rename(src, dst) - If src refers to a symlink, the symlink is 
   *   renamed. If dst refers to a symlink, the symlink is over-written.
   * - getLinkTarget(f) - Returns the target of the symlink. 
   * - getFileLinkStatus(f) - Returns a FileStatus object describing
   *   the symlink.
   * Some functions, create() and mkdir(), expect the final path component
   * does not exist. If they are given a path that refers to a symlink that 
   * does exist they behave as if the path referred to an existing file or 
   * directory. All other functions fully resolve, ie follow, the symlink. 
   * These are: open, setReplication, setOwner, setTimes, setWorkingDirectory,
   * setPermission, getFileChecksum, setVerifyChecksum, getFileBlockLocations,
   * getFsStatus, getFileStatus, exists, and listStatus.
   * 
   * Symlink targets are stored as given to createSymlink, assuming the 
   * underlying file system is capable of storing a fully qualified URI.
   * Dangling symlinks are permitted. FileContext supports four types of 
   * symlink targets, and resolves them as follows
   * <pre>
   * Given a path referring to a symlink of form:
   * 
   *   {@literal <---}X{@literal --->}
   *   fs://host/A/B/link 
   *   {@literal <-----}Y{@literal ----->}
   * 
   * In this path X is the scheme and authority that identify the file system,
   * and Y is the path leading up to the final path component ""link"". If Y is
   * a symlink  itself then let Y' be the target of Y and X' be the scheme and
   * authority of Y'. Symlink targets may:
   * 
   * 1. Fully qualified URIs
   * 
   * fs://hostX/A/B/file  Resolved according to the target file system.
   * 
   * 2. Partially qualified URIs (eg scheme but no host)
   * 
   * fs:///A/B/file  Resolved according to the target file system. Eg resolving
   *                 a symlink to hdfs:///A results in an exception because
   *                 HDFS URIs must be fully qualified, while a symlink to 
   *                 file:///A will not since Hadoop's local file systems 
   *                 require partially qualified URIs.
   * 
   * 3. Relative paths
   * 
   * path  Resolves to [Y'][path]. Eg if Y resolves to hdfs://host/A and path 
   *       is ""../B/file"" then [Y'][path] is hdfs://host/B/file
   * 
   * 4. Absolute paths
   * 
   * path  Resolves to [X'][path]. Eg if Y resolves hdfs://host/A/B and path
   *       is ""/file"" then [X][path] is hdfs://host/file
   * </pre>
   * 
   * @param target the target of the symbolic link
   * @param link the path to be created that points to target
   * @param createParent if true then missing parent dirs are created if 
   *                     false then parent must exist
   *
   *
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If file <code>link</code> already exists
   * @throws FileNotFoundException If <code>target</code> does not exist
   * @throws ParentNotDirectoryException If parent of <code>link</code> is not a
   *           directory.
   * @throws UnsupportedFileSystemException If file system for 
   *           <code>target</code> or <code>link</code> is not supported
   * @throws IOException If an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext:listStatus(org.apache.hadoop.fs.Path),1611,1623,"/**
* Retrieves file status iterator for a given path.
* @param f file path
* @return RemoteIterator of FileStatus objects
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws UnsupportedFileSystemException if file system is unsupported
* @throws IOException if an I/O error occurs
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   * 
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories 
   *         in the given path
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listCorruptFileBlocks,org.apache.hadoop.fs.FileContext:listCorruptFileBlocks(org.apache.hadoop.fs.Path),1633,1644,"/**
* Resolves and returns a remote iterator for the given path.
* @param path the input file path
* @return RemoteIterator of paths or null if unresolved
*/","* List CorruptFile Blocks.
   *
   * @param path the path.
   * @return an iterator over the corrupt files under the given path
   * (may contain duplicates if a file has more than one corrupt block)
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listLocatedStatus,org.apache.hadoop.fs.FileContext:listLocatedStatus(org.apache.hadoop.fs.Path),1673,1686,"/**
* Retrieves file status iterator for a given path.
* @param f the input file path
* @return RemoteIterator of LocatedFileStatus objects
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws UnsupportedFileSystemException if filesystem is unsupported
* @throws IOException for other I/O errors
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory. 
   * Return the file's status and block locations If the path is a file.
   * 
   * If a returned status is a file, it contains the file's block locations.
   *
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories 
   *         in the given path
   * If any IO exception (for example the input directory gets deleted while
   * listing is being executed), next() or hasNext() of the returned iterator
   * may throw a RuntimeException with the io exception as the cause.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolveAbstractFileSystems,org.apache.hadoop.fs.FileContext:resolveAbstractFileSystems(org.apache.hadoop.fs.Path),2372,2386,"/**
 * Resolves file system links and collects linked file systems.
 * @param f target file path
 * @return set of resolved AbstractFileSystem instances
 * @throws IOException if an I/O error occurs
 */","* Returns the list of AbstractFileSystems accessed in the path. The list may
   * contain more than one AbstractFileSystems objects in case of symlinks.
   * 
   * @param f
   *          Path which needs to be resolved
   * @return List of AbstractFileSystems accessed in the path
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,modifyAclEntries,"org.apache.hadoop.fs.FileContext:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",2456,2467,"/**
* Applies ACL to file path.
* @param path target file path
* @param aclSpec list of ACL entries
*/","* Modifies ACL entries of files and directories.  This method can add new ACL
   * entries or modify the permissions on existing ACL entries.  All existing
   * ACL entries that are not specified in this call are retained without
   * changes.  (Modifications are merged into the current ACL.)
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing
   * modifications
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeAclEntries,"org.apache.hadoop.fs.FileContext:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",2478,2489,"/**
* Applies ACL to a file path.
* @param path target file path
* @param aclSpec list of access control entries
* @throws IOException if an I/O error occurs
*/","* Removes ACL entries from files and directories.  Other ACL entries are
   * retained.
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing entries
   * to remove
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeDefaultAcl,org.apache.hadoop.fs.FileContext:removeDefaultAcl(org.apache.hadoop.fs.Path),2497,2508,"/**
* Resolves and processes a file system link.
* @param path the initial file path
*/","* Removes all default ACL entries from files and directories.
   *
   * @param path Path to modify
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeAcl,org.apache.hadoop.fs.FileContext:removeAcl(org.apache.hadoop.fs.Path),2518,2528,"/**
* Resolves and processes a file system link.
* @param path the input file path
* @throws IOException if an I/O error occurs
*/","* Removes all but the base ACL entries of files and directories.  The entries
   * for user, group, and others are retained for compatibility with permission
   * bits.
   *
   * @param path Path to modify
   * @throws IOException if an ACL could not be removed",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setAcl,"org.apache.hadoop.fs.FileContext:setAcl(org.apache.hadoop.fs.Path,java.util.List)",2540,2551,"/**
 * Applies ACL specifications to a file path.
 * @param path the file path to apply ACLs
 * @param aclSpec list of ACL entries to set
 * @throws IOException if an I/O error occurs
 */","* Fully replaces ACL of files and directories, discarding all existing
   * entries.
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing
   * modifications, must include entries for user, group, and others for
   * compatibility with permission bits.
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAclStatus,org.apache.hadoop.fs.FileContext:getAclStatus(org.apache.hadoop.fs.Path),2561,2570,"/**
* Retrieves ACL status for a given path.
* @param path file system path
* @return AclStatus object representing the ACL permissions
* @throws IOException if an I/O error occurs
*/","* Gets the ACLs of files and directories.
   *
   * @param path Path to get
   * @return RemoteIterator{@literal <}AclStatus{@literal >} which returns
   *         each AclStatus
   * @throws IOException if an ACL could not be read",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setXAttr,"org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",2603,2614,"/**
 * Sets an extended attribute on a file path.
 * @param path the file path
 * @param name the attribute name
 * @param value the attribute value
 * @param flag the set of flags for setting the attribute
 * @throws IOException if an I/O error occurs
 */","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @param flag xattr set flag
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttr,"org.apache.hadoop.fs.FileContext:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",2628,2637,"/**
 * Reads file content by path and name.
 * @param path directory path
 * @param name file name
 * @return byte array of file content
 * @throws IOException if an I/O error occurs
 */","* Get an xattr for a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attribute
   * @param name xattr name.
   * @return byte[] xattr value.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttrs,org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path),2651,2660,"/**
* Reads files from a directory.
* @param path directory path
* @return map of file names to their contents
*/","* Get all of the xattrs for a file or directory.
   * Only those xattrs for which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @return Map{@literal <}String, byte[]{@literal >} describing the XAttrs
   * of the file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttrs,"org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",2675,2685,"/**
* Reads files by name from a directory.
* @param path directory path
* @param names list of file names to read
* @return map of file names and their byte contents
*/","* Get all of the xattrs for a file or directory.
   * Only those xattrs for which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @param names XAttr names.
   * @return Map{@literal <}String, byte[]{@literal >} describing the XAttrs
   * of the file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeXAttr,"org.apache.hadoop.fs.FileContext:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",2698,2708,"/**
* Resolves and processes a file system link.
* @param path the initial file path
* @param name the target name for processing
* @throws IOException if an I/O error occurs
*/","* Remove an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to remove extended attribute
   * @param name xattr name
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listXAttrs,org.apache.hadoop.fs.FileContext:listXAttrs(org.apache.hadoop.fs.Path),2722,2731,"/**
* Resolves and lists files in a directory.
* @param path directory path
* @return list of file names or null if error occurs
*/","* Get all of the xattr names for a file or directory.
   * Only those xattr names which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @return List{@literal <}String{@literal >} of the XAttr names of the
   * file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSnapshot,"org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",2766,2777,"/**
* Resolves a path using a filesystem snapshot.
* @param path the original file path
* @param snapshotName name of the snapshot to use
* @return resolved path or throws IOException if fails
*/","* Create a snapshot.
   *
   * @param path The directory where snapshots will be taken.
   * @param snapshotName The name of the snapshot
   * @return the snapshot path.
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,renameSnapshot,"org.apache.hadoop.fs.FileContext:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",2794,2805,"/**
* Renames a file snapshot in the specified path.
* @param path directory containing the snapshot
* @param snapshotOldName current name of the snapshot
* @param snapshotNewName new name for the snapshot
* @throws IOException if an I/O error occurs during renaming
*/","* Rename a snapshot.
   *
   * @param path The directory path where the snapshot was taken
   * @param snapshotOldName Old name of the snapshot
   * @param snapshotNewName New name of the snapshot
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,deleteSnapshot,"org.apache.hadoop.fs.FileContext:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",2821,2832,"/**
 * Creates a snapshot of the specified path.
 * @param path file system path to snapshot
 * @param snapshotName name for the snapshot
 * @throws IOException if an I/O error occurs
 */","* Delete a snapshot of a directory.
   *
   * @param path The directory that the to-be-deleted snapshot belongs to
   * @param snapshotName The name of the snapshot
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,satisfyStoragePolicy,org.apache.hadoop.fs.FileContext:satisfyStoragePolicy(org.apache.hadoop.fs.Path),2839,2850,"/**
 * Resolves and processes a file system link.
 * @param path the path to be processed
 */","* Set the source path to satisfy storage policy.
   * @param path The source path referring to either a directory or a file.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setStoragePolicy,"org.apache.hadoop.fs.FileContext:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",2861,2872,"/**
 * Resolves and applies a file system link policy.
 * @param path the target file path
 * @param policyName the name of the policy to apply
 * @throws IOException if an I/O error occurs
 */","* Set the storage policy for a given file or directory.
   *
   * @param path file or directory path.
   * @param policyName the name of the target storage policy. The list
   *                   of supported Storage policies can be retrieved
   *                   via {@link #getAllStoragePolicies}.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,unsetStoragePolicy,org.apache.hadoop.fs.FileContext:unsetStoragePolicy(org.apache.hadoop.fs.Path),2879,2889,"/**
 * Processes a file path by resolving and manipulating it.
 * @param src source file path to process
 * @throws IOException if an I/O error occurs during processing
 */","* Unset the storage policy set for a given file or directory.
   * @param src file or directory path.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getStoragePolicy,org.apache.hadoop.fs.FileContext:getStoragePolicy(org.apache.hadoop.fs.Path),2898,2908,"/**
* Resolves block storage policy for a given path.
* @param path file system path
* @return BlockStoragePolicySpi object
*/","* Query the effective storage policy ID for the given file or directory.
   *
   * @param path file or directory path.
   * @return storage policy for give file.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,hasPathCapability,"org.apache.hadoop.fs.FileContext:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",3001,3007,"/**
* Checks capability for a file system path.
* @param path file system path to check
* @param capability required capability
* @return true if capability is granted, false otherwise
* @throws IOException if an I/O error occurs
*/","* Return the path capabilities of the bonded {@code AbstractFileSystem}.
   * @param path path to query the capability of.
   * @param capability string to query the stream support for.
   * @return true iff the capability is supported under that FS.
   * @throws IOException path resolution or other IO failure
   * @throws IllegalArgumentException invalid arguments",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getServerDefaults,org.apache.hadoop.fs.FileContext:getServerDefaults(org.apache.hadoop.fs.Path),3015,3020,"/**
* Resolves file system server defaults.
* @param path file path to resolve
* @return FsServerDefaults object
* @throws IOException if an I/O error occurs
*/","* Return a set of server default configuration values based on path.
   * @param path path to fetch server defaults
   * @return server default configuration values for path
   * @throws IOException an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createMultipartUploader,org.apache.hadoop.fs.FileContext:createMultipartUploader(org.apache.hadoop.fs.Path),3029,3035,"/**
* Initializes multipart uploader with base path.
* @param basePath root directory for uploads
* @return MultipartUploaderBuilder instance
* @throws IOException if file system access fails
*/","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.HarFileSystem:getInitialWorkingDirectory(),278,281,"/**
 * Returns the path from function M1.
 * @return Path object representing the function's output
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory(com.jcraft.jsch.ChannelSftp),652,655,"/**
 * Returns a file path using SFTP client.
 * @param client the SFTP client instance
 * @return the file path as a Path object
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,<init>,org.apache.hadoop.fs.RawLocalFileSystem:<init>(),101,103,"/**
* Initializes the file system with the initial working directory.
* Sets the working directory to the default starting location.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,refreshStatus,org.apache.hadoop.fs.shell.PathData:refreshStatus(),198,208,"/**
 * Retrieves file status.
 * @return FileStatus object or null if an error occurs
 * @throws IOException if I/O error occurs during retrieval
 */","* Updates the paths's file status
   * @return the updated FileStatus
   * @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getUsed,org.apache.hadoop.fs.HarFileSystem:getUsed(),1271,1274,"/**
 * Delegates to file system's m1 method.
 * @throws IOException if an I/O error occurs
 * @return result from file system's m1 method
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getUsed,org.apache.hadoop.fs.FilterFileSystem:getUsed(),415,418,"/**
 * Delegates to filesystem's m1 method.
 * @throws IOException if an I/O error occurs
 */",Return the total size of all files in the filesystem.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,resolve,"org.apache.hadoop.fs.viewfs.RegexMountPoint:resolve(java.lang.String,boolean)",168,205,"/**
* Resolves path using regex and interceptors.
* @param srcPath source path to resolve
* @param resolveLastComponent flag to resolve last component
* @return ResolveResult object or null if not resolved
*/","* Get resolved path from regex mount points.
   *  E.g. link: ^/user/(?<username>\\w+) => s3://$user.apache.com/_${user}
   *  srcPath: is /user/hadoop/dir1
   *  resolveLastComponent: true
   *  then return value is s3://hadoop.apache.com/_hadoop
   * @param srcPath - the src path to resolve
   * @param resolveLastComponent - whether resolve the path after last `/`
   * @return mapped path of the mount point.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkDest,"org.apache.hadoop.fs.FileUtil:checkDest(java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",607,630,"/**
 * Recursively copies a file or directory to a destination path.
 * @param srcName source name (null for directories)
 * @param dstFS destination filesystem
 * @param dst destination path
 * @param overwrite flag to overwrite existing files
 * @return destination path after copy
 * @throws IOException if an I/O error occurs
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,advance,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:advance(),561,569,"/**
* Masks files in specified directories.
* @throws IOException if file system operations fail
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,ifExists,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)",615,636,"/**
* Checks if a file exists in local directories.
* @param pathStr file path string
* @param conf configuration object
* @return true if file is found, false otherwise
*/","We search through all the configured dirs for the file's existence
     *  and return true when we find one",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,delete,"org.apache.hadoop.io.MapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)",913,921,"/**
* Marks a directory and its files as deleted.
* @param fs FileSystem object to perform operations on
* @param name directory name
*/","* Deletes the named map file.
   * @param fs input fs.
   * @param name input name.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,delete,"org.apache.hadoop.io.BloomMapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)",59,69,"/**
 * Deletes MapFile components and directory.
 * @param fs FileSystem instance
 * @param name directory name containing MapFile components
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,findCurrentDirectory,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:findCurrentDirectory(java.util.Date),551,558,"/**
* Generates a path for the current directory based on the date.
* @param now current date
* @return Path object representing the directory
*/","* Use the given time to determine the current directory. The current
   * directory will be based on the {@link #rollIntervalMinutes}.
   *
   * @param now the current time
   * @return the current directory",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,createForWrite,"org.apache.hadoop.io.SecureIOUtils:createForWrite(java.io.File,int)",273,280,"/**
* Opens a file output stream with specified permissions.
* @param f the File object to be opened
* @param permissions access permissions for the file
* @return FileOutputStream instance
* @throws IOException if an I/O error occurs
*/","* Open the specified File for write access, ensuring that it does not exist.
   * @param f the file that we want to create
   * @param permissions we want to have on the file (if security is enabled)
   *
   * @throws AlreadyExistsException if the file already exists
   * @throws IOException if any other error occurred
   * @return createForWrite FileOutputStream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:initFileSystem(java.net.URI),165,171,"/**
* Initializes the JKS path from a keystore URI.
* @param keystoreUri URI of the keystore
* @throws IOException if an I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,extractKMSPath,org.apache.hadoop.crypto.key.kms.KMSClientProvider:extractKMSPath(java.net.URI),443,445,"/**
 * Converts URI to Path using utility function.
 * @param uri input URI to be converted
 * @return Path object representing the URI
 * @throws MalformedURLException if URI is malformed
 * @throws IOException if an I/O error occurs
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,checkPathsForReservedRaw,"org.apache.hadoop.fs.shell.CommandWithDestination:checkPathsForReservedRaw(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",385,406,"/**
 * Checks paths for raw reservation and throws exception if mismatch.
 * @param src source path
 * @param target target path
 * @return true if both paths are in raw reservation, false otherwise
 */","* Check the source and target paths to ensure that they are either both in
   * /.reserved/raw or neither in /.reserved/raw. If neither src nor target are
   * in /.reserved/raw, then return false, indicating not to preserve raw.*
   * xattrs. If both src/target are in /.reserved/raw, then return true,
   * indicating raw.* xattrs should be preserved. If only one of src/target is
   * in /.reserved/raw then throw an exception.
   *
   * @param src The source path to check. This should be a fully-qualified
   *            path, not relative.
   * @param target The target path to check. This should be a fully-qualified
   *               path, not relative.
   * @return true if raw.* xattrs should be preserved.
   * @throws PathOperationException is only one of src/target are in
   * /.reserved/raw.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",997,1042,"/**
 * Creates a file with specified options.
 * @param f file path to create
 * @param flag creation flags
 * @param absolutePermission permissions for the new file
 * @param bufferSize buffer size for I/O operations
 * @param replication desired block replication factor
 * @param blockSize desired block size
 * @param progress progress monitor
 * @param checksumOpt checksum option
 * @param createParent whether to create parent directories if needed
 * @throws various exceptions for different error conditions
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",1051,1071,"/**
* Retrieves block locations for a given file path.
* @param f file path
* @param start starting offset
* @param len length of data
* @return array of BlockLocation objects
* @throws FileNotFoundException if the path is invalid or points to a directory
* @throws IOException on I/O errors
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,mkdir,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",1267,1299,"/**
* Creates a directory with specified permissions.
* @param dir path of the directory to create
* @param permission file system permissions for the directory
* @param createParent flag to create parent directories if necessary
* @throws IOException if an I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1458,1496,"/**
 * Creates a new file with specified permissions and options.
 * @param f file path
 * @param permission file permissions
 * @param overwrite flag to overwrite existing file
 * @param bufferSize buffer size for file operations
 * @param replication replication factor
 * @param blockSize block size for the file
 * @param progress progress tracking object
 * @return FSDataOutputStream for writing to the file
 * @throws IOException if an I/O error occurs
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatusForFallbackLink,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatusForFallbackLink(),1638,1657,"/**
 * Retrieves file statuses from a linked fallback filesystem.
 * @return Array of FileStatus objects or empty array if none found
 * @throws IOException on I/O errors during filesystem operations
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1696,1730,"/**
* Creates a directory with specified permission.
* @param dir directory path to create
* @param permission file system permissions for the new directory
* @return true if directory creation is successful, false otherwise
* @throws IOException if an I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,makeTrashRelativePath,"org.apache.hadoop.fs.TrashPolicyDefault:makeTrashRelativePath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",120,122,"/**
 * Constructs a path by combining base and relative paths.
 * @param basePath root directory path
 * @param rmFilePath relative file path to append
 * @return combined full path
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,innerPutPart,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerPutPart(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)",124,153,"/**
 * Handles file upload by part.
 * @param filePath source file path
 * @param inputStream input stream of the file part
 * @param partNumber sequential part number
 * @param uploadId unique upload identifier
 * @param lengthInBytes length of the part in bytes
 * @return PartHandle representing the uploaded part
 * @throws IOException if I/O operations fail
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getParent,org.apache.hadoop.fs.Path:getParent(),429,431,"/**
 * Returns the masked path.
 * @return Path object representing the masked path
 */","* Returns the parent of a path or null if at root. Better alternative is
   * {@link #getOptionalParentPath()} to handle nullable value for root path.
   *
   * @return the parent of a path or null if at root",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getOptionalParentPath,org.apache.hadoop.fs.Path:getOptionalParentPath(),440,442,"/**
 * Returns an optional path.
 * @return Optional containing Path or empty if not applicable
 */","* Returns the parent of a path as {@link Optional} or
   * {@link Optional#empty()} i.e an empty Optional if at root.
   *
   * @return Parent of path wrappen in {@link Optional}.
   * {@link Optional#empty()} i.e an empty Optional if at root.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getDirectoryContentsIterator,org.apache.hadoop.fs.shell.PathData:getDirectoryContentsIterator(),293,299,"/**
 * Retrieves directory contents as an iterator.
 * @return Iterator of PathData objects representing files in the directory
 * @throws IOException if an I/O error occurs
 */","* Returns a RemoteIterator for PathData objects of the items contained in the
   * given directory.
   * @return remote iterator of PathData objects for its children
   * @throws IOException if anything else goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,schemeFromPath,org.apache.hadoop.fs.Globber:schemeFromPath(org.apache.hadoop.fs.Path),171,182,"/**
 * Retrieves the scheme from a given file path.
 * @param path the file path to process
 * @return the scheme of the file path or default value if not found
 * @throws IOException if an I/O error occurs
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,authorityFromPath,org.apache.hadoop.fs.Globber:authorityFromPath(org.apache.hadoop.fs.Path),184,195,"/**
 * Retrieves the authority from a given path.
 * @param path file system path
 * @return authority string or null if not found
 * @throws IOException if an I/O error occurs
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,<init>,"org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",111,122,"/**
* Constructs a FSDataOutputStreamBuilder.
* @param fc FileContext instance
* @param p Path to file
* @throws IOException if an I/O error occurs
*/","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param p path.
   * @throws IOException failure",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setVerifyChecksum,"org.apache.hadoop.fs.FileContext:setVerifyChecksum(boolean,org.apache.hadoop.fs.Path)",1223,1228,"/**
* Verifies file checksum and checks access.
* @param verifyChecksum flag to enable/disable checksum verification
* @param f path to the file
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws UnsupportedFileSystemException if file system is unsupported
* @throws IOException for other I/O errors
*/","* Set the verify checksum flag for the  file system denoted by the path.
   * This is only applicable if the 
   * corresponding FileSystem supports checksum. By default doesn't do anything.
   * @param verifyChecksum verify check sum.
   * @param f set the verifyChecksum for the Filesystem containing this path
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,readFields,org.apache.hadoop.fs.FileStatus:readFields(java.io.DataInput),496,522,"/**
* Reads and processes file status data from input.
* @param in DataInput source containing file status information
* @throws IOException if reading fails or data is invalid
*/","* Read instance encoded as protobuf from stream.
   * @param in Input stream
   * @see PBHelper#convert(FileStatus)
   * @deprecated Use the {@link PBHelper} and protobuf serialization directly.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",140,149,"/**
 * Constructs a FileStatus object with specified attributes.
 * @param length file size in bytes
 * @param isdir true if it's a directory
 * @param block_replication replication factor for blocks
 * @param blocksize block size in bytes
 * @param modification_time last modification time in milliseconds since epoch
 * @param access_time last access time in milliseconds since epoch
 * @param permission file permissions
 * @param owner file owner
 * @param group file group
 * @param symlink symbolic link path, if any
 * @param path full path of the file or directory
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean,org.apache.hadoop.fs.BlockLocation[])",113,123,"/**
 * Constructs a LocatedFileStatus object.
 * @param length file size in bytes
 * @param isdir true if the path is a directory
 * @param block_replication replication factor of blocks
 * @param blocksize block size in bytes
 * @param modification_time last modification time in milliseconds since epoch
 * @param access_time last access time in milliseconds since epoch
 * @param permission file permissions
 * @param owner file owner
 * @param group file group
 * @param symlink symbolic link path (if any)
 * @param path file path
 * @param hasAcl true if the file has an ACL
 * @param isEncrypted true if the file is encrypted
 * @param isErasureCoded true if the file uses erasure coding
 * @param locations block locations
 */","* Constructor.
   *
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param hasAcl entity has associated ACLs
   * @param isEncrypted entity is encrypted
   * @param isErasureCoded entity is erasure coded
   * @param locations a file's block locations",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,<init>,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:<init>(org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder,org.apache.hadoop.fs.FileSystem)",87,96,"/**
* Constructs a new FileSystemMultipartUploader.
* @param builder configuration builder for uploader settings
* @param fs file system to use for uploads
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,append,"org.apache.hadoop.io.MapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",399,416,"/**
 * Adds a key-value pair to the data store.
 * @param key unique identifier for the value
 * @param val associated data value
 * @throws IOException if an I/O error occurs
 */","* Append a key/value pair to the map.  The key must be greater or equal
     * to the previous key added to the map.
     *
     * @param key key.
     * @param val value.
     * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,close,org.apache.hadoop.service.AbstractService:close(),246,249,"/**
 * Calls method m1.
 * @throws IOException if an I/O error occurs in m1
 */","* Relay to {@link #stop()}
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Metadata:equals(java.lang.Object),785,795,"/**
* Compares Metadata objects based on m1 value.
* @param other object to compare with
* @return true if equal, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,equals,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:equals(java.lang.Object),100,108,"/**
* Checks equality with another object.
* @param that the object to compare with
* @return true if equal, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,equals,org.apache.hadoop.security.token.Token$PrivateToken:equals(java.lang.Object),284,297,"/**
* Compares this object with the specified object for equality.
* @param o the object to compare with
* @return true if objects are equal, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,selectDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials),998,1006,"/**
* Generates a token from credentials using multiple services.
* @param creds user credentials
* @return generated Token object or null if unsuccessful
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BasicDiskValidator.java,checkStatus,org.apache.hadoop.util.BasicDiskValidator:checkStatus(java.io.File),30,33,"/**
 * Masks files in a directory.
 * @param dir directory to mask files in
 * @throws DiskErrorException if disk error occurs
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write,org.apache.hadoop.fs.FSOutputSummer:write(int),76,82,"/**
* Writes byte to buffer; flushes if full.
* @param b byte to write
* @throws IOException on I/O error
*/",Write one byte,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write1,"org.apache.hadoop.fs.FSOutputSummer:write1(byte[],int,int)",120,140,"/**
 * Copies bytes from input array to buffer.
 * @param b source byte array
 * @param off offset in source array
 * @param len number of bytes to copy
 * @return number of bytes copied
 * @throws IOException if an I/O error occurs
 */","* Write a portion of an array, flushing to the underlying
   * stream at most once if necessary.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",79,121,"/**
 * Processes input and output buffers for error correction.
 * @param inputs array of input ByteBuffers
 * @param outputs array of output ByteBuffers
 * @throws IOException if processing fails
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingValidator.java,validate,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])",138,143,"/**
 * Decodes input chunks using specified erased indexes.
 * @param inputs array of encoded chunks
 * @param erasedIndexes indexes of erased chunks
 * @param outputs decoded chunk results
 * @throws IOException if an I/O error occurs during decoding
 */","*  Validate outputs decoded from inputs, by decoding an input back from
   *  those outputs and comparing it with the original one.
   * @param inputs input buffers used for decoding
   * @param erasedIndexes indexes of erased units used for decoding
   * @param outputs decoded output buffers
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",54,58,"/**
* Masks input chunks to produce output chunks.
* @param inputChunks array of input EC chunks
* @param outputChunks array to store masked EC chunks
* @throws IOException if an I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartA,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartA(),1077,1104,"/**
* Processes data block during decompression.
* Updates internal state and handles end of block.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartA,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartA(),1106,1126,"/**
 * Updates character and state based on current position.
 * @throws IOException if an I/O error occurs
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,finish,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finish(),718,732,"/**
 * Masks data by writing to output stream.
 * @throws IOException if I/O error occurs
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write0,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write0(int),881,900,"/**
* Updates run-length encoding for given byte.
* @param b input byte to encode
* @throws IOException if I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,finishDataBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:finishDataBlock(boolean),653,671,"/**
 * Finishes block appending if forced or size limit reached.
 * @param bForceFinish flag to force block finish
 * @throws IOException on I/O errors during processing
 */","* Close the current data block if necessary.
     * 
     * @param bForceFinish
     *          Force the closure regardless of the block size.
     * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:close(),3549,3556,"/**
 * Masks segments by processing and clearing them.
 * @throws IOException if an I/O error occurs during segment processing
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,parkCursorAtEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:parkCursorAtEnd(),1561,1571,"/**
* Resets state and releases resources.
* @throws IOException if an I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageStream,org.apache.hadoop.security.Credentials:readTokenStorageStream(java.io.DataInputStream),273,295,"/**
* Reads and validates token storage header, then processes data based on format.
* @param in DataInputStream to read from
* @throws IOException if header is invalid or unsupported format is encountered
*/","* Convenience method for reading a token from a DataInputStream.
   *
   * @param in DataInputStream.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getCandidateTokensForCleanup,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getCandidateTokensForCleanup(),176,197,"/**
 * Fetches and processes delegation tokens for cleanup.
 * @return Map of TokenIdent to DelegationTokenInformation
 */","* Obtain a list of tokens that will be considered for cleanup, based on the last
   * time the token was updated in SQL. This list may include tokens that are not
   * expired and should not be deleted (e.g. if the token was last renewed using a
   * higher renewal interval).
   * The number of results is limited to reduce performance impact. Some level of
   * contention is expected when multiple routers run cleanup simultaneously.
   * @return Map of tokens that have not been updated in SQL after the token renewal
   *         period.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)",644,650,"/**
* Retrieves delegation token information.
* @param ident token identifier object
* @param quiet flag to suppress logging
* @return DelegationTokenInformation or null if not found
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawValue,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes),3866,3869,"/**
 * Reads and returns the length of a value.
 * @param rawValue input data containing the value
 * @return length of the value read from input
 * @throws IOException if an I/O error occurs
 */","* Fills up the passed rawValue with the value corresponding to the key
       * read earlier.
       * @param rawValue input ValueBytes rawValue.
       * @return the length of the value
       * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeExpiredStoredToken,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),213,229,"/**
* Handles token masking logic.
* @param ident Token identifier
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,readFields,org.apache.hadoop.fs.FsServerDefaults:readFields(java.io.DataInput),175,185,"/**
* Reads configuration data from input stream.
* @param in DataInput source for reading values
* @throws IOException if an I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeProto,org.apache.hadoop.security.Credentials:writeProto(java.io.DataOutput),378,397,"/**
* Writes credentials to output stream.
* @param out DataOutput stream to write credentials
* @throws IOException if an I/O error occurs
*/","* Write contents of this instance as CredentialsProto message to DataOutput.
   * @param out
   * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,protoFromToken,org.apache.hadoop.ipc.ProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token),128,130,"/**
 * Converts a generic token to a protocol buffer token.
 * @param tok the input token of any type
 * @return the converted TokenProto object
 */","* Create a {@code TokenProto} instance
   * from a hadoop token.
   * This builds and caches the fields
   * (identifier, password, kind, service) but not
   * renewer or any payload.
   * @param tok token
   * @return a marshallable protobuf class.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenIdentifier.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text),37,39,"/**
 * Constructs a DelegationTokenIdentifier with a specified kind.
 * @param kind the type of delegation token
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invoke,"org.apache.hadoop.io.retry.RetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",358,374,"/**
* Handles method invocation with retry logic.
* @param proxy the proxy object
* @param method the method to invoke
* @param args arguments for the method
* @return result of method invocation or null if async
* @throws Throwable if an error occurs during execution
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,entry,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:entry(),1619,1622,"/**
 * Executes mask function and returns an Entry.
 * @throws IOException if input/output error occurs
 */","* Get an entry to access the key and value.
       * 
       * @return The Entry object to access the key and value.
       * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareCursorKeyTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:compareCursorKeyTo(org.apache.hadoop.io.file.tfile.RawComparable),1642,1646,"/**
 * Compares current object with another using reader.
 * @param other RawComparable to compare against
 * @return Comparison result as an integer
 * @throws IOException if I/O error occurs during comparison
 */","* Internal API. Comparing the key at cursor to user-specified key.
       * 
       * @param other
       *          user-specified key.
       * @return negative if key at cursor is smaller than user key; 0 if equal;
       *         and positive if key at cursor greater than user key.
       * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,get,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:get(org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable)",1675,1679,"/**
 * Masks key and value using helper methods.
 * @param key input key to be masked
 * @param value input value to be masked
 * @throws IOException if an I/O error occurs
 */","* Copy the key and value in one shot into BytesWritables. This is
         * equivalent to getKey(key); getValue(value);
         * 
         * @param key
         *          BytesWritable to hold key.
         * @param value
         *          BytesWritable to hold value
         * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,inBlockAdvance,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(long),1986,1995,"/**
* Iterates and processes data n times.
* @param n number of iterations
*/","* Advance cursor by n positions within the block.
       * 
       * @param n
       *          Number of key-value pairs to skip in block.
       * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String)",168,172,"/**
 * Fetches delegation token from specified URL.
 * @param url target URL for fetching the token
 * @param token existing authentication token
 * @param renewer entity that can renew the token
 * @return Token object containing AbstractDelegationTokenIdentifier
 * @throws IOException if an I/O error occurs
 * @throws AuthenticationException if authentication fails
 */","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return abstract delegation token identifier.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)",217,222,"/**
* Calls m1 with an additional parameter set to null.
* @param url the URL to access
* @param token authentication token
* @param dToken delegation token
* @return result of the call
* @throws IOException if I/O error occurs
* @throws AuthenticationException if authentication fails
*/","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)",257,262,"/**
* Calls overloaded m1 with additional parameter set to null.
* @param url target URL for the request
* @param token authentication token
* @param dToken delegation token
*/","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getInputStream,org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket),470,473,"/**
 * Wraps socket input with specified encoding.
 * @param socket network socket connection
 * @return SocketInputWrapper instance
 * @throws IOException if an I/O error occurs
 */","* Same as <code>getInputStream(socket, socket.getSoTimeout()).</code>
   *
   * @param socket socket.
   * @throws IOException raised on errors performing I/O.
   * @return SocketInputWrapper for reading from the socket.
   * @see #getInputStream(Socket, long)",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getOutputStream,org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket),526,529,"/**
 * Returns an OutputStream from the given Socket.
 * @param socket the connected socket to write data to
 * @return OutputStream for writing data to the socket
 * @throws IOException if an I/O error occurs
 */","* Same as getOutputStream(socket, 0). Timeout of zero implies write will
   * wait until data is available.<br><br>
   * 
   * From documentation for {@link #getOutputStream(Socket, long)} : <br>
   * Returns OutputStream for the socket. If the socket has an associated
   * SocketChannel then it returns a 
   * {@link SocketOutputStream} with the given timeout. If the socket does not
   * have a channel, {@link Socket#getOutputStream()} is returned. In the later
   * case, the timeout argument is ignored and the write will wait until 
   * data is available.<br><br>
   * 
   * Any socket created using socket factories returned by {@link NetUtils},
   * must use this interface instead of {@link Socket#getOutputStream()}.
   * 
   * @see #getOutputStream(Socket, long)
   * 
   * @param socket socket.
   * @return OutputStream for writing to the socket.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,putMetrics,org.apache.hadoop.metrics2.sink.StatsDSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),97,148,"/**
 * Processes a metrics record to generate and log metric lines.
 * @param record the MetricsRecord to process
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,connect,"org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,int)",574,578,"/**
 * Calls overloaded method with default null value.
 * @param socket network connection socket
 * @param address target socket address
 * @param timeout operation timeout in milliseconds
 */","* This is a drop-in replacement for 
   * {@link Socket#connect(SocketAddress, int)}.
   * In the case of normal sockets that don't have associated channels, this 
   * just invokes <code>socket.connect(endpoint, timeout)</code>. If 
   * <code>socket.getChannel()</code> returns a non-null channel,
   * connect is implemented using Hadoop's selectors. This is done mainly
   * to avoid Sun's connect implementation from creating thread-local 
   * selectors, since Hadoop does not have control on when these are closed
   * and could end up taking all the available file descriptors.
   * 
   * @see java.net.Socket#connect(java.net.SocketAddress, int)
   * 
   * @param socket socket.
   * @param address the remote address
   * @param timeout timeout in milliseconds
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,sampleMetrics,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:sampleMetrics(),403,418,"/**
* Collects and returns metrics from sources.
* @return MetricsBuffer containing aggregated metrics
*/","* Sample all the sources for a snapshot of metrics/tags
   * @return  the metrics buffer containing the snapshot",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getAttribute,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttribute(java.lang.String),104,118,"/**
* Retrieves an attribute value by name.
* @param attribute the name of the attribute to retrieve
* @return the value of the specified attribute
* @throws AttributeNotFoundException if the attribute is not found
* @throws MBeanException if an error occurs in the MBean
* @throws ReflectionException if a reflection error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getAttributes,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttributes(java.lang.String[]),127,141,"/**
* Filters and returns a list of attributes.
* @param attributes array of attribute keys to filter
* @return AttributeList containing filtered attributes
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getMBeanInfo,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMBeanInfo(),154,158,"/**
 * Retrieves cached MBeanInfo.
 * Calls m1() and returns infoCache.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,register,"org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.lang.Object)",71,74,"/**
 * Creates an ObjectName with specified service and MBean.
 * @param serviceName name of the service
 * @param nameName name component for the ObjectName
 * @param theMbean MBean object to be associated
 * @return created ObjectName instance
 */","* Register the MBean using our standard MBeanName format
   * ""hadoop:service={@literal <serviceName>,name=<nameName>}""
   * Where the {@literal <serviceName> and <nameName>} are the supplied
   * parameters.
   *
   * @param serviceName serviceName.
   * @param nameName nameName.
   * @param theMbean - the MBean to register
   * @return the named used to register the MBean",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stop,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stop(),196,219,"/**
* Stops the metrics system and executes cleanup tasks.
* @throws MetricsException if the metrics system is not started
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,getMetric,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:getMetric(java.lang.String),86,103,"/**
* Retrieves or creates disk validator metrics for a directory.
* @param dirName name of the directory
* @return ReadWriteDiskValidatorMetrics object
*/","* Get a metric by given directory name.
   *
   * @param dirName directory name
   * @return the metric",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class,java.lang.String)",190,193,"/**
 * Sets type prefix and calls another method.
 * @param protocol Class representing the protocol
 * @param prefix Prefix string to be set
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,init,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:init(int),70,80,"/**
* Initializes RPC stats for a specified number of priority levels.
* @param numLevels the number of priority levels to initialize
*/","* Initialize the metrics for JMX with priority levels.
   * @param numLevels input numLevels.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",116,132,"/**
 * Processes metrics records and updates global metrics.
 * @param rb builder to record metrics
 * @param all flag indicating whether to process all metrics
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,collectThreadLocalStates,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:collectThreadLocalStates(),137,143,"/**
* Processes and updates metrics in a thread-safe manner.
*/","* Collects states maintained in {@link ThreadLocal}, if any.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:<init>(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,java.lang.String,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,int,int,int,float,int)",62,94,"/**
* Initializes a MetricsSinkAdapter with specified parameters.
* @param name adapter name
* @param description adapter description
* @param sink underlying metrics sink object
* @param context additional context for filtering
* @param sourceFilter filter for source metrics
* @param recordFilter filter for record metrics
* @param metricFilter filter for specific metrics
* @param periodMs periodicity in milliseconds
* @param queueCapacity capacity of the internal queue
* @param retryDelay initial retry delay in milliseconds
* @param retryBackoff backoff factor for retries
* @param retryCount maximum number of retries
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String),289,291,"/**
 * Creates a mutable rate with default settings.
 * @param name identifier for the rate
 * @return MutableRate object initialized with given name
 */","* Create a mutable rate metric
   * @param name  of the metric
   * @return a new mutable metric object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String)",299,301,"/**
 * Creates a mutable rate with specified name and description.
 * @param name the name of the rate
 * @param description the description of the rate
 * @return a MutableRate object
 */","* Create a mutable rate metric
   * @param name  of the metric
   * @param description of the metric
   * @return a new mutable rate metric object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,create,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:create(java.lang.String),60,64,"/**
* Creates and registers a detailed metrics scheduler for RPCs.
* @param ns namespace for the metrics
* @return registered DecayRpcSchedulerDetailedMetrics instance
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,create,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:create(int),66,69,"/**
* Creates and returns an RpcDetailedMetrics object.
* @param port the port number to initialize the metrics with
* @return an RpcDetailedMetrics object configured with the given port
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Handler:run(),3151,3230,"/**
* Processes incoming RPC calls in a loop.
* Logs start and end of processing, handles exceptions, and updates metrics.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpKeytab,org.apache.hadoop.security.KDiag:dumpKeytab(java.io.File),596,620,"/**
* Examines and logs details of a Kerberos keytab file.
* @param keytabFile the keytab file to examine
*/","* Dump a keytab: list all principals.
   *
   * @param keytabFile the keytab file
   * @throws IOException IO problems",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateJAAS,org.apache.hadoop.security.KDiag:validateJAAS(boolean),755,771,"/**
* Configures JAAS based on system property.
* @param jaasRequired indicates if JAAS configuration is mandatory
*/","* Validate any JAAS entry referenced in the {@link #SUN_SECURITY_JAAS_FILE}
   * property.
   * @param jaasRequired is JAAS required",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateNTPConf,org.apache.hadoop.security.KDiag:validateNTPConf(),773,784,"/**
 * Handles NTP configuration for non-Windows systems.
 * Checks if NTP file exists and is writable, then configures NTP settings.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTokenRealOwner,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenRealOwner(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),907,917,"/**
* Retrieves the real user from TokenIdent.
* @param id TokenIdent object containing user information
* @return Real user string extracted from TokenIdent
*/","* Return the real owner for a token. If this is a token from a proxy user,
   * the real/effective user will be returned.
   *
   * @param id
   * @return real owner",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,getUgi,org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto),124,131,"/**
* Creates UserGroupInformation from IpcConnectionContextProto.
* @param context IPC connection context
* @return UserGroupInformation object or null if not available
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUid,org.apache.hadoop.security.ShellBasedIdMapping:getUid(java.lang.String),632,644,"/**
* Retrieves user ID for a given username.
* @param user the username of the user
* @return the user ID as an integer
* @throws IOException if user is not found or recently deleted
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGid,org.apache.hadoop.security.ShellBasedIdMapping:getGid(java.lang.String),646,658,"/**
* Retrieves or creates a group ID for the given group name.
* @param group the name of the group
* @return the group ID as an integer
* @throws IOException if the group cannot be created or found
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUserName,"org.apache.hadoop.security.ShellBasedIdMapping:getUserName(int,java.lang.String)",660,676,"/**
 * Retrieves a username for a given UID, using a fallback if necessary.
 * @param uid unique user identifier
 * @param unknown default username to use if no name is found
 * @return the retrieved or fallback username
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGroupName,"org.apache.hadoop.security.ShellBasedIdMapping:getGroupName(int,java.lang.String)",678,694,"/**
* Retrieves group name by ID, using a fallback if not found.
* @param gid group identifier
* @param unknown default group name to use if lookup fails
* @return group name or default if not found
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,ensureParentZNode,org.apache.hadoop.ha.ActiveStandbyElector:ensureParentZNode(),360,396,"/**
 * Ensures the existence of a ZNode directory.
 * @throws IOException if an I/O error occurs
 * @throws InterruptedException if interrupted while waiting
 * @throws KeeperException if a ZooKeeper operation fails
 */","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.
   * @throws KeeperException other zookeeper operation errors.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,getActiveData,org.apache.hadoop.ha.ActiveStandbyElector:getActiveData(),474,491,"/**
* Retrieves mask data from Zookeeper.
* @throws ActiveNotFoundException if active node not found
* @throws KeeperException for Zookeeper operation errors
* @throws InterruptedException if thread is interrupted
* @throws IOException for I/O errors
*/","* get data set by the active leader
   * 
   * @return data set by the active instance
   * @throws ActiveNotFoundException
   *           when there is no active leader
   * @throws KeeperException
   *           other zookeeper operation errors
   * @throws InterruptedException
   *           interrupted exception.
   * @throws IOException
   *           when ZooKeeper connection could not be established",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reEstablishSession,org.apache.hadoop.ha.ActiveStandbyElector:reEstablishSession(),872,892,"/**
* Attempts to establish a zookeeper connection with retries.
* @return true if connection is successful, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,checkTGTAndReloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:checkTGTAndReloginFromKeytab(),1197,1199,"/**
 * Calls method m1 with true parameter.
 * @throws IOException if an I/O error occurs
 */","* Re-login a user from keytab if TGT is expired or is close to expiry.
   * 
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if it's a kerberos login exception.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(),1245,1249,"/**
 * Calls overloaded method with false flag.
 * @throws IOException if an I/O error occurs
 */","* Re-Login a user in from a keytab file. Loads a user identity from a keytab
   * file and logs them in. They become the currently logged-in user. This
   * method assumes that {@link #loginUserFromKeytab(String, String)} had
   * happened already.
   * The Subject field of this UserGroupInformation object is updated to have
   * the new credentials.
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean,boolean)",229,261,"/**
* Parses target address and returns an InetSocketAddress.
* @param target the target address string
* @param defaultPort default port to use if not specified in target
* @param configName configuration property name (optional)
* @param useCacheIfPresent flag to use cached result if available
* @param isResolved whether to resolve the host
* @return InetSocketAddress object or throws IllegalArgumentException for invalid input
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getConnectAddress,org.apache.hadoop.net.NetUtils:getConnectAddress(java.net.InetSocketAddress),450,460,"/**
 * Modifies socket address based on conditions.
 * @param addr original InetSocketAddress
 * @return modified InetSocketAddress or default if exception occurs
 */","* Returns an InetSocketAddress that a client can use to connect to the
   * given listening address.
   * 
   * @param addr of a listener
   * @return socket address that a client can use to connect to the server.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,updateAddress,org.apache.hadoop.ipc.Client$Connection:updateAddress(),588,606,"/**
* Checks and updates server address if changed.
* @return true if address was updated, false otherwise
*/","* Update the server address if the address corresponding to the host
     * name has changed.
     *
     * @return true if an addr change was detected.
     * @throws IOException when the hostname cannot be resolved.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getCanonicalUri,"org.apache.hadoop.net.NetUtils:getCanonicalUri(java.net.URI,int)",333,354,"/**
 * Masks URI host and sets default port if necessary.
 * @param uri original URI to be masked
 * @param defaultPort default port to use if none specified in URI
 * @return masked URI with potential default port set
 */","* Resolve the uri's hostname and add the default port if not in the uri
   * @param uri to resolve
   * @param defaultPort if none is given
   * @return URI",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",1467,1531,"/**
 * Handles RPC request processing and response handling.
 * @param rpcKind type of RPC call
 * @param rpcRequest the RPC request to be sent
 * @param remoteId identifier for the remote connection
 * @param serviceClass class of the service
 * @param fallbackToSimpleAuth flag to fall back to simple authentication
 * @param alignmentContext context for alignment operations
 * @return response from the server or null if asynchronous
 * @throws IOException on I/O errors during processing
 */","* Make a call, passing <code>rpcRequest</code>, to the IPC server defined by
   * <code>remoteId</code>, returning the rpc response.
   *
   * @param rpcKind
   * @param rpcRequest -  contains serialized method and method parameters
   * @param remoteId - the target rpc server
   * @param serviceClass - service class for RPC
   * @param fallbackToSimpleAuth - set to true or false during this method to
   *   indicate if a secure client falls back to simple auth
   * @param alignmentContext - state alignment context
   * @return the rpc response
   * Throws exceptions if there are network problems or if the remote code
   * threw an exception.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection:run(),1082,1109,"/**
* Handles RPC requests and processes responses.
* Logs start and stop events with connection counts.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,isIn,org.apache.hadoop.util.CacheableIPList:isIn(java.lang.String),62,75,"/**
 * Checks if IP address is allowed.
 * @param ipAddress the IP address to check
 * @return true if IP is allowed, false otherwise
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache$CacheEntry),259,300,"/**
* Handles cache entry processing and state management.
* @param newEntry the new cache entry to process
* @return processed CacheEntry object
*/","* This method handles the following conditions:
   * <ul>
   * <li>If retry is not to be processed, return null</li>
   * <li>If there is no cache entry, add a new entry {@code newEntry} and return
   * it.</li>
   * <li>If there is an existing entry, wait for its completion. If the
   * completion state is {@link CacheEntry#FAILED}, the expectation is that the
   * thread that waited for completion, retries the request. the
   * {@link CacheEntry} state is set to {@link CacheEntry#INPROGRESS} again.
   * <li>If the completion state is {@link CacheEntry#SUCCESS}, the entry is
   * returned so that the thread that waits for it can can return previous
   * response.</li>
   * <ul>
   * 
   * @return {@link CacheEntry}.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,addCacheEntry,"org.apache.hadoop.ipc.RetryCache:addCacheEntry(byte[],int)",309,319,"/**
* Adds a new cache entry with masked client ID.
* @param clientId unique client identifier
* @param callId unique call identifier
*/","* Add a new cache entry into the retry cache. The cache entry consists of 
   * clientId and callId extracted from editlog.
   *
   * @param clientId input clientId.
   * @param callId input callId.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,addCacheEntryWithPayload,"org.apache.hadoop.ipc.RetryCache:addCacheEntryWithPayload(byte[],int,java.lang.Object)",321,333,"/**
* Adds a new cache entry with payload.
* @param clientId unique client identifier
* @param callId unique call identifier
* @param payload data to be cached
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,org.apache.hadoop.fs.ContentSummary:toString(boolean),381,384,"/**
 * Calls overloaded method with default second parameter.
 * @param qOption query option flag
 * @return result of the overloaded method
 */","Return the string representation of the object in the output format.
   * if qOption is false, output directory count, file count, and content size;
   * if qOption is true, output quota and remaining quota as well.
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @return the string representation of the object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,org.apache.hadoop.fs.QuotaUsage:toString(),302,305,"/**
 * Calls overloaded method with default false flag.
 * @return result of m1(false)
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,getExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:getExpression(java.lang.String,org.apache.hadoop.conf.Configuration)",108,116,"/**
 * Creates an Expression instance based on the given name and configuration.
 * @param expressionName name of the expression to create
 * @param conf configuration object
 * @return Expression instance or throws NullPointerException if config is null
 */","* Get an instance of the requested expression
   *
   * @param expressionName
   *          name of the command to lookup
   * @param conf
   *          the Hadoop configuration
   * @return the {@link Expression} or null if the expression is unknown",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,createExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.String,org.apache.hadoop.conf.Configuration)",145,155,"/**
* Creates an Expression instance from a class name.
* @param expressionClassname fully qualified class name of the Expression
* @param conf configuration object
* @return Expression instance
* @throws IllegalArgumentException if class not found
*/","* Creates an instance of the requested {@link Expression} class.
   *
   * @param expressionClassname
   *          name of the {@link Expression} class to be instantiated
   * @param conf
   *          the Hadoop configuration
   * @return a new instance of the requested {@link Expression} class",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,buildDescription,org.apache.hadoop.fs.shell.find.Find:buildDescription(org.apache.hadoop.fs.shell.find.ExpressionFactory),109,159,"/**
 * Generates a masked string of recognized expressions.
 * @param factory ExpressionFactory instance
 * @return Masked string describing primary and operator expressions
 */",Build the description used by the help command.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getExpression,org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.Class),438,442,"/**
 * Creates an expression using a mask.
 * @param expressionClass type of expression to create
 * @return masked expression instance
 */",Gets an instance of an expression from the factory.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,getInstance,org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String),108,110,"/**
 * Executes command with default options.
 * @param cmd command to execute
 * @return Command object
 */","* Returns an instance of the class implementing the given command.  The
   * class must have been registered via
   * {@link #addClass(Class, String...)}
   * @param cmd name of the command
   * @return instance of the requested command",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,"org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)",141,154,"/**
* Initializes WritableComparator with key class and configuration.
* @param keyClass the class of keys to compare
* @param conf the configuration object, may be null
* @param createInstances whether to create instances for comparison
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readObject,"org.apache.hadoop.io.ObjectWritable:readObject(java.io.DataInput,org.apache.hadoop.io.ObjectWritable,org.apache.hadoop.conf.Configuration)",261,340,"/**
* Reads an object from a DataInput stream.
* @param in input stream to read from
* @param objectWritable object to store the result, can be null
* @param conf configuration settings
* @return deserialized object or null if not applicable
*/","* Read a {@link Writable}, {@link String}, primitive type, or an array of
   * the preceding.
   *
   * @param in DataInput.
   * @param objectWritable objectWritable.
   * @param conf configuration.
   * @return Object.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableFactories.java,newInstance,org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class),81,83,"/**
 * Creates an instance of a Writable class.
 * @param c Class type of the Writable to create
 * @return Instance of the specified Writable class or null if creation fails
 */","* Create a new instance of a class with a defined factory.
   * @param c input c.
   * @return a new instance of a class with a defined factory.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,decodeTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token),870,872,"/**
 * Masks a token by applying transformation m1.
 * @param token input token to be masked
 * @return transformed TokenIdent object
 * @throws IOException if an I/O error occurs during masking
 */","* Decode the token identifier. The subclass can customize the way to decode
   * the token identifier.
   * 
   * @param token the token where to extract the identifier
   * @return the delegation token identifier
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,identifierToString,org.apache.hadoop.security.token.Token:identifierToString(java.lang.StringBuilder),422,435,"/**
* Masks an identifier in the buffer.
* @param buffer StringBuilder to append masked value
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,printCredentials,"org.apache.hadoop.security.token.DtFileOperations:printCredentials(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text,java.io.PrintStream)",137,163,"/**
 * Masks credentials by printing tokens matching the alias.
 * @param creds user credentials object
 * @param alias text to match against token identifiers
 * @param out print stream for output
 * @throws IOException if an I/O error occurs during output
 */","Print out a Credentials object.
   *  @param creds the Credentials object to be printed out.
   *  @param alias print only tokens matching alias (null matches all).
   *  @param out print to this stream.
   *  @throws IOException failure to unmarshall a token identifier.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskValidatorFactory.java,getInstance,org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.String),72,92,"/**
 * Creates a DiskValidator instance based on the input string.
 * @param diskValidator name of the disk validator
 * @return DiskValidator object
 * @throws DiskErrorException if the validator class is not found
 */","* Returns {@link DiskValidator} instance corresponding to its name.
   * The diskValidator parameter can be ""basic"" for {@link BasicDiskValidator}
   * or ""read-write"" for {@link ReadWriteDiskValidator}.
   * @param diskValidator canonical class name, for example, ""basic""
   * @throws DiskErrorException if the class cannot be located
   * @return disk validator.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,parseMethod,"org.apache.hadoop.ha.NodeFencer:parseMethod(org.apache.hadoop.conf.Configuration,java.lang.String)",151,165,"/**
 * Parses a configuration line and returns a FenceMethodWithArg.
 * @param conf Configuration object
 * @param line Configuration line to parse
 * @return FenceMethodWithArg instance or throws exception if parsing fails
 * @throws BadFencingConfigurationException if line cannot be parsed
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,<init>,"org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.lang.String)",58,65,"/**
* Initializes HostsFileReader with input and exclusion files.
* @param inFile path to the main hosts file
* @param exFile path to the exclusion hosts file
* @throws IOException if an I/O error occurs during initialization
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,org.apache.hadoop.util.HostsFileReader:refresh(),118,121,"/**
* Calls m2 with file inclusion and exclusion details from current's m1.
* @throws IOException if an I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,add,org.apache.hadoop.util.bloom.DynamicBloomFilter:add(org.apache.hadoop.util.bloom.Key),136,153,"/**
 * Processes a key using a Bloom Filter.
 * @param key the key to process, cannot be null
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResourceObject,org.apache.hadoop.conf.Configuration:addResourceObject(org.apache.hadoop.conf.Configuration$Resource),1034,1038,"/**
* Processes resource and updates system properties.
* @param resource the resource to process
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getProps,org.apache.hadoop.conf.Configuration:getProps(),2946,2952,"/**
* Initializes and returns the properties object.
* @return Properties object containing configuration settings
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readVectored,"org.apache.hadoop.fs.BufferedFSInputStream:readVectored(java.util.List,java.util.function.IntFunction)",179,183,"/**
* Reads data from input stream using specified file ranges and allocator.
* @param ranges list of file ranges to read
* @param allocate function to allocate ByteBuffer
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,readVectored,"org.apache.hadoop.fs.FSDataInputStream:readVectored(java.util.List,java.util.function.IntFunction)",304,308,"/**
* Reads data from input stream using specified file ranges and allocator.
* @param ranges list of file ranges to read
* @param allocate function to allocate ByteBuffer for reading
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,getInternal,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getInternal(org.apache.hadoop.fs.impl.prefetch.BufferData),177,208,"/**
 * Masks buffer if not in certain states.
 * @param data BufferData object to process
 * @return true if masking is applied, false otherwise
 * @throws IOException if I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,org.apache.hadoop.fs.BlockLocation:<init>(),82,84,"/**
 * Constructs an empty block location.
 */",* Default Constructor.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",883,901,"/**
* Retrieves block locations for a file segment.
* @param file FileStatus object representing the file
* @param start starting offset of the segment
* @param len length of the segment
* @return array of BlockLocation objects or empty array if no blocks found
* @throws IOException if an I/O error occurs
*/","* Return an array containing hostnames, offset and size of
   * portions of the given file.  For nonexistent
   * file or regions, {@code null} is returned.
   *
   * <pre>
   *   if f == null :
   *     result = null
   *   elif f.getLen() {@literal <=} start:
   *     result = []
   *   else result = [ locations(FS, b) for b in blocks(FS, p, s, s+l)]
   * </pre>
   * This call is most helpful with and distributed filesystem
   * where the hostnames of machines that contain blocks of the given file
   * can be determined.
   *
   * The default implementation returns an array containing one element:
   * <pre>
   * BlockLocation( { ""localhost:9866"" },  { ""localhost"" }, 0, file.getLen())
   * </pre>
   *
   * In HDFS, if file is three-replicated, the returned array contains
   * elements like:
   * <pre>
   * BlockLocation(offset: 0, length: BLOCK_SIZE,
   *   hosts: {""host1:9866"", ""host2:9866, host3:9866""})
   * BlockLocation(offset: BLOCK_SIZE, length: BLOCK_SIZE,
   *   hosts: {""host2:9866"", ""host3:9866, host4:9866""})
   * </pre>
   *
   * And if a file is erasure-coded, the returned BlockLocation are logical
   * block groups.
   *
   * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).
   * 1. If the file size is less than one stripe size, say 2 * CELL_SIZE, then
   * there will be one BlockLocation returned, with 0 offset, actual file size
   * and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks.
   * 3. If the file size is less than one group size but greater than one
   * stripe size, then there will be one BlockLocation returned, with 0 offset,
   * actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting
   * the actual blocks.
   * 4. If the file size is greater than one group size, 3 * BLOCK_SIZE + 123
   * for example, then the result will be like:
   * <pre>
   * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {""host1:9866"",
   *   ""host2:9866"",""host3:9866"",""host4:9866"",""host5:9866""})
   * BlockLocation(offset: 3 * BLOCK_SIZE, length: 123, hosts: {""host1:9866"",
   *   ""host4:9866"", ""host5:9866""})
   * </pre>
   *
   * @param file FilesStatus to get data from
   * @param start offset into the given file
   * @param len length for which to get locations for
   * @throws IOException IO failure
   * @return block location array.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,fetchDurationSummary,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchDurationSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String,boolean)",129,140,"/**
* Generates a duration statistic summary.
* @param source IOStatistics object containing data
* @param key identifier for the statistic
* @param success indicates if the operation was successful
* @return DurationStatisticSummary with calculated statistics
*/","* Fetch the duration timing summary of success or failure operations
   * from an IO Statistics source.
   * If the duration key is unknown, the summary will be incomplete.
   * @param source source of data
   * @param key duration statistic key
   * @param success fetch success statistics, or if false, failure stats.
   * @return a summary of the statistics.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,snapshot,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:snapshot(),92,96,"/**
* Takes a snapshot of IO statistics.
* @return IOStatisticsSnapshot object containing current stats
*/","* Returns a snapshot of the current thread's IOStatistics.
   *
   * @return IOStatisticsSnapshot of the context.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,snapshotIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics(org.apache.hadoop.fs.statistics.IOStatistics),46,50,"/**
* Creates an IOStatisticsSnapshot from given IOStatistics.
* @param statistics source IOStatistics object
* @return IOStatisticsSnapshot instance
*/","* Take a snapshot of the current statistics state.
   * <p>
   * This is not an atomic option.
   * <p>
   * The instance can be serialized, and its
   * {@code toString()} method lists all the values.
   * @param statistics statistics
   * @return a snapshot of the current values.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create(java.lang.Object),123,125,"/**
 * Converts an object to an IOStatisticsSnapshot.
 * @param source the source object, expected to be an IOStatistics instance
 * @return a Serializable IOStatisticsSnapshot or null if conversion fails
 */","* Create a new {@link IOStatisticsSnapshot} instance.
   * @param source optional source statistics
   * @return an IOStatisticsSnapshot.
   * @throws ClassCastException if the {@code source} is not null and not an IOStatistics instance",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,toList,org.apache.hadoop.util.functional.RemoteIterators:toList(org.apache.hadoop.fs.RemoteIterator),232,237,"/**
* Masks data from remote iterator into a list.
* @param source RemoteIterator providing data
* @return List containing masked data
* @throws IOException if an I/O error occurs
*/","* Build a list from a RemoteIterator.
   * @param source source iterator
   * @param <T> type
   * @return a list of the values.
   * @throws IOException if the source RemoteIterator raises it.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackStoreToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackStoreToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1002,1004,"/**
* Masks invocation and stores token.
* @param invocation object representing the invocation
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackUpdateToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackUpdateToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1006,1008,"/**
 * Updates token status using provided invocation.
 * @param invocation object containing request details
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackRemoveToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackRemoveToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1010,1012,"/**
* Masks function by invoking with specific parameters.
* @param invocation Invocation object raising IO exceptions
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",59,65,"/**
 * Processes chunks using buffer conversion.
 * @param inputChunks array of input ECChunk objects
 * @param outputChunks array of output ECChunk objects
 * @throws IOException if an I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,write,org.apache.hadoop.io.EnumSetWritable:write(java.io.DataOutput),135,154,"/**
* Serializes the value to a DataOutput stream.
* @param out the output stream to write data to
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,write,org.apache.hadoop.io.ObjectWritable:write(java.io.DataOutput),89,92,"/**
 * Writes data to output stream.
 * @param out DataOutput stream
 * @throws IOException if I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processArgument,org.apache.hadoop.fs.shell.Command:processArgument(org.apache.hadoop.fs.shell.PathData),299,305,"/**
* Processes PathData item.
* @param item PathData object to process
* @throws IOException if an I/O error occurs
*/","* Processes a {@link PathData} item, calling
   * {@link #processPathArgument(PathData)} or
   * {@link #processNonexistentPath(PathData)} on each item.
   * @param item {@link PathData} item to process
   * @throws IOException if anything goes wrong...",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getUnixGroups,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getUnixGroups(java.lang.String),203,231,"/**
* Executes shell command to fetch user groups.
* @param user username for which to fetch groups
* @return set of group names or empty set if none found
*/","* Get the current user's group list from Unix by running the command 'groups'
   * NOTE. For non-existing user it will return EMPTY list.
   *
   * @param user get groups for this user
   * @return the groups list that the <code>user</code> belongs to. The primary
   *         group is returned first.
   * @throws IOException if encounter any error when running the command",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,runResolveCommand,"org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:runResolveCommand(java.util.List,java.lang.String)",222,262,"/**
* Executes a command script with arguments.
* @param args list of command arguments
* @param commandScriptName name of the command script
* @return combined output of the command execution or null on error
*/","* Build and execute the resolution command. The command is
     * executed in the directory specified by the system property
     * ""user.dir"" if set; otherwise the current working directory is used.
     * @param args a list of arguments
     * @param commandScriptName input commandScriptName.
     * @return null if the number of arguments is out of range,
     * or the output of the command.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[]),1214,1216,"/**
 * Constructs a ShellCommandExecutor with a command string array.
 * @param execString array of strings representing the shell command and its arguments
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,readLink,org.apache.hadoop.fs.FileUtil:readLink(java.io.File),213,230,"/**
 * Masks file content by reading and processing it.
 * @param f File object to be processed
 * @return Masked content as a String or empty string on error
 */","* Returns the target of the given symlink. Returns the empty string if
   * the given path does not refer to a symlink or there is an error
   * accessing the symlink.
   * @param f File representing the symbolic link.
   * @return The target of the symbolic link, empty string on error or if not
   *         a symlink.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,execCommand,"org.apache.hadoop.fs.FileUtil:execCommand(java.io.File,java.lang.String[])",1531,1537,"/**
* Executes a shell command with a file path.
* @param f File object representing the target file
* @param cmd Variable number of command arguments
* @return Command execution output as a string
* @throws IOException if an I/O error occurs during command execution
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setPermission,"org.apache.hadoop.fs.RawLocalFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1108,1119,"/**
* Sets file permissions.
* @param p file path
* @param permission desired file permissions
*/",* Use the command chmod to set permission.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,execShellGetUserForNetgroup,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:execShellGetUserForNetgroup(java.lang.String),134,146,"/**
* Masks a network group name.
* @param netgroup the original network group name
* @return masked network group name or empty string on error
*/","* Calls shell to get users for a netgroup by calling getent
   * netgroup, this is a low level function that just returns string
   * that 
   *
   * @param netgroup get users for this netgroup
   * @return string of users for a given netgroup in getent netgroups format
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,org.apache.hadoop.security.UserGroupInformation$TicketCacheRenewalRunnable:relogin(),1073,1078,"/**
* Renews Kerberos ticket using kinit command.
* @throws IOException if an I/O error occurs during the process
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,stashOriginalFilePermissions,org.apache.hadoop.security.alias.LocalKeyStoreProvider:stashOriginalFilePermissions(),92,114,"/**
 * Sets file permissions based on the operating system.
 * @throws IOException if an I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,<init>,org.apache.hadoop.security.Credentials:<init>(org.apache.hadoop.security.Credentials),103,105,"/**
 * Copies credentials from another Credentials object.
 * @param credentials source Credentials object to copy from
 */","* Create a copy of the given credentials.
   * @param credentials to copy",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addCredentials,org.apache.hadoop.security.UserGroupInformation:addCredentials(org.apache.hadoop.security.Credentials),1753,1757,"/**
 * Masks user credentials.
 * @param credentials user's login details
 */","* Add the given Credentials to this user.
   * @param credentials of tokens and secrets",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",580,590,"/**
* Creates a file with specified permissions and flags.
* @param f file path
* @param permission file permissions
* @param flags creation flags
* @param bufferSize buffer size for output stream
* @param replication replication factor
* @param blockSize block size
* @param progress progress tracking
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,toFile,org.apache.hadoop.fs.shell.PathData:toFile(),494,499,"/**
* Masks a file path using local file system.
* @return masked File object or throws exception if not local
*/","* Get the path to a local file
   * @return File representing the local path
   * @throws IllegalArgumentException if this.fs is not the LocalFileSystem",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,mkdirsWithExistsAndPermissionCheck,"org.apache.hadoop.util.DiskChecker:mkdirsWithExistsAndPermissionCheck(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",224,235,"/**
* Ensures directory has the expected permissions.
* @param localFS LocalFileSystem instance
* @param dir Path to the directory
* @param expected Expected FsPermission
*/","* Create the directory or check permissions if it already exists.
   *
   * The semantics of mkdirsWithExistsAndPermissionCheck method is different
   * from the mkdirs method provided in the Sun's java.io.File class in the
   * following way:
   * While creating the non-existent parent directories, this method checks for
   * the existence of those directories if the mkdir fails at any point (since
   * that directory might have just been created by some other process).
   * If both mkdir() and the exists() check fails for any seemingly
   * non-existent directory, then we signal an error; Sun's mkdir would signal
   * an error (return false) if a directory it is attempting to create already
   * exists or the mkdir fails.
   *
   * @param localFS local filesystem
   * @param dir directory to be created or checked
   * @param expected expected permission
   * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.ViewFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),441,445,"/**
 * Updates working directory and calls setup methods.
 * @param new_dir new directory path to set as working directory
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getNativeFileLinkStatus,"org.apache.hadoop.fs.RawLocalFileSystem:getNativeFileLinkStatus(org.apache.hadoop.fs.Path,boolean)",1300,1306,"/**
* Retrieves file status with optional symlink resolution.
* @param f file path to check
* @param dereference true to resolve symlinks
* @return FileStatus object representing the file's status
* @throws IOException if an I/O error occurs
*/","* Calls out to platform's native stat(1) implementation to get file metadata
   * (permissions, user, group, atime, mtime, etc). This works around the lack
   * of lstat(2) in Java 6.
   * 
   *  Currently, the {@link Stat} class used to do this only supports Linux
   *  and FreeBSD, so the old {@link #deprecatedGetFileLinkStatusInternal(Path)}
   *  implementation (deprecated) remains further OS support is added.
   *
   * @param f File to stat
   * @param dereference whether to dereference symlinks
   * @return FileStatus of f
   * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getResolvedQualifiedPath,org.apache.hadoop.fs.viewfs.ChRootedFs:getResolvedQualifiedPath(org.apache.hadoop.fs.Path),167,171,"/**
* Masks and returns a modified file path.
* @param f original file path
* @return masked file path
* @throws FileNotFoundException if file not found
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.AbstractFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1614,1625,"/**
* Checks if a file system supports capabilities.
* @param path the file path to check
* @param capability the capability to verify
* @return true if capability is supported, false otherwise
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.AbstractFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),1652,1657,"/**
* Applies mask to given path.
* @param path input file path
* @return masked Path object
* @throws IOException if an I/O error occurs
*/","* Return path of the enclosing root for a given path
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   *
   * Call makeQualified on the param path to ensure its part of the correct filesystem
   *
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,makeQualified,org.apache.hadoop.fs.FilterFs:makeQualified(org.apache.hadoop.fs.Path),73,76,"/**
 * Delegates file system operation to `myFs`.
 * @param path file path to operate on
 * @return result of the file system operation
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",1892,1903,"/**
* Populates results with filtered file statuses from directory.
* @param results list to store matching FileStatus objects
* @param f path to the directory
* @param filter PathFilter to apply to files
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file not found
* @throws IOException for other I/O errors
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,listStatus,org.apache.hadoop.fs.Globber:listStatus(org.apache.hadoop.fs.Path),125,136,"/**
 * Retrieves file status for a given path.
 * @param path the file path to check
 * @return an array of FileStatus objects or an empty array if not found
 * @throws IOException if an I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,processDeleteOnExit,org.apache.hadoop.fs.FileContext:processDeleteOnExit(),296,312,"/**
* Marks files for deletion on exit.
* Iterates through and deletes paths associated with file contexts.
*/",* Delete all the paths that were marked as delete-on-exit.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,exists,org.apache.hadoop.fs.FileContext$Util:exists(org.apache.hadoop.fs.Path),1757,1766,"/**
* Checks if a file exists at the specified path.
* @param f file path to check
* @return true if file exists, false otherwise
*/","* Does the file exist?
     * Note: Avoid using this method if you already have FileStatus in hand.
     * Instead reuse the FileStatus 
     * @param f the  file or dir to be checked
     *
     * @throws AccessControlException If access is denied
     * @throws IOException If an I/O error occurred
     * @throws UnsupportedFileSystemException If file system for <code>f</code> is
     *           not supported
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server
     * @return if f exists true, not false.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,getFileStatus,org.apache.hadoop.fs.Globber:getFileStatus(org.apache.hadoop.fs.Path),112,123,"/**
* Retrieves file status for a given path.
* @param path the file path to check
* @return FileStatus object or null if not found
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setWorkingDirectory,org.apache.hadoop.fs.FileContext:setWorkingDirectory(org.apache.hadoop.fs.Path),542,554,"/**
* Sets the working directory and validates it.
* @param newWDir new working directory path
* @throws IOException if an I/O error occurs or if the target is a file
*/","* Set the working directory for wd-relative names (such a ""foo/bar""). Working
   * directory feature is provided by simply prefixing relative names with the
   * working dir. Note this is different from Unix where the wd is actually set
   * to the inode. Hence setWorkingDir does not follow symlinks etc. This works
   * better in a distributed environment that has multiple independent roots.
   * {@link #getWorkingDirectory()} should return what setWorkingDir() set.
   * 
   * @param newWDir new working directory
   * @throws IOException 
   * <br>
   *           NewWdir can be one of:
   *           <ul>
   *           <li>relative path: ""foo/bar"";</li>
   *           <li>absolute without scheme: ""/foo/bar""</li>
   *           <li>fully qualified with scheme: ""xx://auth/foo/bar""</li>
   *           </ul>
   * <br>
   *           Illegal WDs:
   *           <ul>
   *           <li>relative with scheme: ""xx:foo/bar""</li>
   *           <li>non existent directory</li>
   *           </ul>",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,checkDest,"org.apache.hadoop.fs.FileContext:checkDest(java.lang.String,org.apache.hadoop.fs.Path,boolean)",2261,2278,"/**
 * Copies a file to a destination path.
 * @param srcName source file name or null for directories
 * @param dst destination path
 * @param overwrite flag to overwrite existing files
 * @throws AccessControlException if access is denied
 * @throws IOException on I/O errors or if target exists and overwrite is false
 */","* Check if copying srcName to dst would overwrite an existing 
   * file or directory.
   * @param srcName File or directory to be copied.
   * @param dst Destination to copy srcName to.
   * @param overwrite Whether it's ok to overwrite an existing file. 
   * @throws AccessControlException If access is denied.
   * @throws IOException If dst is an existing directory, or dst is an 
   * existing file and the overwrite option is not passed.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getContentSummary,org.apache.hadoop.fs.FileContext$Util:getContentSummary(org.apache.hadoop.fs.Path),1786,1812,"/**
* Calculates content summary for a file or directory.
* @param f Path to the file or directory
* @return ContentSummary object with size, file count, and directory count
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file not found
* @throws UnsupportedFileSystemException if filesystem is unsupported
* @throws IOException for other I/O errors
*/","* Return the {@link ContentSummary} of path f.
     * @param f path
     *
     * @return the {@link ContentSummary} of path f.
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>f</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getDelegationTokens,"org.apache.hadoop.fs.FileContext:getDelegationTokens(org.apache.hadoop.fs.Path,java.lang.String)",2432,2443,"/**
* Retrieves tokens for a given path.
* @param p file system path
* @param renewer entity that can renew the token
* @return list of tokens associated with the path
* @throws IOException if an I/O error occurs
*/","* Get delegation tokens for the file systems accessed for a given
   * path.
   * @param p Path for which delegations tokens are requested.
   * @param renewer the account name that is allowed to renew the token.
   * @return List of delegation tokens.
   * @throws IOException If an I/O error occurred.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setXAttr,"org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",2584,2588,"/**
 * Sets an extended attribute with specified flags.
 * @param path file path
 * @param name attribute name
 * @param value attribute value
 * @throws IOException if I/O error occurs
 */","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException If an I/O error occurred.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSnapshot,org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path),2747,2749,"/**
 * Calls overloaded method with default options.
 * @param path file system path to process
 * @return result of processing the path
 * @throws IOException if an I/O error occurs
 */","* Create a snapshot with a default name.
   *
   * @param path The directory where snapshots will be taken.
   * @return the snapshot path.
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",77,87,"/**
 * Initializes a MultipartUploaderBuilderImpl with file context and path.
 * @param fc FileContext object for file operations
 * @param p Path to the file
 * @throws IOException if an I/O error occurs
 */","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param p path.
   * @throws IOException failure",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,<init>,org.apache.hadoop.fs.LocalFileSystem:<init>(),40,42,"/**
 * Constructs a LocalFileSystem using a default RawLocalFileSystem.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,hasMoreThanOneSourcePaths,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:hasMoreThanOneSourcePaths(java.util.LinkedList),105,118,"/**
 * Checks conditions on linked list of PathData.
 * @param args linked list containing PathData objects
 * @return true if conditions met, otherwise false
 * @throws IOException if an I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,waitForRecovery,org.apache.hadoop.fs.shell.Truncate:waitForRecovery(),102,116,"/**
* Waits for and truncates files in the wait list to a specified length.
* @throws IOException if an I/O error occurs
*/",* Wait for all files in waitList to have length equal to newLength.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,tryResolveInRegexMountpoint,"org.apache.hadoop.fs.viewfs.InodeTree:tryResolveInRegexMountpoint(java.lang.String,boolean)",1022,1032,"/**
* Resolves path using regex mount points.
* @param srcPath source path to resolve
* @param resolveLastComponent flag to resolve last component
* @return ResolveResult or null if no match found
*/","* Walk through all regex mount points to see
   * whether the path match any regex expressions.
   *  E.g. link: ^/user/(?&lt;username&gt;\\w+) =&gt; s3://$user.apache.com/_${user}
   *  srcPath: is /user/hadoop/dir1
   *  resolveLastComponent: true
   *  then return value is s3://hadoop.apache.com/_hadoop
   *
   * @param srcPath srcPath.
   * @param resolveLastComponent resolveLastComponent.
   * @return ResolveResult.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,<init>,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.Path[])",548,554,"/**
 * Initializes a PathIterator for the given file system and path.
 * @param fs FileSystem instance to operate on
 * @param pathStr String representation of the path
 * @param rootDirs Array of root directories to consider
 * @throws IOException if an I/O error occurs during initialization
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,next,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:next(),571,583,"/**
* Returns the next path and checks its existence.
* @return Path object representing the next element
* @throws NoSuchElementException if no more paths are available
* @throws RuntimeException if an I/O error occurs during existence check
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,ifExists,"org.apache.hadoop.fs.LocalDirAllocator:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)",248,251,"/**
* Checks configuration against given path.
* @param pathStr file path string
* @param conf configuration object
* @return true if valid, false otherwise
*/","*  We search through all the configured dirs for the file's existence
   *  and return true when we find.
   *  @param pathStr the requested file (this will be searched)
   *  @param conf the Configuration object
   *  @return true if files exist. false otherwise",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.LocalKeyStoreProvider:initFileSystem(java.net.URI),116,141,"/**
* Initializes a local file from a URI.
* @param uri the source URI for initializing the file
* @throws IOException if there's an error processing the URI or file operations
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,archivePath,org.apache.hadoop.fs.HarFileSystem:archivePath(org.apache.hadoop.fs.Path),200,211,"/**
* Masks the given path by traversing its segments.
* @param p input path to be masked
* @return masked path or null if no "".har"" segment found
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getPathInHar,org.apache.hadoop.fs.HarFileSystem:getPathInHar(org.apache.hadoop.fs.Path),356,373,"/**
 * Masks a given path by finding its archive root.
 * @param path the input file path
 * @return masked path or null if not applicable
 */","* this method returns the path 
   * inside the har filesystem.
   * this is relative path inside 
   * the har filesystem.
   * @param path the fully qualified path in the har filesystem.
   * @return relative path in the filesystem.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,makeRelative,"org.apache.hadoop.fs.HarFileSystem:makeRelative(java.lang.String,org.apache.hadoop.fs.Path)",379,393,"/**
* Masks a path by prepending an initial segment.
* @param initial the initial segment to prepend
* @param p the original path
* @return a new masked path
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getChecksumFile,org.apache.hadoop.fs.ChecksumFs:getChecksumFile(org.apache.hadoop.fs.Path),88,90,"/**
 * Generates a masked path by appending "".crc"" to the filename.
 * @param file original file path
 * @return new Path with modified filename
 */","* Return the name of the checksum file associated with a file.
   *
   * @param file the file path.
   * @return the checksum file associated with a file.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,createCollectorPath,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:createCollectorPath(org.apache.hadoop.fs.Path),155,161,"/**
* Generates a masked file path.
* @param filePath original file path
* @return modified file path with UUID mask
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,parentExists,org.apache.hadoop.fs.shell.PathData:parentExists(),250,253,"/**
* Masks file based on condition.
* @return true if masking successful, false otherwise
* @throws IOException if an I/O error occurs
*/","* Test if the parent directory exists
   * @return boolean indicating parent exists
   * @throws IOException upon unexpected error",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processNonexistentPath,org.apache.hadoop.fs.shell.Mkdir:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),69,90,"/**
* Checks and creates necessary parent paths for a given item.
* @param item the PathData object representing the item to process
* @throws IOException if an I/O error occurs or path is invalid
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,primitiveMkdir,"org.apache.hadoop.fs.FileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",1392,1415,"/**
* Masks file with specified permissions.
* @param f file path
* @param absolutePermission file permissions
* @param createParent flag to create parent directory if missing
* @throws IOException on failure to mask or create parent
*/","* This version of the mkdirs method assumes that the permission is absolute.
   * It has been added to support the FileContext that processes the permission
   * with umask before calling this method.
   * This a temporary method added to support the transition from FileSystem
   * to FileContext for user applications.
   *
   * @param f the path.
   * @param absolutePermission permission.
   * @param createParent create parent.
   * @throws IOException IO failure.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,rename,"org.apache.hadoop.fs.FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",1669,1726,"/**
* Renames a file or directory.
* @param src source path
* @param dst destination path
* @param options optional rename options like OVERWRITE
* @throws IOException if operation fails
*/","* Renames Path src to Path dst
   * <ul>
   *   <li>Fails if src is a file and dst is a directory.</li>
   *   <li>Fails if src is a directory and dst is a file.</li>
   *   <li>Fails if the parent of dst does not exist or is a file.</li>
   * </ul>
   * <p>
   * If OVERWRITE option is not passed as an argument, rename fails
   * if the dst already exists.
   * </p>
   * <p>
   * If OVERWRITE option is passed as an argument, rename overwrites
   * the dst if it is a file or an empty directory. Rename fails if dst is
   * a non-empty directory.
   * </p>
   * Note that atomicity of rename is dependent on the file system
   * implementation. Please refer to the file system documentation for
   * details. This default implementation is non atomic.
   * <p>
   * This method is deprecated since it is a temporary method added to
   * support the transition from FileSystem to FileContext for user
   * applications.
   * </p>
   *
   * @param src path to be renamed
   * @param dst new path after rename
   * @param options rename options.
   * @throws FileNotFoundException src path does not exist, or the parent
   * path of dst does not exist.
   * @throws FileAlreadyExistsException dest path exists and is a file
   * @throws ParentNotDirectoryException if the parent path of dest is not
   * a directory
   * @throws IOException on failure",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getNflyTmpPath,org.apache.hadoop.fs.viewfs.NflyFSystem:getNflyTmpPath(org.apache.hadoop.fs.Path),447,449,"/**
 * Masks a file path by prepending a temporary prefix to its second component.
 * @param f original file path
 * @return modified file path with masked second component
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getChecksumFile,org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFile(org.apache.hadoop.fs.Path),120,122,"/**
 * Generates a masked path by appending "".crc"" to the second part of the input path.
 * @param file original file path
 * @return modified path with appended CRC extension
 */","* Return the name of the checksum file associated with a file.
   *
   * @param file the file path.
   * @return name of the checksum file associated with a file.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BulkDeleteUtils.java,validatePathIsUnderParent,"org.apache.hadoop.fs.BulkDeleteUtils:validatePathIsUnderParent(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",56,64,"/**
* Checks if path is within base path.
* @param p path to check
* @param basePath base directory path
* @return true if path is within base path, false otherwise
*/","* Check if a given path is the base path or under the base path.
   * @param p path to check.
   * @param basePath base path.
   * @return true if the given path is the base path or under the base path.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isRoot,org.apache.hadoop.fs.Path:isRoot(),408,410,"/**
 * Checks if m1 returns null.
 * @return true if m1 is null, false otherwise
 */","* Returns true if and only if this path represents the root of a file system.
   *
   * @return true if and only if this path represents the root of a file system",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,suffix,org.apache.hadoop.fs.Path:suffix(java.lang.String),467,474,"/**
 * Constructs a path with a given suffix.
 * @param suffix the string to append to the base path
 * @return constructed Path object
 */","* Adds a suffix to the final name in the path.
   *
   * @param suffix the suffix to add
   * @return a new path with the suffix added",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSLinkResolver.java,qualifySymlinkTarget,"org.apache.hadoop.fs.FSLinkResolver:qualifySymlinkTarget(java.net.URI,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",46,55,"/**
 * Masks a URI path with another path.
 * @param pathURI the original URI to mask
 * @param pathWithLink the path containing link information
 * @param target the target path for masking
 * @return masked Path or original target if no scheme and auth
 */","* Return a fully-qualified version of the given symlink target if it
   * has no scheme and authority. Partially and fully-qualified paths
   * are returned unmodified.
   * @param pathURI URI of the filesystem of pathWithLink
   * @param pathWithLink Path that contains the symlink
   * @param target The symlink's absolute target
   * @return Fully qualified version of the target.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,renameInternal,"org.apache.hadoop.fs.AbstractFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",848,897,"/**
 * Renames or moves a file from source to destination.
 * @param src source path
 * @param dst destination path
 * @param overwrite flag to allow overwriting existing files
 * @throws IOException if an I/O error occurs
 */","* The specification of this method matches that of
   * {@link FileContext#rename(Path, Path, Options.Rename...)} except that Path
   * f must be for this file system.
   *
   * @param src src.
   * @param dst dst.
   * @param overwrite overwrite flag.
   * @throws AccessControlException access control exception.
   * @throws FileAlreadyExistsException file already exists exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not directory exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createPath,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createPath(org.apache.hadoop.fs.Path,java.lang.String,boolean)",362,377,"/**
* Validates and constructs file path.
* @param dir base directory
* @param path relative file path
* @param checkWrite flag to check write permissions
* @return constructed Path object or null if validation fails
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,createInternal,"org.apache.hadoop.fs.DelegateToFileSystem:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",79,109,"/**
 * Creates or opens a file with specified flags and permissions.
 * @param f file path
 * @param flag create flags
 * @param absolutePermission file permissions
 * @param bufferSize buffer size for I/O operations
 * @param replication block replication factor
 * @param blockSize block size
 * @param progress progress callback
 * @param checksumOpt checksum options
 * @param createParent whether to create parent directories if missing
 * @return FSDataOutputStream for the file
 * @throws IOException if an I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,<init>,"org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",716,721,"/**
* Initializes FCDataOutputStreamBuilder.
* @param fc non-null FileContext instance
* @param p non-null Path instance
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)",131,138,"/**
* Constructs a FileStatus object.
* @param length file size in bytes
* @param isdir true if the path is a directory
* @param block_replication replication factor of file blocks
* @param blocksize block size in bytes
* @param modification_time last modification time in milliseconds since epoch
* @param access_time last access time in milliseconds since epoch
* @param permission file permissions
* @param owner file owner
* @param group file group
* @param path file path
*/","* Constructor for file systems on which symbolic links are not supported
   *
   * @param length length.
   * @param isdir isdir.
   * @param block_replication block replication.
   * @param blocksize block size.
   * @param modification_time modification time.
   * @param access_time access_time.
   * @param permission permission.
   * @param owner owner.
   * @param group group.
   * @param path the path.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,org.apache.hadoop.fs.FileStatus:<init>(org.apache.hadoop.fs.FileStatus),198,206,"/**
* Constructs a new FileStatus by copying another.
* @param other the FileStatus to copy from
* @throws IOException if an I/O error occurs during copying
*/","* Copy constructor.
   *
   * @param other FileStatus to copy
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,parseExecResult,org.apache.hadoop.fs.Stat:parseExecResult(java.io.BufferedReader),110,170,"/**
 * Parses file status from BufferedReader.
 * @param lines input stream containing file stats
 * @throws IOException if parsing fails or file not found
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,cloneStatus,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:cloneStatus(),172,182,"/**
 * Constructs a FileStatus object with specific attributes.
 * @return FileStatus object initialized with various properties from 'status'
 * @throws IOException if an I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.BlockLocation[])",49,62,"/**
 * Initializes a LocatedFileStatus with the given FileStatus and block locations.
 * @param stat FileStatus object containing file metadata
 * @param locations BlockLocation array representing block locations
 */","* Constructor 
   * @param stat a file status
   * @param locations a file's block locations",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.BlockLocation[])",80,92,"/**
* Deprecated constructor for LocatedFileStatus.
* @param length file size in bytes
* @param isdir true if the path is a directory
* @param block_replication replication factor of blocks
* @param blocksize block size in bytes
* @param modification_time last modified timestamp
* @param access_time last accessed timestamp
* @param permission file permissions
* @param owner file owner
* @param group file group
* @param symlink symbolic link path
* @param path file path
* @param locations array of block locations
*/","* Constructor
   * 
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param locations a file's block locations",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,build,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:build(),48,52,"/**
 * Creates and returns a new FileSystemMultipartUploader.
 * @return FileSystemMultipartUploader instance
 * @throws IllegalArgumentException if invalid arguments are provided
 * @throws IOException if an I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,append,org.apache.hadoop.io.ArrayFile$Writer:append(org.apache.hadoop.io.Writable),84,87,"/**
* Increments count and calls superclass method.
* @param value object to be written
* @throws IOException if an I/O error occurs
*/","* Append a value to the file.
     * @param value value.
     * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,selectDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials),147,165,"/**
 * Retrieves a token from credentials using multiple providers.
 * @param creds user credentials
 * @return Token object or null if not found
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write,"org.apache.hadoop.fs.FSOutputSummer:write(byte[],int,int)",102,114,"/**
* Masks bytes in array.
* @param b byte array to mask
* @param off offset to start masking
* @param len number of bytes to mask
* @throws IOException if I/O error occurs
*/","* Writes <code>len</code> bytes from the specified byte array 
   * starting at offset <code>off</code> and generate a checksum for
   * each data chunk.
   *
   * <p> This method stores bytes from the given array into this
   * stream's buffer before it gets checksumed. The buffer gets checksumed 
   * and flushed to the underlying output stream when all data 
   * in a checksum chunk are in the buffer.  If the buffer is empty and
   * requested length is at least as large as the size of next checksum chunk
   * size, this method will checksum and write the chunk directly 
   * to the underlying output stream.  Thus it avoids unnecessary data copy.
   *
   * @param      b     the data.
   * @param      off   the start offset in the data.
   * @param      len   the number of bytes to write.
   * @exception  IOException  if an I/O error occurs.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",67,77,"/**
 * Processes chunks using erased indexes.
 * @param inputChunks input data chunks
 * @param outputChunks output data chunks
 * @throws IOException if an I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartC,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartC(),1156,1167,"/**
 * Handles character processing based on current state.
 * Updates CRC and state if conditions met; otherwise, transitions to random part A state.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupBlock(),1039,1075,"/**
 * Initializes decompression data structures.
 * @throws IOException if stream is corrupted
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartC,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartC(),1183,1195,"/**
 * Handles character processing and state transition based on conditions.
 * Updates current character, CRC, counters, and state.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,finalize,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finalize(),711,715,"/**
 * Calls method m1 and then invokes superclass's m2.
 * @throws Throwable if either method throws an exception
 */",* Overriden to close the stream.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,close,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:close(),734,746,"/**
* Calls m2 and delegates m3 to the output stream.
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,finish,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:finish(),263,272,"/**
* Resets and processes output.
* @throws IOException if an I/O error occurs during processing
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(int),645,652,"/**
* Writes masked byte to output stream.
* @param b byte value to mask and write
* @throws IOException if output stream is closed
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(byte[],int,int)",859,879,"/**
 * Masks data in buffer and writes to output stream.
 * @param buf input byte array containing data to mask
 * @param offs starting offset in the buffer
 * @param len number of bytes to process
 * @throws IOException if an I/O error occurs or stream is closed
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer$ValueRegister:close(),492,510,"/**
* Increments error count, executes superclass method, updates record count, and marks as ready.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekToEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekToEnd(),1447,1449,"/**
 * Calls function m1 and handles potential IOExceptions.
 */","* Seek to the end of the scanner. The entry returned by the previous
       * entry() call will be invalid.
       * 
       * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:close(),1578,1581,"/**
 * Overrides and calls method m1.
 * Throws IOException if m1 does.
 */","* Close the scanner. Release all resources. The behavior of using the
       * scanner after calling close is not defined. The entry returned by the
       * previous entry() call will be invalid.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageFile,"org.apache.hadoop.security.Credentials:readTokenStorageFile(java.io.File,org.apache.hadoop.conf.Configuration)",250,265,"/**
 * Reads and parses credentials from a file.
 * @param filename the file containing credentials
 * @param conf configuration object (unused)
 * @return Credentials object populated with data from the file
 * @throws IOException if an error occurs during file reading
 */","* Convenience method for reading a token storage file and loading its Tokens.
   * @param filename filename.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return Token.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),639,642,"/**
 * Retrieves delegation token information.
 * @param ident token identifier
 * @return DelegationTokenInformation object
 * @throws IOException if an I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredToken,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)",791,831,"/**
 * Removes a delegation token from ZooKeeper.
 * @param ident token identifier
 * @param checkAgainstZkBeforeDeletion flag to check token freshness before deletion
 * @throws IOException if an I/O error occurs during the operation
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeProtobufOutputStream,org.apache.hadoop.security.Credentials:writeProtobufOutputStream(java.io.DataOutputStream),328,333,"/**
 * Writes header and format to output stream.
 * @param os DataOutputStream to write to
 * @throws IOException if writing fails
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,decodeToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:decodeToken(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",223,232,"/**
* Masks a delegation token identifier.
* @param token the input token to mask
* @param tokenKind kind of the token
* @return masked DelegationTokenIdentifier
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createIdentifier,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:createIdentifier(),80,83,"/**
* Generates a new delegation token identifier.
* @return DelegationTokenIdentifier object initialized with tokenKind
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createIdentifier,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:createIdentifier(),103,106,"/**
* Generates a new delegation token identifier.
* @return DelegationTokenIdentifier object with specified token kind
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSDelegationToken.java,<init>,org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier:<init>(),43,45,"/**
 * Constructs a new KMSDelegationTokenIdentifier.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,invoke,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",41,46,"/**
* Resets retry count and delegates to superclass method.
* @param proxy proxy object
* @param method method being invoked
* @param args arguments for the method
* @return result of superclass method invocation
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,inBlockAdvance,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",2008,2028,"/**
 * Searches for a key in the current block.
 * @param key the key to search for
 * @param greater true if searching for keys greater than or equal to the given key
 * @return true if a match is found, false otherwise
 * @throws IOException if an I/O error occurs during the search
 */","* Advance cursor in block until we find a key that is greater than or
       * equal to the input key.
       * 
       * @param key
       *          Key to compare.
       * @param greater
       *          advance until we find a key greater than the input key.
       * @return true if we find a equal key.
       * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$IpcStreams:<init>(java.net.Socket,int)",1897,1903,"/**
* Initializes IPC streams with a given socket and maximum response length.
* @param socket the network socket for communication
* @param maxResponseLength the maximum allowed length of responses
* @throws IOException if an I/O error occurs during initialization
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,onTimerEvent,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:onTimerEvent(),382,387,"/**
* Increments logical time and processes sinks.
* @param none
* @return void
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,publishMetricsNow,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetricsNow(),392,397,"/**
 * Checks and processes sink status.
 * Calls m3 with m2's result if sinks' count is greater than zero.
 */",* Requests an immediate publish of all metrics from sources to sinks.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,initSystemMBean,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initSystemMBean(),583,588,"/**
* Masks functionality with prefix and sets MBean name.
* @param prefix used for masking
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,startMBeans,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:startMBeans(),216,224,"/**
* Initializes MBean if not already initialized.
* @param prefix MBean prefix
* @param name MBean name
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:<init>(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)",859,867,"/**
* Initializes MetricsProxy with namespace, levels, and scheduler.
* @param namespace unique namespace identifier
* @param numLevels number of response time levels
* @param drs DecayRpcScheduler instance
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:<init>(java.lang.String),403,408,"/**
* Initializes a MetricsProxy with a given namespace.
* @param namespace the unique namespace for metrics registration
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,shutdown,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown(),590,614,"/**
* Shuts down the metrics system.
* Decrements refCount and stops monitoring if necessary.
* @return true if shutdown is complete, false otherwise
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidator.java,checkStatus,org.apache.hadoop.util.ReadWriteDiskValidator:checkStatus(java.io.File),42,94,"/**
* Performs disk read/write operations to validate disk functionality.
* @param dir directory to perform checks on
* @throws DiskErrorException if any disk operation fails
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,init,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:init(java.lang.Class),75,79,"/**
* Updates metrics for a given protocol.
* @param protocol the protocol class to update metrics for
*/","* Initialize the metrics for JMX with protocol methods
   * @param protocol the protocol class",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,run,org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:run(),223,240,"/**
* Collects metrics, validates records, updates snapshot, and performs cleanup.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,collectThreadLocalStates,org.apache.hadoop.metrics2.lib.MutableRollingAverages:collectThreadLocalStates(),202,204,"/**
 * Delegates to innerMetrics' m1 method.
 */","* Collects states maintained in {@link ThreadLocal}, if any.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,newSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,org.apache.hadoop.metrics2.impl.MetricsConfig)",521,532,"/**
* Creates a MetricsSinkAdapter with specified configurations.
* @param name   sink name
* @param desc   description of the sink
* @param sink   underlying MetricsSink instance
* @param conf   configuration for metrics settings
* @return configured MetricsSinkAdapter instance
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,add,"org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,long)",358,373,"/**
* Updates a mutable metric with a given value.
* @param name the name of the metric
* @param value the value to update the metric with
*/","* Add sample to a stat metric by name.
   * @param name  of the metric
   * @param value of the snapshot to add",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addTokenForOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),924,928,"/**
* Updates token owner statistics.
* @param id Token identifier
*/","* Add token stats to the owner to token count mapping.
   *
   * @param id token id.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeTokenForOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),935,945,"/**
* Masks a token by updating its owner's statistics.
* @param id Token identifier to be masked
*/","* Remove token stats to the owner to token count mapping.
   *
   * @param id",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUidAllowingUnknown,org.apache.hadoop.security.ShellBasedIdMapping:getUidAllowingUnknown(java.lang.String),697,707,"/**
* Maps a user to an ID.
* @param user username string
* @return unique integer ID, using hashcode if mapping fails
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGidAllowingUnknown,org.apache.hadoop.security.ShellBasedIdMapping:getGidAllowingUnknown(java.lang.String),710,720,"/**
* Masks a group by ID.
* @param group the group to mask
* @return masked group ID, using hashcode if mapping fails
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,getCurrentActive,org.apache.hadoop.ha.ZKFailoverController:getCurrentActive(),784,802,"/**
 * Retrieves the current active HA service target.
 * @return HAServiceTarget object or null if not found
 * @throws IOException on unexpected ZooKeeper issues
 * @throws InterruptedException if thread is interrupted during operation
 */","* @return an {@link HAServiceTarget} for the current active node
   * in the cluster, or null if no node is active.
   * @throws IOException if a ZK-related issue occurs
   * @throws InterruptedException if thread is interrupted",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,<init>,"org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,boolean,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",273,299,"/**
* Initializes an ActiveStandbyElector with ZK configuration and callback.
* @param zookeeperHostPorts comma-separated list of ZK host:port pairs
* @param zookeeperSessionTimeout session timeout in milliseconds
* @param parentZnodeName ZK node path for election
* @param acl access control list for ZK nodes
* @param authInfo authentication information for ZK
* @param app callback for election events
* @param maxRetryNum maximum retry attempts on connection failure
* @param failFast if true, attempt to create a new connection immediately; otherwise, re-establish session
* @param truststoreKeystore keystore and truststore configuration
* @throws IOException if an I/O error occurs
* @throws HadoopIllegalArgumentException for invalid arguments
* @throws KeeperException if there is a ZK error
*/","* Create a new ActiveStandbyElector object <br>
   * The elector is created by providing to it the Zookeeper configuration, the
   * parent znode under which to create the znode and a reference to the
   * callback interface. <br>
   * The parent znode name must be the same for all service instances and
   * different across services. <br>
   * After the leader has been lost, a new leader will be elected after the
   * session timeout expires. Hence, the app must set this parameter based on
   * its needs for failure response time. The session timeout must be greater
   * than the Zookeeper disconnect timeout and is recommended to be 3X that
   * value to enable Zookeeper to retry transient disconnections. Setting a very
   * short session timeout may result in frequent transitions between active and
   * standby states during issues like network outages/GS pauses.
   * 
   * @param zookeeperHostPorts
   *          ZooKeeper hostPort for all ZooKeeper servers
   * @param zookeeperSessionTimeout
   *          ZooKeeper session timeout
   * @param parentZnodeName
   *          znode under which to create the lock
   * @param acl
   *          ZooKeeper ACL's
   * @param authInfo a list of authentication credentials to add to the
   *                 ZK connection
   * @param app
   *          reference to callback interface object
   * @param failFast
   *          whether need to add the retry when establishing ZK connection.
   * @param maxRetryNum max Retry Num
   * @param truststoreKeystore truststore keystore, that we will use for ZK if SSL/TLS is enabled
   * @throws IOException
   *          raised on errors performing I/O.
   * @throws HadoopIllegalArgumentException
   *          if valid data is not supplied.
   * @throws KeeperException
   *          other zookeeper operation errors.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,joinElectionInternal,org.apache.hadoop.ha.ActiveStandbyElector:joinElectionInternal(),783,796,"/**
* Prepares for election by validating app data and establishing ZooKeeper connection.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,org.apache.hadoop.security.UserGroupInformation$KeytabRenewalRunnable:relogin(),1094,1097,"/**
 * Calls method m1 and handles potential IOExceptions.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddrUnresolved,org.apache.hadoop.net.NetUtils:createSocketAddrUnresolved(java.lang.String),166,168,"/**
* Creates an InetSocketAddress from a target string.
* @param target the target address as a string
* @return InetSocketAddress object
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean)",222,227,"/**
* Calls m1 with additional parameter.
* @param target target address
* @param defaultPort default port number
* @param configName configuration name
* @param useCacheIfPresent flag to use cache if available
* @return InetSocketAddress object
*/","* Create an InetSocketAddress from the given target string and
   * default port. If the string cannot be parsed correctly, the
   * <code>configName</code> parameter is used as part of the
   * exception message, allowing the user to better diagnose
   * the misconfiguration.
   *
   * @param target a string of either ""host"" or ""host:port""
   * @param defaultPort the default port if <code>target</code> does not
   *                    include a port number
   * @param configName the name of the configuration from which
   *                   <code>target</code> was loaded. This is used in the
   *                   exception message in the case that parsing fails.
   * @param useCacheIfPresent Whether use cache when create URI
   * @return  socket addr",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getConnectAddress,org.apache.hadoop.net.NetUtils:getConnectAddress(org.apache.hadoop.ipc.Server),439,441,"/**
 * Retrieves InetSocketAddress from Server using helper method.
 * @param server Server instance to query
 * @return InetSocketAddress associated with the server
 */","* Returns InetSocketAddress that a client can use to 
   * connect to the server. Server.getListenerAddress() is not correct when
   * the server binds to ""0.0.0.0"". This returns ""hostname:port"" of the server,
   * or ""127.0.0.1:port"" when the getListenerAddress() returns ""0.0.0.0:port"".
   * 
   * @param server server.
   * @return socket address that a client can use to connect to the server.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupConnection,org.apache.hadoop.ipc.Client$Connection:setupConnection(org.apache.hadoop.security.UserGroupInformation),608,695,"/**
* Establishes a connection to the server using provided credentials.
* @param ticket user's Kerberos information for secure connection
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean)",1415,1420,"/**
* Calls m1 with default service class.
* @param rpcKind type of RPC kind
* @param rpcRequest request data
* @param remoteId connection identifier
* @param fallbackToSimpleAuth flag for fallback authentication
* @return Writable response or throws IOException
*/","* Make a call, passing <code>rpcRequest</code>, to the IPC server defined by
   * <code>remoteId</code>, returning the rpc respond.
   *
   * @param rpcKind - input rpcKind.
   * @param rpcRequest -  contains serialized method and method parameters
   * @param remoteId - the target rpc server
   * @param fallbackToSimpleAuth - set to true or false during this method to
   *   indicate if a secure client falls back to simple auth
   * @return the rpc response
   * Throws exceptions if there are network problems or if the remote code
   * threw an exception.
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",1422,1428,"/**
* Calls m1 with default service class.
* @param rpcKind type of RPC
* @param rpcRequest request data
* @param remoteId connection identifier
* @param fallbackToSimpleAuth flag for simple auth fallback
* @param alignmentContext context for alignment
* @return Writable response or throws IOException
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean)",1444,1450,"/**
* Calls m1 with an additional null parameter.
* @param rpcKind type of RPC request
* @param rpcRequest the RPC request object
* @param remoteId connection ID for the remote server
* @param serviceClass class of the service
* @param fallbackToSimpleAuth flag to allow fallback to simple auth
* @return result of m1 call
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,"org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,byte[],int)",354,362,"/**
* Retrieves a CacheEntry from the RetryCache.
* @param cache RetryCache instance to query
* @param clientId client identifier in byte array format
* @param callId unique call identifier
* @return CacheEntry object or null if not found or conditions are met
*/","* Static method that provides null check for retryCache.
   * @param cache input Cache.
   * @param clientId client id of this request
   * @param callId client call id of this request
   * @return CacheEntry.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,"org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,java.lang.Object,byte[],int)",372,380,"/**
* Retrieves or creates a cache entry.
* @param cache the RetryCache instance
* @param payload data to be cached
* @param clientId unique client identifier
* @param callId unique call identifier
* @return CacheEntryWithPayload object or null if not found or creation fails
*/","* Static method that provides null check for retryCache.
   * @param cache input cache.
   * @param payload input payload.
   * @param clientId client id of this request
   * @param callId client call id of this request
   * @return CacheEntryWithPayload.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,org.apache.hadoop.fs.ContentSummary:toString(),369,372,"/**
 * Calls overloaded method with default flag set to true.
 * @return Result of m1(true) call
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getExpression,org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.String),432,435,"/**
 * Creates an expression using a factory.
 * @param expressionName name of the expression to create
 * @return Expression object created by the factory
 */",Gets a named expression from the factory.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInfo,"org.apache.hadoop.fs.FsShell:printInfo(java.io.PrintStream,java.lang.String,boolean)",212,247,"/**
 * Handles command execution and help display.
 * @param out PrintStream for output
 * @param cmd Command to execute or null for list
 * @param showHelp Flag to display help information
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,displayError,"org.apache.hadoop.fs.FsShell:displayError(java.lang.String,java.lang.String)",353,365,"/**
* Logs and checks command syntax.
* @param cmd command string
* @param message input message to process
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,get,"org.apache.hadoop.io.WritableComparator:get(java.lang.Class,org.apache.hadoop.conf.Configuration)",65,81,"/**
* Retrieves or creates a WritableComparator for a class.
* @param c the class of WritableComparable
* @param conf configuration settings
* @return configured WritableComparator instance
*/","* Get a comparator for a {@link WritableComparable} implementation.
   * @param c class.
   * @param conf configuration.
   * @return WritableComparator.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class),132,134,"/**
 * Constructs a comparator for the specified writable class.
 * @param keyClass the class of keys to be compared
 */","* Construct for a {@link WritableComparable} implementation.
   * @param keyClass WritableComparable Class.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,"org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,boolean)",136,139,"/**
* Constructs a comparator for WritableComparables.
* @param keyClass class of keys to compare
* @param tagName name tag for the comparator (null if not used)
* @param createInstances whether to create instances during initialization
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readFields,org.apache.hadoop.io.ObjectWritable:readFields(java.io.DataInput),84,87,"/**
 * Masks data from input stream.
 * @param in DataInput source
 * @throws IOException if I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,readFields,org.apache.hadoop.ipc.WritableRpcEngine$Invocation:readFields(java.io.DataInput),148,164,"/**
 * Reads RPC method call data from input stream.
 * @param in DataInput source for reading method details
 * @throws IOException if an I/O error occurs while reading
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,readFields,org.apache.hadoop.io.ArrayWritable:readFields(java.io.DataInput),93,101,"/**
* Reads array of Writable objects from input stream.
* @param in DataInput to read from
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,verifyToken,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:verifyToken(org.apache.hadoop.security.token.Token),208,215,"/**
* Processes a delegation token and returns user group information.
* @param token the delegation token to process
* @return UserGroupInformation object associated with the token
* @throws IOException if an I/O error occurs during processing
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,toString,org.apache.hadoop.security.token.Token:toString(),437,447,"/**
* Constructs a formatted string with kind, service, and identifier.
* @return Formatted string containing kind, service, and ident details
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,<init>,org.apache.hadoop.fs.LocalDirAllocator:<init>(java.lang.String),85,93,"/**
* Initializes a LocalDirAllocator with a configuration item name.
* @param contextCfgItemName the configuration item name for the allocator
*/","* Create an allocator object.
   * @param contextCfgItemName contextCfgItemName.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,parseMethods,"org.apache.hadoop.ha.NodeFencer:parseMethods(org.apache.hadoop.conf.Configuration,java.lang.String)",134,149,"/**
* Parses configuration specification to extract fencing methods.
* @param conf Configuration object
* @param spec Configuration specification string
* @return List of FenceMethodWithArg objects
* @throws BadFencingConfigurationException if parsing fails
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,append,"org.apache.hadoop.io.BloomMapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",182,190,"/**
 * Overrides m1 to process key and value.
 * @param key unique identifier
 * @param val associated data
 * @throws IOException if I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.lang.String,boolean)",927,929,"/**
 * Masks resource by name and parser restriction.
 * @param name resource identifier
 * @param restrictedParser flag to restrict parser usage
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.net.URL,boolean)",945,947,"/**
 * Masks a URL with a resource.
 * @param url the URL to be masked
 * @param restrictedParser flag indicating if parser is restricted
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path,boolean)",963,965,"/**
 * Masks a file using a resource.
 * @param file Path to the file to be masked
 * @param restrictedParser Flag indicating if a restricted parser should be used
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,boolean)",984,986,"/**
* Processes input stream with optional restricted parsing.
* @param in input stream to process
* @param restrictedParser flag to enable restricted parsing mode
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String,boolean)",1002,1005,"/**
 * Masks input stream with resource details.
 * @param in input stream to be masked
 * @param name name of the resource
 * @param restrictedParser flag for restricted parsing
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setDeprecatedProperties,org.apache.hadoop.conf.Configuration:setDeprecatedProperties(),688,706,"/**
* Migrates deprecated keys to new ones in properties.
*/","* Sets all deprecated properties that are not currently set but have a
   * corresponding new property that is set. Useful for iterating the
   * properties when all deprecated properties for currently set properties
   * need to be present.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updatePropertiesWithDeprecatedKeys,"org.apache.hadoop.conf.Configuration:updatePropertiesWithDeprecatedKeys(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String[])",764,775,"/**
* Masks deprecated keys with new names.
* @param deprecations context for deprecation handling
* @param newNames array of new key names
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(org.apache.hadoop.conf.Configuration),843,875,"/**
 * Creates a deep copy of the given Configuration object.
 * @param other the Configuration object to copy
 */","* A new configuration with the same settings cloned from another.
   * 
   * @param other the configuration from which to clone settings.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.conf.Configuration),1015,1017,"/**
 * Masks configuration properties.
 * @param conf Configuration object containing settings to mask
 */","* Add a configuration resource.
   *
   * The properties of this resource will override properties of previously
   * added resources, unless they were marked <a href=""#Final"">final</a>.
   *
   * @param conf Configuration object from which to load properties",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getAlternativeNames,org.apache.hadoop.conf.Configuration:getAlternativeNames(java.lang.String),1372,1393,"/**
 * Retrieves alternative names for a deprecated key.
 * @param name the original key name
 * @return array of alternative names or null if none exist
 */","* Returns alternative names (non-deprecated keys or previously-set deprecated keys)
   * for a given non-deprecated key.
   * If the given key is deprecated, return null.
   *
   * @param name property name.
   * @return alternative names.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPropertySources,org.apache.hadoop.conf.Configuration:getPropertySources(java.lang.String),2109,2129,"/**
* Masks a resource name.
* @param name the resource name to mask
* @return masked resource names or null if not applicable
*/","* Gets information about why a property was set.  Typically this is the 
   * path to the resource objects (file, URL, etc.) the property came from, but
   * it can also indicate that it was set programmatically, or because of the
   * command line.
   *
   * @param name - The property name to get the source of.
   * @return null - If the property or its source wasn't found. Otherwise, 
   * returns a list of the sources of the resource.  The older sources are
   * the first ones in the list.  So for example if a configuration is set from
   * the command line, and then written out to a file that is read back in the
   * first entry would indicate that it was set from the command line, while
   * the second one would indicate the file that the new configuration was read
   * in from.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,size,org.apache.hadoop.conf.Configuration:size(),2988,2990,"/**
 * Calls m2 on the result of m1().
 * @return Result of m2() called on the object returned by m1()
 */","* Return the number of keys in the configuration.
   *
   * @return number of keys in the configuration.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,clear,org.apache.hadoop.conf.Configuration:clear(),2995,2998,"/**
 * Calls m2 on objects returned by m1 and m3.
 */",* Clears all keys from the configuration.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,iterator,org.apache.hadoop.conf.Configuration:iterator(),3006,3022,"/**
* Converts Properties to Map and returns an iterator of entries.
* @return Iterator<Map.Entry<String, String>> containing key-value pairs
*/","* Get an {@link Iterator} to go through the list of <code>String</code> 
   * key-value pairs in the configuration.
   * 
   * @return an iterator over the entries.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,write,org.apache.hadoop.conf.Configuration:write(java.io.DataOutput),3965,3975,"/**
 * Writes properties to DataOutput.
 * @param out the output stream to write to
 * @throws IOException if an I/O error occurs
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getValByRegex,org.apache.hadoop.conf.Configuration:getValByRegex(java.lang.String),3982,4001,"/**
 * Masks values in a map based on regex.
 * @param regex pattern to match keys for masking
 * @return Map with masked values
 */","* get keys matching the the regex.
   * @param regex the regex to match against.
   * @return {@literal Map<String,String>} with matching keys",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,readVectored,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readVectored(java.util.List,java.util.function.IntFunction)",438,482,"/**
 * Processes file ranges with checksum validation.
 * @param ranges list of file ranges to process
 * @param allocate function to allocate ByteBuffer
 * @throws IOException if an I/O error occurs
 */","* Vectored read.
     * If the file has no checksums: delegate to the underlying stream.
     * If the file is checksummed: calculate the checksum ranges as
     * well as the data ranges, read both, and validate the checksums
     * as well as returning the data.
     * @param ranges the byte ranges to read
     * @param allocate the function to allocate ByteBuffer
     * @throws IOException",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:get(int),144,175,"/**
* Retrieves buffer data by block number with retries.
* @param blockNumber the identifier of the block to retrieve
* @return BufferData object if successful
* @throws IOException if an I/O error occurs or stream is closed
*/","* Gets the block having the given {@code blockNumber}.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,waitForReplication,org.apache.hadoop.fs.shell.SetReplication:waitForReplication(),108,141,"/**
* Processes items in waitList, adjusting replications and monitoring progress.
*/",* Wait for all files in waitList to have replication number equal to rep.,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",924,931,"/**
 * Retrieves block locations for a specified path range.
 * @param p the file path
 * @param start starting offset in bytes
 * @param len length of the range in bytes
 * @return array of BlockLocation objects
 * @throws IOException if an I/O error occurs
 */","* Return an array containing hostnames, offset and size of
   * portions of the given file.  For a nonexistent
   * file or regions, {@code null} is returned.
   *
   * This call is most helpful with location-aware distributed
   * filesystems, where it returns hostnames of machines that
   * contain the given file.
   *
   * A FileSystem will normally return the equivalent result
   * of passing the {@code FileStatus} of the path to
   * {@link #getFileBlockLocations(FileStatus, long, long)}
   *
   * @param p path is used to identify an FS since an FS could have
   *          another FS that it could be delegating the call to
   * @param start offset into the given file
   * @param len length for which to get locations for
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException IO failure
   * @return block location array.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FilterFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",151,155,"/**
 * Retrieves block locations for a file segment.
 * @param file FileStatus object representing the file
 * @param start Starting offset of the segment
 * @param len Length of the segment
 * @return Array of BlockLocation objects
 * @throws IOException if an I/O error occurs
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,fetchSuccessSummary,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchSuccessSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String)",149,153,"/**
 * Masks and summarizes duration statistics.
 * @param source IOStatistics object containing data
 * @param key identifier for the statistic to mask
 * @return DurationStatisticSummary of masked statistics
 */","* Fetch the duration timing summary from an IOStatistics source.
   * If the duration key is unknown, the summary will be incomplete.
   * @param source source of data
   * @param key duration statistic key
   * @return a summary of the statistics.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create(),113,115,"/**
 * Calls overloaded m1 with null parameter.
 * @return result of m1(null)
 */","* Create a new {@link IOStatisticsSnapshot} instance.
   * @return an empty IOStatisticsSnapshot.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_retrieve,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object),146,152,"/**
* Applies mask function to source object.
* @param source input data object
* @return processed Serializable object or null
*/","* Extract the IOStatistics from an object in a serializable form.
   * @param source source object, may be null/not a statistics source/instance
   * @return {@link IOStatisticsSnapshot} or null if the object is null/doesn't have statistics",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,toArray,"org.apache.hadoop.util.functional.RemoteIterators:toArray(org.apache.hadoop.fs.RemoteIterator,java.lang.Object[])",248,252,"/**
 * Converts RemoteIterator to array.
 * @param source iterator of remote elements
 * @param a target array type
 * @return array containing elements from iterator
 * @throws IOException if an I/O error occurs
 */","* Build an array from a RemoteIterator.
   * @param source source iterator
   * @param a destination array; if too small a new array
   * of the same type is created
   * @param <T> type
   * @return an array of the values.
   * @throws IOException if the source RemoteIterator raises it.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,createPassword,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),493,515,"/**
* Generates a delegation token for the given identifier.
* @param identifier TokenIdent object containing token details
* @return byte array representing the generated password
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,renewToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)",592,643,"/**
* Renews a delegation token.
* @param token the token to be renewed
* @param renewer the user attempting to renew the token
* @return new expiration time of the token
* @throws InvalidToken if the token is invalid or expired
* @throws IOException on I/O errors
*/","* Renew a delegation token.
   * @param token the token to renew
   * @param renewer the full principal name of the user doing the renewal
   * @return the new expiration time
   * @throws InvalidToken if the token is invalid
   * @throws AccessControlException if the user can't renew token",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",654,685,"/**
* Cancels a token by identifier and checks authorization.
* @param token the token to be cancelled
* @param canceller user attempting to cancel the token
* @return TokenIdent of the cancelled token
* @throws IOException if an I/O error occurs
*/","* Cancel a token by removing it from cache.
   *
   * @param token token.
   * @param canceller canceller.
   * @return Identifier of the canceled token
   * @throws InvalidToken for invalid token
   * @throws AccessControlException if the user isn't allowed to cancel",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processArguments,org.apache.hadoop.fs.shell.Command:processArguments(java.util.LinkedList),281,290,"/**
* Processes a list of PathData objects.
* @param args list of PathData to process
*/","*  Processes the command's list of expanded arguments.
   *  {@link #processArgument(PathData)} will be invoked with each item
   *  in the list.  The loop catches IOExceptions, increments the error
   *  count, and displays the exception.
   *  @param args a list of {@link PathData} to process
   *  @throws IOException if anything goes wrong...",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroups,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroups(java.lang.String),98,101,"/**
 * Masks sensitive information in user name.
 * @param userName original user name
 * @return list of masked characters
 * @throws IOException if an I/O error occurs
 */","* Returns list of groups for a user
   *
   * @param userName get groups for this user
   * @return list of groups for a given user",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsSet(java.lang.String),121,124,"/**
 * Masks the username by calling another function.
 * @param userName user's name to be masked
 * @return set of masked strings
 * @throws IOException if an I/O error occurs
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,resolve,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:resolve(java.util.List),174,211,"/**
* Masks names using a script or default rack.
* @param names list of names to be masked
* @return masked names or null if processing fails
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,getLinkCount,org.apache.hadoop.fs.HardLink:getLinkCount(java.io.File),214,255,"/**
* Executes a shell command to get the link count of a file.
* @param fileName the file to check
* @return the number of hard links
* @throws IOException if an I/O error occurs or file is not found
*/","* Retrieves the number of links to the specified file.
    *
    * @param fileName file name.
    * @throws IOException raised on errors performing I/O.
    * @return link count.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingTar,"org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.File,java.io.File,boolean)",1053,1084,"/**
 * Untars a file to a specified directory.
 * @param inFile input tar file
 * @param untarDir destination directory for extraction
 * @param gzipped indicates if the file is gzip compressed
 * @throws IOException if there's an error during untarring
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,symLink,"org.apache.hadoop.fs.FileUtil:symLink(java.lang.String,java.lang.String)",1226,1279,"/**
* Creates a symbolic link from target to linkname.
* @param target path of the target file
* @param linkname path where the symlink should be created
* @return exit code of the command execution, 0 on success
* @throws IOException if an I/O error occurs
*/","* Create a soft link between a src and destination
   * only on a local disk. HDFS does not support this.
   * On Windows, when symlink creation fails due to security
   * setting, we will log a warning. The return code in this
   * case is 2.
   *
   * @param target the target for symlink
   * @param linkname the symlink
   * @return 0 on success
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,chmod,"org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String,boolean)",1303,1319,"/**
* Changes file permissions using a shell command.
* @param filename path to the file
* @param perm permission string
* @param recursive true for recursive operation
* @return exit status of the shell command
* @throws IOException if an I/O error occurs
*/","* Change the permissions on a file / directory, recursively, if
   * needed.
   * @param filename name of the file whose permissions are to change
   * @param perm permission string
   * @param recursive true, if permissions should be changed recursively
   * @return the exit code from the command.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getConf,org.apache.hadoop.util.SysInfoLinux:getConf(java.lang.String),158,170,"/**
 * Executes a shell command to fetch configuration attribute value.
 * @param attr configuration attribute name
 * @return long value of the attribute or -1 if failed
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getSystemInfoInfoFromShell,org.apache.hadoop.util.SysInfoWindows:getSystemInfoInfoFromShell(),81,92,"/**
* Executes systeminfo command via shell.
* @return Command output as String or null on error
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkIsBashSupported,org.apache.hadoop.util.Shell:checkIsBashSupported(),810,834,"/**
 * Checks if bash is supported on the OS.
 * @throws InterruptedIOException if interrupted during execution
 * @return true if bash is supported, false otherwise
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,isSetsidSupported,org.apache.hadoop.util.Shell:isSetsidSupported(),845,879,"/**
* Checks if 'setsid' is supported on the system.
* @return true if 'setsid' is available and allowed, false otherwise
*/","* Look for <code>setsid</code>.
   * @return true if <code>setsid</code> was present",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfoByNonNativeIO,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNonNativeIO(),998,1045,"/**
* Fetches and sets file permissions, owner, and group.
* Handles exceptions by setting null values or rethrowing as RuntimeException.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setOwner,"org.apache.hadoop.fs.FileUtil:setOwner(java.io.File,java.lang.String,java.lang.String)",1329,1338,"/**
 * Masks a file with specified user and group.
 * @param file the file to mask
 * @param username the username for masking
 * @param groupname the groupname for masking
 */","* Set the ownership on a file / directory. User name and group name
   * cannot both be null.
   * @param file the file to change
   * @param username the new user owner name
   * @param groupname the new group owner name
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,execSetPermission,"org.apache.hadoop.fs.FileUtil:execSetPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1520,1529,"/**
* Masks file with specified permission.
* @param f the target file
* @param permission desired file permissions
* @throws IOException if an I/O error occurs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,getUsersForNetgroup,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getUsersForNetgroup(java.lang.String),97,123,"/**
* Parses netgroup to extract user list.
* @param netgroup netgroup string to be parsed
* @return List of usernames extracted from the netgroup
* @throws IOException if an I/O error occurs during parsing
*/","* Gets users for a netgroup
   *
   * @param netgroup return users for this netgroup
   * @return list of users for a given netgroup
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCredentials,org.apache.hadoop.security.UserGroupInformation:getCredentials(),1736,1747,"/**
* Generates and returns user credentials.
* @return Credentials object containing user tokens
*/","* Obtain the tokens in credentials form associated with this user.
   * 
   * @return Credentials of tokens associated with this user",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,flush,org.apache.hadoop.security.alias.UserProvider:flush(),94,97,"/**
 * Masks user credentials.
 * Calls m1 with current credentials.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,flush,org.apache.hadoop.crypto.key.UserProvider:flush(),138,141,"/**
 * Masks user credentials.
 * Calls m1 with current credentials.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirInternal,"org.apache.hadoop.util.DiskChecker:checkDirInternal(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",138,143,"/**
 * Masks directory permissions.
 * @param localFS Local file system instance
 * @param dir Directory path to mask
 * @param expected Expected file permissions
 * @throws DiskErrorException if disk error occurs
 * @throws IOException if I/O error occurs
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,hasPathCapability,"org.apache.hadoop.fs.FilterFs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",451,455,"/**
 * Checks file system permission.
 * @param path file path to check
 * @param capability required permission
 * @return true if permitted, false otherwise
 * @throws IOException on I/O error
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getEnclosingRoot,org.apache.hadoop.fs.FilterFs:getEnclosingRoot(org.apache.hadoop.fs.Path),463,466,"/**
 * Delegates file system operation to myFs.
 * @param path file path to operate on
 * @return result of the file system operation
 * @throws IOException if an I/O error occurs
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",1850,1856,"/**
 * Filters files in a directory.
 * @param f path to the directory
 * @param filter criteria for file selection
 * @return array of filtered FileStatus objects
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if file not found
 * @throws UnsupportedFileSystemException if file system is unsupported
 * @throws IOException if I/O error occurs
 */","* Filter files/directories in the given path using the user-supplied path
     * filter.
     * 
     * @param f is the path name
     * @param filter is the user-supplied path filter
     *
     * @return an array of FileStatus objects for the files under the given path
     *         after applying the filter
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)",1879,1886,"/**
* Filters and processes file statuses.
* @param files array of file paths to process
* @param filter criteria for filtering files
* @return array of filtered FileStatus objects
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if a file is not found
* @throws IOException for other I/O errors
*/","* Filter files/directories in the given list of paths using user-supplied
     * path filter.
     * 
     * @param files is a list of paths
     * @param filter is the filter
     *
     * @return a list of statuses for the files under the given paths after
     *         applying the filter
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If a file in <code>files</code> does not 
     *           exist
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,run,org.apache.hadoop.fs.FileContext$FileContextFinalizer:run(),2318,2321,"/**
 * Calls method m1 in a thread-safe manner.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,isMultiThreadNecessary,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:isMultiThreadNecessary(java.util.LinkedList),98,102,"/**
* Checks if function should mask based on thread count and input arguments.
* @param args list of PathData objects
* @return true if masking is needed, false otherwise
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processArguments,org.apache.hadoop.fs.shell.Truncate:processArguments(java.util.LinkedList),68,73,"/**
* Overrides and extends base method to process PathData list.
* @param args LinkedList of PathData objects
* @throws IOException if an I/O error occurs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,resolve,"org.apache.hadoop.fs.viewfs.InodeTree:resolve(java.lang.String,boolean)",893,988,"/**
 * Resolves a path to a filesystem node.
 * @param p the input path string
 * @param resolveLastComponent whether to resolve the last component
 * @return ResolveResult containing the resolved node and details
 * @throws IOException if an I/O error occurs
 */","* Resolve the pathname p relative to root InodeDir.
   * @param p - input path
   * @param resolveLastComponent resolveLastComponent.
   * @return ResolveResult which allows further resolution of the remaining path
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileHarStatus,org.apache.hadoop.fs.HarFileSystem:getFileHarStatus(org.apache.hadoop.fs.Path),646,659,"/**
* Retrieves archive status for a file.
* @param f file path to check
* @return HarStatus object representing the file's archive status
* @throws IOException if file name is invalid or I/O error occurs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,int)",153,178,"/**
* Initializes a ChecksumFSInputChecker for a file.
* @param fs File system containing the file
* @param file Path to the file
* @param bufferSize Buffer size for reading data and checksums
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link cannot be resolved
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,setReplication,"org.apache.hadoop.fs.ChecksumFs:setReplication(org.apache.hadoop.fs.Path,short)",463,475,"/**
 * Checks and replicates a file.
 * @param src source file path
 * @param replication number of replicas
 * @return true if successful, false otherwise
 * @throws IOException if an I/O error occurs
 * @throws UnresolvedLinkException if a symbolic link cannot be resolved
 */","* Set replication for an existing file.
   * Implement the abstract <tt>setReplication</tt> of <tt>FileSystem</tt>
   * @param src file name
   * @param replication new replication
   * @throws IOException if an I/O error occurs.
   * @return true if successful;
   *         false if file does not exist or is a directory",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,delete,"org.apache.hadoop.fs.ChecksumFs:delete(org.apache.hadoop.fs.Path,boolean)",529,550,"/**
* Checks and processes file status recursively.
* @param f file path to check
* @param recursive flag for recursive processing
* @return true if processed, false otherwise
*/","* Implement the delete(Path, boolean) in checksum
   * file system.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",363,389,"/**
* Initializes a ChecksumFSOutputSummer for writing files with checksum.
* @param fs filesystem to use
* @param file path of the file to write
* @param createFlag flags for creating the file
* @param absolutePermission permissions for the file
* @param bufferSize buffer size for data and checksum streams
* @param replication replication factor for the file
* @param blockSize block size for the file
* @param progress progressable object to report progress
* @param checksumOpt checksum options
* @param createParent flag to create parent directories if needed
* @throws IOException if an I/O error occurs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processArguments,org.apache.hadoop.fs.shell.CommandWithDestination:processArguments(java.util.LinkedList),223,245,"/**
 * Handles file operations based on destination path and arguments.
 * @param args list of path data arguments
 * @throws IOException if an I/O error occurs
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,mkdir,"org.apache.hadoop.fs.DelegateToFileSystem:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",185,192,"/**
* Applies mask to directory and sets permissions.
* @param dir target directory path
* @param permission file system permissions
* @param createParent flag to create parent directories if needed
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,rename,"org.apache.hadoop.fs.FileUtil:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",2068,2082,"/**
 * Renames a file from source to destination in the given FileSystem.
 * @param srcFs source FileSystem object
 * @param src source path
 * @param dst destination path
 * @param options optional rename options
 * @throws IOException if an I/O error occurs
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,renameInternal,"org.apache.hadoop.fs.DelegateToFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",206,212,"/**
* Copies file from source to destination.
* @param src source file path
* @param dst destination file path
* @throws IOException if an I/O error occurs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,rename,"org.apache.hadoop.fs.FilterFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",254,258,"/**
 * Moves a file from source to destination with specified rename options.
 * @param src source file path
 * @param dst destination file path
 * @param options optional rename options
 * @throws IOException if an I/O error occurs
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InternalOperations.java,rename,"org.apache.hadoop.fs.InternalOperations:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",35,39,"/**
 * Moves a file from source to destination.
 * @param fs FileSystem instance
 * @param src Source Path object
 * @param dst Destination Path object
 * @param options Rename options (varargs)
 * @throws IOException if an I/O error occurs
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,run,org.apache.hadoop.fs.ChecksumFileSystem$FsOperation:run(org.apache.hadoop.fs.Path),771,780,"/**
* Checks file and processes it if valid.
* @param p file path to process
* @return true if initial check passes, false otherwise
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,int)",187,214,"/**
* Initializes a ChecksumFSInputChecker for a given file.
* @param fs the ChecksumFileSystem instance
* @param file the Path of the file to check
* @param bufferSize size of buffer for reading data and checksums
* @throws IOException if an I/O error occurs opening files or reading version
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,rename,"org.apache.hadoop.fs.ChecksumFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",890,915,"/**
 * Moves file from source to destination path.
 * Handles existing files and checks for specific conditions.
 * @param src source file path
 * @param dst destination file path
 * @return true if operation successful, false otherwise
 */",* Rename files/dirs,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,delete,"org.apache.hadoop.fs.ChecksumFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",921,941,"/**
 * Checks file status and processes recursively or individually.
 * @param f file path to check
 * @param recursive flag for recursive processing
 * @return true if operation is successful, false otherwise
 * @throws IOException if an I/O error occurs
 */","* Implement the delete(Path, boolean) in checksum
   * file system.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)",621,642,"/**
* Initializes a ChecksumFSOutputSummer for file output.
* @param fs the ChecksumFileSystem instance
* @param file the Path of the file to write
* @param overwrite true if existing files should be overwritten
* @param bufferSize size of the buffer used for writing
* @param replication desired block replication factor
* @param blockSize desired block size
* @param progress Progressable object for tracking progress
* @param permission FsPermission for the new file
* @throws IOException if an I/O error occurs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isAncestor,"org.apache.hadoop.fs.shell.find.Find:isAncestor(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",335,343,"/**
* Checks if target path is a descendant of any ancestor in source path.
* @param source PathData representing the root path
* @param target PathData representing the path to check
* @return true if target is a descendant, false otherwise
*/",Returns true if the target is an ancestor of the source.,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,fullPath,org.apache.hadoop.fs.viewfs.ChRootedFs:fullPath(org.apache.hadoop.fs.Path),91,95,"/**
 * Masks a given path.
 * @param path the original file path
 * @return masked file path
 */","* 
   * @param path
   * @return return full path including the chroot",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,stripOutRoot,org.apache.hadoop.fs.viewfs.ChRootedFs:stripOutRoot(org.apache.hadoop.fs.Path),137,148,"/**
* Masks the given path.
* @param p input file path
* @return masked path string or empty if matches root path
*/","*  
   * Strip out the root from the path.
   * 
   * @param p - fully qualified path p
   * @return -  the remaining path  without the beginning /",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,stripOutRoot,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:stripOutRoot(org.apache.hadoop.fs.Path),153,163,"/**
 * Masks the given path by removing or modifying its root part.
 * @param p input file path
 * @return masked path string or empty if unchanged
 * @throws IOException if internal error occurs
 */","* Strip out the root from the path.
   * @param p - fully qualified path p
   * @return -  the remaining path  without the beginning /
   * @throws IOException if the p is not prefixed with root",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,"org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(org.apache.hadoop.fs.Path,java.util.Date)",335,359,"/**
 * Creates a trash checkpoint.
 * @param trashRoot root directory of the trash
 * @param date current date for naming the checkpoint
 * @throws IOException if an I/O error occurs or checkpoint creation fails
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSLinkResolver.java,resolve,"org.apache.hadoop.fs.FSLinkResolver:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",79,112,"/**
* Reads file content from given path.
* @param fc file context
* @param path file path
* @return file content or null if not found
* @throws IOException on I/O errors or symlink resolution issues
*/","* Performs the operation specified by the next function, calling it
   * repeatedly until all symlinks in the given path are resolved.
   * @param fc FileContext used to access file systems.
   * @param path The path to resolve symlinks on.
   * @return Generic type determined by the implementation of next.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,rename,"org.apache.hadoop.fs.AbstractFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",795,808,"/**
 * Moves a file from source to destination with optional overwrite.
 * @param src source file path
 * @param dst destination file path
 * @param options rename options (e.g., OVERWRITE)
 * @throws IOException if an I/O error occurs
 */","* The specification of this method matches that of
   * {@link FileContext#rename(Path, Path, Options.Rename...)} except that Path
   * f must be for this file system.
   *
   * @param src src.
   * @param dst dst.
   * @param options options.
   * @throws AccessControlException access control exception.
   * @throws FileAlreadyExistsException file already exists exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not directory exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameInternal,"org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",253,259,"/**
 * Moves a file from source to destination.
 * @param src source file path
 * @param dst destination file path
 * @param overwrite flag to overwrite existing files
 * @throws AccessControlException if access is denied
 * @throws FileAlreadyExistsException if destination file exists and overwrite is false
 * @throws FileNotFoundException if source file does not exist
 * @throws ParentNotDirectoryException if parent of destination is not a directory
 * @throws UnresolvedLinkException if a link cannot be resolved
 * @throws IOException for other I/O errors
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,toFileStatus,org.apache.hadoop.fs.HarFileSystem:toFileStatus(org.apache.hadoop.fs.HarFileSystem$HarStatus),530,553,"/**
* Creates a FileStatus object based on HarStatus.
* @param h HarStatus instance containing file metadata
* @return FileStatus object representing the file or directory
* @throws IOException if an I/O error occurs
*/","* Combine the status stored in the index and the underlying status. 
   * @param h status stored in the index
   * @return the combined file status
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,<init>,"org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:<init>(java.io.File,long,org.apache.hadoop.fs.FileSystem)",942,949,"/**
 * Constructs a file status for a local file.
 * @param f local file
 * @param defaultBlockSize default block size to use if not specified
 * @param fs filesystem reference
 * @throws IOException if an I/O error occurs
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,org.apache.hadoop.fs.FileStatus:<init>(),107,107,"/**
 * Default constructor initializing FileStatus with default values.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,org.apache.hadoop.fs.Path)",110,115,"/**
 * Constructs a FileStatus object.
 * @param length file size in bytes
 * @param isdir true if the path is a directory
 * @param block_replication replication factor of blocks
 * @param blocksize block size in bytes
 * @param modification_time last modification time in milliseconds
 * @param path file or directory path
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPFile,org.apache.hadoop.fs.Path)",557,572,"/**
* Creates a FileStatus object from an FTP file.
* @param ftpFile the FTP file to process
* @param parentPath the parent directory path
* @return FileStatus representing the FTP file
*/","* Convert the file information in FTPFile to a {@link FileStatus} object. *
   * 
   * @param ftpFile
   * @param parentPath
   * @return FileStatus",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartB,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartB(),1128,1154,"/**
* Handles state transitions and data processing in BZip2 decompression.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,changeStateToProcessABlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:changeStateToProcessABlock(),355,362,"/**
* Handles function masking logic.
* Skips result processing if skipResult is true, otherwise sets currentState to EOF.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,init,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:init(),492,509,"/**
* Validates and processes the initial bytes of a BZip2 stream.
* Throws IOException if the stream is not properly formatted.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartB,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartB(),1169,1181,"/**
* Updates state based on character comparison and count.
* Throws IOException if an I/O error occurs.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,close,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:close(),302,308,"/**
 * Calls superclass method and ensures output cleanup.
 * @throws IOException if an I/O error occurs during execution
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,write,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(int),288,293,"/**
* Calls m1 if needsReset is true, then invokes output's m2.
* @param b integer parameter for output's m2 method
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,write,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(byte[],int,int)",295,300,"/**
 * Writes bytes to output stream.
 * @param b byte array containing data to write
 * @param off offset of the first byte to write
 * @param len number of bytes to write
 * @throws IOException if an I/O error occurs
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verify,"org.apache.hadoop.security.KDiag:verify(java.io.File,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",990,1003,"/**
* Masks credentials using a token file and configuration.
* @param tokenFile file containing the token
* @param conf configuration settings
* @param category error category
* @param message error message
* @return true if successful, false otherwise
*/","* Verify that tokenFile contains valid Credentials.
   *
   * If not, an exception is raised, or, if {@link #nofail} is set,
   * an error will be logged and the method return false.
   *",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,printTokenFile,"org.apache.hadoop.security.token.DtFileOperations:printTokenFile(java.io.File,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration,java.io.PrintStream)",123,129,"/**
 * Masks credentials from a file and prints the result.
 * @param tokenFile file containing credentials
 * @param alias text alias for masking
 * @param conf configuration settings
 * @param out output stream for results
 * @throws IOException if an I/O error occurs
 */","Print out a Credentials file from the local filesystem.
   *  @param tokenFile a local File object.
   *  @param alias print only tokens matching alias (null matches all).
   *  @param conf Configuration object passed along.
   *  @param out print to this stream.
   *  @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfo,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),600,617,"/**
* Retrieves delegation token information.
* @param ident token identifier
* @return DelegationTokenInformation object or null if not found
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,syncLocalCacheWithZk,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:syncLocalCacheWithZk(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),625,637,"/**
* Updates or removes a token based on retrieval status.
* @param ident identifier for the token to process
*/","* This method synchronizes the state of a delegation token information in
   * local cache with its actual value in Zookeeper.
   *
   * @param ident Identifier of the token",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredToken,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),785,789,"/**
 * Calls overloaded method with default flag.
 * @param ident token identifier
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageToStream,"org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream,org.apache.hadoop.security.Credentials$SerializedFormat)",306,319,"/**
 * Writes data to output stream based on format.
 * @param os DataOutputStream for writing
 * @param format SerializedFormat indicating the write method
 * @throws IOException if an I/O error occurs
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>(java.lang.String),118,134,"/**
* Initializes a MetricsSystem with a given prefix.
* @param prefix the metric system's prefix or null for default initialization
*/","* Construct the metrics system
   * @param prefix  for the system",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,startMetricsMBeans,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:startMetricsMBeans(),334,339,"/**
* Iterates over MetricsSourceAdapters and calls m1 on each.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,start,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:start(),100,102,"/**
 * Calls m1() if startMBeans is true.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getInstance,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getInstance(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)",869,881,"/**
* Retrieves or creates a MetricsProxy instance.
* @param namespace unique identifier for metrics
* @param numLevels number of levels in the metric hierarchy
* @param drs DecayRpcScheduler for handling RPC scheduling
* @return existing or newly created MetricsProxy instance
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getInstance,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getInstance(java.lang.String),410,418,"/**
* Retrieves or creates a MetricsProxy instance for the given namespace.
* @param namespace unique identifier for metrics
* @return MetricsProxy object associated with the namespace
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,call,"org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)",546,642,"/**
 * Handles RPC requests for Writable protocol.
 * @param server RPC server instance
 * @param protocolName name of the protocol
 * @param rpcRequest request object
 * @param receivedTime time when request was received
 * @return response object or throws exception if error occurs
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,processCall,"org.apache.hadoop.ipc.ProtobufRpcEngine$Server:processCall(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)",463,504,"/**
 * Handles RPC method invocation.
 * @param server RPC server instance
 * @param connectionProtocolName protocol name for the connection
 * @param request buffer containing request data
 * @param methodName name of the method to invoke
 * @param protocolImpl implementation of the protocol
 * @return RpcWritable result or null if callback is set
 * @throws Exception if method invocation fails
 */","* This implementation is same as
     * ProtobufRpcEngine2.Server.ProtobufInvoker#call(..)
     * except this implementation uses non-shaded protobuf classes from legacy
     * protobuf version (default 2.5.0).",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)",599,641,"/**
 * Handles RPC method invocation.
 * @param server the RPC server instance
 * @param connectionProtocolName name of the connection protocol
 * @param request the request buffer containing method parameters
 * @param methodName name of the method to invoke
 * @param protocolImpl implementation of the protocol
 * @return RpcWritable result of the method call or null if callback is set
 * @throws Exception if an error occurs during method invocation
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,<init>,org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:<init>(org.apache.hadoop.metrics2.lib.MutableRollingAverages),219,221,"/**
 * Initializes a RatesRoller with a parent mutable rolling averages.
 * @param parent the parent MutableRollingAverages instance
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)",296,306,"/**
* Registers a metrics sink with configuration.
* @param name sink identifier
* @param desc description of the sink
* @param sink MetricsSink instance to register
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,newSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.impl.MetricsConfig)",534,537,"/**
* Creates a MetricsSinkAdapter instance.
* @param name metric name
* @param desc metric description
* @param conf configuration object
* @return MetricsSinkAdapter object
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,add,"org.apache.hadoop.metrics2.lib.MutableRates:add(java.lang.String,long)",76,78,"/**
 * Records a metric with a given name and elapsed time.
 * @param name   the name of the metric
 * @param elapsed the elapsed time in milliseconds
 */","* Add a rate sample for a rate metric
   * @param name of the rate metric
   * @param elapsed time",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addPersistedDelegationToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)",404,433,"/**
* Adds a persisted delegation token.
* @param identifier unique token identifier
* @param renewDate token's renewal date
* @throws IOException if adding fails or token is already active
*/","* This method is intended to be used for recovering persisted delegation
   * tokens. Tokens that have an unknown <code>DelegationKey</code> are
   * marked as expired and automatically cleaned up.
   * This method must be called before this secret manager is activated (before
   * startThreads() is called)
   * @param identifier identifier read from persistent storage
   * @param renewDate token renew time
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,syncTokenOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:syncTokenOwnerStats(),952,957,"/**
 * Updates stats and processes tokens.
 */","* This method syncs token information from currentTokens to tokenOwnerStats.
   * It is used when the currentTokens is initialized or refreshed. This is
   * called from a single thread thus no synchronization is needed.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredToken(),762,780,"/**
* Removes expired delegation tokens.
* @throws IOException if an I/O error occurs
*/",Remove expired delegation tokens from cache,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,<init>,"org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",226,233,"/**
* Constructs an ActiveStandbyElector with TLS enabled.
* @param zookeeperHostPorts ZooKeeper host:port list
* @param zookeeperSessionTimeout session timeout in milliseconds
* @param parentZnodeName parent ZNode name
* @param acl Access control lists for ZNodes
* @param authInfo authentication information for ZooKeeper
* @param app callback for election events
* @param maxRetryNum maximum number of retries on connection loss
* @param truststoreKeystore TLS truststore and keystore configuration
* @throws IOException if an I/O error occurs
* @throws HadoopIllegalArgumentException if invalid arguments are provided
* @throws KeeperException if a ZooKeeper operation fails
*/","* Create a new ActiveStandbyElector object <br>
   * The elector is created by providing to it the Zookeeper configuration, the
   * parent znode under which to create the znode and a reference to the
   * callback interface. <br>
   * The parent znode name must be the same for all service instances and
   * different across services. <br>
   * After the leader has been lost, a new leader will be elected after the
   * session timeout expires. Hence, the app must set this parameter based on
   * its needs for failure response time. The session timeout must be greater
   * than the Zookeeper disconnect timeout and is recommended to be 3X that
   * value to enable Zookeeper to retry transient disconnections. Setting a very
   * short session timeout may result in frequent transitions between active and
   * standby states during issues like network outages/GS pauses.
   * 
   * @param zookeeperHostPorts
   *          ZooKeeper hostPort for all ZooKeeper servers
   * @param zookeeperSessionTimeout
   *          ZooKeeper session timeout
   * @param parentZnodeName
   *          znode under which to create the lock
   * @param acl
   *          ZooKeeper ACL's
   * @param authInfo a list of authentication credentials to add to the
   *                 ZK connection
   * @param app
   *          reference to callback interface object
   * @param maxRetryNum maxRetryNum.
   * @param truststoreKeystore truststore keystore, that we will use for ZK if SSL/TLS is enabled
   * @throws IOException raised on errors performing I/O.
   * @throws HadoopIllegalArgumentException
   *         if valid data is not supplied.
   * @throws KeeperException
   *         other zookeeper operation errors.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,joinElection,org.apache.hadoop.ha.ActiveStandbyElector:joinElection(byte[]),315,334,"/**
* Masks data and initiates an election if not already in one.
* @param data byte array to be masked
* @throws HadoopIllegalArgumentException if data is null
*/","* To participate in election, the app will call joinElection. The result will
   * be notified by a callback on either the becomeActive or becomeStandby app
   * interfaces. <br>
   * After this the elector will automatically monitor the leader status and
   * perform re-election if necessary<br>
   * The app could potentially start off in standby mode and ignore the
   * becomeStandby call.
   * 
   * @param data
   *          to be set by the app. non-null data must be set.
   * @throws HadoopIllegalArgumentException
   *           if valid data is not supplied",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reJoinElection,org.apache.hadoop.ha.ActiveStandbyElector:reJoinElection(int),798,823,"/**
* Attempts to re-establish ZK session with a specified sleep time.
* @param sleepTime time in milliseconds to wait before retrying
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String)",200,204,"/**
* Creates an InetSocketAddress with optional configuration.
* @param target hostname or IP address
* @param defaultPort port number to use if not specified in config
* @param configName name of the configuration file
* @return InetSocketAddress object
*/","* Create an InetSocketAddress from the given target string and
   * default port. If the string cannot be parsed correctly, the
   * <code>configName</code> parameter is used as part of the
   * exception message, allowing the user to better diagnose
   * the misconfiguration.
   *
   * @param target a string of either ""host"" or ""host:port""
   * @param defaultPort the default port if <code>target</code> does not
   *                    include a port number
   * @param configName the name of the configuration from which
   *                   <code>target</code> was loaded. This is used in the
   *                   exception message in the case that parsing fails.
   * @return socket addr.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,invoke,"org.apache.hadoop.ipc.WritableRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",232,261,"/**
* Invokes a remote method with tracing and logging.
* @param proxy proxy object for the method invocation
* @param method method to be invoked remotely
* @param args arguments for the method
* @return result of the remote method invocation
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,invoke,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",212,294,"/**
 * Handles RPC method invocation with parameter validation and tracing.
 * @param proxy the proxy object
 * @param method the method being invoked
 * @param args the arguments for the method
 * @return the result of the method call or null for async processing
 * @throws ServiceException if parameters are invalid or an error occurs
 */","* This is the client side invoker of RPC method. It only throws
     * ServiceException, since the invocation proxy expects only
     * ServiceException to be thrown by the method in case protobuf service.
     * 
     * ServiceException has the following causes:
     * <ol>
     * <li>Exceptions encountered on the client side in this method are 
     * set as cause in ServiceException as is.</li>
     * <li>Exceptions from the server are wrapped in RemoteException and are
     * set as cause in ServiceException</li>
     * </ol>
     * 
     * Note that the client calling protobuf RPC methods, must handle
     * ServiceException by getting the cause from the ServiceException. If the
     * cause is RemoteException, then unwrap it to get the exception thrown by
     * the server.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,invoke,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",220,304,"/**
 * Handles RPC method invocation with parameter validation and tracing.
 * @param proxy the proxy object
 * @param method the method to invoke
 * @param args the arguments for the method
 * @return the result of the method call or null if async
 * @throws ServiceException if there's an issue during processing
 */","* This is the client side invoker of RPC method. It only throws
     * ServiceException, since the invocation proxy expects only
     * ServiceException to be thrown by the method in case protobuf service.
     *
     * ServiceException has the following causes:
     * <ol>
     * <li>Exceptions encountered on the client side in this method are
     * set as cause in ServiceException as is.</li>
     * <li>Exceptions from the server are wrapped in RemoteException and are
     * set as cause in ServiceException</li>
     * </ol>
     *
     * Note that the client calling protobuf RPC methods, must handle
     * ServiceException by getting the cause from the ServiceException. If the
     * cause is RemoteException, then unwrap it to get the exception thrown by
     * the server.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,parseExpression,org.apache.hadoop.fs.shell.find.Find:parseExpression(java.util.Deque),272,332,"/**
* Parses and constructs an expression from a deque of string arguments.
* @param args deque containing expression tokens
* @return constructed Expression object
* @throws IOException if unexpected argument is encountered
*/","* Parse a list of arguments to to extract the {@link Expression} elements.
   * The input Deque will be modified to remove the used elements.
   * 
   * @param args arguments to be parsed
   * @return list of {@link Expression} elements applicable to this command
   * @throws IOException if list can not be parsed",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printUsage,org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream),193,195,"/**
 * Calls helper function with specific parameters.
 * @param out PrintStream to output data
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printUsage,"org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream,java.lang.String)",198,200,"/**
 * Masks and prints command.
 * @param out output stream
 * @param cmd command to mask and print
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printHelp,org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream),203,205,"/**
 * Calls m1 with specified PrintStream and default parameters.
 * @param out output stream to write to
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printHelp,"org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream,java.lang.String)",208,210,"/**
 * Masks and processes command.
 * @param out output stream for results
 * @param cmd command string to process
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,get,org.apache.hadoop.io.WritableComparator:get(java.lang.Class),55,57,"/**
 * Returns a comparator for the specified writable class.
 * @param c class of WritableComparable
 * @return WritableComparator instance
 */","* For backwards compatibility.
   *
   * @param c WritableComparable Type.
   * @return WritableComparator.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ByteWritable.java,<init>,org.apache.hadoop.io.ByteWritable$Comparator:<init>(),88,90,"/**
* Constructs a comparator for ByteWritable objects.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IntWritable.java,<init>,org.apache.hadoop.io.IntWritable$Comparator:<init>(),90,92,"/**
 * Constructs a comparator for IntWritable objects.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,org.apache.hadoop.io.WritableComparator:<init>(),124,126,"/**
 * Constructs a comparator with no natural ordering.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text$Comparator:<init>(),429,431,"/**
* Constructs a comparator for Text objects.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/NullWritable.java,<init>,org.apache.hadoop.io.NullWritable$Comparator:<init>(),62,64,"/**
 * Constructs a comparator for NullWritable class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,<init>,org.apache.hadoop.io.LongWritable$Comparator:<init>(),90,92,"/**
 * Constructs a comparator for LongWritable.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DoubleWritable.java,<init>,org.apache.hadoop.io.DoubleWritable$Comparator:<init>(),88,90,"/**
 * Constructs a comparator for DoubleWritable objects.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,<init>,org.apache.hadoop.io.MD5Hash$Comparator:<init>(),249,251,"/**
* Initializes a comparator for MD5Hash objects.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ShortWritable.java,<init>,org.apache.hadoop.io.ShortWritable$Comparator:<init>(),98,100,"/**
 * Initializes a comparator for ShortWritable class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FloatWritable.java,<init>,org.apache.hadoop.io.FloatWritable$Comparator:<init>(),85,87,"/**
 * Constructs a comparator for FloatWritable.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,<init>,org.apache.hadoop.io.BytesWritable$Comparator:<init>(),224,226,"/**
 * Constructs a comparator for BytesWritable.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,<init>,org.apache.hadoop.io.BooleanWritable$Comparator:<init>(),111,113,"/**
 * Constructs a comparator for BooleanWritable class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8$Comparator:<init>(),212,214,"/**
 * Constructs a comparator using UTF-8 encoding.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,authenticate,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",380,410,"/**
* Authenticates user using delegation token or fallback method.
* @param request HTTP request object
* @param response HTTP response object
* @return AuthenticationToken if successful, null otherwise
*/","* Authenticates a request looking for the <code>delegation</code>
   * query-string parameter and verifying it is a valid token. If there is not
   * <code>delegation</code> query-string parameter, it delegates the
   * authentication to the {@link KerberosAuthenticationHandler} unless it is
   * disabled.
   *
   * @param request the HTTP client request.
   * @param response the HTTP client response.
   * @return the authentication token for the authenticated request.
   * @throws IOException thrown if an IO error occurred.
   * @throws AuthenticationException thrown if the authentication failed.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,<init>,"org.apache.hadoop.ha.NodeFencer:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)",78,81,"/**
 * Initializes node fencer with configuration and specification.
 * @param conf Configuration object containing settings
 * @param spec Specification string for fencing rules
 * @throws BadFencingConfigurationException if configuration is invalid
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleDeprecation,"org.apache.hadoop.conf.Configuration:handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String)",724,762,"/**
 * Masks a property name based on deprecation context.
 * @param deprecations context for deprecated keys
 * @param name original property name
 * @return array of masked or new property names
 */","* Checks for the presence of the property <code>name</code> in the
   * deprecation map. Returns the first of the list of new keys if present
   * in the deprecation map or the <code>name</code> itself. If the property
   * is not presently set but the property map contains an entry for the
   * deprecated key, the value of the deprecated key is set as the value for
   * the provided property name.
   *
   * Also updates properties and overlays with deprecated keys, if the new
   * key does not already exist.
   *
   * @param deprecations deprecation context
   * @param name the property name
   * @return the first property in the list of properties mapping
   *         the <code>name</code> or the <code>name</code> itself.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,setConfAsEnvVars,org.apache.hadoop.ha.ShellCommandFencer:setConfAsEnvVars(java.util.Map),207,211,"/**
 * Masks environment variables by replacing dots with underscores.
 * @param env map containing environment variables to be masked
 */","* Set the environment of the subprocess to be the Configuration,
   * with '.'s replaced by '_'s.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,setHeaders,org.apache.hadoop.http.HttpServer2:setHeaders(org.apache.hadoop.conf.Configuration),1958,1970,"/**
* Configures frame options for security headers.
* @param conf Configuration object containing header settings
* @return Map of security headers with frame options configured
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,getFilterParameters,"org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getFilterParameters(org.apache.hadoop.conf.Configuration,java.lang.String)",54,65,"/**
* Filters configuration properties by prefix.
* @param conf Configuration object
* @param prefix Property key prefix to filter
* @return Map of filtered properties
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processArguments,org.apache.hadoop.fs.shell.SetReplication:processArguments(java.util.LinkedList),74,79,"/**
* Overrides base method to process path data.
* @param args list of PathData objects
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.DelegateToFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",117,122,"/**
 * Retrieves block locations for a file range.
 * @param f file path
 * @param start starting offset
 * @param len length of the range
 * @return array of BlockLocation objects
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",1512,1535,"/**
 * Retrieves block locations for a file.
 * @param fs FileStatus object representing the file
 * @param start starting byte position
 * @param len length of data to retrieve
 * @return array of BlockLocation objects
 * @throws FileNotFoundException if path is not a file
 * @throws IOException on I/O errors
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,renewToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)",190,196,"/**
* Renews a delegation token.
* @param token the token to be renewed
* @param renewer the entity that is attempting to renew the token
* @return the new expiration time of the token in milliseconds
* @throws IOException if an I/O error occurs while renewing the token
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",198,206,"/**
* Cancels a delegation token.
* @param token the token to be cancelled
* @param canceler the user attempting to cancel the token
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",154,164,"/**
* Overrides to process and validate token.
* @param token input token object
* @param canceller cancellation identifier
* @return processed TokenIdent or null if invalid
* @throws IOException on processing error
*/","* Cancels a token by removing it from the SQL database. This will
   * call the corresponding method in {@link AbstractDelegationTokenSecretManager}
   * to perform validation and remove the token from the cache.
   * @return Identifier of the canceled token",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,getGroups,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String),52,58,"/**
* Retrieves and updates user's group membership.
* @param user the username
* @return list of groups the user belongs to
* @throws IOException if an I/O error occurs
*/","* Get unix groups (parent) and netgroups for given user
   *
   * @param user get groups and netgroups for this user
   * @return groups and netgroups for user",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTar,"org.apache.hadoop.fs.FileUtil:unTar(java.io.File,java.io.File)",1015,1033,"/**
* Unpacks a tar file to the specified directory.
* @param inFile input tar file
* @param untarDir destination directory for unpacking
* @throws IOException if directory creation fails or unpacking errors occur
*/","* Given a Tar File as input it will untar the file in a the untar directory
   * passed as the second parameter
   *
   * This utility will untar "".tar"" files and "".tar.gz"",""tgz"" files.
   *
   * @param inFile The tar file as input.
   * @param untarDir The untar directory where to untar the tar file.
   * @throws IOException an exception occurred.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,chmod,"org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String)",1289,1292,"/**
 * Calls m1 with default value for overwrite.
 * @param filename name of the file
 * @param perm file permissions
 * @return result of m1 method call
 * @throws IOException if an I/O error occurs
 * @throws InterruptedException if the operation is interrupted
 */","* Change the permissions on a filename.
   * @param filename the name of the file to change
   * @param perm the permission string
   * @return the exit code from the command
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException command interrupted.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setReadable,"org.apache.hadoop.fs.FileUtil:setReadable(java.io.File,boolean)",1347,1359,"/**
* Sets file readability on Windows; otherwise, delegates to m1.
* @param f the File object to modify
* @param readable true to make file readable, false to make it unreadable
* @return true if operation is successful, false otherwise
*/","* Platform independent implementation for {@link File#setReadable(boolean)}
   * File#setReadable does not work as expected on Windows.
   * @param f input file
   * @param readable readable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setWritable,"org.apache.hadoop.fs.FileUtil:setWritable(java.io.File,boolean)",1368,1380,"/**
* Sets file permissions on Windows.
* @param f the target file
* @param writable indicates if file should be writable
* @return true if operation succeeds, false otherwise
*/","* Platform independent implementation for {@link File#setWritable(boolean)}
   * File#setWritable does not work as expected on Windows.
   * @param f input file
   * @param writable writable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setExecutable,"org.apache.hadoop.fs.FileUtil:setExecutable(java.io.File,boolean)",1392,1404,"/**
* Sets file executable permission.
* @param f File object to modify
* @param executable boolean indicating if the file should be executable
* @return true if operation succeeds, false otherwise
*/","* Platform independent implementation for {@link File#setExecutable(boolean)}
   * File#setExecutable does not work as expected on Windows.
   * Note: revoking execute permission on folders does not have the same
   * behavior on Windows as on Unix platforms. Creating, deleting or renaming
   * a file within that folder will still succeed on Windows.
   * @param f input file
   * @param executable executable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,refreshIfNeeded,org.apache.hadoop.util.SysInfoWindows:refreshIfNeeded(),94,141,"/**
 * Refreshes system information periodically.
 * Updates CPU, memory, and network stats.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfo,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfo(),983,995,"/**
 * Executes native operations conditionally.
 * Calls m4() if m1() is false and NativeIO.m2() is true, logs exception on failure.
 * Always calls m5() if m1() is false.
 */","* Load file permission information (UNIX symbol rwxrwxrwx, sticky bit info).
     *
     * To improve peformance, give priority to native stat() call. First try get
     * permission information by using native JNI call then fall back to use non
     * native (ProcessBuilder) call in case native lib is not loaded or native
     * call is not successful",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setOwner,"org.apache.hadoop.fs.RawLocalFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1099,1103,"/**
* Modifies file permissions for a user and group.
* @param p path to the file
* @param username user name
* @param groupname group name
* @throws IOException if an I/O error occurs
*/",* Use the command chown to set owner.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setPermission,"org.apache.hadoop.fs.FileUtil:setPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1470,1508,"/**
* Applies file permissions with masking.
* @param f the target File object
* @param permission FsPermission to apply
*/","* Set permissions to the required value. Uses the java primitives instead
   * of forking if group == other.
   * @param f the file to change
   * @param permission the new permissions
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,cacheGroupsAdd,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List),75,88,"/**
* Processes groups for masking logic.
* @param groups list of group names to process
*/","* Add a group to cache, only netgroups are cached
   *
   * @param groups list of group names to add to cache",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpTokens,org.apache.hadoop.security.KDiag:dumpTokens(org.apache.hadoop.security.UserGroupInformation),811,819,"/**
* Logs user group information and tokens.
* @param ugi UserGroupInformation object containing user details
*/","* Dump all tokens of a UGI.
   * @param ugi UGI to examine",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logUserInfo,"org.apache.hadoop.security.UserGroupInformation:logUserInfo(org.slf4j.Logger,java.lang.String,org.apache.hadoop.security.UserGroupInformation)",1980,1991,"/**
* Logs user group information and tokens if logging is enabled.
* @param log the Logger instance to use for logging
* @param caption a descriptive caption for the log entry
* @param ugi UserGroupInformation object containing user details and tokens
*/","* Log current UGI and token information into specified log.
   * @param ugi - UGI
   * @param log log.
   * @param caption caption.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,containsKmsDt,org.apache.hadoop.crypto.key.kms.KMSClientProvider:containsKmsDt(org.apache.hadoop.security.UserGroupInformation),1155,1165,"/**
* Checks for KMS delegation token in user's credentials.
* @param ugi UserGroupInformation object
* @return true if token found, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDir,"org.apache.hadoop.util.DiskChecker:checkDir(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",113,117,"/**
* Masks directory permissions to expected.
* @param localFS Local file system instance
* @param dir Directory path to mask
* @param expected Expected file permissions
* @throws DiskErrorException if disk error occurs
* @throws IOException if I/O error occurs
*/","* Create the local directory if necessary, check permissions and also ensure
   * it can be read from and written into.
   *
   * @param localFS local filesystem
   * @param dir directory
   * @param expected permission
   * @throws DiskErrorException disk problem.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirWithDiskIo,"org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",131,136,"/**
* Masks directory permissions.
* @param localFS Local file system instance
* @param dir Directory path to mask
* @param expected Expected file permissions
*/","* Create the local directory if necessary, also ensure permissions
   * allow it to be read from and written into. Perform some diskIO
   * to ensure that the disk is usable for writes. 
   *
   * @param localFS local filesystem
   * @param dir directory
   * @param expected permission
   * @throws DiskErrorException disk problem.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[]),1823,1826,"/**
* Checks file statuses with default filter.
* @param files array of file paths to check
* @return array of FileStatus objects
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if a file is not found
* @throws IOException for other I/O errors
*/","* See {@link #listStatus(Path[], PathFilter)}
     *
     * @param files files.
     * @throws AccessControlException If access is denied.
     * @throws FileNotFoundException If <code>files</code> does not exist.
     * @throws IOException If an I/O error occurred.
     * @return file status array.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path),1481,1496,"/**
* Resolves the full path of a given file system path.
* @param path input file system path
* @return resolved full path or original path if not deeper
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ViewFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",447,453,"/**
 * Opens a file for writing with specified buffer size and progress tracker.
 * @param f file path to open
 * @param bufferSize size of the buffer used for writing
 * @param progress object to track write progress
 * @return FSDataOutputStream for writing to the file
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.viewfs.ViewFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",455,468,"/**
 * Creates a new file with specified permissions and options.
 * @param f file path
 * @param permission file permissions
 * @param flags creation flags
 * @param bufferSize buffer size for data transfer
 * @param replication number of replicas
 * @param blockSize block size
 * @param progress progress monitor
 * @return FSDataOutputStream for writing to the file
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ViewFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",470,483,"/**
 * Creates a file with specified permissions and options.
 * @param f file path
 * @param permission file permissions
 * @param overwrite flag to overwrite existing file
 * @param bufferSize buffer size for file operations
 * @param replication replication factor for the file
 * @param blockSize block size for the file
 * @param progress progress monitor
 * @return FSDataOutputStream for writing to the file
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",486,496,"/**
* Deletes a file or directory.
* @param f path to the file or directory
* @param recursive true if deletion should be recursive for directories
* @return true if delete is successful, false otherwise
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws IOException on other I/O errors
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),514,521,"/**
* Computes checksum for a file.
* @param f path to the file
* @return FileChecksum object
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",523,530,"/**
 * Computes checksum for a file.
 * @param f path to the file
 * @param length length of the file
 * @return FileChecksum object
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if file does not exist
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listLocatedStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",630,655,"/**
* Retrieves file statuses from a given path.
* @param f target path
* @param filter path filter
* @return iterator of LocatedFileStatus objects
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path),670,675,"/**
* Checks if path exists in filesystem.
* @param dir directory path to check
* @return true if path exists, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",677,683,"/**
* Sets directory permissions.
* @param dir target directory path
* @param permission new file system permissions
* @return true if operation successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ViewFileSystem:open(org.apache.hadoop.fs.Path,int)",685,691,"/**
* Opens a file input stream for reading.
* @param f path to the file
* @param bufferSize size of the buffer
* @return FSDataInputStream for reading the file
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if the file does not exist
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFileSystem:truncate(org.apache.hadoop.fs.Path,long)",800,806,"/**
* Truncates a file to a specified length.
* @param f the file path to truncate
* @param newLength the new length of the file
* @return true if truncation is successful, false otherwise
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",808,815,"/**
* Resolves file path and sets ownership.
* @param f file path
* @param username user name for ownership
* @param groupname group name for ownership
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws IOException for other I/O errors
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",817,823,"/**
* Sets file permissions.
* @param f file path
* @param permission new file permissions
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws IOException for other I/O errors
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",825,831,"/**
* Sets file replication factor.
* @param f file path
* @param replication desired replication factor
* @return success status
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws IOException for other I/O errors
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",833,839,"/**
 * Updates file timestamps.
 * @param f file path
 * @param mtime modification time in milliseconds
 * @param atime access time in milliseconds
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if file does not exist
 * @throws IOException for other I/O errors
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",841,847,"/**
 * Applies ACL entries to a file system path.
 * @param path the file system path
 * @param aclSpec list of ACL entries to apply
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",849,855,"/**
 * Applies ACL entries to a file system path.
 * @param path the file system path
 * @param aclSpec list of ACL entries to apply
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),857,863,"/**
* Resolves and processes a file system path.
* @param path the file system path to process
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAcl(org.apache.hadoop.fs.Path),865,871,"/**
* Resolves and processes a file system path.
* @param path the file path to process
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",873,878,"/**
* Applies ACL to a file system path.
* @param path the file system path
* @param aclSpec list of ACL entries to apply
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getAclStatus(org.apache.hadoop.fs.Path),880,885,"/**
* Resolves ACL status for a given path.
* @param path file system path to check
* @return AclStatus object representing the ACLs
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",887,893,"/**
 * Sets an extended attribute on a file.
 * @param path file path
 * @param name attribute name
 * @param value attribute value
 * @param flag set flags
 * @throws IOException if operation fails
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",895,900,"/**
* Reads file content.
* @param path file path
* @param name file name
* @return byte array of file content
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path),902,907,"/**
 * Retrieves a map of file names to their contents from the given path.
 * @param path the directory path to read files from
 * @return a map with file names as keys and byte arrays as values
 * @throws IOException if an I/O error occurs during file reading
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",909,915,"/**
* Resolves and retrieves files by name from a given path.
* @param path file system path
* @param names list of file names to retrieve
* @return map of file names to their byte content
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem:listXAttrs(org.apache.hadoop.fs.Path),917,922,"/**
* Resolves and lists files in a given path.
* @param path file system path to resolve
* @return list of file names in the resolved path
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",924,929,"/**
* Resolves and renames a file in the file system.
* @param path original file path
* @param name new file name
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),990,1002,"/**
* Resolves and retrieves replication factor for a file.
* @param f file path
* @return short replication factor
* @throws NotInMountpointException if file not found in mountpoint
* @throws RuntimeException for IO errors during resolution
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ViewFileSystem:getContentSummary(org.apache.hadoop.fs.Path),1015,1020,"/**
 * Retrieves content summary for a file path.
 * @param f file path to summarize
 * @return ContentSummary object
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ViewFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),1022,1027,"/**
* Resolves file path and retrieves quota usage.
* @param f file path to resolve
* @return QuotaUsage object for the resolved path
* @throws IOException if an I/O error occurs during resolution
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1072,1078,"/**
* Resolves and creates a snapshot of a file system path.
* @param path the original file system path
* @param snapshotName name of the snapshot to create
* @return Path object representing the snapshot
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1080,1087,"/**
 * Renames a snapshot in the file system.
 * @param path full path to the snapshot location
 * @param snapshotOldName current name of the snapshot
 * @param snapshotNewName new name for the snapshot
 * @throws IOException if an I/O error occurs during the operation
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1089,1095,"/**
 * Resolves and applies a snapshot to a file system path.
 * @param path the file system path
 * @param snapshotName name of the snapshot to apply
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1097,1102,"/**
* Resolves and processes a file path.
* @param src source file path
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1104,1109,"/**
* Applies a policy to a file path.
* @param src source file path
* @param policyName name of the policy to apply
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),1111,1116,"/**
* Resolves and processes a file path.
* @param src source file path
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),1118,1123,"/**
 * Retrieves block storage policy for a given path.
 * @param src source file path
 * @return BlockStoragePolicySpi object
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus(org.apache.hadoop.fs.Path),1304,1312,"/**
* Retrieves file system status for a given path.
* @param p file path, defaults to root if null
* @return FsStatus object representing the file system status
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getUsed,org.apache.hadoop.fs.viewfs.ViewFileSystem:getUsed(),1321,1330,"/**
* Resolves and retrieves used space from file system.
* @throws IOException if an I/O error occurs
* @return used space in bytes
*/","* Return the total size of all files under ""/"", if {@link
   * Constants#CONFIG_VIEWFS_LINK_MERGE_SLASH} is supported and is a valid
   * mount point. Else, throw NotInMountpointException.
   *
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),1332,1341,"/**
* Resolves path in file system.
* @param path input path to resolve
* @return resolved path in target file system
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.HarFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",468,481,"/**
* Retrieves block locations for a file segment.
* @param file file status object
* @param start starting offset of the segment
* @param len length of the segment
* @return array of BlockLocation objects
*/","* Get block locations from the underlying fs and fix their
   * offsets and lengths.
   * @param file the input file status to get block locations
   * @param start the start of the desired range in the contained file
   * @param len the length of the desired range
   * @return block locations for this segment of file
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,open,"org.apache.hadoop.fs.HarFileSystem:open(org.apache.hadoop.fs.Path,int)",679,690,"/**
 * Opens an input stream for reading from a specified path.
 * @param f the file path to read from
 * @param bufferSize size of the buffer used for reading
 * @return FSDataInputStream for the specified file
 * @throws IOException if file is not found or other I/O errors occur
 */","* Returns a har input stream which fakes end of 
   * file. It reads the index files to get the part 
   * file name and the size and start of the file.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path)",148,151,"/**
* Constructs a ChecksumFSInputChecker.
* @param fs the ChecksumFs instance
* @param file the Path to the file
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symlink cannot be resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,open,"org.apache.hadoop.fs.ChecksumFs:open(org.apache.hadoop.fs.Path,int)",334,339,"/**
 * Returns an input stream for reading data from a file.
 * @param f the file path
 * @param bufferSize size of the buffer to use
 * @return FSDataInputStream with checksum enabled
 * @throws IOException if an I/O error occurs
 * @throws UnresolvedLinkException if a symbolic link could not be resolved
 */","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,createInternal,"org.apache.hadoop.fs.ChecksumFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",418,428,"/**
* Creates a file output stream with specified options.
* @param f file path
* @param createFlag flags for file creation
* @param absolutePermission permissions to set on the file
* @param bufferSize size of buffer used in I/O operations
* @param replication number of block replicas
* @param blockSize size of blocks in the file system
* @param progress callback for reporting progress
* @param checksumOpt options for checksum computation
* @param createParent flag to create parent directories if necessary
* @return FSDataOutputStream for writing data
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,processArguments,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:processArguments(java.util.LinkedList),81,94,"/**
* Processes path data and executes additional steps.
* @param args linked list of PathData objects
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,open,"org.apache.hadoop.fs.ChecksumFileSystem:open(org.apache.hadoop.fs.Path,int)",566,578,"/**
* Opens an input stream for the given file path with specified buffer size.
* @param f file path to open
* @param bufferSize size of the buffer
* @return FSDataInputStream for reading the file
* @throws IOException if an I/O error occurs
*/","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * @throws IOException if an I/O error occurs.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable)",704,734,"/**
* Creates a file output stream with specified permissions and options.
* @param f file path
* @param permission file permissions
* @param overwrite whether to overwrite existing file
* @param createParent whether to create parent directories if needed
* @param bufferSize buffer size for data transfer
* @param replication replication factor for the file
* @param blockSize block size for the file
* @param progress progress monitor
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,isValidName,org.apache.hadoop.fs.viewfs.ChRootedFs:isValidName(java.lang.String),97,100,"/**
* Checks if source exists.
* @param src file path as string
* @return true if source exists, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",173,182,"/**
* Creates a file output stream with specified options.
* @param f file path
* @param flag creation flags
* @param absolutePermission file permissions
* @param bufferSize buffer size for data transfer
* @param replication number of replicas
* @param blockSize block size for the file
* @param progress progress tracking object
* @param checksumOpt checksum option
* @param createParent flag to create parent directories
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symlink cannot be resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,delete,"org.apache.hadoop.fs.viewfs.ChRootedFs:delete(org.apache.hadoop.fs.Path,boolean)",184,188,"/**
 * Checks if a file exists in the filesystem.
 * @param f path to the file
 * @param recursive whether to search recursively in directories
 * @return true if the file exists, false otherwise
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ChRootedFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",190,194,"/**
* Retrieves block locations for a file path.
* @param f file path
* @param start starting offset
* @param len length of data
* @return array of BlockLocation objects
* @throws IOException if I/O error occurs
* @throws UnresolvedLinkException if link cannot be resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileChecksum(org.apache.hadoop.fs.Path),196,200,"/**
 * Computes checksum for a file.
 * @param f file path
 * @return FileChecksum object
 * @throws IOException if I/O error occurs
 * @throws UnresolvedLinkException if symbolic link cannot be resolved
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileStatus(org.apache.hadoop.fs.Path),202,206,"/**
* Retrieves file status using transformed path.
* @param f original file path
* @return FileStatus object
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link cannot be resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileLinkStatus(org.apache.hadoop.fs.Path),213,217,"/**
* Retrieves file status after processing path.
* @param f file path to process
* @return FileStatus object
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link cannot be resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFs:getServerDefaults(org.apache.hadoop.fs.Path),230,233,"/**
 * Retrieves server defaults for a given file path.
 * @param f file path to query
 * @return FsServerDefaults object
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:listStatus(org.apache.hadoop.fs.Path),240,244,"/**
* Retrieves file status for given path.
* @param f file path
* @return array of FileStatus objects
* @throws IOException if I/O error occurs
* @throws UnresolvedLinkException if symbolic link cannot be resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listStatusIterator,org.apache.hadoop.fs.viewfs.ChRootedFs:listStatusIterator(org.apache.hadoop.fs.Path),246,250,"/**
* Retrieves file statuses from the filesystem.
* @param f path to the file or directory
* @return iterator of FileStatus objects
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symlink cannot be resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:listLocatedStatus(org.apache.hadoop.fs.Path),252,256,"/**
* Retrieves file status iterator for a given path.
* @param f file path
* @return iterator of LocatedFileStatus objects
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symlink cannot be resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,mkdir,"org.apache.hadoop.fs.viewfs.ChRootedFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",258,263,"/**
 * Sets directory permissions and optionally creates parent directories.
 * @param dir target directory path
 * @param permission file system permissions to set
 * @param createParent flag to create parent directories if necessary
 * @throws IOException if an I/O error occurs
 * @throws UnresolvedLinkException if a symbolic link cannot be resolved
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,open,"org.apache.hadoop.fs.viewfs.ChRootedFs:open(org.apache.hadoop.fs.Path,int)",265,269,"/**
* Opens an input stream for reading a file.
* @param f file path
* @param bufferSize size of the buffer
* @return FSDataInputStream for reading the file
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symlink cannot be resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,truncate,"org.apache.hadoop.fs.viewfs.ChRootedFs:truncate(org.apache.hadoop.fs.Path,long)",271,275,"/**
* Resizes a file to a specified length.
* @param f path to the file
* @param newLength new size of the file in bytes
* @return true if resizing is successful, false otherwise
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link cannot be resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",277,283,"/**
 * Moves file from source to destination path.
 * @param src source file path
 * @param dst destination file path
 * @throws IOException if I/O error occurs
 * @throws UnresolvedLinkException if symbolic link cannot be resolved
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",285,292,"/**
* Moves file from source to destination.
* @param src source file path
* @param dst destination file path
* @param overwrite flag to overwrite destination if it exists
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link is unresolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setOwner,"org.apache.hadoop.fs.viewfs.ChRootedFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",294,300,"/**
* Updates file metadata in filesystem.
* @param f path to the file
* @param username owner of the file
* @param groupname group associated with the file
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link cannot be resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setPermission,"org.apache.hadoop.fs.viewfs.ChRootedFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",302,306,"/**
* Sets file permissions.
* @param f file path
* @param permission desired file permissions
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symlink cannot be resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setReplication,"org.apache.hadoop.fs.viewfs.ChRootedFs:setReplication(org.apache.hadoop.fs.Path,short)",308,312,"/**
 * Checks if a file can be replicated.
 * @param f path to the file
 * @param replication desired replication factor
 * @return true if replication is possible, false otherwise
 * @throws IOException if an I/O error occurs
 * @throws UnresolvedLinkException if a symbolic link cannot be resolved
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setTimes,"org.apache.hadoop.fs.viewfs.ChRootedFs:setTimes(org.apache.hadoop.fs.Path,long,long)",314,318,"/**
* Updates file metadata.
* @param f file path
* @param mtime modification time in milliseconds
* @param atime access time in milliseconds
* @throws IOException if I/O error occurs
* @throws UnresolvedLinkException if unresolved link is encountered
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",320,324,"/**
* Applies ACL entries to a file system path.
* @param path the file system path
* @param aclSpec list of access control entries
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",326,330,"/**
 * Modifies ACLs for a file.
 * @param path file path
 * @param aclSpec list of ACL entries to apply
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ChRootedFs:removeDefaultAcl(org.apache.hadoop.fs.Path),332,335,"/**
 * Processes file at given path.
 * @param path file path to process
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ChRootedFs:removeAcl(org.apache.hadoop.fs.Path),337,340,"/**
 * Processes a file path.
 * @param path file path to process
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ChRootedFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",342,345,"/**
* Applies ACL entries to a file.
* @param path file path
* @param aclSpec list of ACL specifications
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getAclStatus(org.apache.hadoop.fs.Path),347,350,"/**
 * Retrieves ACL status for a given path.
 * @param path file system path
 * @return AclStatus object
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",352,356,"/**
 * Sets an extended attribute on a file.
 * @param path the file path
 * @param name the attribute name
 * @param value the attribute value
 * @param flag flags for setting the attribute
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",358,361,"/**
* Reads file content as bytes.
* @param path file path
* @param name file name
* @return byte array of file content
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path),363,366,"/**
 * Reads files from a directory.
 * @param path directory path
 * @return map of file names to their contents
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",368,372,"/**
* Reads files and returns their contents as a map.
* @param path directory path to read from
* @param names list of file names to read
* @return map of file names to byte arrays or throws IOException if an error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFs:listXAttrs(org.apache.hadoop.fs.Path),374,377,"/**
 * Processes a file path to return a list of strings.
 * @param path file path to process
 * @return list of strings resulting from processing
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",379,382,"/**
 * Calls myFs.m2 with processed path and name.
 * @param path file system path
 * @param name file or directory name
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",384,387,"/**
 * Modifies file path by appending a name.
 * @param path original file path
 * @param name name to append
 * @return modified file path
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",389,393,"/**
* Renames a snapshot within a file system.
* @param path the path to the file containing the snapshots
* @param snapshotOldName the current name of the snapshot
* @param snapshotNewName the new name for the snapshot
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",395,399,"/**
 * Calls m2 on myFs with processed directory and snapshot name.
 * @param snapshotDir directory containing snapshots
 * @param snapshotName name of the snapshot
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ChRootedFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",406,410,"/**
* Applies a policy to a file path.
* @param path the file path to apply the policy to
* @param policyName the name of the policy to apply
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),412,416,"/**
 * Processes file at given path.
 * @param src source file path
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ChRootedFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",441,451,"/**
* Creates a symbolic link to a target path.
* @param target the path where the symlink points to
* @param link the path of the symlink to be created
* @param createParent if true, creates parent directories if necessary
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ChRootedFs:getLinkTarget(org.apache.hadoop.fs.Path),453,456,"/**
 * Processes file path through two transformations.
 * @param f input file path
 * @return transformed file path
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",568,636,"/**
* Renames a file or directory from source to destination.
* @param src source path
* @param dst destination path
* @param overwrite flag to allow overwriting the destination if it exists
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link cannot be resolved
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,next,org.apache.hadoop.fs.viewfs.ViewFs$WrappingRemoteIterator:next(),945,952,"/**
* Processes and returns a transformed status object.
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getChrootedPath,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getChrootedPath(org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",657,668,"/**
 * Determines and constructs the final file path based on resolve result and file status.
 * @param res ResolveResult containing target filesystem and resolved path
 * @param status FileStatus object holding metadata about the file
 * @param f Original file path
 * @return Constructed Path with potential modifications
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,renameInternal,"org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",480,497,"/**
* Masks a file by moving it to a destination path.
* @param src source file path
* @param dst destination file path
* @throws IOException if an I/O error occurs
* @throws UnresolvedLinkException if a symbolic link cannot be resolved
*/",* Rename files/dirs.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,renameInternal,"org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",499,523,"/**
 * Moves a file from source to destination with optional overwrite.
 * @param src source file path
 * @param dst destination file path
 * @param overwrite flag to indicate if existing files should be overwritten
 * @throws AccessControlException if access is denied
 * @throws FileAlreadyExistsException if destination file exists and not overwriting
 * @throws FileNotFoundException if source file does not exist
 * @throws ParentNotDirectoryException if parent of destination is not a directory
 * @throws UnresolvedLinkException if symbolic link cannot be resolved
 * @throws IOException for other I/O errors
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,rename,"org.apache.hadoop.fs.FileContext:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",1031,1060,"/**
 * Renames a file from source to destination path.
 * @param src source file path
 * @param dst destination file path
 * @param options optional rename options
 * @throws IOException if renaming fails for various reasons
 */","* Renames Path src to Path dst
   * <ul>
   * <li>Fails if src is a file and dst is a directory.
   * <li>Fails if src is a directory and dst is a file.
   * <li>Fails if the parent of dst does not exist or is a file.
   * </ul>
   * <p>
   * If OVERWRITE option is not passed as an argument, rename fails if the dst
   * already exists.
   * <p>
   * If OVERWRITE option is passed as an argument, rename overwrites the dst if
   * it is a file or an empty directory. Rename fails if dst is a non-empty
   * directory.
   * <p>
   * Note that atomicity of rename is dependent on the file system
   * implementation. Please refer to the file system documentation for details
   * <p>
   * 
   * @param src path to be renamed
   * @param dst new path after rename
   * @param options rename options.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If <code>dst</code> already exists and
   *           <code>options</code> has {@link Options.Rename#OVERWRITE}
   *           option false.
   * @throws FileNotFoundException If <code>src</code> does not exist
   * @throws ParentNotDirectoryException If parent of <code>dst</code> is not a
   *           directory
   * @throws UnsupportedFileSystemException If file system for <code>src</code>
   *           and <code>dst</code> is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameInternal,"org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",245,251,"/**
 * Masks and moves a file from source to destination.
 * @param src source file path
 * @param dst destination file path
 * @throws IOException if an I/O error occurs
 * @throws UnresolvedLinkException if a symbolic link cannot be resolved
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,fileStatusesInIndex,"org.apache.hadoop.fs.HarFileSystem:fileStatusesInIndex(org.apache.hadoop.fs.HarFileSystem$HarStatus,java.util.List)",511,522,"/**
 * Masks file statuses under a given parent.
 * @param parent the parent directory status
 * @param statuses list to store masked file statuses
 * @throws IOException if an I/O error occurs
 */","* Get filestatuses of all the children of a given directory. This just reads
   * through index file and reads line by line to get all statuses for children
   * of a directory. Its a brute force way of getting all such filestatuses
   * 
   * @param parent
   *          the parent path directory
   * @param statuses
   *          the list to add the children filestatuses to",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileStatus,org.apache.hadoop.fs.HarFileSystem:getFileStatus(org.apache.hadoop.fs.Path),640,644,"/**
* Retrieves file status using mask function.
* @param f file path
* @return FileStatus object or null if not found
*/","* return the filestatus of files in har archive.
   * The permission returned are that of the archive
   * index files. The permissions are not persisted 
   * while creating a hadoop archive.
   * @param f the path in har filesystem
   * @return filestatus.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,deprecatedGetFileStatus,org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileStatus(org.apache.hadoop.fs.Path),910,919,"/**
* Retrieves file status.
* @param f file path
* @return FileStatus object if exists, throws FileNotFoundException otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,org.apache.hadoop.fs.LocatedFileStatus:<init>(),40,42,"/**
 * Default constructor for LocatedFileStatus.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:<init>(org.apache.hadoop.fs.viewfs.ChRootedFileSystem,org.apache.hadoop.fs.FileStatus)",464,468,"/**
* Initializes NflyStatus with file system and status.
* @param realFs the underlying file system
* @param realStatus the actual file status
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFsFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",35,38,"/**
 * Initializes file status with new path.
 * @param fs original FileStatus object
 * @param newPath updated path for the file
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",205,250,"/**
 * Retrieves file status from SFTP server.
 * @param client SFTP client instance
 * @param file local path of the file
 * @return FileStatus object or throws IOException if not found
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.AbstractHttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
* Creates a FileStatus object with default values.
* @param path file path to evaluate
* @return FileStatus object initialized with specific parameters
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,doGlob,org.apache.hadoop.fs.Globber:doGlob(),208,396,"/**
 * Masks file status based on glob patterns.
 * @return Array of FileStatus objects or null if no matches found
 * @throws IOException if an I/O error occurs
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,notFoundStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:notFoundStatus(org.apache.hadoop.fs.Path),623,625,"/**
* Creates a masked FileStatus object.
* @param f file path to be used in the status
* @return FileStatus with default values except for the path
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",516,548,"/**
* Retrieves file status from FTP server.
* @param client FTP client instance
* @param file path to the file
* @return FileStatus object or throws FileNotFoundException if not found
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE,boolean)",299,321,"/**
* Initializes a CBZip2InputStream.
* @param in input stream containing BZip2 compressed data
* @param readMode mode of reading blocks (CONTINUOUS or BYBLOCK)
* @param skipDecompression flag to skip decompression process
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read0,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read0(),450,490,"/**
* Processes current state and returns character.
* @return processed character or specific end code
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Print:execute(),193,198,"/**
* Masks tokens in specified files.
* @throws Exception if file operations fail
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",833,843,"/**
 * Processes a token to generate an identifier.
 * @param token input token containing data
 * @param canceller cancellation string for processing
 * @return TokenIdent object representing the processed token
 * @throws IOException if I/O operations fail
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageToStream,org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream),300,304,"/**
 * Writes data using specified format.
 * @param os output stream to write data to
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>(),139,141,"/**
 * Constructs a new MetricsSystemImpl with default configuration.
 */",* Construct the system but not initializing (read config etc.) it.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSource,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)",260,270,"/**
 * Registers a metrics source with configuration.
 * @param name unique source identifier
 * @param desc description of the source
 * @param source MetricsSource object to register
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,long,java.lang.String,java.lang.String,long)",577,597,"/**
* Processes RPC call.
* @param server RPC server instance
* @param connectionProtocolName name of the connection protocol
* @param request request buffer
* @param receiveTime time when request was received
* @param methodName method being called
* @param declaringClassProtoName class prototype name
* @param clientVersion client version
* @return Writable response or null if not processed
* @throws Exception if processing fails
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,<init>,org.apache.hadoop.metrics2.lib.MutableRollingAverages:<init>(java.lang.String),144,155,"/**
 * Initializes a MutableRollingAverages instance.
 * @param metricValueName name of the metric for rolling averages
 */","* Constructor for {@link MutableRollingAverages}.
   * @param metricValueName input metricValueName.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,replaceScheduledTask,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:replaceScheduledTask(int,long,java.util.concurrent.TimeUnit)",160,167,"/**
 * Configures and schedules task execution.
 * @param windows number of intervals to consider
 * @param interval duration between executions
 * @param timeUnit unit of the interval duration
 */",* This method is for testing only to replace the scheduledTask.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)",272,294,"/**
* Registers a metrics sink with a name and description.
* @param name unique identifier for the sink
* @param description details about the sink
* @param sink MetricsSink object to register
* @return registered MetricsSink object
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSinks,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSinks(),492,519,"/**
* Initializes and configures metrics sinks.
* @param none
* @return void
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,recheckElectability,org.apache.hadoop.ha.ZKFailoverController:recheckElectability(),808,860,"/**
* Handles master election logic based on health state.
* @param elector the election manager instance
* @param localTarget the target for election
*/","* Check the current state of the service, and join the election
   * if it should be in the election.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reJoinElectionAfterFailureToBecomeActive,org.apache.hadoop.ha.ActiveStandbyElector:reJoinElectionAfterFailureToBecomeActive(),627,629,"/**
 * Masks functionality with sleep after failure to become active.
 */","* We failed to become active. Re-join the election, but
   * sleep for a few seconds after terminating our existing
   * session, so that other nodes have a chance to become active.
   * The failure to become active is already logged inside
   * becomeActive().",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processWatchEvent,"org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)",635,713,"/**
* Handles ZooKeeper watcher events.
* @param zk ZooKeeper instance
* @param event WatchedEvent received
*/","* interface implementation of Zookeeper watch events (connection and node),
   * proxied by {@link WatcherWithClientRef}.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int)",180,183,"/**
 * Creates an InetSocketAddress with a target and default port.
 * @param target the target address (hostname or IP)
 * @param defaultPort the default port number
 * @return InetSocketAddress object
 */","* Util method to build socket addr from either.
   *   {@literal <host>}
   *   {@literal <host>:<port>}
   *   {@literal <fs>://<host>:<port>/<path>}
   *
   * @param target target.
   * @param defaultPort default port.
   * @return socket addr.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processOptions,org.apache.hadoop.fs.shell.find.Find:processOptions(java.util.LinkedList),166,212,"/**
* Processes command arguments, sets options, and evaluates expressions.
* @param args list of command line arguments
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,authenticate,"org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",154,182,"/**
* Handles authentication based on request headers.
* @param request HTTP request containing authorization info
* @param response HTTP response to send challenges if needed
* @return AuthenticationToken or null if unauthorized
* @throws IOException if an I/O error occurs
* @throws AuthenticationException if authentication fails
*/","* This method is overridden to restrict HTTP authentication schemes
   * available for delegation token management functionality. The
   * authentication schemes to be used for delegation token management are
   * configured using {@link DELEGATION_TOKEN_SCHEMES_PROPERTY}
   *
   * The basic logic here is to check if the current request is for delegation
   * token management. If yes then check if the request contains an
   * ""Authorization"" header. If it is missing, then return the HTTP 401
   * response with WWW-Authenticate header for each scheme configured for
   * delegation token management.
   *
   * It is also possible for a client to preemptively send Authorization header
   * for a scheme not configured for delegation token management. We detect
   * this case and return the HTTP 401 response with WWW-Authenticate header
   * for each scheme configured for delegation token management.
   *
   * If a client has sent a request with ""Authorization"" header for a scheme
   * configured for delegation token management, then it is forwarded to
   * underlying {@link MultiSchemeAuthenticationHandler} for actual
   * authentication.
   *
   * Finally all other requests (excluding delegation token management) are
   * forwarded to underlying {@link MultiSchemeAuthenticationHandler} for
   * actual authentication.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleDeprecation,org.apache.hadoop.conf.Configuration:handleDeprecation(),777,786,"/**
* Handles deprecation for all properties in config.
* Logs and processes each deprecated property.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,onlyKeyExists,org.apache.hadoop.conf.Configuration:onlyKeyExists(java.lang.String),1295,1305,"/**
* Checks if a given name matches any masked pattern.
* @param name the name to check
* @return true if name matches any mask, false otherwise
*/","* Return existence of the <code>name</code> property, but only for
   * names which have no valid value, usually non-existent or commented
   * out in XML.
   *
   * @param name the property name
   * @return true if the property <code>name</code> exists without value",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRaw,org.apache.hadoop.conf.Configuration:getRaw(java.lang.String),1355,1362,"/**
* Masks a given name using context-dependent rules.
* @param name the original name to be masked
* @return masked version of the input name
*/","* Get the value of the <code>name</code> property, without doing
   * <a href=""#VariableExpansion"">variable expansion</a>.If the key is 
   * deprecated, it returns the value of the first key which replaces 
   * the deprecated key and is not null.
   * 
   * @param name the property name.
   * @return the value of the <code>name</code> property or 
   *         its replacing property and null if no such property exists.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,set,"org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String,java.lang.String)",1420,1458,"/**
* Masks a property with a given name and value.
* @param name the property name to mask
* @param value the value to assign to the masked property
* @param source the source of the masking action or null if programmatically set
*/","* Set the <code>value</code> of the <code>name</code> property. If 
   * <code>name</code> is deprecated, it also sets the <code>value</code> to
   * the keys that replace the deprecated key. Name will be trimmed before put
   * into configuration.
   *
   * @param name property name.
   * @param value property value.
   * @param source the place that this configuration value came from 
   * (For debugging).
   * @throws IllegalArgumentException when the value or name is null.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,unset,org.apache.hadoop.conf.Configuration:unset(java.lang.String),1476,1492,"/**
 * Masks a given name by processing it through various methods.
 * @param name the name to be masked
 */","* Unset a previously set property.
   * @param name the property name",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,tryFence,"org.apache.hadoop.ha.ShellCommandFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",81,135,"/**
* Executes a fencing command on the target service.
* @param target HAServiceTarget object representing the target service
* @param args arguments for the fencing command
* @return true if the command executes successfully, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,grantPermissions,org.apache.hadoop.fs.FileUtil:grantPermissions(java.io.File),235,239,"/**
 * Masks file by applying multiple utility functions.
 * @param f target file to be masked
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getVirtualMemorySize,org.apache.hadoop.util.SysInfoWindows:getVirtualMemorySize(),144,148,"/**
 * Calls m1 and returns the value of vmemSize.
 * @return size of virtual memory
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getPhysicalMemorySize,org.apache.hadoop.util.SysInfoWindows:getPhysicalMemorySize(),151,155,"/**
 * Returns memory size after executing mask function.
 * @return Memory size in bytes
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getAvailableVirtualMemorySize,org.apache.hadoop.util.SysInfoWindows:getAvailableVirtualMemorySize(),158,162,"/**
 * Returns available memory size.
 * Calls m1() before returning the value.
 * @return Available memory in bytes
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getAvailablePhysicalMemorySize,org.apache.hadoop.util.SysInfoWindows:getAvailablePhysicalMemorySize(),165,169,"/**
 * Returns available memory.
 * Calls m1() before returning.
 * @return amount of available memory
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumProcessors,org.apache.hadoop.util.SysInfoWindows:getNumProcessors(),172,176,"/**
 * Returns the number of processors.
 * Calls m1() before returning.
 * @return Number of processors
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCpuFrequency,org.apache.hadoop.util.SysInfoWindows:getCpuFrequency(),185,189,"/**
 * Returns CPU frequency in kHz.
 * Calls m1() before returning the value.
 * @return CPU frequency in kHz
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCumulativeCpuTime,org.apache.hadoop.util.SysInfoWindows:getCumulativeCpuTime(),192,196,"/**
 * Returns cumulative CPU time in milliseconds.
 * Calls m1() before returning the value.
 * @return cumulative CPU time in milliseconds
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCpuUsagePercentage,org.apache.hadoop.util.SysInfoWindows:getCpuUsagePercentage(),199,207,"/**
* Calculates CPU usage per processor.
* @return CPU usage percentage or -1 if not available
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumVCoresUsed,org.apache.hadoop.util.SysInfoWindows:getNumVCoresUsed(),210,218,"/**
* Returns CPU usage percentage.
* @return CPU usage as a float between 0 and 1, or -1 if not available
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNetworkBytesRead,org.apache.hadoop.util.SysInfoWindows:getNetworkBytesRead(),221,225,"/**
 * Calls m1() and returns netBytesRead.
 * @return Value of netBytesRead after calling m1()
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNetworkBytesWritten,org.apache.hadoop.util.SysInfoWindows:getNetworkBytesWritten(),228,232,"/**
 * Calls m1 and returns the net bytes written.
 * @return number of net bytes written
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getStorageBytesRead,org.apache.hadoop.util.SysInfoWindows:getStorageBytesRead(),234,238,"/**
 * Calls m1 and returns storage bytes read.
 * @return Number of bytes read from storage
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getStorageBytesWritten,org.apache.hadoop.util.SysInfoWindows:getStorageBytesWritten(),240,244,"/**
 * Calls m1 and returns storageBytesWritten.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getPermission,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getPermission(),951,957,"/**
* Overrides default permission setting.
* Calls m2 if m1 condition fails.
* @return FsPermission object
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getOwner,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getOwner(),959,965,"/**
 * Overrides method m3 to conditionally call m2.
 * @return result of super.m3()
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getGroup,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getGroup(),967,973,"/**
 * Overrides m3 to conditionally call m2 based on m1's result.
 * @return Result of super.m3()
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:write(java.io.DataOutput),1087,1093,"/**
* Writes data to output stream.
* Calls m2() if m1() returns false, then invokes superclass method.
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,flush,org.apache.hadoop.security.alias.LocalKeyStoreProvider:flush(),143,160,"/**
* Overrides method m1 to reset file permissions.
* Logs the permission change and applies it based on OS.
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,cacheGroupsRefresh,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh(),63,68,"/**
* Masks network group data.
* Fetches groups, updates cache, and processes them.
*/",* Refresh the netgroup cache,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path),498,503,"/**
* Checks file access and delegates to m1 with default flag.
* @param f file path to check
* @return result of m1 call
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file does not exist
* @throws IOException for other I/O errors
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus(),1299,1302,"/**
 * Calls overloaded m1 with null argument.
 * @return FsStatus result of operation
 * @throws IOException if an I/O error occurs
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,updateMountPointFsStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:updateMountPointFsStatus(org.apache.hadoop.fs.viewfs.ViewFileSystem,java.util.Map,org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint,org.apache.hadoop.fs.Path)",169,175,"/**
 * Updates filesystem status for a given mount point and path.
 * @param viewFileSystem the ViewFileSystem instance to use
 * @param mountPointMap map of MountPoint to FsStatus
 * @param mountPoint the target mount point
 * @param path the filesystem path
 */","* Update FsStatus for the given the mount point.
   *
   * @param viewFileSystem
   * @param mountPointMap
   * @param mountPoint
   * @param path",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,read,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:read(long,byte[],int,int)",194,210,"/**
 * Reads data from a specified position into a byte array.
 * @param position starting position in the file
 * @param b destination byte array
 * @param off offset within the byte array
 * @param len number of bytes to read
 * @return number of bytes actually read
 * @throws IOException if an I/O error occurs
 * @throws UnresolvedLinkException if a symbolic link could not be resolved
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$Put:processArguments(java.util.LinkedList),306,315,"/**
 * Handles specific path data processing.
 * @param args linked list of path data arguments
 * @throws IOException if an I/O error occurs
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",696,702,"/**
* Opens a file for writing with specified parameters.
* @param f file path
* @param permission file permissions
* @param overwrite flag to overwrite existing file
* @param bufferSize buffer size in bytes
* @param replication replication factor
* @param blockSize block size in bytes
* @param progress progress monitor
* @return FSDataOutputStream for writing
* @throws IOException if an I/O error occurs
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",736,742,"/**
* Creates a file output stream with specified parameters.
* @param f file path
* @param permission file permissions
* @param overwrite flag to overwrite existing file
* @param bufferSize buffer size for I/O operations
* @param replication number of data replicas
* @param blockSize block size in bytes
* @param progress progress callback
* @return FSDataOutputStream for writing data
* @throws IOException if an I/O error occurs
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",757,768,"/**
* Creates or overwrites a file with specified permissions and settings.
* @param f file path
* @param permission file permissions
* @param flags file creation flags
* @param bufferSize buffer size for I/O operations
* @param replication replication factor for the file
* @param blockSize block size for the file
* @param progress progress monitor for I/O operations
* @return FSDataOutputStream for writing to the file
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",638,644,"/**
 * Copies a file from source to destination.
 * @param src source file path
 * @param dst destination file path
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,listStatus,org.apache.hadoop.fs.HarFileSystem:listStatus(org.apache.hadoop.fs.Path),784,804,"/**
* Retrieves file status for a given path.
* @param f the input file path
* @return array of FileStatus objects
* @throws IOException if an I/O error occurs
*/","* liststatus returns the children of a directory 
   * after looking up the index files.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileLinkStatusInternal,"org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatusInternal(org.apache.hadoop.fs.Path,boolean)",1233,1242,"/**
 * Determines file status.
 * @param f file path
 * @param dereference whether to follow symbolic links
 * @return FileStatus object
 * @throws IOException if an I/O error occurs
 */","* Public {@link FileStatus} methods delegate to this function, which in turn
   * either call the new {@link Stat} based implementation or the deprecated
   * methods based on platform support.
   * 
   * @param f Path to stat
   * @param dereference whether to dereference the final path component if a
   *          symlink
   * @return FileStatus of f
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:<init>(org.apache.hadoop.fs.LocatedFileStatus,org.apache.hadoop.fs.Path)",32,35,"/**
* Initializes file status with given LocatedFileStatus and path.
* @param locatedFileStatus file status object
* @param path file path
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",505,512,"/**
 * Retrieves block locations for a file.
 * @param fs File status object
 * @param start Start offset
 * @param len Length of data
 * @return Array of BlockLocation objects
 * @throws IOException if an I/O error occurs
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFileStatus(org.apache.hadoop.fs.Path),407,426,"/**
* Retrieves file status for a given path.
* @param f file path
* @return FileStatus object representing the file's status
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if the file does not exist
* @throws UnresolvedLinkException if a symlink cannot be resolved
* @throws IOException if an I/O error occurs
*/","* {@inheritDoc}
   *
   * If the given path is a symlink(mount link), the path will be resolved to a
   * target path and it will get the resolved path's FileStatus object. It will
   * not be represented as a symlink and isDirectory API returns true if the
   * resolved path is a directory, false otherwise.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFs:listStatus(org.apache.hadoop.fs.Path),518,538,"/**
* Retrieves file statuses for a given path.
* @param f file path to query
* @return array of FileStatus objects
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file not found
* @throws UnresolvedLinkException if link cannot be resolved
* @throws IOException on I/O errors
*/","* {@inheritDoc}
   *
   * Note: listStatus considers listing from fallbackLink if available. If the
   * same directory path is present in configured mount path as well as in
   * fallback fs, then only the fallback path will be listed in the returned
   * result except for link.
   *
   * If any of the the immediate children of the given path f is a symlink(mount
   * link), the returned FileStatus object of that children would be represented
   * as a symlink. It will not be resolved to the target path and will not get
   * the target path FileStatus object. The target path will be available via
   * getSymlink on that children's FileStatus object. Since it represents as
   * symlink, isDirectory on that children's FileStatus will return false.
   * This behavior can be changed by setting an advanced configuration
   * fs.viewfs.mount.links.as.symlinks to false. In this case, mount points will
   * be represented as non-symlinks and all the file/directory attributes like
   * permissions, isDirectory etc will be assigned from it's resolved target
   * directory/file.
   *
   * If you want to get the FileStatus of target path for that children, you may
   * want to use GetFileStatus API with that children's symlink path. Please see
   * {@link ViewFs#getFileStatus(Path f)}
   *
   * Note: In ViewFs, by default the mount links are represented as symlinks.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,exists,"org.apache.hadoop.fs.sftp.SFTPFileSystem:exists(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",189,198,"/**
* Masks a file using SFTP channel.
* @param channel SFTP channel for file operations
* @param file path of the file to mask
* @return true if masking successful, false if file not found
* @throws IOException if an I/O error occurs during operation
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,com.jcraft.jsch.ChannelSftp$LsEntry,org.apache.hadoop.fs.Path)",260,297,"/**
 * Recursively retrieves file status from SFTP.
 * @param channel SFTP channel
 * @param sftpFile SFTP entry to process
 * @param parentPath parent directory path
 * @return FileStatus object representing the file or directory
 * @throws IOException if an I/O error occurs
 */","* Convert the file information in LsEntry to a {@link FileStatus} object. *
   *
   * @param sftpFile
   * @param parentPath
   * @return file status
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,isFile,"org.apache.hadoop.fs.sftp.SFTPFileSystem:isFile(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",355,363,"/**
 * Checks if a file is not masked.
 * @param channel SFTP channel for file operations
 * @param file path to the file
 * @return true if file is not masked, false otherwise
 * @throws IOException if an I/O error occurs
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.HttpsFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
* Returns file status with default values.
* @param path file path to check
* @return FileStatus object with specific attributes
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.HttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
 * Returns file status with default block size.
 * @param path file path to check
 * @return FileStatus object with specified properties
 * @throws IOException if an I/O error occurs
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,glob,org.apache.hadoop.fs.Globber:glob(),197,206,"/**
* Retrieves file status for paths matching a pattern.
* @return FileStatus array of matched files
* @throws IOException if an I/O error occurs
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,exists,"org.apache.hadoop.fs.ftp.FTPFileSystem:exists(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",391,398,"/**
 * Applies mask operation on FTP file.
 * @param client FTP client instance
 * @param file local file path
 * @return true if successful, false if file not found
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException on IO problems other than FileNotFoundException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,listStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",484,498,"/**
* Retrieves file status from FTP server.
* @param client FTPClient instance
* @param file Path to the file or directory
* @return Array of FileStatus objects representing files and subdirectories
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,isFile,"org.apache.hadoop.fs.ftp.FTPFileSystem:isFile(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",617,625,"/**
* Checks if file exists on FTP server.
* @param client FTPClient instance
* @param file local path to the file
* @return true if file exists, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",294,297,"/**
* Constructs a CBZip2InputStream with specified input stream and read mode.
* @param in input stream containing BZip2 compressed data
* @param readMode specifies the reading mode (e.g., NORMAL or SMALL)
*/","* Constructs a new CBZip2InputStream which decompresses bytes read from the
  * specified stream.
  *
  * <p>
  * Although BZip2 headers are marked with the magic <tt>""Bz""</tt> this
  * constructor expects the next byte in the stream to be the first one after
  * the magic. Thus callers have to skip the first two bytes. Otherwise this
  * constructor will throw an exception.
  * </p>
  * @param in in.
  * @param readMode READ_MODE.
  * @throws IOException
  *             if the stream content is malformed or an I/O error occurs.
  * @throws NullPointerException
  *             if <tt>in == null</tt>",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,numberOfBytesTillNextMarker,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:numberOfBytesTillNextMarker(java.io.InputStream),346,349,"/**
 * Decompresses BZip2 input stream and returns a value.
 * @param in BZip2 compressed input stream
 * @return decompressed value as long
 * @throws IOException if I/O error occurs
 */","* Returns the number of bytes between the current stream position
   * and the immediate next BZip2 block marker.
   *
   * @param in
   *             The InputStream
   *
   * @return long Number of bytes between current stream position and the
   * next BZip2 block start marker.
 * @throws IOException raised on errors performing I/O.
   *",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read(byte[],int,int)",400,448,"/**
 * Reads and masks data into a destination buffer.
 * @param dest destination byte array
 * @param offs offset in the destination array
 * @param len number of bytes to read
 * @return number of bytes read or -1 if end of stream
 * @throws IOException on I/O error or invalid parameters
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)",529,575,"/**
* Processes RPC request by delegating to another method.
* @param server RPC server instance
* @param connectionProtocolName protocol name for the connection
* @param writableRequest request object wrapped in Writable
* @param receiveTime timestamp when request was received
* @return processed response as Writable
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newMutableRollingAverages,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newMutableRollingAverages(java.lang.String,java.lang.String)",339,346,"/**
* Creates and registers a mutable rolling averages metric.
* @param name unique identifier for the metric
* @param valueName name of the values to average
* @return MutableRollingAverages instance
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,verifyChangedServiceState,org.apache.hadoop.ha.ZKFailoverController:verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState),884,922,"/**
* Handles state changes in High Availability services.
* @param changedState new state of the HA service
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processResult,"org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,java.lang.String)",496,551,"/**
 * Handles ZooKeeper node creation result.
 * @param rc result code from ZooKeeper operation
 * @param path path of the created node
 * @param ctx context object for the operation
 * @param name name associated with the operation
 */",* interface implementation of Zookeeper callback for create,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processResult,"org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)",556,613,"/**
* Handles Zookeeper StatNode result.
* @param rc Zookeeper operation result code
* @param path Path of the node
* @param ctx Context object
* @param stat Node statistics
*/",* interface implementation of Zookeeper callback for monitor (exists),,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,process,org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:process(org.apache.zookeeper.WatchedEvent),1233,1247,"/**
* Handles ZooKeeper events.
* @param event WatchedEvent received from ZooKeeper
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String),162,164,"/**
 * Creates an InetSocketAddress from target and default port.
 * @param target host or IP address
 * @return InetSocketAddress with specified target and default port
 */","* Util method to build socket addr from either.
   *   {@literal <host>:<port>}
   *   {@literal <fs>://<host>:<port>/<path>}
   *
   * @param target target.
   * @return socket addr.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Servers.java,parse,"org.apache.hadoop.metrics2.util.Servers:parse(java.lang.String,int)",50,62,"/**
* Parses address specifications and returns a list of InetSocketAddress.
* @param specs comma-separated host:port or just host strings
* @param defaultPort port to use if not specified in specs
* @return List of InetSocketAddress objects
*/","* Parses a space and/or comma separated sequence of server specifications
   * of the form <i>hostname</i> or <i>hostname:port</i>.  If
   * the specs string is null, defaults to localhost:defaultPort.
   *
   * @param specs   server specs (see description)
   * @param defaultPort the default port if not specified
   * @return a list of InetSocketAddress objects.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildDTServiceName,"org.apache.hadoop.security.SecurityUtil:buildDTServiceName(java.net.URI,int)",338,345,"/**
* Masks URI by extracting and returning its host.
* @param uri the input URI to process
* @param defPort default port number if not specified in URI
* @return masked host string or null if authority is missing
*/","* create the service name for a Delegation token
   * @param uri of the service
   * @param defPort is used if the uri lacks a port
   * @return the token service, or null if no authority
   * @see #buildTokenService(InetSocketAddress)",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,asXmlDocument,"org.apache.hadoop.conf.Configuration:asXmlDocument(java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3650,3685,"/**
* Generates a masked configuration document.
* @param propertyName specific property to mask; if null, masks all properties
* @param redactor tool for masking sensitive data
* @return Document object with masked configurations
* @throws IOException if there's an error parsing XML
* @throws IllegalArgumentException if specified property is not found
*/",* Return the XML DOM corresponding to this Configuration.,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,substituteVars,org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String),1150,1218,"/**
 * Substitutes variables in expression.
 * @param expr input expression with variables
 * @return modified expression with substituted values or original if no substitutions
 */","* Attempts to repeatedly expand the value {@code expr} by replacing the
   * left-most substring of the form ""${var}"" in the following precedence order
   * <ol>
   *   <li>by the value of the environment variable ""var"" if defined</li>
   *   <li>by the value of the Java system property ""var"" if defined</li>
   *   <li>by the value of the configuration key ""var"" if defined</li>
   * </ol>
   *
   * If var is unbounded the current state of expansion ""prefix${var}suffix"" is
   * returned.
   * <p>
   * This function also detects self-referential substitutions, i.e.
   * <pre>
   *   {@code
   *   foo.bar = ${foo.bar}
   *   }
   * </pre>
   * If a cycle is detected then the original expr is returned. Loops
   * involving multiple substitutions are not detected.
   *
   * In order not to introduce breaking changes (as Oozie for example contains a method with the
   * same name and same signature) do not make this method public, use substituteCommonVariables
   * in this case.
   *
   * @param expr the literal value of a config key
   * @return null if expr is null, otherwise the value resulting from expanding
   * expr using the algorithm above.
   * @throws IllegalArgumentException when more than
   * {@link Configuration#MAX_SUBST} replacements are required",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,applyChanges,"org.apache.hadoop.conf.ReconfigurationServlet:applyChanges(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable,javax.servlet.http.HttpServletRequest)",140,197,"/**
* Handles configuration reconfiguration and logging changes.
* @param out PrintWriter for outputting change logs
* @param reconf Reconfigurable object to apply new configurations
* @param req HttpServletRequest containing configuration parameters
*/",* Apply configuratio changes after admin has approved them.,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,set,"org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String)",1404,1406,"/**
 * Calls overloaded method with default null for third parameter.
 * @param name  first parameter
 * @param value second parameter
 */","* Set the <code>value</code> of the <code>name</code> property. If 
   * <code>name</code> is deprecated or there is a deprecated name associated to it,
   * it sets the value to both names. Name will be trimmed before put into
   * configuration.
   * 
   * @param name property name.
   * @param value property value.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,set,"org.apache.hadoop.conf.ConfigurationWithLogging:set(java.lang.String,java.lang.String,java.lang.String)",107,112,"/**
* Logs and sets a configuration property.
* @param name property name
* @param value property value
* @param source source of the property (optional)
*/","* See {@link Configuration#set(String, String, String)}.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDelete,"org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File,boolean)",187,203,"/**
* Masks a directory with optional permission granting.
* @param dir the directory to mask
* @param tryGrantPermissions flag to attempt granting permissions
* @return true if masking is successful, false otherwise
*/","* Delete a directory and all its contents.  If
   * we return false, the directory may be partially-deleted.
   * (1) If dir is symlink to a file, the symlink is deleted. The file pointed
   *     to by the symlink is not deleted.
   * (2) If dir is symlink to a directory, symlink is deleted. The directory
   *     pointed to by symlink is not deleted.
   * (3) If dir is a normal file, it is deleted.
   * (4) If dir is a normal directory, then dir and all its contents recursively
   *     are deleted.
   * @param dir the file or directory to be deleted
   * @param tryGrantPermissions true if permissions should be modified to delete a file.
   * @return true on success false on failure.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumCores,org.apache.hadoop.util.SysInfoWindows:getNumCores(),179,182,"/**
 * Returns the mask value from function m1.
 * @return integer mask value
 */",{@inheritDoc},,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,getStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:getStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",106,159,"/**
 * Retrieves file system status for given path.
 * @param fileSystem the FileSystem instance
 * @param path the Path to check
 * @return Map of MountPoint to FsStatus
 * @throws IOException if an I/O error occurs
 */","* Get FsStatus for all ViewFsMountPoints matching path for the given
   * ViewFileSystem.
   *
   * Say ViewFileSystem has following mount points configured
   *  (1) hdfs://NN0_host:port/sales mounted on /dept/sales
   *  (2) hdfs://NN1_host:port/marketing mounted on /dept/marketing
   *  (3) hdfs://NN2_host:port/eng_usa mounted on /dept/eng/usa
   *  (4) hdfs://NN3_host:port/eng_asia mounted on /dept/eng/asia
   *
   * For the above config, here is a sample list of paths and their matching
   * mount points while getting FsStatus
   *
   *  Path                  Description                      Matching MountPoint
   *
   *  ""/""                   Root ViewFileSystem lists all    (1), (2), (3), (4)
   *                         mount points.
   *
   *  ""/dept""               Not a mount point, but a valid   (1), (2), (3), (4)
   *                         internal dir in the mount tree
   *                         and resolved down to ""/"" path.
   *
   *  ""/dept/sales""         Matches a mount point            (1)
   *
   *  ""/dept/sales/india""   Path is over a valid mount point (1)
   *                         and resolved down to
   *                         ""/dept/sales""
   *
   *  ""/dept/eng""           Not a mount point, but a valid   (1), (2), (3), (4)
   *                         internal dir in the mount tree
   *                         and resolved down to ""/"" path.
   *
   *  ""/erp""                Doesn't match or leads to or
   *                         over any valid mount points     None
   *
   *
   * @param fileSystem - ViewFileSystem on which mount point exists
   * @param path - URI for which FsStatus is requested
   * @return Map of ViewFsMountPoint and FsStatus
   * @throws IOException raised on errors performing I/O.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",744,755,"/**
* Creates or overwrites a file with specified permissions and options.
* @param f file path
* @param permission file permissions
* @param flags creation flags including overwrite option
* @param bufferSize buffer size for data transfer
* @param replication number of replicas for the file
* @param blockSize block size for the file
* @param progress progress monitor
* @param checksumOpt checksum options
* @return FSDataOutputStream for writing to the file
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileStatus,org.apache.hadoop.fs.RawLocalFileSystem:getFileStatus(org.apache.hadoop.fs.Path),905,908,"/**
 * Checks file status with masking.
 * @param f file path to check
 * @return FileStatus object
 * @throws IOException if an I/O error occurs
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),1209,1220,"/**
* Retrieves file status and resolves symbolic links.
* @param f file path to check
* @return FileStatus object with resolved link
* @throws IOException if an I/O error occurs
*/","* Return a FileStatus representing the given path. If the path refers
   * to a symlink return a FileStatus representing the link rather than
   * the object the link refers to.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getLinkTarget,org.apache.hadoop.fs.RawLocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),1308,1313,"/**
* Masks a file path.
* @param f input file path
* @return masked file path
* @throws IOException if an I/O error occurs
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,wrapLocalFileStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:wrapLocalFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",552,557,"/**
* Wraps original file status with view-specific details.
* @param orig original FileStatus object
* @param qualified fully qualified path
* @return wrapped FileStatus object
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,rename,"org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",464,491,"/**
 * Renames a file on an SFTP server.
 * @param channel SFTP channel connection
 * @param src source path of the file to rename
 * @param dst destination path for the renamed file
 * @return true if renaming is successful, false otherwise
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   *
   * @param channel
   * @param src
   * @param dst
   * @return rename successful?
   * @throws IOException",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,listStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",421,451,"/**
 * Retrieves file status for a directory in SFTP.
 * @param client SFTP client instance
 * @param file local path to the directory
 * @return array of FileStatus objects representing files and directories
 * @throws IOException if an I/O error occurs during operation
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",314,347,"/**
* Creates a directory with specified permissions.
* @param client SFTP client instance
* @param file target directory path
* @param permission desired file system permissions
* @return true if directory is successfully created or already exists, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,globStatus,org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path),2122,2126,"/**
 * Retrieves file statuses matching a given path pattern.
 * @param pathPattern the pattern to match file paths
 * @return array of FileStatus objects
 * @throws AccessControlException if access is denied
 * @throws UnsupportedFileSystemException if file system is unsupported
 * @throws IOException if an I/O error occurs
 */","* <p>Return all the files that match filePattern and are not checksum
     * files. Results are sorted by their names.
     * 
     * <p>
     * A filename pattern is composed of <i>regular</i> characters and
     * <i>special pattern matching</i> characters, which are:
     *
     * <dl>
     *  <dd>
     *   <dl>
     *    <dt> <tt> ? </tt>
     *    <dd> Matches any single character.
     *
     *    <dt> <tt> * </tt>
     *    <dd> Matches zero or more characters.
     *
     *    <dt> <tt> [<i>abc</i>] </tt>
     *    <dd> Matches a single character from character set
     *     <tt>{<i>a,b,c</i>}</tt>.
     *
     *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>
     *    <dd> Matches a single character from the character range
     *     <tt>{<i>a...b</i>}</tt>. Note: character <tt><i>a</i></tt> must be
     *     lexicographically less than or equal to character <tt><i>b</i></tt>.
     *
     *    <dt> <tt> [^<i>a</i>] </tt>
     *    <dd> Matches a single char that is not from character set or range
     *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur
     *     immediately to the right of the opening bracket.
     *
     *    <dt> <tt> \<i>c</i> </tt>
     *    <dd> Removes (escapes) any special meaning of character <i>c</i>.
     *
     *    <dt> <tt> {ab,cd} </tt>
     *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>
     *
     *    <dt> <tt> {ab,c{de,fh}} </tt>
     *    <dd> Matches a string from string set <tt>{<i>ab, cde, cfh</i>}</tt>
     *
     *   </dl>
     *  </dd>
     * </dl>
     *
     * @param pathPattern a glob specifying a path pattern
     *
     * @return an array of paths that match the path pattern
     *
     * @throws AccessControlException If access is denied
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,globStatus,"org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2151,2155,"/**
* Applies a mask to find files matching a pattern.
* @param pathPattern the file path pattern to match
* @param filter additional filter for file selection
* @return array of FileStatus objects or empty if none found
*/","* Return an array of FileStatus objects whose path names match pathPattern
     * and is accepted by the user-supplied path filter. Results are sorted by
     * their path names.
     * Return null if pathPattern has no glob and the path does not exist.
     * Return an empty array if pathPattern has a glob and no path matches it. 
     * 
     * @param pathPattern glob specifying the path pattern
     * @param filter user-supplied path filter
     *
     * @return an array of FileStatus objects
     *
     * @throws AccessControlException If access is denied
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,globStatus,org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path),2219,2226,"/**
* Retrieves file status for paths matching a pattern.
* @param pathPattern glob pattern to match file paths
* @return array of FileStatus objects
* @throws IOException if an I/O error occurs
*/","* <p>Return all the files that match filePattern and are not checksum
   * files. Results are sorted by their names.
   *
   * <p>
   * A filename pattern is composed of <i>regular</i> characters and
   * <i>special pattern matching</i> characters, which are:
   *
   * <dl>
   *  <dd>
   *   <dl>
   *    <dt> <tt> ? </tt>
   *    <dd> Matches any single character.
   *
   *    <dt> <tt> * </tt>
   *    <dd> Matches zero or more characters.
   *
   *    <dt> <tt> [<i>abc</i>] </tt>
   *    <dd> Matches a single character from character set
   *     <tt>{<i>a,b,c</i>}</tt>.
   *
   *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>
   *    <dd> Matches a single character from the character range
   *     <tt>{<i>a...b</i>}</tt>.  Note that character <tt><i>a</i></tt> must be
   *     lexicographically less than or equal to character <tt><i>b</i></tt>.
   *
   *    <dt> <tt> [^<i>a</i>] </tt>
   *    <dd> Matches a single character that is not from character set or range
   *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur
   *     immediately to the right of the opening bracket.
   *
   *    <dt> <tt> \<i>c</i> </tt>
   *    <dd> Removes (escapes) any special meaning of character <i>c</i>.
   *
   *    <dt> <tt> {ab,cd} </tt>
   *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>
   *
   *    <dt> <tt> {ab,c{de,fh}} </tt>
   *    <dd> Matches a string from the string set <tt>{<i>ab, cde, cfh</i>}</tt>
   *
   *   </dl>
   *  </dd>
   * </dl>
   *
   * @param pathPattern a glob specifying a path pattern

   * @return an array of paths that match the path pattern
   * @throws IOException IO failure",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,globStatus,"org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2241,2244,"/**
* Retrieves file statuses matching the given path pattern and filter.
* @param pathPattern path pattern to match files
* @param filter custom filter for file selection
* @return array of FileStatus objects
*/","* Return an array of {@link FileStatus} objects whose path names match
   * {@code pathPattern} and is accepted by the user-supplied path filter.
   * Results are sorted by their path names.
   *
   * @param pathPattern a glob specifying the path pattern
   * @param filter a user-supplied path filter
   * @return null if {@code pathPattern} has no glob and the path does not exist
   *         an empty array if {@code pathPattern} has a glob and no path
   *         matches it else an array of {@link FileStatus} objects matching the
   *         pattern
   * @throws IOException if any I/O error occurs when fetching file status",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,rename,"org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",670,705,"/**
* Renames a file on an FTP server.
* @param client FTPClient instance
* @param src source file path
* @param dst destination file path
* @return true if rename is successful, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * 
   * @param client
   * @param src
   * @param dst
   * @return
   * @throws IOException",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,delete,"org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,boolean)",416,438,"/**
 * Deletes a file or directory on an FTP server.
 * @param client FTPClient instance
 * @param file path of the file or directory to delete
 * @param recursive true if deletion should be recursive for directories
 * @return true if deletion is successful, false otherwise
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",590,610,"/**
* Creates directory on FTP server with specified permissions.
* @param client FTPClient instance
* @param file Path to create directory
* @param permission FsPermission for the new directory
* @return true if directory was successfully created, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",358,407,"/**
 * Initializes BZip2CompressionInputStream with specified range and read mode.
 * @param in input stream to be compressed
 * @param start starting position in the stream
 * @param end ending position in the stream
 * @param readMode mode for reading blocks
 * @throws IOException if an I/O error occurs
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,internalReset,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:internalReset(),530,537,"/**
* Resets and initializes the input stream for decompression.
* @throws IOException if an I/O error occurs during stream initialization
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream),351,353,"/**
 * Constructs a new CBZip2InputStream.
 * @param in input stream containing Bzip2 compressed data
 * @throws IOException if an I/O error occurs
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read(),365,376,"/**
* Reads and processes data from input stream.
* @return processed integer value or throws exception if stream is closed
* @throws IOException if input stream is not available
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,newForField,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)",40,92,"/**
* Creates and registers a metric based on field and annotation.
* @param field the field with metric annotation
* @param annotation the metric annotation
* @param registry the metrics registry
* @return the created MutableMetric or throws MetricsException if unsupported
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,reportServiceStatus,org.apache.hadoop.ha.ZKFailoverController$ServiceStateCallBacks:reportServiceStatus(org.apache.hadoop.ha.HAServiceStatus),999,1002,"/**
 * Masks service status by invoking another function.
 * @param status current HAServiceStatus object
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,normalizeIP2HostName,org.apache.hadoop.net.NetUtils:normalizeIP2HostName(java.lang.String),731,738,"/**
* Masks IP and port in a given string.
* @param ipPort input string containing IP and port
* @return masked IP and port or original string if invalid
*/","* Attempt to normalize the given string to ""host:port""
   * if it like ""ip:port"".
   *
   * @param ipPort maybe lik ip:port or host:port.
   * @return host:port",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getTokenServiceAddr,org.apache.hadoop.security.SecurityUtil:getTokenServiceAddr(org.apache.hadoop.security.token.Token),447,449,"/**
 * Converts token to InetSocketAddress.
 * @param token input token object
 * @return InetSocketAddress derived from token
 */","* Decode the given token's service field into an InetAddress
   * @param token from which to obtain the service
   * @return InetAddress for the service",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildTokenService,org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.URI),495,497,"/**
 * Converts URI to Text using NetUtils.
 * @param uri input URI object
 * @return Text representation of the URI
 */","* Construct the service key for a token
   * @param uri of remote connection with a token
   * @return ""ip:port"" or ""host:port"" depending on the value of
   *          hadoop.security.token.service.use_ip",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,init,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:init(org.apache.commons.configuration2.SubsetConfiguration),120,172,"/**
* Initializes GangliaSink with configuration settings.
* @param conf configuration for Ganglia metrics
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.FileSystem:getCanonicalServiceName(),453,460,"/**
* Returns masked string based on condition.
* @return Masked string or null if condition not met
*/","* Get a canonical service name for this FileSystem.
   * The token cache is the only user of the canonical service name,
   * and uses it to lookup this FileSystem's service tokens.
   * If the file system provides a token of its own then it must have a
   * canonical name, otherwise the canonical name can be null.
   *
   * Default implementation: If the FileSystem has child file systems
   * (such as an embedded file system) then it is assumed that the FS has no
   * tokens of its own and hence returns a null name; otherwise a service
   * name is built using Uri and port.
   *
   * @return a service string that uniquely identifies this file system, null
   *         if the filesystem does not implement tokens
   * @see SecurityUtil#buildDTServiceName(URI, int)",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.AbstractFileSystem:getCanonicalServiceName(),1243,1245,"/**
 * Masks data using security utility functions.
 * @return masked string result
 */","* Get a canonical name for this file system.
   * @return a URI string that uniquely identifies this file system",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,substituteCommonVariables,org.apache.hadoop.conf.Configuration:substituteCommonVariables(java.lang.String),1115,1117,"/**
 * Masks the given expression.
 * @param expr input expression to be masked
 * @return masked expression as a string
 */","* Provides a public wrapper over substituteVars in order to avoid compatibility issues.
   * See HADOOP-18021 for further details.
   *
   * @param expr the literal value of a config key
   * @return null if expr is null, otherwise the value resulting from expanding
   * expr using the algorithm above.
   * @throws IllegalArgumentException when more than
   * {@link Configuration#MAX_SUBST} replacements are required",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,get,org.apache.hadoop.conf.Configuration:get(java.lang.String),1263,1270,"/**
* Generates a processed string based on input name.
* @param name the input string to process
* @return the final processed string or null if none found
*/","* Get the value of the <code>name</code> property, <code>null</code> if
   * no such property exists. If the key is deprecated, it returns the value of
   * the first key which replaces the deprecated key and is not null.
   * 
   * Values are processed for <a href=""#VariableExpansion"">variable expansion</a> 
   * before being returned.
   *
   * As a side effect get loads the properties from the sources if called for
   * the first time as a lazy init.
   * 
   * @param name the property name, will be trimmed before get value.
   * @return the value of the <code>name</code> or its replacing property, 
   *         or null if no such property exists.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,get,"org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)",1524,1531,"/**
* Processes a name to derive a result.
* @param name the input name string
* @param defaultValue default value if processing fails
* @return processed result or null
*/","* Get the value of the <code>name</code>. If the key is deprecated,
   * it returns the value of the first key which replaces the deprecated key
   * and is not null.
   * If no such property exists,
   * then <code>defaultValue</code> is returned.
   * 
   * @param name property name, will be trimmed before get value.
   * @param defaultValue default value.
   * @return property value, or <code>defaultValue</code> if the property 
   *         doesn't exist.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,doPost,"org.apache.hadoop.conf.ReconfigurationServlet:doPost(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",214,236,"/**
 * Handles POST request for masking configuration.
 * @param req HTTP request object
 * @param resp HTTP response object
 * @throws ServletException if a servlet-specific error occurs
 * @throws IOException if an I/O error occurs
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String)",169,175,"/**
* Applies a function mask with key and value.
* @param key unique identifier for the operation
* @param value associated data for the operation
* @return result of the masked function application
*/",* Set optional Builder parameter.,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String)",256,261,"/**
* Applies mask to configuration.
* @param key configuration key
* @param value configuration value
* @return result of the mask application
*/","* Set mandatory option to the Builder.
   *
   * If the option is not supported or unavailable on the {@link FileSystem},
   * the client should expect {@link #build()} throws IllegalArgumentException.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setDefaultUri,"org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.net.URI)",311,313,"/**
 * Sets default file system name in configuration.
 * @param conf Configuration object to update
 * @param uri URI containing the file system name
 */","* Set the default FileSystem URI in a configuration.
   * @param conf the configuration to alter
   * @param uri the new default filesystem uri",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLink,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.net.URI)",55,59,"/**
* Updates configuration with a new link.
* @param conf Configuration object to update
* @param mountTableName Name of the mount table
* @param src Source path for the link
* @param target Target URI for the link
*/","* Add a link to the config for the specified mount table
   * @param conf - add the link to this conf
   * @param mountTableName mountTable.
   * @param src - the src path name
   * @param target - the target URI link",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMergeSlash,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",79,83,"/**
* Updates configuration with a masked target URI.
* @param conf Configuration object to update
* @param mountTableName Name of the mount table
* @param target Target URI for masking
*/","* Add a LinkMergeSlash to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param target target.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkFallback,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",102,106,"/**
 * Updates configuration with a fallback link.
 * @param conf Configuration object to update
 * @param mountTableName Name of the mount table
 * @param target URI for the fallback target
 */","* Add a LinkFallback to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param target targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMerge,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])",125,129,"/**
* Configures target URIs for a specified mount table.
* @param conf Configuration object to update
* @param mountTableName name of the mount table
* @param targets array of target URIs
*/","* Add a LinkMerge to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param targets targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",150,156,"/**
* Updates configuration setting.
* @param conf Configuration object
* @param mountTableName Table name for mounting
* @param src Source path or identifier
* @param settings Configuration settings key
* @param targets Target paths or identifiers
*/","* Add nfly link to configuration for the given mount table.
   *
   * @param conf configuration.
   * @param mountTableName mount table.
   * @param src src.
   * @param settings settings.
   * @param targets targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkRegex,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkRegex(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",191,202,"/**
 * Updates configuration with masked source regex.
 * @param conf Configuration object to update
 * @param mountTableName Name of the mount table
 * @param srcRegex Source regex pattern to mask
 * @param targetStr Target string for masking
 * @param interceptorSettings Optional interceptor settings
 */","* Add a LinkRegex to the config for the specified mount table.
   * @param conf - get mountable config from this conf
   * @param mountTableName - the mountable name of the regex config item
   * @param srcRegex - the src path regex expression that applies to this config
   * @param targetStr - the string of target path
   * @param interceptorSettings - the serialized interceptor string to be
   *                            applied while resolving the mapping",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setHomeDirConf,"org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",220,228,"/**
* Sets HDFS home directory in configuration.
* @param conf Configuration object to update
* @param mountTableName name of the mount table
* @param homedir HDFS home directory path
*/","* Add config variable for homedir the specified mount table
   * @param conf - add to this conf
   * @param homedir - the home dir path starting with slash
   * @param mountTableName - the mount table.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,setUMask,"org.apache.hadoop.fs.permission.FsPermission:setUMask(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.permission.FsPermission)",400,402,"/**
* Sets umask in configuration.
* @param conf Configuration object to update
* @param umask File permission umask to apply
*/","* Set the user file creation mask (umask)
   * @param conf configuration.
   * @param umask umask.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,setCodecClasses,"org.apache.hadoop.io.compress.CompressionCodecFactory:setCodecClasses(org.apache.hadoop.conf.Configuration,java.util.List)",156,169,"/**
* Masks configuration with class names.
* @param conf Configuration object to update
* @param classes List of Class objects to process
*/","* Sets a list of codec classes in the configuration. In addition to any
   * classes specified using this method, {@link CompressionCodec} classes on
   * the classpath are discovered using a Java ServiceLoader.
   * @param conf the configuration to modify
   * @param classes the list of classes to set",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,setDefaultCompressionType,"org.apache.hadoop.io.SequenceFile:setDefaultCompressionType(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$CompressionType)",262,265,"/**
* Sets compression type for a job configuration.
* @param job Configuration object to modify
* @param val CompressionType value to set
*/","* Set the default compression type for sequence files.
   * @param job the configuration to modify
   * @param val the new compression type (none, block, record)",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,getProxyuserConfiguration,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig),107,119,"/**
* Creates a Configuration object from FilterConfig.
* @param filterConfig configuration settings for the filter
* @return Configuration object with proxy user settings
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,getProxyuserConfiguration,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig),158,174,"/**
* Creates configuration from FilterConfig with proxy user settings.
* @param filterConfig filter configuration object
* @return Configuration object containing proxy user settings
*/","* Returns the proxyuser configuration. All returned properties must start
   * with <code>proxyuser.</code>'
   * <p>
   * Subclasses may override this method if the proxyuser configuration is 
   * read from other place than the filter init parameters.
   *
   * @param filterConfig filter configuration object
   * @return the proxyuser configuration properties.
   * @throws ServletException thrown if the configuration could not be created.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,prepareConf,org.apache.hadoop.security.CompositeGroupsMapping:prepareConf(java.lang.String),175,191,"/**
* Filters configuration entries for a specific provider.
* @param providerName name of the provider to filter by
* @return Configuration object with filtered entries
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,init,org.apache.hadoop.security.alias.CredentialShell:init(java.lang.String[]),78,126,"/**
* Parses command-line arguments and executes corresponding actions.
* @param args array of command-line arguments
* @return exit code: 0 for success, 1 for error
*/","* Parse the command line arguments and initialize the data.
   * <pre>
   * % hadoop credential create alias [-provider providerPath]
   * % hadoop credential list [-provider providerPath]
   * % hadoop credential check alias [-provider providerPath]
   * % hadoop credential delete alias [-provider providerPath] [-f]
   * </pre>
   * @param args args.
   * @return 0 if the argument(s) were recognized, 1 otherwise
   * @throws IOException raised on errors performing I/O.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setAuthenticationMethod,"org.apache.hadoop.security.SecurityUtil:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,org.apache.hadoop.conf.Configuration)",741,748,"/**
 * Configures security authentication method.
 * @param authenticationMethod the authentication method to use; defaults to SIMPLE if null
 * @param conf configuration object to apply settings to
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setInt,"org.apache.hadoop.conf.Configuration:setInt(java.lang.String,int)",1582,1584,"/**
 * Masks and processes a given integer value.
 * @param name identifier for the value processing
 * @param value integer to be masked and processed
 */","* Set the value of the <code>name</code> property to an <code>int</code>.
   * 
   * @param name property name.
   * @param value <code>int</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setLong,"org.apache.hadoop.conf.Configuration:setLong(java.lang.String,long)",1655,1657,"/**
 * Masks and processes a given value with a name.
 * @param name identifier for the value
 * @param value numeric value to be masked and processed
 */","* Set the value of the <code>name</code> property to a <code>long</code>.
   * 
   * @param name property name.
   * @param value <code>long</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setFloat,"org.apache.hadoop.conf.Configuration:setFloat(java.lang.String,float)",1684,1686,"/**
 * Masks and processes a float value.
 * @param name identifier for the value
 * @param value the float value to be processed
 */","* Set the value of the <code>name</code> property to a <code>float</code>.
   * 
   * @param name property name.
   * @param value property value.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setDouble,"org.apache.hadoop.conf.Configuration:setDouble(java.lang.String,double)",1713,1715,"/**
 * Masks and processes a given name and numeric value.
 * @param name the input string to be processed
 * @param value the numeric value to be masked and passed
 */","* Set the value of the <code>name</code> property to a <code>double</code>.
   * 
   * @param name property name.
   * @param value property value.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setBoolean,"org.apache.hadoop.conf.Configuration:setBoolean(java.lang.String,boolean)",1750,1752,"/**
 * Masks a function with a given name and boolean value.
 * @param name the name of the function to mask
 * @param value the boolean value to apply for masking
 */","* Set the value of the <code>name</code> property to a <code>boolean</code>.
   * 
   * @param name property name.
   * @param value <code>boolean</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setTimeDuration,"org.apache.hadoop.conf.Configuration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)",1868,1870,"/**
* Masks and logs a time duration.
* @param name identifier for the log entry
* @param value duration value
* @param unit time unit for the duration
*/","* Set the value of <code>name</code> to the given time duration. This
   * is equivalent to <code>set(&lt;name&gt;, value + &lt;time suffix&gt;)</code>.
   * @param name Property name
   * @param value Time duration
   * @param unit Unit of time",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setStorageSize,"org.apache.hadoop.conf.Configuration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)",2039,2041,"/**
 * Masks and stores a value with a given unit.
 * @param name unique identifier for the value
 * @param value numeric value to be masked
 * @param unit storage unit associated with the value
 */","* Sets Storage Size for the specified key.
   *
   * @param name - Key to set.
   * @param value - The numeric value to set.
   * @param unit - Storage Unit to be used.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setPattern,"org.apache.hadoop.conf.Configuration:setPattern(java.lang.String,java.util.regex.Pattern)",2089,2092,"/**
* Masks a string using a regex pattern.
* @param name input string to mask
* @param pattern regex pattern for masking
*/","* Set the given property to <code>Pattern</code>.
   * If the pattern is passed as null, sets the empty pattern which results in
   * further calls to getPattern(...) returning the default value.
   *
   * @param name property name
   * @param pattern new value",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setStrings,"org.apache.hadoop.conf.Configuration:setStrings(java.lang.String,java.lang.String[])",2405,2407,"/**
 * Masks sensitive information in log messages.
 * @param name field name
 * @param values variable number of string values to mask
 */","* Set the array of string values for the <code>name</code> property as 
   * as comma delimited values.  
   * 
   * @param name property name.
   * @param values The values",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setSocketAddr,"org.apache.hadoop.conf.Configuration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)",2578,2580,"/**
 * Masks and processes user information.
 * @param name username to be masked
 * @param addr network address associated with the user
 */","* Set the socket address for the <code>name</code> property as
   * a <code>host:port</code>.
   * @param name property name.
   * @param addr inetSocketAddress addr.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setClass,"org.apache.hadoop.conf.Configuration:setClass(java.lang.String,java.lang.Class,java.lang.Class)",2811,2815,"/**
* Validates class against interface and registers it.
* @param name registration name for the class
* @param theClass class to be validated and registered
* @param xface interface for validation
*/","* Set the value of the <code>name</code> property to the name of a 
   * <code>theClass</code> implementing the given interface <code>xface</code>.
   * 
   * An exception is thrown if <code>theClass</code> does not implement the 
   * interface <code>xface</code>. 
   * 
   * @param name property name.
   * @param theClass property value.
   * @param xface the interface implemented by the named class.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,readFields,org.apache.hadoop.conf.Configuration:readFields(java.io.DataInput),3949,3962,"/**
 * Reads and processes data from input stream.
 * @param in DataInput to read from
 * @throws IOException on I/O errors
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDelete,org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File),169,171,"/**
 * Checks if directory is empty.
 * @param dir directory to check
 * @return true if directory is empty, false otherwise
 */","* Delete a directory and all its contents.  If
   * we return false, the directory may be partially-deleted.
   * (1) If dir is symlink to a file, the symlink is deleted. The file pointed
   *     to by the symlink is not deleted.
   * (2) If dir is symlink to a directory, symlink is deleted. The directory
   *     pointed to by symlink is not deleted.
   * (3) If dir is a normal file, it is deleted.
   * (4) If dir is a normal directory, then dir and all its contents recursively
   *     are deleted.
   * @param dir dir.
   * @return fully delete status.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDeleteContents,"org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File,boolean)",282,316,"/**
* Masks a directory by deleting its contents.
* @param dir the directory to mask
* @param tryGrantPermissions flag to attempt granting permissions before deletion
* @return true if all deletions succeed, false otherwise
*/","* Delete the contents of a directory, not the directory itself.  If
   * we return false, the directory may be partially-deleted.
   * If dir is a symlink to a directory, all the contents of the actual
   * directory pointed to by dir will be deleted.
   *
   * @param dir dir.
   * @param tryGrantPermissions if 'true', try grant +rwx permissions to this
   * and all the underlying directories before trying to delete their contents.
   * @return fully delete contents status.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPath,org.apache.hadoop.fs.shell.FsUsage$Df:processPath(org.apache.hadoop.fs.shell.PathData),129,157,"/**
 * Masks file system data for a given path.
 * @param item PathData object containing file system and path information
 * @throws IOException if an I/O error occurs
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,open,"org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.Path,int)",392,397,"/**
* Opens an input stream for the specified file.
* @param f Path to the file
* @param bufferSize Buffer size for reading
* @return FSDataInputStream for the file
* @throws IOException if an I/O error occurs
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,open,"org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.PathHandle,int)",399,409,"/**
 * Opens a file for reading with a specified buffer size.
 * @param fd file descriptor handle
 * @param bufferSize size of the buffer to use
 * @return FSDataInputStream for reading the file
 * @throws IOException if an I/O error occurs
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,append,"org.apache.hadoop.fs.RawLocalFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",535,545,"/**
 * Opens a file for writing.
 * @param f path of the file
 * @param bufferSize size of buffer
 * @param progress progressable object
 * @return FSDataOutputStream for writing to the file
 * @throws IOException if an I/O error occurs or if appending to a directory
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,truncate,"org.apache.hadoop.fs.RawLocalFileSystem:truncate(org.apache.hadoop.fs.Path,long)",675,698,"/**
* Truncates a file to a specified length.
* @param f the path of the file to truncate
* @param newLength the new length of the file
* @return true if truncation is successful
* @throws IOException if an I/O error occurs
* @throws FileNotFoundException if the file does not exist
* @throws IllegalArgumentException if new length is larger than current size
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,listStatus,org.apache.hadoop.fs.RawLocalFileSystem:listStatus(org.apache.hadoop.fs.Path),729,766,"/**
* Retrieves file statuses for a given path.
* @param f the path to check
* @return array of FileStatus objects or empty if none found
*/","* {@inheritDoc}
   *
   * (<b>Note</b>: Returned list is not sorted in any given order,
   * due to reliance on Java's {@link File#list()} API.)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,deprecatedGetFileLinkStatusInternal,org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileLinkStatusInternal(org.apache.hadoop.fs.Path),1248,1285,"/**
* Deprecated method to mask file status.
* @param f input file path
* @return FileStatus object with masked details or null if target does not exist
*/","* Deprecated. Remains for legacy support. Should be removed when {@link Stat}
   * gains support for Windows and other operating systems.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,fixFileStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:fixFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",532,550,"/**
 * Masks file status based on path.
 * @param orig original FileStatus object
 * @param qualified Path to be used for masking
 * @return masked FileStatus object
 * @throws IOException if an I/O error occurs
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,delete,"org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,boolean)",370,414,"/**
 * Deletes a file or directory recursively.
 * @param channel SFTP channel for operations
 * @param file path of the file or directory to delete
 * @param recursive true to delete directories recursively
 * @return true if deletion is successful, false otherwise
 * @throws IOException on I/O errors
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream),354,356,"/**
* Constructs a BZip2 input stream.
* @param in source InputStream to read from
* @throws IOException if an I/O error occurs
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,"org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",200,211,"/**
* Creates a SplitCompressionInputStream for decompression.
* @param seekableIn input stream that supports seeking
* @param decompressor decompression algorithm to use
* @param start starting position in the stream
* @param end ending position in the stream
* @param readMode mode of reading the stream
* @return SplitCompressionInputStream for decompression
* @throws IOException if seekableIn does not support seeking
*/","* Creates CompressionInputStream to be used to read off uncompressed data
   * in one of the two reading modes. i.e. Continuous or Blocked reading modes
   *
   * @param seekableIn The InputStream
   * @param start The start offset into the compressed stream
   * @param end The end offset into the compressed stream
   * @param readMode Controls whether progress is reported continuously or
   *                 only at block boundaries.
   *
   * @return CompressionInputStream for BZip2 aligned at block boundaries",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,read,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(byte[],int,int)",483,522,"/**
 * Reads data from a byte array.
 * @param b destination buffer
 * @param off starting offset in the buffer
 * @param len number of bytes to read
 * @return number of bytes read or END_OF_BLOCK if at block end
 * @throws IOException if an I/O error occurs
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,add,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Field)",133,159,"/**
* Processes fields annotated with Metric.
* Updates field access and sets metrics if valid.
*/","* Change the declared field {@code field} in {@code source} Object to
   * {@link MutableMetric}",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,init,org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:init(org.apache.commons.configuration2.SubsetConfiguration),57,84,"/**
* Processes configuration settings for tags.
* @param conf configuration object containing subset settings
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.DelegateToFileSystem:getCanonicalServiceName(),262,265,"/**
 * Calls implementation-specific method m1.
 * @return Result of fsImpl.m1()
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getCanonicalServiceName,org.apache.hadoop.fs.FilterFs:getCanonicalServiceName(),312,315,"/**
 * Delegates to myFs's m1 method.
 * @return result of myFs.m1()
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getConf,"org.apache.hadoop.fs.StorageType:getConf(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.StorageType,java.lang.String)",120,123,"/**
 * Masks configuration key by combining storage type and name.
 * @param conf Configuration object
 * @param t Storage type
 * @param name Key component
 * @return Masked configuration key as a string
 */","* Get the configured values for different StorageType.
   * @param conf - absolute or fully qualified path
   * @param t - the StorageType
   * @param name - the sub-name of key
   * @return the file system of the path",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getTransferMode,org.apache.hadoop.fs.ftp.FTPFileSystem:getTransferMode(org.apache.hadoop.conf.Configuration),188,209,"/**
* Determines transfer mode from configuration.
* @param conf Configuration object
* @return Transfer mode code (FTP.BLOCK_TRANSFER_MODE, FTP.STREAM_TRANSFER_MODE, or FTP.COMPRESSED_TRANSFER_MODE)
*/","* Set FTP's transfer mode based on configuration. Valid values are
   * STREAM_TRANSFER_MODE, BLOCK_TRANSFER_MODE and COMPRESSED_TRANSFER_MODE.
   * <p>
   * Defaults to BLOCK_TRANSFER_MODE.
   *
   * @param conf
   * @return",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,setDataConnectionMode,"org.apache.hadoop.fs.ftp.FTPFileSystem:setDataConnectionMode(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)",222,240,"/**
 * Configures FTP client data connection mode.
 * @param client FTPClient instance to configure
 * @param conf Configuration object containing settings
 * @throws IOException if an I/O error occurs
 */","* Set the FTPClient's data connection mode based on configuration. Valid
   * values are ACTIVE_LOCAL_DATA_CONNECTION_MODE,
   * PASSIVE_LOCAL_DATA_CONNECTION_MODE and PASSIVE_REMOTE_DATA_CONNECTION_MODE.
   * <p>
   * Defaults to ACTIVE_LOCAL_DATA_CONNECTION_MODE.
   *
   * @param client
   * @param conf
   * @throws IOException",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getHomeDirValue,"org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration,java.lang.String)",245,249,"/**
* Masks table name with configuration settings.
* @param conf Configuration object
* @param mountTableName original table name
* @return masked table name as a string
*/","* Get the value of the home dir conf value for specified mount table
   * @param conf - from this conf
   * @param mountTableName - the mount table
   * @return home dir value, null if variable is not in conf",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getUMask,org.apache.hadoop.fs.permission.FsPermission:getUMask(org.apache.hadoop.conf.Configuration),325,349,"/**
 * Retrieves file system permissions from configuration.
 * @param conf Configuration object containing permission settings
 * @return FsPermission object with parsed umask, defaulting to DEFAULT_UMASK if invalid or null
 */","* Get the user file creation mask (umask)
   * 
   * {@code UMASK_LABEL} config param has umask value that is either symbolic 
   * or octal.
   * 
   * Symbolic umask is applied relative to file mode creation mask; 
   * the permission op characters '+' clears the corresponding bit in the mask, 
   * '-' sets bits in the mask.
   * 
   * Octal umask, the specified bits are set in the file mode creation mask.
   *
   * @param conf configuration.
   * @return FsPermission UMask.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",791,796,"/**
 * Constructs a DiskBlockFactory.
 * @param keyToBufferDir configuration key for buffer directory
 * @param conf Hadoop configuration object
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecClasses,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClasses(org.apache.hadoop.conf.Configuration),111,147,"/**
* Retrieves list of compression codecs from configuration.
* @param conf Hadoop configuration object
* @return List of CompressionCodec classes
*/","* Get the list of codecs discovered via a Java ServiceLoader, or
   * listed in the configuration. Codecs specified in configuration come
   * later in the returned list, and are considered to override those
   * from the ServiceLoader.
   * @param conf the configuration to look in
   * @return a list of the {@link CompressionCodec} classes",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getDefaultCompressionType,org.apache.hadoop.io.SequenceFile:getDefaultCompressionType(org.apache.hadoop.conf.Configuration),251,255,"/**
* Determines compression type from configuration.
* @param job Configuration object containing settings
* @return CompressionType based on configuration, default is RECORD
*/","* Get the compression type for the reduce outputs
   * @param job the job config to look in
   * @return the kind of compression to use",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,setConf,org.apache.hadoop.net.SocksSocketFactory:setConf(org.apache.hadoop.conf.Configuration),135,142,"/**
* Configures the system with provided settings.
* @param conf Configuration object containing system settings
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,isSingleSwitchByScriptPolicy,org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitchByScriptPolicy(),135,138,"/**
* Checks if network topology script is not configured.
* @return true if script is missing, false otherwise
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createWebAppContext,"org.apache.hadoop.http.HttpServer2:createWebAppContext(org.apache.hadoop.http.HttpServer2$Builder,org.apache.hadoop.security.authorize.AccessControlList,java.lang.String)",834,860,"/**
* Configures and returns a WebAppContext for a web application.
* @param b Builder object containing configuration details
* @param adminsAcl AccessControlList for admin permissions
* @param appDir Directory of the web application
* @return Configured WebAppContext instance
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,stringifySecurityProperty,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:stringifySecurityProperty(java.lang.String),314,333,"/**
 * Masks a property value based on configuration.
 * @param property the name of the property to mask
 * @return masked property string or default if not set
 */","* Turn a security property into a nicely formatted set of <i>name=value</i>
   * strings, allowing for either the property or the configuration not to be
   * set.
   *
   * @param property the property to stringify
   * @return the stringified property",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateHadoopTokenFiles,org.apache.hadoop.security.KDiag:validateHadoopTokenFiles(org.apache.hadoop.conf.Configuration),521,553,"/**
* Processes Hadoop token files from configuration.
* @param conf Configuration object containing settings
*/","* Validate that hadoop.token.files (if specified) exist and are valid.
   * @throws ClassNotFoundException
   * @throws SecurityException
   * @throws NoSuchMethodException
   * @throws KerberosDiagsFailure",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,locateKeystore,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:locateKeystore(),314,339,"/**
* Initializes the key store with credentials.
* @throws IOException if there's an issue creating or loading the key store
*/","* Open up and initialize the keyStore.
   *
   * @throws IOException If there is a problem reading the password file
   * or a problem reading the keystore.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,needsPassword,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:needsPassword(),341,346,"/**
* Checks if CREDENTIAL_PASSWORD_ENV_VAR is not set.
* @return true if not set, false otherwise
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getLocalHostName,org.apache.hadoop.security.SecurityUtil:getLocalHostName(org.apache.hadoop.conf.Configuration),255,272,"/**
* Masks configuration for DNS settings.
* @param conf Configuration object, can be null
* @return Masked IP address string
* @throws UnknownHostException if unable to resolve local host
*/","* Retrieve the name of the current host. Multihomed hosts may restrict the
   * hostname lookup to a specific interface and nameserver with {@link
   * org.apache.hadoop.fs.CommonConfigurationKeysPublic#HADOOP_SECURITY_DNS_INTERFACE_KEY}
   * and {@link org.apache.hadoop.fs.CommonConfigurationKeysPublic#HADOOP_SECURITY_DNS_NAMESERVER_KEY}
   *
   * @param conf Configuration object. May be null.
   * @return
   * @throws UnknownHostException",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getClientPrincipal,"org.apache.hadoop.security.SecurityUtil:getClientPrincipal(java.lang.Class,org.apache.hadoop.conf.Configuration)",404,413,"/**
* Masks function for protocol and configuration.
* @param protocol the protocol class
* @param conf the configuration object
* @return masked user or null if not applicable
*/","* Look up the client principal for a given protocol. It searches all known
   * SecurityInfo providers.
   * @param protocol the protocol class to get the information for
   * @param conf configuration object
   * @return client principal or null if it has no client principal defined.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,needsPassword,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:needsPassword(),308,313,"/**
* Checks if keystore password is null.
* @throws IOException if an I/O error occurs
* @return true if password is null, false otherwise
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getMetricsTimeUnit,org.apache.hadoop.ipc.metrics.RpcMetrics:getMetricsTimeUnit(org.apache.hadoop.conf.Configuration),189,204,"/**
* Retrieves TimeUnit from configuration; defaults to DEFAULT_METRIC_TIME_UNIT if invalid.
* @param conf Configuration object
* @return TimeUnit based on config or default value
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,validateSslConfiguration,org.apache.hadoop.util.curator.ZKCuratorManager:validateSslConfiguration(org.apache.hadoop.conf.Configuration),196,221,"/**
* Validates SSL configuration parameters for ZooKeeper client.
* Throws IOException if any required SSL parameter is missing.
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmed,org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String),1320,1328,"/**
 * Masks a given name using a transformation function.
 * @param name the original name to be masked
 * @return the masked name or null if transformation fails
 */","* Get the value of the <code>name</code> property as a trimmed <code>String</code>, 
   * <code>null</code> if no such property exists. 
   * If the key is deprecated, it returns the value of
   * the first key which replaces the deprecated key and is not null
   * 
   * Values are processed for <a href=""#VariableExpansion"">variable expansion</a> 
   * before being returned. 
   * 
   * @param name the property name.
   * @return the value of the <code>name</code> or its replacing property, 
   *         or null if no such property exists.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setIfUnset,"org.apache.hadoop.conf.Configuration:setIfUnset(java.lang.String,java.lang.String)",1499,1503,"/**
 * Masks a value under a given name.
 * @param name unique identifier for the value
 * @param value the value to be masked
 */","* Sets a property if it is currently unset.
   * @param name the property name
   * @param value the new value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1906,1914,"/**
 * Masks a value based on configuration.
 * @param name configuration key
 * @param defaultValue default value if config not found
 * @param defaultUnit unit of the default value
 * @param returnUnit desired unit for the returned value
 * @return masked value in specified units or default value
 */","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d). If no unit is
   * provided, the default unit is applied.
   *
   * @param name Property name
   * @param defaultValue Value returned if no mapping exists.
   * @param defaultUnit Default time unit if no valid suffix is provided.
   * @param returnUnit The unit used for the returned value.
   * @throws NumberFormatException If the property stripped of its unit is not
   *         a number
   * @return time duration in given time unit",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1916,1924,"/**
 * Fetches and converts a value by name.
 * @param name key for the value to fetch
 * @param defaultValue value to use if none found
 * @param defaultUnit unit of the default value
 * @param returnUnit desired unit for the returned value
 * @return converted value in specified units
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStorageSize,"org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)",1988,2005,"/**
* Converts a storage size value to the specified unit.
* @param name key for fetching the storage size value
* @param defaultValue default value if original is blank or invalid
* @param targetUnit unit to convert the storage size to
* @return converted storage size as a double
*/","* Gets the Storage Size from the config, or returns the defaultValue. The
   * unit of return value is specified in target unit.
   *
   * @param name - Key Name
   * @param defaultValue - Default Value -- e.g. 100MB
   * @param targetUnit - The units that we want result to be in.
   * @return double -- formatted in target Units",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStorageSize,"org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)",2017,2030,"/**
* Converts a storage value to the specified unit.
* @param name name of the storage size
* @param defaultValue default value if conversion fails
* @param targetUnit target unit for conversion
* @return converted value in target units or default value
*/","* Gets storage size from a config file.
   *
   * @param name - Key to read.
   * @param defaultValue - The default value to return in case the key is
   * not present.
   * @param targetUnit - The Storage unit that should be used
   * for the return value.
   * @return - double value in the Storage Unit specified.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPattern,"org.apache.hadoop.conf.Configuration:getPattern(java.lang.String,java.util.regex.Pattern)",2067,2079,"/**
* Retrieves a pattern by name, returning default if invalid.
* @param name the key to fetch the pattern from
* @param defaultValue the fallback pattern to use
* @return compiled Pattern or default if creation fails
*/","* Get the value of the <code>name</code> property as a <code>Pattern</code>.
   * If no such property is specified, or if the specified value is not a valid
   * <code>Pattern</code>, then <code>DefaultValue</code> is returned.
   * Note that the returned value is NOT trimmed by this method.
   *
   * @param name property name
   * @param defaultValue default value
   * @return property value as a compiled Pattern, or defaultValue",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStringCollection,org.apache.hadoop.conf.Configuration:getStringCollection(java.lang.String),2310,2313,"/**
 * Processes and returns a collection of strings based on input name.
 * @param name input string to process
 * @return Collection of processed strings
 */","* Get the comma delimited values of the <code>name</code> property as 
   * a collection of <code>String</code>s.  
   * If no such property is specified then empty collection is returned.
   * <p>
   * This is an optimized version of {@link #getStrings(String)}
   * 
   * @param name property name.
   * @return property value as a collection of <code>String</code>s.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStrings,org.apache.hadoop.conf.Configuration:getStrings(java.lang.String),2324,2327,"/**
 * Processes input name through two methods.
 * @param name input string to process
 * @return processed string array or null if empty
 */","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s.  
   * If no such property is specified then <code>null</code> is returned.
   * 
   * @param name property name.
   * @return property value as an array of <code>String</code>s, 
   *         or <code>null</code>.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStrings,"org.apache.hadoop.conf.Configuration:getStrings(java.lang.String,java.lang.String[])",2339,2346,"/**
 * Retrieves a processed string value by name.
 * @param name the key to look up the value
 * @param defaultValue default values to return if no value is found
 * @return an array of strings, either processed or default
 */","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s.  
   * If no such property is specified then default value is returned.
   * 
   * @param name property name.
   * @param defaultValue The default value
   * @return property value as an array of <code>String</code>s, 
   *         or default value.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStringCollection,org.apache.hadoop.conf.Configuration:getTrimmedStringCollection(java.lang.String),2356,2363,"/**
* Converts a name to a collection of strings.
* @param name input string to process
* @return Collection of processed strings or empty collection if input is null
*/","* Get the comma delimited values of the <code>name</code> property as 
   * a collection of <code>String</code>s, trimmed of the leading and trailing whitespace.  
   * If no such property is specified then empty <code>Collection</code> is returned.
   *
   * @param name property name.
   * @return property value as a collection of <code>String</code>s, or empty <code>Collection</code>",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStrings,org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String),2374,2377,"/**
 * Processes and splits a string using another method.
 * @param name input string to process
 * @return array of strings resulting from the split operation
 */","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s, trimmed of the leading and trailing whitespace.
   * If no such property is specified then an empty array is returned.
   * 
   * @param name property name.
   * @return property value as an array of trimmed <code>String</code>s, 
   *         or empty array.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStrings,"org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String,java.lang.String[])",2389,2396,"/**
* Retrieves a string array for a given name.
* @param name key to fetch the string value
* @param defaultValue default values to return if not found
* @return String array from m1 or defaultValue
*/","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s, trimmed of the leading and trailing whitespace.
   * If no such property is specified then default value is returned.
   * 
   * @param name property name.
   * @param defaultValue The default value
   * @return property value as an array of trimmed <code>String</code>s, 
   *         or default value.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPropsWithPrefix,org.apache.hadoop.conf.Configuration:getPropsWithPrefix(java.lang.String),3032,3043,"/**
* Filters and masks configuration properties.
* @param confPrefix prefix for filtering property names
* @return Map of filtered and masked properties
*/","* Constructs a mapping of configuration and includes all properties that
   * start with the specified configuration prefix.  Property names in the
   * mapping are trimmed to remove the configuration prefix.
   *
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties with prefix stripped",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,appendJSONProperty,"org.apache.hadoop.conf.Configuration:appendJSONProperty(com.fasterxml.jackson.core.JsonGenerator,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3861,3881,"/**
* Writes masked configuration to JSON.
* @param jsonGen JSON generator instance
* @param config configuration settings
* @param name parameter name
* @param redactor object for masking values
*/","* Write property and its attributes as json format to given
   * {@link JsonGenerator}.
   *
   * @param jsonGen json writer
   * @param config configuration
   * @param name property name
   * @throws IOException raised on errors performing I/O.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationUtil.java,getChangedProperties,"org.apache.hadoop.conf.ReconfigurationUtil:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",39,65,"/**
* Compares two configurations and returns property changes.
* @param newConf updated configuration
* @param oldConf original configuration
* @return collection of PropertyChange objects representing differences
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,reconfigureProperty,"org.apache.hadoop.conf.ReconfigurableBase:reconfigureProperty(java.lang.String,java.lang.String)",223,241,"/**
 * Updates a property with a new value if conditions are met.
 * @param property the name of the property to update
 * @param newVal the new value for the property
 * @throws ReconfigurationException if the property cannot be updated
 */","* {@inheritDoc}
   *
   * This method makes the change to this objects {@link Configuration}
   * and calls reconfigurePropertyImpl to update internal data structures.
   * This method cannot be overridden, subclasses should instead override
   * reconfigurePropertyImpl.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,get,org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String),46,51,"/**
 * Overrides and logs the result of a method call.
 * @param name input parameter for the method
 * @return result of the overridden method
 */",* See {@link Configuration#get(String)}.,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,create,"org.apache.hadoop.ha.NodeFencer:create(org.apache.hadoop.conf.Configuration,java.lang.String)",83,90,"/**
* Creates a NodeFencer using configuration.
* @param conf Configuration object
* @param confKey key for configuration setting
* @return NodeFencer instance or null if config is missing
* @throws BadFencingConfigurationException if invalid configuration
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getDefaultMountTableName,org.apache.hadoop.fs.viewfs.ConfigUtil:getDefaultMountTableName(org.apache.hadoop.conf.Configuration),260,263,"/**
 * Retrieves configuration property with default value.
 * @param conf Configuration object
 * @return Value of the specified config key or default if not found
 */","* Get the name of the default mount table to use. If
   * {@link Constants#CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE_NAME_KEY} is specified,
   * it's value is returned. Otherwise,
   * {@link Constants#CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE} is returned.
   *
   * @param conf Configuration to use.
   * @return the name of the default mount table to use.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,getCodecClassName,"org.apache.hadoop.io.erasurecode.CodecUtil:getCodecClassName(org.apache.hadoop.conf.Configuration,java.lang.String)",256,285,"/**
* Returns the codec class for a given erasure code.
* @param conf Configuration object
* @param codec Name of the codec
* @return Class name of the codec or throws IllegalArgumentException if not found
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,isNativeBzip2Loaded,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration),47,70,"/**
* Loads and initializes bzip2 library.
* @param conf configuration object
* @return true if native library is loaded, false otherwise
*/","* Check if native-bzip2 code is loaded &amp; initialized correctly and
   * can be loaded for this job.
   * 
   * @param conf configuration
   * @return <code>true</code> if native-bzip2 is loaded &amp; initialized
   *         and can be loaded for this job, else <code>false</code>",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getDefaultSocketFactory,org.apache.hadoop.net.NetUtils:getDefaultSocketFactory(org.apache.hadoop.conf.Configuration),121,130,"/**
 * Creates a socket factory based on configuration.
 * @param conf configuration object
 * @return SocketFactory instance
 */","* Get the default socket factory as specified by the configuration
   * parameter <tt>hadoop.rpc.socket.factory.default</tt>
   * 
   * @param conf the configuration
   * @return the default socket factory as specified in the configuration or
   *         the JVM default socket factory if the configuration does not
   *         contain a default socket factory property.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,load,org.apache.hadoop.net.TableMapping$RawTableMapping:load(),93,125,"/**
 * Loads a mapping from a file into a map.
 * @return Map containing key-value pairs from the file, or null if file is not configured or readable
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,getUsernameFromConf,org.apache.hadoop.http.lib.StaticUserWebFilter:getUsernameFromConf(org.apache.hadoop.conf.Configuration),133,146,"/**
* Masks deprecated UGI key with new static user.
* @param conf Configuration object
* @return Masked username or default if not found
*/",* Retrieve the static username from the configuration.,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,setEnabledProtocols,org.apache.hadoop.http.HttpServer2$Builder:setEnabledProtocols(org.eclipse.jetty.util.ssl.SslContextFactory),665,696,"/**
* Configures SSL context with specified enabled protocols.
* @param sslContextFactory factory to configure SSL settings
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,parseStaticMapping,org.apache.hadoop.security.Groups:parseStaticMapping(org.apache.hadoop.conf.Configuration),164,191,"/**
* Parses user-group mappings from configuration and updates static map.
* @param conf Configuration object containing user-group overrides
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printConfOpt,org.apache.hadoop.security.KDiag:printConfOpt(java.lang.String),907,909,"/**
* Masks an option by setting its value to unset.
* @param option the name of the option to mask
*/","* Print a configuration option, or {@link #UNSET} if unset.
   *
   * @param option option to print",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,<init>,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore:<init>(org.apache.hadoop.conf.Configuration),868,873,"/**
* Initializes TruststoreKeystore with configuration settings.
* @param conf Configuration object containing SSL keystore and truststore details
*/","* Configuration for the ZooKeeper connection when SSL/TLS is enabled.
     * When a value is not configured, ensure that empty string is set instead of null.
     *
     * @param conf ZooKeeper Client configuration",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getDirContext,org.apache.hadoop.security.LdapGroupsMapping:getDirContext(),653,706,"/**
 * Initializes and returns a DirContext for LDAP operations.
 * @return DirContext object for LDAP communication
 * @throws NamingException if LDAP context initialization fails
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,spawnAutoRenewalThreadForUserCreds,org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForUserCreds(boolean),882,899,"/**
* Renews Kerberos ticket if conditions are met.
* @param force whether to force renewal regardless of current state
*/","* Spawn a thread to do periodic renewals of kerberos credentials. NEVER
   * directly call this method. This method should only be used for ticket cache
   * based kerberos credentials.
   *
   * @param force - used by tests to forcibly spawn thread",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,getHostnameVerifier,org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier(org.apache.hadoop.conf.Configuration),206,210,"/**
* Retrieves and configures SSL hostname verifier.
* @param conf configuration object
* @return HostnameVerifier instance
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getAuthenticationMethod,org.apache.hadoop.security.SecurityUtil:getAuthenticationMethod(org.apache.hadoop.conf.Configuration),730,739,"/**
* Retrieves and parses the authentication method from configuration.
* @param conf Configuration object containing security settings
* @return AuthenticationMethod enum or throws if invalid
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getCodecClasses,"org.apache.hadoop.crypto.CryptoCodec:getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)",105,140,"/**
* Retrieves crypto codec classes for a given cipher suite.
* @param conf configuration object
* @param cipherSuite encryption algorithm suite
* @return list of CryptoCodec classes or null if none configured
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.JceCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),83,102,"/**
* Configures security settings using provided Configuration.
* @param conf configuration object containing security properties
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,setConf,org.apache.hadoop.crypto.random.OsSecureRandom:setConf(org.apache.hadoop.conf.Configuration),83,90,"/**
* Updates configuration and initializes random device path.
* @param conf new Configuration object to set
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,<init>,org.apache.hadoop.crypto.key.KeyProvider:<init>(org.apache.hadoop.conf.Configuration),403,417,"/**
* Initializes KeyProvider with configuration.
* @param conf Hadoop configuration object
*/","* Constructor.
   * 
   * @param conf configuration for the provider",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,"org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",143,146,"/**
* Initializes builder with context and configuration.
* @param context execution context string
* @param conf configuration object containing settings
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getZKAcls,org.apache.hadoop.util.curator.ZKCuratorManager:getZKAcls(org.apache.hadoop.conf.Configuration),97,109,"/**
 * Parses and returns ACL list from configuration.
 * @param conf Configuration object containing ACL settings
 * @return List of ACL objects
 * @throws IOException if parsing fails
 */","* Utility method to fetch the ZK ACLs from the configuration.
   *
   * @param conf configuration.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @return acl list.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getHashType,org.apache.hadoop.util.hash.Hash:getHashType(org.apache.hadoop.conf.Configuration),64,68,"/**
* Retrieves hash type configuration and returns its mask value.
* @param conf Configuration object containing settings
* @return Integer representing the mask value of the hash type
*/","* This utility method converts the name of the configured
   * hash type to a symbolic constant.
   * @param conf configuration
   * @return one of the predefined constants",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getEnumSet,"org.apache.hadoop.conf.Configuration:getEnumSet(java.lang.String,java.lang.Class,boolean)",1803,1809,"/**
* Retrieves and parses an enum set from configuration.
* @param key config key for the enum set
* @param enumClass enum class type
* @param ignoreUnknown flag to ignore unknown enum values
* @return EnumSet of parsed enums or empty set if none found
*/","* Build an enumset from a comma separated list of values.
   * Case independent.
   * Special handling of ""*"" meaning: all values.
   * @param key key to look for
   * @param enumClass class of enum
   * @param ignoreUnknown should unknown values raise an exception?
   * @return a mutable set of the identified enum values declared in the configuration
   * @param <E> enumeration type
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   *           or there are two entries in the enum which differ only by case.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRange,"org.apache.hadoop.conf.Configuration:getRange(java.lang.String,java.lang.String)",2296,2298,"/**
 * Creates an IntegerRanges object using a mask function.
 * @param name parameter name for masking
 * @param defaultValue default value if masking fails
 * @return IntegerRanges object based on the mask result
 */","* Parse the given attribute as a set of integer ranges.
   * @param name the attribute name
   * @param defaultValue the default value if it is not set
   * @return a new set of ranges from the configured value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,<init>,org.apache.hadoop.conf.ConfigRedactor:<init>(org.apache.hadoop.conf.Configuration),44,55,"/**
* Initializes ConfigRedactor with configuration settings.
* @param conf Configuration object containing security settings
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,get,"org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String,java.lang.String)",56,62,"/**
* Retrieves and logs a configuration value.
* @param name the configuration key
* @param defaultValue the default value if not found
* @return the configuration value or default
*/","* See {@link Configuration#get(String, String)}.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,getParentZnode,org.apache.hadoop.ha.ZKFailoverController:getParentZnode(),384,391,"/**
* Constructs a masked path for configuration.
* @return Formatted string representing the masked path
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,boolean)",182,185,"/**
 * Calls m2 with key and boxed boolean.
 * @param key unique identifier
 * @param value boolean value to be boxed
 * @return result of m2 call
 */","* Set optional boolean parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,optLong,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optLong(java.lang.String,long)",202,205,"/**
* Applies mask to a numeric value using a key.
* @param key unique identifier for masking
* @param value numeric value to be masked
* @return masked result of type B
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,optDouble,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optDouble(java.lang.String,double)",232,235,"/**
 * Applies mask to a numeric value using a key.
 * @param key unique identifier for masking
 * @param value numeric value to be masked
 * @return result of the masking operation
 */","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,boolean)",268,271,"/**
 * Calls m2 with key and boxed boolean value.
 * @param key unique identifier
 * @param value boolean flag
 * @return result of m2 call
 */","* Set mandatory boolean option.
   *
   * @see #must(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,mustLong,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustLong(java.lang.String,long)",273,276,"/**
 * Masks and processes a key with a long value.
 * @param key unique identifier
 * @param value numeric value to process
 * @return processed result of type B
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,mustDouble,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustDouble(java.lang.String,double)",283,286,"/**
 * Applies mask to key with double value.
 * @param key unique identifier
 * @param value numeric value to be masked
 * @return processed result of type B
 */","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setDefaultUri,"org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.lang.String)",319,321,"/**
 * Recursively processes URI configuration.
 * @param conf Configuration object
 * @param uri initial URI string
 */","Set the default FileSystem URI in a configuration.
   * @param conf the configuration to alter
   * @param uri the new default filesystem uri",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.net.URI[])",167,175,"/**
* Calls m2 with default settings if none provided.
* @param conf Configuration object
* @param mountTableName name of the mount table
* @param src source path
* @param settings optional settings string
* @param targets variable number of target URIs
*/","* Add nfly link to configuration for the given mount table.
   *
   * @param conf configuration.
   * @param mountTableName mount table.
   * @param src src.
   * @param settings settings.
   * @param targets targets.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,addMappingProvider,"org.apache.hadoop.security.CompositeGroupsMapping:addMappingProvider(java.lang.String,java.lang.Class)",162,168,"/**
 * Registers a group mapping service provider.
 * @param providerName name of the provider
 * @param providerClass class of the provider implementation
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,setBlockSize,"org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setBlockSize(org.apache.hadoop.conf.Configuration,int)",126,128,"/**
 * Sets the block size for BZip2 compression.
 * @param conf Configuration object to modify
 * @param blockSize size of the compression block
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,setWorkFactor,"org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setWorkFactor(org.apache.hadoop.conf.Configuration,int)",135,137,"/**
* Sets BZip2 compression work factor.
* @param conf Configuration object to modify
* @param workFactor integer representing the work factor (1-9)
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,setIndexInterval,"org.apache.hadoop.io.MapFile$Writer:setIndexInterval(org.apache.hadoop.conf.Configuration,int)",380,382,"/**
 * Sets the interval in the configuration.
 * @param conf Configuration object to update
 * @param interval new interval value
 */","* Sets the index interval and stores it in conf.
     * @see #getIndexInterval()
     *
     * @param conf configuration.
     * @param interval interval.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setPingInterval,"org.apache.hadoop.ipc.Client:setPingInterval(org.apache.hadoop.conf.Configuration,int)",175,178,"/**
* Sets IPC ping interval in configuration.
* @param conf Configuration object to update
* @param pingInterval new ping interval value
*/","* set the ping interval value in configuration
   * 
   * @param conf Configuration
   * @param pingInterval the ping interval",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setConnectTimeout,"org.apache.hadoop.ipc.Client:setConnectTimeout(org.apache.hadoop.conf.Configuration,int)",233,235,"/**
* Sets IPC client connection timeout.
* @param conf Configuration object to update
* @param timeout Timeout value in milliseconds
*/","* set the connection timeout value in configuration
   * 
   * @param conf Configuration
   * @param timeout the socket connect timeout value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setIsNestedMountPointSupported,"org.apache.hadoop.fs.viewfs.ConfigUtil:setIsNestedMountPointSupported(org.apache.hadoop.conf.Configuration,boolean)",279,281,"/**
* Sets nested mount point support in configuration.
* @param conf Configuration object to update
* @param isNestedMountPointSupported flag indicating support for nested mount points
*/","* Set the bool value isNestedMountPointSupported in config.
   * @param conf - from this conf
   * @param isNestedMountPointSupported - whether nested mount point is supported",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String[])",242,248,"/**
* Processes a key with associated values.
* @param key unique identifier for the operation
* @param values variable number of values to process with the key
* @return result of processing as type B
*/","* Set an array of string values as optional parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String[])",318,324,"/**
* Processes a key with associated values.
* @param key unique identifier for the operation
* @param values variable number of values associated with the key
* @return result of the processing operation
*/","* Set a string array as mandatory option.
   *
   * @see #must(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updateConnectAddr,"org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)",2624,2629,"/**
* Masks and processes an address.
* @param name associated with the address
* @param addr original socket address
* @return processed InetSocketAddress
*/","* Set the socket address a client can use to connect for the
   * <code>name</code> property as a <code>host:port</code>.  The wildcard
   * address is replaced with the local host's address.
   * @param name property name.
   * @param addr InetSocketAddress of a listener to store in the given property
   * @return InetSocketAddress for clients to connect",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,setProtocolEngine,"org.apache.hadoop.ipc.RPC:setProtocolEngine(org.apache.hadoop.conf.Configuration,java.lang.Class,java.lang.Class)",211,217,"/**
* Sets RPC engine in configuration.
* @param conf Configuration object
* @param protocol Protocol class
* @param engine Engine class to set
*/","* Set a protocol to use a non-default RpcEngine if one
   * is not specified in the configuration.
   * @param conf configuration to use
   * @param protocol the protocol interface
   * @param engine the RpcEngine impl",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,delete,"org.apache.hadoop.fs.RawLocalFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",707,721,"/**
 * Checks and processes a file or directory.
 * @param p path to the file or directory
 * @param recursive whether to process recursively
 * @return true if processed successfully, false otherwise
 * @throws IOException if an I/O error occurs
 */","* Delete the given path to a file or directory.
   * @param p the path to delete
   * @param recursive to delete sub-directories
   * @return true if the file or directory and all its contents were deleted
   * @throws IOException if p is non-empty and recursive is false",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDeleteContents,org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File),267,269,"/**
 * Checks directory existence.
 * @param dir directory to check
 * @return true if directory exists, false otherwise
 */","* Delete the contents of a directory, not the directory itself.  If
   * we return false, the directory may be partially-deleted.
   * If dir is a symlink to a directory, all the contents of the actual
   * directory pointed to by dir will be deleted.
   *
   * @param dir dir.
   * @return fullyDeleteContents Status.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileStatus(org.apache.hadoop.fs.Path),567,574,"/**
* Retrieves file status.
* @param f file path
* @return FileStatus object
*/","* {@inheritDoc}
   *
   * If the given path is a symlink(mount link), the path will be resolved to a
   * target path and it will get the resolved path's FileStatus object. It will
   * not be represented as a symlink and isDirectory API returns true if the
   * resolved path is a directory, false otherwise.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:listStatus(org.apache.hadoop.fs.Path),611,628,"/**
* Retrieves file statuses from a given path.
* @param f file path to resolve
* @return FileStatus array of resolved files
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file is not found
* @throws IOException for other I/O errors
*/","* {@inheritDoc}
   *
   * Note: listStatus considers listing from fallbackLink if available. If the
   * same directory path is present in configured mount path as well as in
   * fallback fs, then only the fallback path will be listed in the returned
   * result except for link.
   *
   * If any of the the immediate children of the given path f is a symlink(mount
   * link), the returned FileStatus object of that children would be represented
   * as a symlink. It will not be resolved to the target path and will not get
   * the target path FileStatus object. The target path will be available via
   * getSymlink on that children's FileStatus object. Since it represents as
   * symlink, isDirectory on that children's FileStatus will return false.
   * This behavior can be changed by setting an advanced configuration
   * fs.viewfs.mount.links.as.symlinks to false. In this case, mount points will
   * be represented as non-symlinks and all the file/directory attributes like
   * permissions, isDirectory etc will be assigned from it's resolved target
   * directory/file.
   *
   * If you want to get the FileStatus of target path for that children, you may
   * want to use GetFileStatus API with that children's symlink path. Please see
   * {@link ViewFileSystem#getFileStatus(Path f)}
   *
   * Note: In ViewFileSystem, by default the mount links are represented as
   * symlinks.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,read,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(),524,528,"/**
* Reads a single byte from the input stream.
* @return the byte value or -1 if end of stream is reached
* @throws IOException if an I/O error occurs
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,<init>,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:<init>(java.lang.Object,org.apache.hadoop.metrics2.lib.MutableMetricsFactory)",62,74,"/**
* Initializes metrics registry from source object.
* @param source the object to extract metrics from
* @param factory mutable metrics factory instance
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkOneDirWithMode,"org.apache.hadoop.fs.RawLocalFileSystem:mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission)",777,802,"/**
 * Sets file permissions and creates directory.
 * @param p Path to the directory
 * @param p2f File object for the directory
 * @param permission Desired file permissions
 * @return true if successful, false otherwise
 * @throws IOException on I/O errors
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1226,1236,"/**
* Creates a file output stream with specified parameters.
* @param f file path
* @param overwrite flag to allow overwriting existing files
* @param bufferSize buffer size for I/O operations
* @param replication number of data replications
* @param blockSize block size for the file
* @param progress object to monitor progress
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize the size of the buffer to be used.
   * @param progress to report progress.
   * @throws IOException IO failure
   * @return output stream.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getUMask,org.apache.hadoop.fs.FileContext:getUMask(),585,587,"/**
 * Returns file system permission.
 * Uses umask if set, otherwise retrieves from configuration.
 * @return FsPermission object
 */","* 
   * @return the umask of this FileContext",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,createFactory,"org.apache.hadoop.fs.store.DataBlocks:createFactory(java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String)",129,144,"/**
* Creates a BlockFactory based on the specified type.
* @param keyToBufferDir directory for key to buffer mapping
* @param configuration configuration settings
* @param name type of block factory to create
* @return BlockFactory instance or throws IllegalArgumentException if unsupported
*/","* Create a factory.
   *
   * @param keyToBufferDir Key to buffer directory config for a FS.
   * @param configuration  factory configurations.
   * @param name           factory name -the option from {@link CommonConfigurationKeys}.
   * @return the factory, ready to be initialized.
   * @throws IllegalArgumentException if the name is unknown.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,<init>,org.apache.hadoop.io.compress.CompressionCodecFactory:<init>(org.apache.hadoop.conf.Configuration),177,191,"/**
* Initializes the compression codec factory with configuration.
* @param conf Hadoop configuration object
*/","* Find the codecs specified in the config value io.compression.codecs 
   * and register them. Defaults to gzip and deflate.
   *
   * @param conf configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",81,90,"/**
* Initializes a keystore provider with given URI and configuration.
* @param uri location of the keystore
* @param conf configuration settings
* @throws IOException if initialization fails
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,replacePattern,"org.apache.hadoop.security.SecurityUtil:replacePattern(java.lang.String[],java.lang.String)",235,243,"/**
* Masks a component with the hostname.
* @param components array of path components
* @param hostname server name or IP address
* @return masked string combining component and processed hostname
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,registerProtocolAndImpl,"org.apache.hadoop.ipc.RPC$Server:registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)",1101,1135,"/**
* Registers a protocol with RPC and logs details.
* @param rpcKind type of RPC communication
* @param protocolClass class representing the protocol
* @param protocolImpl implementation of the protocol
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,getKeyProviderUri,"org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration,java.lang.String)",71,79,"/**
 * Retrieves and returns a masked URI from configuration.
 * @param conf Configuration object containing settings
 * @param configKeyName key for the configuration setting
 * @return URI object or null if not found or invalid
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmed,"org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String,java.lang.String)",1340,1343,"/**
* Retrieves value by name, returning default if null.
* @param name key to look up
* @param defaultValue value to return if lookup fails
* @return retrieved value or default
*/","* Get the value of the <code>name</code> property as a trimmed <code>String</code>, 
   * <code>defaultValue</code> if no such property exists. 
   * See @{Configuration#getTrimmed} for more details.
   * 
   * @param name          the property name.
   * @param defaultValue  the property default value.
   * @return              the value of the <code>name</code> or defaultValue
   *                      if it is not set.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInt,"org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)",1546,1555,"/**
 * Converts a string to an integer using a mask.
 * @param name key for value retrieval
 * @param defaultValue default integer value if conversion fails
 * @return masked integer value or default if null
 */","* Get the value of the <code>name</code> property as an <code>int</code>.
   *   
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>int</code>,
   * then an error is thrown.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as an <code>int</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLong,"org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)",1599,1608,"/**
* Converts a named value to a long using a mask.
* @param name key for retrieving the value string
* @param defaultValue default long value if conversion fails
* @return converted long value or default if null or invalid
*/","* Get the value of the <code>name</code> property as a <code>long</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>long</code>,
   * then an error is thrown.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>long</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLongBytes,"org.apache.hadoop.conf.Configuration:getLongBytes(java.lang.String,long)",1624,1629,"/**
* Converts string to long using traditional binary prefix.
* @param name key for fetching value
* @param defaultValue default long value if conversion fails
* @return converted long value or default
*/","* Get the value of the <code>name</code> property as a <code>long</code> or
   * human readable format. If no such property exists, the provided default
   * value is returned, or if the specified value is not a valid
   * <code>long</code> or human readable format, then an error is thrown. You
   * can use the following suffix (case insensitive): k(kilo), m(mega), g(giga),
   * t(tera), p(peta), e(exa)
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>long</code>,
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getFloat,"org.apache.hadoop.conf.Configuration:getFloat(java.lang.String,float)",1671,1676,"/**
* Retrieves and converts a named configuration value to a float.
* @param name configuration key
* @param defaultValue default float value if not found
* @return converted float value or default if conversion fails
*/","* Get the value of the <code>name</code> property as a <code>float</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>float</code>,
   * then an error is thrown.
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>float</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDouble,"org.apache.hadoop.conf.Configuration:getDouble(java.lang.String,double)",1700,1705,"/**
* Retrieves and parses a double value by name.
* @param name key for the value to retrieve
* @param defaultValue default double value if retrieval fails
* @return parsed double value or default if not found
*/","* Get the value of the <code>name</code> property as a <code>double</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>double</code>,
   * then an error is thrown.
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>double</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getBoolean,"org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)",1727,1742,"/**
* Converts string to boolean with a default value.
* @param name key for the value in configuration
* @param defaultValue fallback boolean value if conversion fails
* @return parsed boolean or default value if invalid
*/","* Get the value of the <code>name</code> property as a <code>boolean</code>.  
   * If no such property is specified, or if the specified value is not a valid
   * <code>boolean</code>, then <code>defaultValue</code> is returned.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @return property value as a <code>boolean</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClass,"org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class)",2730,2739,"/**
* Fetches class by name, returns default if not found.
* @param name class name as string
* @param defaultValue default Class object to return if class not found
* @return Class object or default if class is not found
*/","* Get the value of the <code>name</code> property as a <code>Class</code>.  
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * @param name the conf key name.
   * @param defaultValue default value.
   * @return property value as a <code>Class</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setBooleanIfUnset,"org.apache.hadoop.conf.Configuration:setBooleanIfUnset(java.lang.String,boolean)",1759,1761,"/**
 * Masks a function with a given name and boolean value.
 * @param name the name of the function to mask
 * @param value the boolean value to apply for masking
 */","* Set the given property, if it is currently unset.
   * @param name property name
   * @param value new value",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)",1884,1886,"/**
 * Calls m1 with identical start and end units.
 * @param name unique identifier
 * @param defaultValue default value if not found
 * @param unit time unit for both start and end
 * @return result from m1 method
 */","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param defaultValue Value returned if no mapping exists.
   * @param unit Unit to convert the stored property, if it exists.
   * @throws NumberFormatException If the property stripped of its unit is not
   *         a number
   * @return time duration in given time unit",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)",1888,1890,"/**
 * Calls another method with additional parameters.
 * @param name parameter name
 * @param defaultValue default value if not found
 * @param unit time unit for conversion
 * @return result of the called method
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialProviderFactory.java,getProviders,org.apache.hadoop.security.alias.CredentialProviderFactory:getProviders(org.apache.hadoop.conf.Configuration),73,112,"/**
* Loads credential providers from specified paths.
* @param conf Configuration object containing provider paths
* @return List of CredentialProvider instances
* @throws IOException if loading fails or no factory is found
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderFactory.java,getProviders,org.apache.hadoop.crypto.key.KeyProviderFactory:getProviders(org.apache.hadoop.conf.Configuration),62,81,"/**
* Loads KeyProviders from configuration paths.
* @param conf Configuration object containing key provider paths
* @return List of KeyProvider instances
* @throws IOException if loading fails or no KeyProviderFactory is found
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseServiceUserNames,"org.apache.hadoop.ipc.DecayRpcScheduler:parseServiceUserNames(java.lang.String,org.apache.hadoop.conf.Configuration)",409,413,"/**
* Retrieves and returns a set of service users from configuration.
* @param ns namespace identifier
* @param conf configuration object
* @return set of service user names
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java,getPackages,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getPackages(),65,73,"/**
* Masks packages based on AVRO_REFLECT_PACKAGES.
* Initializes and populates the 'packages' set.
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,getRawCoderNames,"org.apache.hadoop.io.erasurecode.CodecUtil:getRawCoderNames(org.apache.hadoop.conf.Configuration,java.lang.String)",168,174,"/**
* Retrieves raw coders configuration for a given codec.
* @param conf Configuration object
* @param codecName name of the codec
* @return array of raw coder configurations
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getSaslProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getSaslProperties(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.SaslRpcServer$QualityOfProtection)",136,150,"/**
* Generates SASL properties for authentication.
* @param conf configuration object
* @param configKey key for fetching QOP values
* @param defaultQOP default QualityOfProtection setting
* @return Map containing SASL properties
*/","* A util function to retrieve specific additional sasl property from config.
   * Used by subclasses to read sasl properties used by themselves.
   * @param conf the configuration
   * @param configKey the config key to look for
   * @param defaultQOP the default QOP if the key is missing
   * @return sasl property associated with the given key",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,getKeyFiles,org.apache.hadoop.ha.SshFenceByTcpPort:getKeyFiles(),220,222,"/**
 * Retrieves masked identities configuration.
 * @return Collection of identity keys or empty collection if none found
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,refresh,org.apache.hadoop.security.authorize.ProxyServers:refresh(org.apache.hadoop.conf.Configuration),35,45,"/**
* Masks proxy servers from configuration.
* @param conf Configuration object containing server settings
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInts,org.apache.hadoop.conf.Configuration:getInts(java.lang.String),1567,1574,"/**
* Converts string array to integer array using mask function.
* @param name input string
* @return integer array from masked string values
*/","* Get the value of the <code>name</code> property as a set of comma-delimited
   * <code>int</code> values.
   * 
   * If no such property exists, an empty array is returned.
   * 
   * @param name property name
   * @return property value interpreted as an array of comma-delimited
   *         <code>int</code> values",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurations,"org.apache.hadoop.conf.Configuration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)",1971,1978,"/**
 * Converts names to durations using specified time unit.
 * @param name base name for conversion
 * @param unit time unit for duration calculation
 * @return array of durations in the specified unit
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClasses,"org.apache.hadoop.conf.Configuration:getClasses(java.lang.String,java.lang.Class[])",2703,2718,"/**
* Retrieves an array of Class objects based on a name.
* @param name the key to fetch class names
* @param defaultValue default classes if none found
* @return array of Class objects or default if not found
*/","* Get the value of the <code>name</code> property
   * as an array of <code>Class</code>.
   * The value of the property specifies a list of comma separated class names.  
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * @param name the property name.
   * @param defaultValue default value.
   * @return property value as a <code>Class[]</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getFile,"org.apache.hadoop.conf.Configuration:getFile(java.lang.String,java.lang.String)",2861,2874,"/**
* Finds a valid directory for storing a file.
* @param dirsProp property containing directory paths
* @param path file path to be stored
* @return File object representing the chosen directory
* @throws IOException if no valid directory is found
*/","* Get a local file name under a directory named in <i>dirsProp</i> with
   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,
   * then one is chosen based on <i>path</i>'s hash code.  If the selected
   * directory does not exist, an attempt is made to create it.
   *
   * @param dirsProp directory in which to locate the file.
   * @param path file-path.
   * @return local file under the directory with the given path.
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,<init>,org.apache.hadoop.io.serializer.SerializationFactory:<init>(org.apache.hadoop.conf.Configuration),58,67,"/**
* Initializes SerializationFactory with configurations.
* @param conf Configuration object containing serialization settings
*/","* <p>
   * Serializations are found by reading the <code>io.serializations</code>
   * property from <code>conf</code>, which is a comma-delimited list of
   * classnames.
   * </p>
   *
   * @param conf configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,setConf,org.apache.hadoop.security.SaslPropertiesResolver:setConf(org.apache.hadoop.conf.Configuration),60,73,"/**
* Configures security settings for the service.
* @param conf configuration object containing security parameters
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,getFilterParams,"org.apache.hadoop.security.http.RestCsrfPreventionFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)",229,232,"/**
 * Masks configuration values based on prefix.
 * @param conf Configuration object containing settings
 * @param confPrefix Prefix to identify settings to mask
 * @return Map of masked configuration key-value pairs
 */","* Constructs a mapping of configuration properties to be used for filter
   * initialization.  The mapping includes all properties that start with the
   * specified configuration prefix.  Property names in the mapping are trimmed
   * to remove the configuration prefix.
   *
   * @param conf configuration to read
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties to be used for filter
   *     initialization",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/XFrameOptionsFilter.java,getFilterParams,"org.apache.hadoop.security.http.XFrameOptionsFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)",80,83,"/**
 * Masks configuration values based on prefix.
 * @param conf Configuration object containing settings
 * @param confPrefix Prefix to identify config keys to mask
 * @return Map of masked configuration key-value pairs
 */","* Constructs a mapping of configuration properties to be used for filter
   * initialization.  The mapping includes all properties that start with the
   * specified configuration prefix.  Property names in the mapping are trimmed
   * to remove the configuration prefix.
   *
   * @param conf configuration to read
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties to be used for filter
   *     initialization",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,propagateOptions,"org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",356,374,"/**
* Masks configuration properties with a prefix.
* @param builder FSBuilder instance to apply properties
* @param conf Configuration object containing properties
* @param prefix Prefix for filtering properties
* @param mandatory Flag indicating if properties are mandatory
*/","* Propagate options to any builder, converting everything with the
   * prefix to an option where, if there were 2+ dot-separated elements,
   * it is converted to a schema.
   * <pre>
   *   fs.example.s3a.option becomes ""s3a.option""
   *   fs.example.fs.io.policy becomes ""fs.io.policy""
   *   fs.example.something becomes ""something""
   * </pre>
   * @param builder builder to modify
   * @param conf configuration to read
   * @param prefix prefix to scan/strip
   * @param mandatory are the options to be mandatory or optional?",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationUtil.java,parseChangedProperties,"org.apache.hadoop.conf.ReconfigurationUtil:parseChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",67,70,"/**
 * Compares two configurations and returns property changes.
 * @param newConf new configuration object
 * @param oldConf old configuration object
 * @return collection of PropertyChange objects
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,printConf,"org.apache.hadoop.conf.ReconfigurationServlet:printConf(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable)",88,130,"/**
* Generates a form for reconfiguring settings.
* @param out PrintWriter to output HTML
* @param reconf Reconfigurable object for configuration management
*/",* Print configuration options that can be changed.,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLink,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",67,70,"/**
 * Calls m2 with additional parameters.
 * @param conf configuration object
 * @param src source string
 * @param target target URI
 */","* Add a link to the config for the default mount table
   * @param conf - add the link to this conf
   * @param src - the src path name
   * @param target - the target URI link",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMergeSlash,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.net.URI)",91,93,"/**
 * Recursively calls m2 with modified configuration.
 * @param conf Configuration object
 * @param target Target URI
 */","* Add a LinkMergeSlash to the config for the default mount table.
   *
   * @param conf configuration.
   * @param target targets.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkFallback,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.net.URI)",114,116,"/**
 * Calls m2 with configuration, processed result from m1, and target URI.
 * @param conf Configuration object
 * @param target Target URI
 */","* Add a LinkFallback to the config for the default mount table.
   *
   * @param conf configuration.
   * @param target targets.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMerge,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.net.URI[])",137,139,"/**
 * Calls m2 with modified configuration and targets.
 * @param conf Configuration object
 * @param targets Array of target URIs
 */","* Add a LinkMerge to the config for the default mount table.
   *
   * @param conf configuration.
   * @param targets targets array.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setHomeDirConf,"org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String)",209,212,"/**
* Calls overloaded m2 with additional parameter.
* @param conf configuration object
* @param homedir home directory path
*/","* Add config variable for homedir for default mount table
   * @param conf - add to this conf
   * @param homedir - the home dir path starting with slash",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getHomeDirValue,org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration),235,237,"/**
 * Calls another method with configuration and default value.
 * @param conf application configuration
 * @return result of m2 with configuration and computed default
 */","* Get the value of the home dir conf value for default mount table
   * @param conf - from this conf
   * @return home dir value, null if variable is not in conf",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createEncoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createEncoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",94,104,"/**
* Creates an ErasureEncoder using configuration and options.
* @param conf Configuration object
* @param options ErasureCodecOptions specifying encoder settings
* @return ErasureEncoder instance
*/","* Create encoder corresponding to given codec.
   * @param options Erasure codec options
   * @param conf configuration.
   * @return erasure encoder",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createDecoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createDecoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",112,122,"/**
* Creates an ErasureDecoder instance.
* @param conf configuration settings
* @param options erasure codec options
* @return ErasureDecoder object initialized with given config and options
*/","* Create decoder corresponding to given codec.
   * @param options Erasure codec options
   * @param conf configuration.
   * @return erasure decoder",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getLibraryName,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getLibraryName(org.apache.hadoop.conf.Configuration),72,78,"/**
* Determines compression method based on configuration.
* @param conf configuration object
* @return compressor name or library name
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2CompressorType,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2CompressorType(org.apache.hadoop.conf.Configuration),86,90,"/**
* Returns compressor class based on configuration.
* @param conf configuration settings
* @return Bzip2Compressor or BZip2DummyCompressor class
*/","* Return the appropriate type of the bzip2 compressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the bzip2 compressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2DecompressorType,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2DecompressorType(org.apache.hadoop.conf.Configuration),109,113,"/**
* Determines decompressor class based on configuration.
* @param conf Configuration object
* @return Class of decompressor to use
*/","* Return the appropriate type of the bzip2 decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the bzip2 decompressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2Decompressor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Decompressor(org.apache.hadoop.conf.Configuration),121,124,"/**
* Returns a decompressor based on configuration.
* @param conf configuration settings
* @return Bzip2Decompressor if enabled, otherwise BZip2DummyDecompressor
*/","* Return the appropriate implementation of the bzip2 decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the bzip2 decompressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getSocketFactory,"org.apache.hadoop.net.NetUtils:getSocketFactory(org.apache.hadoop.conf.Configuration,java.lang.Class)",96,110,"/**
* Creates a SocketFactory based on configuration.
* @param conf Configuration object
* @param clazz Class for which the socket factory is created
* @return SocketFactory instance or default if not specified
*/","* Get the socket factory for the given class according to its
   * configuration parameter
   * <tt>hadoop.rpc.socket.factory.class.&lt;ClassName&gt;</tt>. When no
   * such parameter exists then fall back on the default socket factory as
   * configured by <tt>hadoop.rpc.socket.factory.class.default</tt>. If
   * this default socket factory is not configured, then fall back on the JVM
   * default socket factory.
   * 
   * @param conf the configuration
   * @param clazz the class (usually a {@link VersionedProtocol})
   * @return a socket factory",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,resolve,org.apache.hadoop.net.TableMapping$RawTableMapping:resolve(java.util.List),127,147,"/**
* Masks node names using a mapping.
* @param names list of node names to mask
* @return list of masked node names
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings(),149,160,"/**
* Reloads mapping from m1, logs error if null.
* @param none
* @return void
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,initFilter,"org.apache.hadoop.http.lib.StaticUserWebFilter:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",122,128,"/**
* Applies a static user filter to the container.
* @param container FilterContainer instance
* @param conf Configuration object
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,<init>,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String,java.lang.String,java.lang.String)",520,524,"/**
* Initializes a Hadoop Zookeeper Factory with specified security settings.
* @param zkPrincipal ZooKeeper principal
* @param kerberosPrincipal Kerberos principal
* @param kerberosKeytab Path to Kerberos keytab file
*/","* Constructor for the helper class to configure the ZooKeeper client connection.
     * @param zkPrincipal Optional.
     * @param kerberosPrincipal Optional. Use along with kerberosKeytab.
     * @param kerberosKeytab Optional. Use along with kerberosPrincipal.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,goUpGroupHierarchy,"org.apache.hadoop.security.LdapGroupsMapping:goUpGroupHierarchy(java.util.Set,int,java.util.Set)",592,618,"/**
* Recursively retrieves LDAP groups and members.
* @param groupDNs set of group DNs to search
* @param goUpHierarchy levels to traverse up the hierarchy
* @param groups set to store retrieved groups
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,init,org.apache.hadoop.security.ssl.SSLFactory:init(),194,204,"/**
 * Initializes SSL context and configures it for client or server mode.
 * @throws GeneralSecurityException if a security issue occurs
 * @throws IOException if an I/O error occurs
 */","* Initializes the factory.
   *
   * @throws  GeneralSecurityException thrown if an SSL initialization error
   * happened.
   * @throws IOException thrown if an IO error happened while reading the SSL
   * configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,isSimpleAuthentication,org.apache.hadoop.security.KDiag:isSimpleAuthentication(org.apache.hadoop.conf.Configuration),427,430,"/**
 * Checks security configuration mask.
 * @param conf Configuration object
 * @return true if mask is valid, false otherwise
 */","* Is the authentication method of this configuration ""simple""?
   * @param conf configuration to check
   * @return true if auth is simple (i.e. not kerberos)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HadoopKerberosName.java,setConfiguration,org.apache.hadoop.security.HadoopKerberosName:setConfiguration(org.apache.hadoop.conf.Configuration),63,85,"/**
* Configures security rules based on authentication type.
* @param conf Configuration object containing security settings
* @throws IOException if Kerberos realm cannot be retrieved
*/","* Set the static configuration to get and evaluate the rules.
   * <p>
   * IMPORTANT: This method does a NOP if the rules have been set already.
   * If there is a need to reset the rules, the {@link KerberosName#setRules(String)}
   * method should be invoked directly.
   * 
   * @param conf the new configuration
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuthMethods,"org.apache.hadoop.ipc.Server:getAuthMethods(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.conf.Configuration)",3472,3491,"/**
* Determines authentication methods based on configuration.
* @param secretManager manager for handling secrets
* @param conf application configuration
* @return list of enabled AuthMethod objects
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getInstance,"org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)",59,88,"/**
* Selects and returns a CryptoCodec compatible with the given CipherSuite.
* @param conf configuration settings
* @param cipherSuite desired cryptographic suite
* @return selected CryptoCodec or null if none found
*/","* Get crypto codec for specified algorithm/mode/padding.
   * 
   * @param conf
   *          the configuration
   * @param cipherSuite
   *          algorithm/mode/padding
   * @return CryptoCodec the codec object. Null value will be returned if no
   *         crypto codec classes with cipher suite configured.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,<init>,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(org.apache.hadoop.crypto.key.JavaKeyStoreProvider),115,127,"/**
* Copy constructor for JavaKeyStoreProvider.
* @param other the instance to copy from
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getInstance,org.apache.hadoop.util.hash.Hash:getInstance(org.apache.hadoop.conf.Configuration),92,95,"/**
 * Generates a hash based on configuration.
 * @param conf Configuration object
 * @return Hash object
 */","* Get a singleton instance of hash function of a type
   * defined in the configuration.
   * @param conf current configuration
   * @return defined hash type, or null if type is invalid",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,buildFlagSet,"org.apache.hadoop.fs.impl.FlagSet:buildFlagSet(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",318,325,"/**
* Creates a FlagSet from configuration.
* @param enumClass the enum class type
* @param conf configuration object
* @param key config key prefix
* @param ignoreUnknown flag to ignore unknown values
* @return FlagSet containing parsed enum values
*/","* Build a FlagSet from a comma separated list of values.
   * Case independent.
   * Special handling of ""*"" meaning: all values.
   * @param enumClass class of enum
   * @param conf configuration
   * @param key key to look for
   * @param ignoreUnknown should unknown values raise an exception?
   * @param <E> enumeration type
   * @return a mutable FlagSet
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   * or there are two entries in the enum which differ only by case.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,bind,"org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int,org.apache.hadoop.conf.Configuration,java.lang.String)",690,720,"/**
* Binds server socket to specified address or range of ports.
* @param socket ServerSocket instance to bind
* @param address InetSocketAddress specifying host and port
* @param backlog maximum length of the pending connection queue
* @param conf Configuration object for range parsing
* @param rangeConf configuration key for port range
* @throws IOException if binding fails
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,"org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer,org.apache.hadoop.conf.Configuration)",3622,3640,"/**
 * Masks sensitive configuration properties in XML output.
 * @param propertyName property to mask; null masks all
 * @param out Writer to output the masked XML
 * @param config Configuration object containing sensitive data
 * @throws IOException if an I/O error occurs during processing
 * @throws IllegalArgumentException for invalid input parameters
 */","* Write out the non-default properties in this configuration to the
   * given {@link Writer}.
   * <ul>
   * <li>
   * When property name is not empty and the property exists in the
   * configuration, this method writes the property and its attributes
   * to the {@link Writer}.
   * </li>
   *
   * <li>
   * When property name is null or empty, this method writes all the
   * configuration properties and their attributes to the {@link Writer}.
   * </li>
   *
   * <li>
   * When property name is not empty but the property doesn't exist in
   * the configuration, this method throws an {@link IllegalArgumentException}.
   * </li>
   * </ul>
   * @param propertyName xml property name.
   * @param out the writer to write to.
   * @param config configuration.
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpConfiguration,"org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.io.Writer)",3832,3850,"/**
* Masks configuration properties and writes to output.
* @param config Configuration object containing sensitive data
* @param out Writer for outputting the masked JSON
*/","*  Writes out all properties and their attributes (final and resource) to
   *  the given {@link Writer}, the format of the output would be,
   *
   *  <pre>
   *  { ""properties"" :
   *      [ { key : ""key1"",
   *          value : ""value1"",
   *          isFinal : ""key1.isFinal"",
   *          resource : ""key1.resource"" },
   *        { key : ""key2"",
   *          value : ""value2"",
   *          isFinal : ""ke2.isFinal"",
   *          resource : ""key2.resource"" }
   *       ]
   *   }
   *  </pre>
   *
   *  It does not output the properties of the configuration object which
   *  is loaded from an input stream.
   *  <p>
   *
   * @param config the configuration
   * @param out the Writer to write to
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,<init>,org.apache.hadoop.conf.ConfigurationWithLogging:<init>(org.apache.hadoop.conf.Configuration),37,41,"/**
* Initializes configuration with logging.
* @param conf base configuration object
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,confirmFormat,org.apache.hadoop.ha.ZKFailoverController:confirmFormat(),301,317,"/**
* Prompts user confirmation to clear failover info from ZooKeeper.
* @return true if user confirms, false otherwise
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,int)",192,195,"/**
 * Calls m1 with given key and value.
 * @param key unique identifier string
 * @param value integer value
 * @return result of m1 call
 */","* Set optional int parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,long)",197,200,"/**
 * Masks a value using a key.
 * @param key unique identifier for masking
 * @param value value to be masked
 * @return result of the masking operation
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,float)",212,215,"/**
 * Converts and delegates key-value pair to another method.
 * @param key unique identifier
 * @param value numeric value to be converted to long
 * @return result of the delegated method
 */","* Set optional float parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,double)",222,225,"/**
 * Converts and delegates a masked function call.
 * @param key unique identifier string
 * @param value numeric value to be converted to long
 * @return result of the delegated function
 */","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,int)",293,296,"/**
* Masks a function with a given key and value.
* @param key unique identifier for the function
* @param value integer value associated with the function
* @return result of the masked function execution
*/","* Set mandatory int option.
   *
   * @see #must(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,long)",298,301,"/**
 * Masks a function with a given key and value.
 * @param key unique identifier for masking
 * @param value associated value for masking
 * @return result of the masked function
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,float)",303,306,"/**
* Converts and delegates a string key with a float value to another method.
* @param key unique identifier as a string
* @param value numeric value to be converted to long
* @return result of m1 method call
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,double)",308,311,"/**
 * Converts and processes a key with a double value.
 * @param key unique identifier string
 * @param value numeric value to be converted to long
 * @return result of processing by m1 method
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])",177,180,"/**
* Calls another method with additional parameters.
* @param conf configuration object
* @param src source string
* @param targets target URIs
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,handleEmptyDstDirectoryOnWindows,"org.apache.hadoop.fs.RawLocalFileSystem:handleEmptyDstDirectoryOnWindows(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.Path,java.io.File)",646,673,"/**
 * Masks a file by moving it from source to destination.
 * @param src source path
 * @param srcFile source file object
 * @param dst destination path
 * @param dstFile destination file object
 * @return true if operation successful, false otherwise
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsAnnotations.java,makeSource,org.apache.hadoop.metrics2.lib.MetricsAnnotations:makeSource(java.lang.Object),36,39,"/**
 * Wraps an object in a MetricsSource with default factory.
 * @param source the object to be wrapped
 * @return MetricsSource instance
 */","* Make an metrics source from an annotated object.
   * @param source  the annotated object.
   * @return a metrics source",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsAnnotations.java,newSourceBuilder,org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder(java.lang.Object),41,44,"/**
 * Creates a MetricsSourceBuilder with default factory.
 * @param source data source object
 * @return MetricsSourceBuilder instance
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkOneDir,org.apache.hadoop.fs.RawLocalFileSystem:mkOneDir(java.io.File),773,775,"/**
* Checks file mask against given path.
* @param p2f target file to check
* @return true if mask matches, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirsWithOptionalPermission,"org.apache.hadoop.fs.RawLocalFileSystem:mkdirsWithOptionalPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",818,839,"/**
 * Masks file with specified permissions.
 * @param f target file path
 * @param permission file permissions to apply
 * @return true if masking is successful, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)",1205,1211,"/**
* Opens a file output stream with specified parameters.
* @param f file path
* @param overwrite flag to overwrite existing file
* @param bufferSize size of buffer in bytes
* @param replication number of block replicas
* @param blockSize size of each block in bytes
* @return FSDataOutputStream for writing to the file
*/","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize the size of the buffer to be used.
   * @throws IOException IO failure
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,create,"org.apache.hadoop.fs.FileContext:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",681,706,"/**
* Creates a file with specified options.
* @param f file path
* @param createFlag flags for file creation
* @param opts additional options
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/","* Create or overwrite file on indicated path and returns an output stream for
   * writing into the file.
   * 
   * @param f the file name to open
   * @param createFlag gives the semantics of create; see {@link CreateFlag}
   * @param opts file creation options; see {@link Options.CreateOpts}.
   *          <ul>
   *          <li>Progress - to report progress on the operation - default null
   *          <li>Permission - umask is applied against permission: default is
   *          FsPermissions:getDefault()
   * 
   *          <li>CreateParent - create missing parent path; default is to not
   *          to create parents
   *          <li>The defaults for the following are SS defaults of the file
   *          server implementing the target path. Not all parameters make sense
   *          for all kinds of file system - eg. localFS ignores Blocksize,
   *          replication, checksum
   *          <ul>
   *          <li>BufferSize - buffersize used in FSDataOutputStream
   *          <li>Blocksize - block size for file blocks
   *          <li>ReplicationFactor - replication for blocks
   *          <li>ChecksumParam - Checksum parameters. server default is used
   *          if not specified.
   *          </ul>
   *          </ul>
   * 
   * @return {@link FSDataOutputStream} for created file
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If file <code>f</code> already exists
   * @throws FileNotFoundException If parent of <code>f</code> does not exist
   *           and <code>createParent</code> is false
   * @throws ParentNotDirectoryException If parent of <code>f</code> is not a
   *           directory.
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is not valid",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,mkdir,"org.apache.hadoop.fs.FileContext:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",799,816,"/**
* Creates a directory with specified permissions.
* @param dir target directory path
* @param permission directory permissions
* @param createParent flag to create parent directories if necessary
*/","* Make(create) a directory and all the non-existent parents.
   * 
   * @param dir - the dir to make
   * @param permission - permissions is set permission{@literal &~}umask
   * @param createParent - if true then missing parent dirs are created if false
   *          then parent must exist
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If directory <code>dir</code> already
   *           exists
   * @throws FileNotFoundException If parent of <code>dir</code> does not exist
   *           and <code>createParent</code> is false
   * @throws ParentNotDirectoryException If parent of <code>dir</code> is not a
   *           directory
   * @throws UnsupportedFileSystemException If file system for <code>dir</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>dir</code> is not valid",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,main,org.apache.hadoop.io.compress.CompressionCodecFactory:main(java.lang.String[]),301,358,"/**
* Handles file compression and decompression based on command-line arguments.
* @param args command-line arguments specifying input/output files and operations
*/","* A little test program.
   * @param args arguments.
   * @throws Exception exception.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.KeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",47,50,"/**
* Constructs a KeyStoreProvider.
* @param uri location of the key store
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",54,57,"/**
 * Initializes a LocalKeyStoreProvider with URI and configuration.
 * @param uri location of the key store
 * @param conf configuration settings
 * @throws IOException if initialization fails
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getServerPrincipal,"org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.lang.String)",185,196,"/**
* Masks the principal configuration with a given hostname.
* @param principalConfig original principal configuration string
* @param hostname target hostname for masking
* @return masked principal configuration or original if invalid
*/","* Convert Kerberos principal name pattern to valid Kerberos principal
   * names. It replaces hostname pattern with hostname, which should be
   * fully-qualified domain name. If hostname is null or ""0.0.0.0"", it uses
   * dynamically looked-up fqdn of the current host instead.
   * 
   * @param principalConfig
   *          the Kerberos principal name conf value to convert
   * @param hostname
   *          the fully-qualified domain name used for substitution
   * @return converted Kerberos principal name
   * @throws IOException if the client address cannot be determined",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getServerPrincipal,"org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.net.InetAddress)",212,227,"/**
 * Masks the principal configuration with the client's domain name.
 * @param principalConfig the original principal configuration string
 * @param addr the client's InetAddress
 * @return masked principal configuration or original if no replacement is needed
 * @throws IOException if the client address is null or pattern replacement fails
 */","* Convert Kerberos principal name pattern to valid Kerberos principal names.
   * This method is similar to {@link #getServerPrincipal(String, String)},
   * except 1) the reverse DNS lookup from addr to hostname is done only when
   * necessary, 2) param addr can't be null (no default behavior of using local
   * hostname when addr is null).
   * 
   * @param principalConfig
   *          Kerberos principal name pattern to convert
   * @param addr
   *          InetAddress of the host used for substitution
   * @return converted Kerberos principal name
   * @throws IOException if the client address cannot be determined",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,addProtocol,"org.apache.hadoop.ipc.RPC$Server:addProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)",1218,1222,"/**
* Registers an RPC service.
* @param rpcKind type of RPC communication
* @param protocolClass interface class of the protocol
* @param protocolImpl implementation instance of the protocol
* @return Server instance for method chaining
*/","* Add a protocol to the existing server.
     * @param rpcKind - input rpcKind
     * @param protocolClass - the protocol class
     * @param protocolImpl - the impl of the protocol that will be called
     * @return the server (for convenience)",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,createKeyProvider,"org.apache.hadoop.util.KMSUtil:createKeyProvider(org.apache.hadoop.conf.Configuration,java.lang.String)",59,64,"/**
* Creates a KeyProvider using the specified configuration key.
* @param conf Configuration object containing settings
* @param configKeyName key for accessing the configuration
* @return KeyProvider instance or null if URI is invalid
* @throws IOException if an I/O error occurs during creation
*/","* Creates a new KeyProvider from the given Configuration
   * and configuration key name.
   *
   * @param conf Configuration
   * @param configKeyName The configuration key name
   * @return new KeyProvider, or null if no provider was found.
   * @throws IOException if the KeyProvider is improperly specified in
   *                             the Configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,getKeyProviderUri,org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration),66,69,"/**
 * Retrieves KMS URI from configuration.
 * @param conf application configuration
 * @return KMS URI or null if not found
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultUri,org.apache.hadoop.fs.FileSystem:getDefaultUri(org.apache.hadoop.conf.Configuration),297,304,"/**
* Constructs a URI from configuration.
* @param conf Configuration object
* @return Constructed URI with valid scheme
*/","* Get the default FileSystem URI from a configuration.
   * @param conf the configuration to use
   * @return the uri of the default filesystem",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,setConf,org.apache.hadoop.io.compress.PassthroughCodec:setConf(org.apache.hadoop.conf.Configuration),95,102,"/**
* Sets configuration and processes file extension.
* @param conf Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKinitExecutable,org.apache.hadoop.security.KDiag:validateKinitExecutable(),715,727,"/**
* Executes Kerberos kinit command and checks if it's executable.
* Logs path and issues warning if not found in PATH.
*/","* A cursory look at the {@code kinit} executable.
   *
   * If it is an absolute path: it must exist with a size > 0.
   * If it is just a command, it has to be on the path. There's no check
   * for that -but the PATH is printed out.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getSocketAddr,"org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,int)",2566,2570,"/**
* Creates an InetSocketAddress from a given name or default values.
* @param name the name to resolve for the address
* @param defaultAddress fallback IP address if resolution fails
* @param defaultPort fallback port number
* @return InetSocketAddress instance using resolved or default values
*/","* Get the socket address for <code>name</code> property as a
   * <code>InetSocketAddress</code>.
   * @param name property name.
   * @param defaultAddress the default value
   * @param defaultPort the default port
   * @return InetSocketAddress",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updateConnectAddr,"org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)",2596,2614,"/**
 * Resolves InetSocketAddress using host and address properties.
 * @param hostProperty property key for the host
 * @param addressProperty property key for the address
 * @param defaultAddressValue default address value if not found
 * @param addr initial InetSocketAddress
 * @return resolved InetSocketAddress or result of m4 with default address
 */","* Set the socket address a client can use to connect for the
   * <code>name</code> property as a <code>host:port</code>.  The wildcard
   * address is replaced with the local host's address. If the host and address
   * properties are configured the host component of the address will be combined
   * with the port component of the addr to generate the address.  This is to allow
   * optional control over which host name is used in multi-home bind-host
   * cases where a host can have multiple names
   * @param hostProperty the bind-host configuration name
   * @param addressProperty the service address configuration name
   * @param defaultAddressValue the service default address configuration value
   * @param addr InetSocketAddress of the service listener
   * @return InetSocketAddress for clients to connect",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,initializeMetadataCache,org.apache.hadoop.fs.HarFileSystem:initializeMetadataCache(org.apache.hadoop.conf.Configuration),108,113,"/**
* Initializes metadata cache with specified size from configuration.
* @param conf Configuration object containing cache settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,build,org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:build(),2971,2990,"/**
* Opens a file asynchronously with specified parameters.
* @return CompletableFuture of FSDataInputStream
*/","* Perform the open operation.
     *
     * @return a future to the input stream.
     * @throws IOException early failure to open
     * @throws UnsupportedOperationException if the specific operation
     * is not supported.
     * @throws IllegalArgumentException if the parameters are not valid.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,build,org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:build(),4941,4958,"/**
* Opens a file stream with specified parameters.
* @return CompletableFuture of FSDataInputStream or throws IOException
*/","* Perform the open operation.
     * Returns a future which, when get() or a chained completion
     * operation is invoked, will supply the input stream of the file
     * referenced by the path/path handle.
     * @return a future to the input stream.
     * @throws IOException early failure to open
     * @throws UnsupportedOperationException if the specific operation
     * is not supported.
     * @throws IllegalArgumentException if the parameters are not valid.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,setConfigurationFromURI,"org.apache.hadoop.fs.sftp.SFTPFileSystem:setConfigurationFromURI(java.net.URI,org.apache.hadoop.conf.Configuration)",97,135,"/**
 * Configures SFTP settings from URI and configuration.
 * @param uriInfo URI containing SFTP information
 * @param conf Configuration object to store settings
 * @throws IOException if host is null or decoding fails
 */","* Set configuration from UI.
   *
   * @param uri
   * @param conf
   * @throws IOException",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,connect,org.apache.hadoop.fs.sftp.SFTPFileSystem:connect(),143,157,"/**
* Establishes an SFTP channel using configuration settings.
* @return ChannelSftp object representing the established connection
* @throws IOException if connection fails
*/","* Connecting by using configuration parameters.
   *
   * @return An FTPClient instance
   * @throws IOException",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path)",180,185,"/**
* Initializes a ChecksumFSInputChecker with specified filesystem and path.
* @param fs the filesystem containing the file
* @param file the path to the file to check
* @throws IOException if an I/O error occurs
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,initFromFS,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:initFromFS(),113,116,"/**
* Sets buffer size using file system configuration.
* @param bufferSize size of the buffer to be set
*/",* Initialize from a filesystem.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,create,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.shell.PathData,boolean)",515,544,"/**
* Creates a file output stream for the given path.
* @param item PathData object containing file details
* @param lazyPersist flag indicating lazy persistence
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,open,org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.Path),996,999,"/**
* Opens file input stream with specified buffer size.
* @param f file path
* @return FSDataInputStream for the file
* @throws IOException if an I/O error occurs
*/","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file to open
   * @throws IOException IO failure
   * @return input stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,open,org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.PathHandle),1014,1017,"/**
 * Opens a file stream with specified buffer size.
 * @param fd file handle for the path
 * @return FSDataInputStream for reading the file
 * @throws IOException if an I/O error occurs
 */","* Open an FSDataInputStream matching the PathHandle instance. The
   * implementation may encode metadata in PathHandle to address the
   * resource directly and verify that the resource referenced
   * satisfies constraints specified at its construciton.
   * @param fd PathHandle object returned by the FS authority.
   * @throws InvalidPathHandleException If {@link PathHandle} constraints are
   *                                    not satisfied
   * @throws IOException IO failure
   * @throws UnsupportedOperationException If {@link #open(PathHandle, int)}
   *                                       not overridden by subclass
   * @return input stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,append,org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path),1516,1519,"/**
 * Opens file for writing with specified buffer size.
 * @param f file path
 * @return FSDataOutputStream for writing
 * @throws IOException if an I/O error occurs
 */","* Append to an existing file (optional operation).
   * Same as
   * {@code append(f, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,
   *     IO_FILE_BUFFER_SIZE_DEFAULT), null)}
   * @param f the existing file to be appended.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default).
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,append,"org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,boolean)",1558,1561,"/**
 * Opens a file for writing with specified buffer size and block appending option.
 * @param f file path to write to
 * @param appendToNewBlock flag to append data to a new block if true
 * @return FSDataOutputStream for writing to the file
 * @throws IOException if an I/O error occurs
 */","* Append to an existing file (optional operation).
   * @param f the existing file to be appended.
   * @param appendToNewBlock whether to append data to a new block
   * instead of the end of the last partial block
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default).
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,setConf,org.apache.hadoop.fs.ChecksumFileSystem:setConf(org.apache.hadoop.conf.Configuration),83,93,"/**
* Overrides m1 to set bytesPerChecksum from configuration.
* @param conf Configuration object containing file system settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getSumBufferSize,"org.apache.hadoop.fs.ChecksumFileSystem:getSumBufferSize(int,int)",156,163,"/**
 * Calculates the masked buffer size.
 * @param bytesPerSum number of bytes per sum
 * @param bufferSize initial buffer size
 * @return adjusted buffer size based on calculations
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem$Cache:<init>(org.apache.hadoop.conf.Configuration),3657,3663,"/**
* Initializes cache with configuration settings.
* @param conf configuration object containing parallel count
*/","* Instantiate. The configuration is used to read the
     * count of permits issued for concurrent creation
     * of filesystem instances.
     * @param conf configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Metadata)",2948,2974,"/**
* Initializes a Sorter with specified parameters.
* @param fs FileSystem instance
* @param comparator RawComparator for key comparison
* @param keyClass Class of the sort keys
* @param valClass Class of the values to be sorted
* @param conf Configuration settings
* @param metadata Metadata associated with sorting
*/","* Sort and merge using an arbitrary {@link RawComparator}.
     * @param fs input FileSystem.
     * @param comparator input RawComparator.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.
     * @param metadata input metadata.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBlockSize,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBlockSize(org.apache.hadoop.conf.Configuration),130,133,"/**
* Retrieves block size for Bzip2 compression.
* @param conf configuration object
* @return block size value, default is DEFAULT_BLOCK_SIZE
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getWorkFactor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getWorkFactor(org.apache.hadoop.conf.Configuration),139,142,"/**
* Retrieves the work factor for bzip2 compression.
* @param conf Configuration object containing settings
* @return Work factor value, defaulting to DEFAULT_WORK_FACTOR if not set
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createOutputStream,"org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",60,67,"/**
* Creates a compression output stream.
* @param out underlying output stream
* @param compressor compressor to use
* @return CompressionOutputStream for writing compressed data
* @throws IOException if an I/O error occurs
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createInputStream,"org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",86,93,"/**
 * Creates a compression input stream using a decompressor.
 * @param in the input stream to be decompressed
 * @param decompressor the decompressor to use
 * @return a CompressionInputStream for reading decompressed data
 * @throws IOException if an I/O error occurs
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createOutputStream,"org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",82,94,"/**
* Creates a compression output stream with specified buffer size and overhead.
* @param out the output stream to compress data into
* @param compressor the compressor to use for compression
* @return CompressionOutputStream configured with given parameters
* @throws IOException if an I/O error occurs
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createInputStream,"org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",146,153,"/**
* Creates a compression input stream with the specified decompressor.
* @param in input stream to be compressed
* @param decompressor decompressor to use for compression
* @return CompressionInputStream for reading compressed data
* @throws IOException if an I/O error occurs
*/","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createDecompressor,org.apache.hadoop.io.compress.Lz4Codec:createDecompressor(),170,176,"/**
* Creates a LZ4 decompressor with configured buffer size.
* @return Lz4Decompressor instance initialized with buffer size from configuration
*/","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createOutputStream,"org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",122,130,"/**
 * Creates a compression output stream.
 * @param out the output stream to wrap
 * @param compressor the compressor to use
 * @return CompressionOutputStream for writing compressed data
 * @throws IOException if an I/O error occurs
 */","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to, to have it 
   *         compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,"org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",177,186,"/**
* Creates a compression input stream for the given input and decompressor.
* @param in input stream to be compressed
* @param decompressor decompressor used for compression
* @return CompressionInputStream based on configuration
* @throws IOException if an I/O error occurs
*/","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}, and return a 
   * stream for uncompressed data.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressionLevel,org.apache.hadoop.io.compress.ZStandardCodec:getCompressionLevel(org.apache.hadoop.conf.Configuration),88,92,"/**
* Retrieves ZSTD compression level from configuration.
* @param conf Configuration object
* @return Compression level as an integer
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getBufferSize(org.apache.hadoop.conf.Configuration),108,111,"/**
* Retrieves ZSTD buffer size from configuration.
* @param conf Configuration object
* @return Buffer size integer value
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createOutputStream,"org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",51,60,"/**
* Creates a compression output stream.
* @param out the output stream to wrap
* @param compressor the compressor to use
* @return CompressionOutputStream instance
* @throws IOException if an I/O error occurs
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createInputStream,"org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",83,93,"/**
 * Creates a CompressionInputStream using the provided InputStream and Decompressor.
 * @param in the input stream to be compressed
 * @param decompressor the decompressor to use; if null, defaults to m1()
 * @return a CompressionInputStream for reading compressed data
 * @throws IOException if an I/O error occurs
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createOutputStream,"org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",82,94,"/**
* Creates a compressed output stream.
* @param out the underlying output stream
* @param compressor the compressor to use
* @return CompressionOutputStream for writing compressed data
* @throws IOException if an I/O error occurs
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createCompressor,org.apache.hadoop.io.compress.SnappyCodec:createCompressor(),111,117,"/**
 * Creates a Snappy compressor with configured buffer size.
 * @return SnappyCompressor instance initialized with bufferSize
 */","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createInputStream,"org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",143,150,"/**
* Creates a compression input stream using the specified decompressor.
* @param in input stream to be compressed
* @param decompressor decompressor to use for compression
* @return CompressionInputStream for reading compressed data
*/","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createDecompressor,org.apache.hadoop.io.compress.SnappyCodec:createDecompressor(),167,173,"/**
* Creates a Snappy decompressor with configured buffer size.
* @return SnappyDecompressor instance initialized with specified buffer size
*/","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration)",114,118,"/**
 * Copies data from input stream to output stream with specified buffer size.
 * @param in input stream source
 * @param out output stream destination
 * @param conf configuration object
 * @throws IOException if an I/O error occurs
 */","* Copies from one stream to another. <strong>closes the input and output streams 
   * at the end</strong>.
   *
   * @param in InputStrem to read from
   * @param out OutputStream to write to
   * @param conf the Configuration object.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration,boolean)",130,134,"/**
 * Copies data from InputStream to OutputStream with configurable buffer size.
 * @param in source input stream
 * @param out destination output stream
 * @param conf configuration object
 * @param close whether to close streams after operation
 * @throws IOException if I/O error occurs
 */","* Copies from one stream to another.
   *
   * @param in InputStream to read from
   * @param out OutputStream to write to
   * @param conf the Configuration object
   * @param close whether or not close the InputStream and 
   * OutputStream at the end. The streams are closed in the finally clause.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getChunkBufferSize,org.apache.hadoop.io.file.tfile.TFile:getChunkBufferSize(org.apache.hadoop.conf.Configuration),142,145,"/**
* Retrieves configuration value for buffer size.
* @param conf Configuration object
* @return Buffer size in bytes, default is 1MB if not configured
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFSInputBufferSize,org.apache.hadoop.io.file.tfile.TFile:getFSInputBufferSize(org.apache.hadoop.conf.Configuration),147,149,"/**
* Retrieves input buffer size from configuration.
* @param conf Configuration object
* @return Buffer size in bytes, default is 256KB
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFSOutputBufferSize,org.apache.hadoop.io.file.tfile.TFile:getFSOutputBufferSize(org.apache.hadoop.conf.Configuration),151,153,"/**
* Retrieves output buffer size from configuration.
* @param conf Configuration object
* @return Buffer size in bytes, default is 256KB
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getBufferSize,org.apache.hadoop.io.SequenceFile:getBufferSize(org.apache.hadoop.conf.Configuration),1738,1740,"/**
 * Retrieves the file buffer size from configuration.
 * @param conf Configuration object
 * @return File buffer size as an integer
 */",Get the configured buffer size,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,setConf,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration),156,166,"/**
* Configures the component using provided settings.
* @param conf Configuration object containing settings
*/","* Set the configuration and extract the configuration parameters of interest
     * @param conf the new configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createHttpChannelConnector,"org.apache.hadoop.http.HttpServer2$Builder:createHttpChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)",566,581,"/**
* Creates a ServerConnector for the given server and config.
* @param server the server instance
* @param httpConfig HTTP configuration settings
* @return configured ServerConnector
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,doOp,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:doOp(org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$ProviderCallable,int,boolean)",167,234,"/**
* Executes an operation using a failover mechanism.
* @param op the operation to execute
* @param currPos current provider position
* @param isIdempotent indicates if the operation is idempotent
* @return result of the operation
* @throws IOException if all providers fail or an I/O error occurs
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,<init>,org.apache.hadoop.crypto.key.KeyProvider$Options:<init>(org.apache.hadoop.conf.Configuration),341,344,"/**
* Initializes options with configuration settings.
* @param conf Configuration object containing cipher and bit length settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,getBufferSize,org.apache.hadoop.crypto.CryptoStreamUtils:getBufferSize(org.apache.hadoop.conf.Configuration),65,68,"/**
* Retrieves crypto buffer size from configuration.
* @param conf Configuration object
* @return Buffer size as integer
*/","* Read crypto buffer size.
   *
   * @param conf configuration.
   * @return hadoop.security.crypto.buffer.size.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,parseNumLevels,"org.apache.hadoop.ipc.CallQueueManager:parseNumLevels(java.lang.String,org.apache.hadoop.conf.Configuration)",391,411,"/**
* Retrieves IPC priority levels from configuration.
* @param ns namespace for configuration keys
* @param conf Configuration object
* @return number of priority levels, throws exception if invalid
*/","* Read the number of levels from the configuration.
   * This will affect the FairCallQueue's overall capacity.
   * @throws IllegalArgumentException on invalid queue count",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getRpcTimeout,org.apache.hadoop.ipc.RPC:getRpcTimeout(org.apache.hadoop.conf.Configuration),829,832,"/**
 * Retrieves the IPC client RPC timeout from configuration.
 * @param conf Configuration object containing settings
 * @return Timeout value in milliseconds
 */","* Get the RPC time from configuration;
   * If not set in the configuration, return the default value.
   *
   * @param conf Configuration
   * @return the RPC timeout (ms)",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getPingInterval,org.apache.hadoop.ipc.Client:getPingInterval(org.apache.hadoop.conf.Configuration),187,190,"/**
* Retrieves IPC ping interval from configuration.
* @param conf Configuration object
* @return Ping interval in milliseconds
*/","* Get the ping interval from configuration;
   * If not set in the configuration, return the default value.
   * 
   * @param conf Configuration
   * @return the ping interval",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getRpcTimeout,org.apache.hadoop.ipc.Client:getRpcTimeout(org.apache.hadoop.conf.Configuration),221,226,"/**
* Retrieves RPC timeout from configuration.
* @param conf Configuration object
* @return Timeout value in milliseconds or 0 if negative
*/","* The time after which a RPC will timeout.
   *
   * @param conf Configuration
   * @return the timeout period in milliseconds.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedTimeCostProvider.java,init,"org.apache.hadoop.ipc.WeightedTimeCostProvider:init(java.lang.String,org.apache.hadoop.conf.Configuration)",65,90,"/**
* Initializes weights based on configuration settings.
* @param namespace configuration namespace
* @param conf Configuration object containing weight values
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,"org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration)",94,96,"/**
* Constructs a LineReader with an input stream and configuration.
* @param in input stream to read from
* @param conf configuration containing buffer size settings
* @throws IOException if there is an I/O error initializing the reader
*/","* Create a line reader that reads from the given stream using the
   * <code>io.file.buffer.size</code> specified in the given
   * <code>Configuration</code>.
   * @param in input stream
   * @param conf configuration
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,"org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration,byte[])",138,144,"/**
* Initializes a LineReader with the given input stream and configuration.
* @param in input stream to read from
* @param conf configuration settings
* @param recordDelimiterBytes delimiter for record separation
* @throws IOException if an I/O error occurs
*/","* Create a line reader that reads from the given stream using the
   * <code>io.file.buffer.size</code> specified in the given
   * <code>Configuration</code>, and using a custom delimiter of array of
   * bytes.
   * @param in input stream
   * @param conf configuration
   * @param recordDelimiterBytes The delimiter
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getInt,"org.apache.hadoop.conf.ConfigurationWithLogging:getInt(java.lang.String,int)",87,92,"/**
* Calls superclass method and logs the result.
* @param name key for fetching value
* @param defaultValue default value if not found
* @return fetched or default value
*/","* See {@link Configuration#getInt(String, int)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,setConf,org.apache.hadoop.ha.HAAdmin:setConf(org.apache.hadoop.conf.Configuration),337,345,"/**
* Overrides base method to set RPC timeout from configuration.
* @param conf Configuration object containing settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,getSshConnectTimeout,org.apache.hadoop.ha.SshFenceByTcpPort:getSshConnectTimeout(),215,218,"/**
 * Retrieves connection timeout configuration.
 * @return configured connection timeout value
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,getGracefulFenceTimeout,org.apache.hadoop.ha.FailoverController:getGracefulFenceTimeout(org.apache.hadoop.conf.Configuration),82,86,"/**
* Retrieves graceful fence timeout from configuration.
* @param conf Configuration object
* @return Timeout value as an integer
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,getRpcTimeoutToNewActive,org.apache.hadoop.ha.FailoverController:getRpcTimeoutToNewActive(org.apache.hadoop.conf.Configuration),88,92,"/**
 * Retrieves configuration value for HA timeout.
 * @param conf Configuration object
 * @return Timeout value in milliseconds
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,setTimeout,"org.apache.hadoop.fs.ftp.FTPFileSystem:setTimeout(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)",173,177,"/**
* Configures FTP client with specified timeout.
* @param client FTPClient instance to configure
* @param conf Configuration object containing timeout settings
*/","* Set the FTPClient's timeout based on configuration.
   * FS_FTP_TIMEOUT is set as timeout (defaults to DEFAULT_TIMEOUT).",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FSBuilderSupport.java,getLong,"org.apache.hadoop.fs.impl.FSBuilderSupport:getLong(java.lang.String,long)",77,93,"/**
* Retrieves a long value for a given key with a default.
* @param key configuration key
* @param defVal default long value if retrieval fails
* @return long value or default if invalid format
*/","* Get a long value with resilience to unparseable values.
   * @param key key to log
   * @param defVal default value
   * @return long value",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,canBeSafelyDeleted,org.apache.hadoop.fs.shell.Delete$Rm:canBeSafelyDeleted(org.apache.hadoop.fs.shell.PathData),129,149,"/**
* Determines if a path should be deleted based on file count and user confirmation.
* @param item PathData object representing the directory to check
* @return true if deletion should proceed, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FileSystem:getDefaultBlockSize(),2753,2757,"/**
* Deprecated method to get block size.
* @return Block size in bytes (default: 32 MB)
*/","* Return the number of bytes that large input files should be optimally
   * be split into to minimize I/O time.
   * @deprecated use {@link #getDefaultBlockSize(Path)} instead
   * @return default block size.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,<init>,"org.apache.hadoop.fs.DF:<init>(java.io.File,org.apache.hadoop.conf.Configuration)",49,52,"/**
* Constructs a DF object with specified file path and configuration.
* @param path file system path to monitor
* @param conf configuration settings for the file system
* @throws IOException if an I/O error occurs
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getInterval,org.apache.hadoop.fs.GetSpaceUsed$Builder:getInterval(),58,67,"/**
* Returns the file system du interval.
* @return interval value or default if not set
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getJitter,org.apache.hadoop.fs.GetSpaceUsed$Builder:getJitter(),118,129,"/**
* Returns jitter value for space used calculation.
* @return jitter value or default if not set
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,ensureInitialized,org.apache.hadoop.io.nativeio.NativeIO:ensureInitialized(),1039,1048,"/**
* Initializes the cache timeout for UID to User mapping.
* Sets up the cache if not already initialized.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,"org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration,boolean)",106,128,"/**
* Initializes ShellBasedIdMapping with configuration.
* @param conf Hadoop configuration object
* @param constructFullMapAtInit flag to determine map construction at initialization
* @throws IOException if file operations fail
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",71,78,"/**
* Initializes a DelegationTokenSecretManager with configuration settings.
* @param conf Configuration object containing security settings
* @param tokenKind Type of delegation tokens to manage
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseDecayPeriodMillis,"org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayPeriodMillis(java.lang.String,org.apache.hadoop.conf.Configuration)",359,377,"/**
* Retrieves scheduler period from configuration.
* @param ns namespace for configuration keys
* @param conf Configuration object
* @return period in milliseconds, throws exception if invalid
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceInit,org.apache.hadoop.util.JvmPauseMonitor:serviceInit(org.apache.hadoop.conf.Configuration),74,79,"/**
* Configures warning and info thresholds from configuration.
* @param conf Configuration object containing threshold settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getLong,"org.apache.hadoop.conf.ConfigurationWithLogging:getLong(java.lang.String,long)",97,102,"/**
* Calls superclass method and logs the result.
* @param name key for fetching the value
* @param defaultValue default value if not found
* @return fetched or default value
*/","* See {@link Configuration#getLong(String, long)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,<init>,"org.apache.hadoop.ha.HealthMonitor:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)",115,135,"/**
* Initializes a HealthMonitor with configuration and target.
* @param conf Configuration settings for the monitor
* @param target HAServiceTarget to monitor
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,initBloomFilter,org.apache.hadoop.io.BloomMapFile$Writer:initBloomFilter(org.apache.hadoop.conf.Configuration),166,180,"/**
* Initializes Bloom filter parameters from configuration.
* @param conf Configuration object containing settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getFloat,"org.apache.hadoop.conf.ConfigurationWithLogging:getFloat(java.lang.String,float)",77,82,"/**
* Retrieves a float value by name with logging.
* @param name the key for the value to retrieve
* @param defaultValue the default value if not found
* @return the retrieved float value or default
*/","* See {@link Configuration#getFloat(String, float)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseDecayFactor,"org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayFactor(java.lang.String,org.apache.hadoop.conf.Configuration)",339,357,"/**
* Retrieves decay factor from configuration.
* @param ns namespace for configuration keys
* @param conf Configuration object
* @return Decay factor as double
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,initialize,"org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",87,99,"/**
* Configures trash management settings.
* @param conf configuration object
* @param fs file system instance
* @param home home directory path
*/","* @deprecated Use {@link #initialize(Configuration, FileSystem)} instead.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,initialize,"org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",101,118,"/**
 * Initializes trash settings from configuration.
 * @param conf Configuration object containing trash settings
 * @param fs FileSystem instance
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,processRawArguments,org.apache.hadoop.fs.shell.FsCommand:processRawArguments(java.util.LinkedList),102,122,"/**
* Handles function execution with argument processing and warning display.
* @param args list of string arguments for the function
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,closeChildFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:closeChildFileSystems(org.apache.hadoop.fs.FileSystem),1966,1984,"/**
 * Disables and closes child file systems if caching is disabled.
 * @param fs parent FileSystem object
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,isNestedMountPointSupported,org.apache.hadoop.fs.viewfs.ConfigUtil:isNestedMountPointSupported(org.apache.hadoop.conf.Configuration),270,272,"/**
 * Checks if nested mount points are supported.
 * @param conf configuration object
 * @return true if supported, false otherwise
 */","* Check the bool config whether nested mount point is supported. Default: true
   * @param conf - from this conf
   * @return whether nested mount point is supported",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.viewfs.InodeTree)",1410,1426,"/**
 * Initializes a new InternalDirOfViewFs instance.
 * @param dir directory inode tree
 * @param cTime creation time of the directory
 * @param ugi user group information
 * @param uri file system URI
 * @param config configuration settings
 * @param fsState file system state
 * @throws URISyntaxException if the URI is invalid
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createCompressor,org.apache.hadoop.io.compress.Lz4Codec:createCompressor(),111,120,"/**
* Creates an LZ4 compressor.
* @return Compressor instance configured with buffer size and HC setting
*/","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,handleChecksumException,org.apache.hadoop.io.SequenceFile$Reader:handleChecksumException(org.apache.hadoop.fs.ChecksumException),2792,2801,"/**
* Handles checksum exceptions based on configuration.
* @param e ChecksumException to be handled
* @throws IOException if not configured to skip checksum errors
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,getMultipleLinearRandomRetry,"org.apache.hadoop.io.retry.RetryUtils:getMultipleLinearRandomRetry(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String)",179,201,"/**
 * Creates a retry policy based on configuration.
 * @param conf Configuration object
 * @param retryPolicyEnabledKey key for checking if retry policy is enabled
 * @param defaultRetryPolicyEnabled default value for retry policy enablement
 * @param retryPolicySpecKey key for retry policy specification
 * @param defaultRetryPolicySpec default retry policy specification
 * @return configured RetryPolicy or null if disabled
 */","* Return the MultipleLinearRandomRetry policy specified in the conf,
   * or null if the feature is disabled.
   * If the policy is specified in the conf but the policy cannot be parsed,
   * the default policy is returned.
   * 
   * Retry policy spec:
   *   N pairs of sleep-time and number-of-retries ""s1,n1,s2,n2,...""
   * 
   * @param conf configuration.
   * @param retryPolicyEnabledKey     conf property key for enabling retry
   * @param defaultRetryPolicyEnabled default retryPolicyEnabledKey conf value 
   * @param retryPolicySpecKey        conf property key for retry policy spec
   * @param defaultRetryPolicySpec    default retryPolicySpecKey conf value
   * @return the MultipleLinearRandomRetry policy specified in the conf,
   *         or null if the feature is disabled.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,isSupported,org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:isSupported(),283,283,"/**
 * Abstract method to determine mask functionality.
 * @return true if mask is enabled, false otherwise
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addPrometheusServlet,org.apache.hadoop.http.HttpServer2:addPrometheusServlet(org.apache.hadoop.conf.Configuration),818,828,"/**
 * Initializes Prometheus support based on configuration.
 * @param conf Configuration object containing settings
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addDefaultApps,"org.apache.hadoop.http.HttpServer2:addDefaultApps(org.eclipse.jetty.server.handler.ContextHandlerCollection,java.lang.String,org.apache.hadoop.conf.Configuration)",926,973,"/**
* Configures log and static contexts for a web application.
* @param parent parent context handler collection
* @param appDir application directory path
* @param conf configuration settings
* @throws IOException if an I/O error occurs during setup
*/","* Add default apps.
   *
   * @param parent contexthandlercollection.
   * @param appDir The application directory
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addDefaultServlets,org.apache.hadoop.http.HttpServer2:addDefaultServlets(org.apache.hadoop.conf.Configuration),985,995,"/**
* Registers servlets for various configurations.
* @param configuration app configuration settings
*/","* Add default servlets.
   * @param configuration the hadoop configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,create,"org.apache.hadoop.metrics2.source.JvmMetrics:create(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSystem)",112,123,"/**
* Creates JVM metrics for a given process and session.
* @param processName name of the process
* @param sessionId unique session identifier
* @param ms MetricsSystem instance
* @return JvmMetrics object containing JVM statistics
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,initFilter,"org.apache.hadoop.security.HttpCrossOriginFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",39,52,"/**
 * Configures a cross-origin filter based on configuration.
 * @param container FilterContainer for adding filters
 * @param conf Configuration object containing settings
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$ListCommand:validate(),242,250,"/**
* Initializes provider and metadata.
* @return true if initialization is successful, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,getServerFailOverEnable,"org.apache.hadoop.ipc.CallQueueManager:getServerFailOverEnable(java.lang.String,org.apache.hadoop.conf.Configuration)",119,141,"/**
* Checks failover enable property for a namespace.
* @param namespace configuration namespace
* @param conf Configuration object
* @return boolean value of the property or default if not found
*/","* Return boolean value configured by property 'ipc.<port>.callqueue.overflow.trigger.failover'
   * if it is present. If the config is not present, default config
   * (without port) is used to derive class i.e 'ipc.callqueue.overflow.trigger.failover',
   * and derived value is returned if configured. Otherwise, default value
   * {@link CommonConfigurationKeys#IPC_CALLQUEUE_SERVER_FAILOVER_ENABLE_DEFAULT} is returned.
   *
   * @param namespace Namespace ""ipc"" + ""."" + Server's listener port.
   * @param conf Configuration properties.
   * @return Value returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseBackOffByResponseTimeEnabled,"org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffByResponseTimeEnabled(java.lang.String,org.apache.hadoop.conf.Configuration)",467,472,"/**
* Checks if response time backoff is enabled for a namespace.
* @param ns namespace string
* @return Boolean indicating if feature is enabled
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",1325,1342,"/**
* Initializes a new Client instance.
* @param valueClass class type for writable values
* @param conf configuration settings
* @param factory socket factory for network connections
*/","* Construct an IPC client whose values are of the given {@link Writable}
   * class.
   *
   * @param valueClass input valueClass.
   * @param conf input configuration.
   * @param factory input factory.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getClientBackoffEnable,"org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,org.apache.hadoop.conf.Configuration)",920,927,"/**
* Checks if IPC backoff is enabled.
* @param prefix configuration key prefix
* @param conf Configuration object
* @return true if enabled, false otherwise
*/",* Get from config if client backoff is enabled on that port.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getClientBackoffEnable,"org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,int,org.apache.hadoop.conf.Configuration)",941,953,"/**
* Checks IPC backoff setting for a namespace and port.
* @param namespace configuration namespace
* @param port service port number
* @param conf Configuration object
* @return true if IPC backoff is enabled, false otherwise
*/","* Return boolean value configured by property 'ipc.<port>.backoff.enable'
   * if it is present. If the config is not present, default config
   * (without port) is used to derive class i.e 'ipc.backoff.enable',
   * and derived value is returned if configured. Otherwise, default value
   * {@link CommonConfigurationKeys#IPC_BACKOFF_ENABLE_DEFAULT} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Value returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPasswordFromConfig,org.apache.hadoop.conf.Configuration:getPasswordFromConfig(java.lang.String),2513,2524,"/**
* Masks password for given username.
* @param name username to fetch password for
* @return masked password as char array or null if not found
*/","* Fallback to clear text passwords in configuration.
   * @param name the property name.
   * @return clear text password or null",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getBoolean,"org.apache.hadoop.conf.ConfigurationWithLogging:getBoolean(java.lang.String,boolean)",67,72,"/**
* Overrides base method to fetch a boolean value by name and logs the result.
* @param name key for the value to retrieve
* @param defaultValue default boolean value if not found
* @return retrieved boolean value or default value
*/","* See {@link Configuration#getBoolean(String, boolean)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileSystemClass,"org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration)",3559,3594,"/**
* Retrieves FileSystem implementation class by scheme.
* @param scheme URI scheme for the filesystem
* @param conf configuration object containing filesystem properties
* @return FileSystem class or throws UnsupportedFileSystemException if not found
*/","* Get the FileSystem implementation class of a filesystem.
   * This triggers a scan and load of all FileSystem implementations listed as
   * services and discovered via the {@link ServiceLoader}
   * @param scheme URL scheme of FS
   * @param conf configuration: can be null, in which case the check for
   * a filesystem binding declaration in the configuration is skipped.
   * @return the filesystem
   * @throws UnsupportedFileSystemException if there was no known implementation
   *         for the scheme.
   * @throws IOException if the filesystem could not be loaded",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,createFileSystem,"org.apache.hadoop.fs.AbstractFileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",169,181,"/**
* Creates an AbstractFileSystem instance for a given URI and configuration.
* @param uri the file system URI
* @param conf the configuration object
* @return AbstractFileSystem instance
* @throws UnsupportedFileSystemException if implementation class is not found or null
*/","* Create a file system instance for the specified uri using the conf. The
   * conf is used to find the class name that implements the file system. The
   * conf is also passed to the file system for its configuration.
   *
   * @param uri URI of the file system
   * @param conf Configuration for the file system
   * 
   * @return Returns the file system for the given URI
   *
   * @throws UnsupportedFileSystemException file system for <code>uri</code> is
   *           not found",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,loadMappingProviders,org.apache.hadoop.security.CompositeGroupsMapping:loadMappingProviders(),147,160,"/**
 * Initializes mapping providers from configuration.
 * Logs error for invalid provider classes.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolEngine,"org.apache.hadoop.ipc.RPC:getProtocolEngine(java.lang.Class,org.apache.hadoop.conf.Configuration)",220,230,"/**
 * Retrieves or creates an RpcEngine for a given protocol.
 * @param protocol the protocol class
 * @param conf configuration settings
 * @return RpcEngine instance
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getQueueClass,"org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,org.apache.hadoop.conf.Configuration)",796,802,"/**
* Retrieves a BlockingQueue implementation for IPC calls.
* @param prefix configuration key prefix
* @param conf configuration object
* @return Class of the BlockingQueue implementation
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getQueueClass,"org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)",816,827,"/**
 * Retrieves a BlockingQueue implementation for IPC calls.
 * @param namespace configuration namespace
 * @param port service port number
 * @param conf configuration object
 * @return Class of BlockingQueue or LinkedBlockingQueue if not specified
 */","* Return class configured by property 'ipc.<port>.callqueue.impl' if it is
   * present. If the config is not present, default config (without port) is
   * used to derive class i.e 'ipc.callqueue.impl', and derived class is
   * returned if class value is present and valid. If default config is also
   * not present, default class {@link LinkedBlockingQueue} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Class returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getSchedulerClass,"org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,org.apache.hadoop.conf.Configuration)",829,853,"/**
* Deprecated method to determine RPC scheduler class.
* @param prefix configuration key prefix
* @param conf Configuration object
* @return RpcScheduler subclass or default implementation
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getSchedulerClass,"org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)",869,898,"/**
 * Retrieves RPC scheduler class for a given namespace and port.
 * @param namespace configuration namespace
 * @param port service port
 * @param conf configuration object
 * @return RpcScheduler subclass or default implementation
 */","* Return class configured by property 'ipc.<port>.scheduler.impl' if it is
   * present. If the config is not present, and if property
   * 'ipc.<port>.callqueue.impl' represents FairCallQueue class,
   * return DecayRpcScheduler. If config 'ipc.<port>.callqueue.impl'
   * does not have value FairCallQueue, default config (without port) is used
   * to derive class i.e 'ipc.scheduler.impl'. If default config is also not
   * present, default class {@link DefaultRpcScheduler} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Class returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClass,"org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class,java.lang.Class)",2758,2772,"/**
 * Retrieves and validates a class by name.
 * @param name class name to retrieve
 * @param defaultValue default class if not found
 * @param xface interface or superclass the retrieved class must implement/extend
 * @return Class object if valid, otherwise null
 */","* Get the value of the <code>name</code> property as a <code>Class</code>
   * implementing the interface specified by <code>xface</code>.
   *   
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * An exception is thrown if the returned class does not implement the named
   * interface. 
   * 
   * @param name the conf key name.
   * @param defaultValue default value.
   * @param xface the interface implemented by the named class.
   * @param <U> Interface class type.
   * @return property value as a <code>Class</code>, 
   *         or <code>defaultValue</code>.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getInternal,"org.apache.hadoop.fs.FileSystem$Cache:getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key)",3689,3765,"/**
 * Acquires a FileSystem instance for the given URI.
 * @param uri target file system URI
 * @param conf configuration settings
 * @param key unique identifier for caching
 * @return FileSystem object or throws IOException if creation fails
 */","* Get the FS instance if the key maps to an instance, creating and
     * initializing the FS if it is not found.
     * If this is the first entry in the map and the JVM is not shutting down,
     * this registers a shutdown hook to close filesystems, and adds this
     * FS to the {@code toAutoClose} set if {@code ""fs.automatic.close""}
     * is set in the configuration (default: true).
     * @param uri filesystem URI
     * @param conf configuration
     * @param key key to store/retrieve this FileSystem in the cache
     * @return a cached or newly instantiated FileSystem.
     * @throws IOException If an I/O error occurred.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration),81,102,"/**
* Initializes the SQLDelegationTokenSecretManager with configuration settings.
* @param conf Configuration object containing token management parameters
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,setConf,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),61,72,"/**
* Overrides m1 to set timeout from configuration.
* @param conf Configuration object containing settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,getShutdownTimeout,org.apache.hadoop.util.ShutdownHookManager:getShutdownTimeout(org.apache.hadoop.conf.Configuration),180,191,"/**
* Retrieves shutdown timeout from configuration.
* @param conf Configuration object
* @return Minimum of configured or default shutdown timeout in milliseconds
*/","* Get the shutdown timeout in seconds, from the supplied
   * configuration.
   * @param conf configuration to use.
   * @return a timeout, always greater than or equal to {@link #TIMEOUT_MINIMUM}",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,getCredentialProvider,org.apache.hadoop.security.alias.CredentialShell$Command:getCredentialProvider(),144,166,"/**
* Retrieves a credential provider.
* @return CredentialProvider object or null if none found
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPasswordFromCredentialProviders,org.apache.hadoop.conf.Configuration:getPasswordFromCredentialProviders(java.lang.String),2478,2506,"/**
* Masks a password by fetching it from credential providers.
* @param name key name for the password
* @return char array of the masked password or null if not found
* @throws IOException if there's an issue accessing the credential provider
*/","* Try and resolve the provided element name as a credential provider
   * alias.
   * @param name alias of the provisioned credential
   * @return password or null if not found
   * @throws IOException when error in fetching password",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,getKeyProvider,org.apache.hadoop.crypto.key.KeyShell$Command:getKeyProvider(),191,213,"/**
* Retrieves a key provider based on configuration.
* @return KeyProvider instance or null if none available
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java,accept,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:accept(java.lang.Class),55,63,"/**
* Checks if class is maskable.
* @param c the Class object to check
* @return true if class is maskable, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawEncoderWithFallback,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",176,202,"/**
* Creates a raw erasure encoder using specified codec.
* @param conf configuration settings
* @param codecName name of the codec
* @param coderOptions options for the erasure coder
* @return RawErasureEncoder instance or throws exception if creation fails
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawDecoderWithFallback,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",204,230,"/**
* Creates a raw erasure decoder for the specified codec.
* @param conf configuration object
* @param codecName name of the codec
* @param coderOptions options for the erasure coder
* @return RawErasureDecoder instance or throws IllegalArgumentException if creation fails
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,createSession,"org.apache.hadoop.ha.SshFenceByTcpPort:createSession(java.lang.String,org.apache.hadoop.ha.SshFenceByTcpPort$Args)",118,128,"/**
* Establishes an SSH session to the specified host.
* @param host target server address
* @param args connection parameters including user and port
* @return JSch Session object for the established connection
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,refresh,org.apache.hadoop.security.authorize.ProxyServers:refresh(),31,33,"/**
 * Calls m1 with a new default configuration.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,parseCapacityWeights,"org.apache.hadoop.ipc.CallQueueManager:parseCapacityWeights(int,java.lang.String,org.apache.hadoop.conf.Configuration)",417,440,"/**
* Retrieves and validates call queue capacity weights.
* @param priorityLevels number of priority levels
* @param ns namespace string
* @param conf configuration object
* @return array of validated weights
*/","* Read the weights of capacity in callqueue and pass the value to
   * callqueue constructions.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,<init>,"org.apache.hadoop.ipc.metrics.RpcMetrics:<init>(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)",58,111,"/**
* Initializes RPC metrics for a server.
* @param server the RPC server instance
* @param conf configuration settings for metrics
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseThresholds,"org.apache.hadoop.ipc.DecayRpcScheduler:parseThresholds(java.lang.String,org.apache.hadoop.conf.Configuration,int)",379,407,"/**
 * Computes decay thresholds from configuration.
 * @param ns namespace string
 * @param conf Configuration object
 * @param numLevels number of levels
 * @return array of threshold percentages as decimals
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,<init>,"org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)",56,79,"/**
* Initializes Weighted Round Robin Multiplexer.
* @param aNumQueues number of queues, must be greater than zero
* @param ns namespace for configuration keys
* @param conf configuration object containing queue weights
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseBackOffResponseTimeThreshold,"org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffResponseTimeThreshold(java.lang.String,org.apache.hadoop.conf.Configuration,int)",433,456,"/**
* Retrieves and validates response time thresholds from configuration.
* @param ns namespace for configuration key
* @param conf Configuration object
* @param numLevels number of priority levels
* @return array of response time thresholds in milliseconds
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getFilterInitializers,org.apache.hadoop.http.HttpServer2:getFilterInitializers(org.apache.hadoop.conf.Configuration),894,916,"/**
* Initializes filter initializers from configuration.
* @param conf configuration object
* @return array of FilterInitializer or null if config is invalid
*/",Get an array of FilterConfiguration specified in the conf,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInstances,"org.apache.hadoop.conf.Configuration:getInstances(java.lang.String,java.lang.Class)",2787,2798,"/**
* Retrieves and casts a list of classes implementing a given interface.
* @param name class name identifier
* @param xface interface type to check implementation against
* @return List of instances cast to U or throws RuntimeException if not implemented
*/","* Get the value of the <code>name</code> property as a <code>List</code>
   * of objects implementing the interface specified by <code>xface</code>.
   * 
   * An exception is thrown if any of the classes does not exist, or if it does
   * not implement the named interface.
   * 
   * @param name the property name.
   * @param xface the interface implemented by the classes named by
   *        <code>name</code>.
   * @param <U> Interface class type.
   * @return a <code>List</code> of objects implementing <code>xface</code>.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,<init>,"org.apache.hadoop.io.DefaultStringifier:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)",60,73,"/**
* Initializes DefaultStringifier with configuration and class type.
* @param conf Hadoop configuration object
* @param c Class type for serialization/deserialization
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,init,"org.apache.hadoop.io.SequenceFile$Writer:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,boolean,java.lang.Class,java.lang.Class,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,int)",1292,1354,"/**
* Initializes and configures output stream for writing data.
* @param config configuration settings
* @param outStream output stream to write data
* @param ownStream flag indicating if the stream is owned by this class
* @param key key class type
* @param val value class type
* @param compCodec compression codec for data
* @param meta metadata associated with the data
* @param syncIntervalVal synchronization interval
* @throws IOException if an I/O error occurs during setup
*/",Initialize.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,getFactory,org.apache.hadoop.util.ReflectionUtils:getFactory(org.apache.hadoop.conf.Configuration),330,335,"/**
* Returns a singleton instance of SerializationFactory.
* @param conf configuration settings
* @return SerializationFactory object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/IngressPortBasedResolver.java,setConf,org.apache.hadoop.security.IngressPortBasedResolver:setConf(org.apache.hadoop.conf.Configuration),65,79,"/**
* Configures port to quality of protection mapping.
* @param conf configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,setConf,org.apache.hadoop.security.WhitelistBasedResolver:setConf(org.apache.hadoop.conf.Configuration),91,109,"/**
* Initializes security configurations and sets up IP whitelists.
* @param conf Configuration object containing security settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,propagateOptions,"org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",159,166,"/**
* Deprecated method. Calls FutureIO.m1 with given parameters.
* @param builder FSBuilder instance
* @param conf Configuration object
* @param prefix string prefix
* @param mandatory boolean indicating if mandatory
*/","* Propagate options to any builder.
   * {@link FutureIO#propagateOptions(FSBuilder, Configuration, String, boolean)}
   * @param builder builder to modify
   * @param conf configuration to read
   * @param prefix prefix to scan/strip
   * @param mandatory are the options to be mandatory or optional?",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,propagateOptions,"org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",330,340,"/**
 * Configures a builder with optional and mandatory prefixes.
 * @param <T> the type of the FSBuilder
 * @param <U> the subclass of FSBuilder
 * @param builder the FSBuilder instance to configure
 * @param conf configuration settings
 * @param optionalPrefix prefix for optional configurations
 * @param mandatoryPrefix prefix for mandatory configurations
 * @return configured FSBuilder instance
 */","* Propagate options to any builder, converting everything with the
   * prefix to an option where, if there were 2+ dot-separated elements,
   * it is converted to a schema.
   * See {@link #propagateOptions(FSBuilder, Configuration, String, boolean)}.
   * @param builder builder to modify
   * @param conf configuration to read
   * @param optionalPrefix prefix for optional settings
   * @param mandatoryPrefix prefix for mandatory settings
   * @param <T> type of result
   * @param <U> type of builder
   * @return the builder passed in.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,getChangedProperties,"org.apache.hadoop.conf.ReconfigurableBase:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",99,103,"/**
* Compares two configurations and returns property changes.
* @param newConf new configuration settings
* @param oldConf old configuration settings
* @return Collection of PropertyChange objects representing differences
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,doGet,"org.apache.hadoop.conf.ReconfigurationServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",199,212,"/**
* Handles HTTP GET request for masking functionality.
* @param req HttpServletRequest object containing client request details
* @param resp HttpServletResponse object to send response back to client
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/NativeLibraryChecker.java,main,org.apache.hadoop.util.NativeLibraryChecker:main(java.lang.String[]),45,156,"/**
* Checks availability of native libraries and Winutils.
* @param args command-line arguments for options
*/","* A tool to test native library availability.
   * @param args args.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,getCompressorType,org.apache.hadoop.io.compress.BZip2Codec:getCompressorType(),137,140,"/**
 * Returns a Bzip2 compressor class.
 * @return Class of type extending Compressor
 */","* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of compressor needed by this codec.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,getDecompressorType,org.apache.hadoop.io.compress.BZip2Codec:getDecompressorType(),218,221,"/**
* Returns decompressor class based on configuration.
* @return Class of decompressor or null if not applicable
*/","* Get the type of {@link Decompressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of decompressor needed by this codec.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createDecompressor,org.apache.hadoop.io.compress.BZip2Codec:createDecompressor(),228,231,"/**
 * Returns a decompressor instance configured with the current settings.
 * @return Decompressor object for Bzip2 format
 */","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping:reloadCachedMappings(),82,86,"/**
 * Calls superclass method and chained method.
 * @see #m2()
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings(java.util.List),162,167,"/**
 * Calls overloaded method m1 with no parameters.
 * @param names list of string names (unused in this implementation)
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,<init>,org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String),510,512,"/**
 * Constructs a HadoopZookeeperFactory with a specified principal.
 * @param zkPrincipal ZooKeeper principal string
 */","* Constructor for the helper class to configure the ZooKeeper client connection.
     * @param zkPrincipal Optional.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,lookupGroup,"org.apache.hadoop.security.LdapGroupsMapping:lookupGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext,int)",438,475,"/**
* Extracts groups from search result.
* @param result search result object
* @param c directory context
* @param goUpHierarchy flag to determine hierarchy traversal
* @return set of group names
*/","* Perform the second query to get the groups of the user.
   *
   * If posixGroups is enabled, use use posix gid/uid to find.
   * Otherwise, use the general group member attribute to find it.
   *
   * @param result the result object returned from the prior user lookup.
   * @param c the context object of the LDAP connection.
   * @return a list of strings representing group names of the user.
   * @throws NamingException if unable to find group names",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HadoopKerberosName.java,main,org.apache.hadoop.security.HadoopKerberosName:main(java.lang.String[]),87,93,"/**
* Processes arguments with Kerberos names.
* @param args array of input strings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getInstance,org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration),99,103,"/**
* Retrieves and initializes a CryptoCodec based on configuration.
* @param conf Configuration object containing security settings
* @return CryptoCodec instance configured with specified cipher suite
*/","* Get crypto codec for algorithm/mode/padding in config value
   * hadoop.security.crypto.cipher.suite
   * 
   * @param conf
   *          the configuration
   * @return CryptoCodec the codec object Null value will be returned if no
   *         crypto codec classes with cipher suite configured.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,bind,"org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int)",685,688,"/**
 * Initializes server socket with default handlers.
 * @param socket ServerSocket instance to initialize
 * @param address InetSocketAddress for binding the server
 * @param backlog maximum length of pending connection queue
 */","* A convenience method to bind to a given address and report 
   * better exceptions if the address is not a valid host.
   * @param socket the socket to bind
   * @param address the address to bind to
   * @param backlog the number of connections allowed in the queue
   * @throws BindException if the address can't be bound
   * @throws UnknownHostException if the address isn't a valid host name
   * @throws IOException other random errors from bind",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,"org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer)",3642,3645,"/**
 * Writes property value to output.
 * @param propertyName name of the property (can be null)
 * @param out writer to output the result
 * @throws IOException if an I/O error occurs
 * @throws IllegalArgumentException for invalid input
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpConfiguration,"org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String,java.io.Writer)",3787,3804,"/**
 * Writes property value to output.
 * @param config Configuration object
 * @param propertyName Name of the property
 * @param out Writer for output
 * @throws IOException if an I/O error occurs
 */","*  Writes properties and their attributes (final and resource)
   *  to the given {@link Writer}.
   *  <ul>
   *  <li>
   *  When propertyName is not empty, and the property exists
   *  in the configuration, the format of the output would be,
   *  <pre>
   *  {
   *    ""property"": {
   *      ""key"" : ""key1"",
   *      ""value"" : ""value1"",
   *      ""isFinal"" : ""key1.isFinal"",
   *      ""resource"" : ""key1.resource""
   *    }
   *  }
   *  </pre>
   *  </li>
   *
   *  <li>
   *  When propertyName is null or empty, it behaves same as
   *  {@link #dumpConfiguration(Configuration, Writer)}, the
   *  output would be,
   *  <pre>
   *  { ""properties"" :
   *      [ { key : ""key1"",
   *          value : ""value1"",
   *          isFinal : ""key1.isFinal"",
   *          resource : ""key1.resource"" },
   *        { key : ""key2"",
   *          value : ""value2"",
   *          isFinal : ""ke2.isFinal"",
   *          resource : ""key2.resource"" }
   *       ]
   *   }
   *  </pre>
   *  </li>
   *
   *  <li>
   *  When propertyName is not empty, and the property is not
   *  found in the configuration, this method will throw an
   *  {@link IllegalArgumentException}.
   *  </li>
   *  </ul>
   *  <p>
   * @param config the configuration
   * @param propertyName property name
   * @param out the Writer to write to
   * @throws IOException raised on errors performing I/O.
   * @throws IllegalArgumentException when property name is not
   *   empty and the property is not found in configuration
   *",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,formatZK,"org.apache.hadoop.ha.ZKFailoverController:formatZK(boolean,boolean)",282,299,"/**
* Performs election and clears ZK node if conditions are met.
* @param force bypasses format check
* @param interactive user interaction required
* @return error code or 0 on success
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSystemSource,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSystemSource(),561,567,"/**
* Initializes metrics source with configuration.
* @param none
* @return void
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,java.lang.Object)",221,243,"/**
 * Registers and logs a metrics source.
 * @param name optional custom name for the source
 * @param desc optional description of the source
 * @param source the source object to register
 * @return the original source object
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirs,org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path),808,811,"/**
 * Checks file mask against given path.
 * @param f file path to check
 * @return true if matches mask, false otherwise
 */","* Creates the specified directory hierarchy. Does not
   * treat existence as an error.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirs,"org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",813,816,"/**
 * Masks file with specified permissions.
 * @param f file path
 * @param permission file permissions
 * @return true if masking successful, false otherwise
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,build,org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:build(),728,748,"/**
* Creates a file output stream with specified options.
* @return FSDataOutputStream for the created file
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/BouncyCastleFipsKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",41,44,"/**
* Initializes a BouncyCastle FIPS key store provider.
* @param uri location of the key store
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/JavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",40,43,"/**
 * Initializes a JavaKeyStoreProvider with a URI and configuration.
 * @param uri location of the key store
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalJavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",40,43,"/**
 * Initializes a new LocalJavaKeyStoreProvider instance.
 * @param uri location of the key store
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs during initialization
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalBouncyCastleFipsKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",41,44,"/**
* Initializes a new LocalBouncyCastleFipsKeyStoreProvider.
* @param uri key store URI
* @param conf configuration settings
* @throws IOException if initialization fails
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,initSpnego,"org.apache.hadoop.http.HttpServer2:initSpnego(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Properties,java.lang.String,java.lang.String)",1356,1375,"/**
* Configures Kerberos authentication for a web application.
* @param conf configuration object
* @param hostName hostname for Kerberos principal
* @param authFilterConfigurationPrefixes properties for authentication filter
* @param usernameConfKey key for retrieving username from config
* @param keytabConfKey key for retrieving keytab path from config
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java,getFilterConfigMap,"org.apache.hadoop.security.AuthenticationFilterInitializer:getFilterConfigMap(org.apache.hadoop.conf.Configuration,java.lang.String)",66,91,"/**
* Masks configuration properties with a prefix.
* @param conf Configuration object
* @param prefix Property key prefix
* @return Map of masked configuration properties
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,createCuratorClient,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:createCuratorClient(org.apache.hadoop.conf.Configuration,java.lang.String)",185,241,"/**
* Creates a CuratorFramework instance with ZooKeeper configuration.
* @param conf Configuration object containing ZooKeeper settings
* @param namespace Namespace for ZooKeeper operations
* @return Configured CuratorFramework instance
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,setJaasConfiguration,org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:setJaasConfiguration(org.apache.zookeeper.client.ZKClientConfig),585,597,"/**
 * Configures Kerberos authentication for ZK client.
 * @param zkClientConfig configuration for ZooKeeper client
 * @throws IOException if there is an I/O error during configuration
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getServerPrincipal,org.apache.hadoop.security.SaslRpcClient:getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),303,356,"/**
* Generates Kerberos principal for authentication.
* @param authType authentication type object
* @return generated server principal or null if Kerberos info is missing
* @throws IOException on I/O errors during configuration retrieval
*/","* Get the remote server's principal.  The value will be obtained from
   * the config and cross-checked against the server's advertised principal.
   * 
   * @param authType of the SASL client
   * @return String of the server's principal
   * @throws IOException - error determining configured principal",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,initProtocolMetaInfo,org.apache.hadoop.ipc.RPC$Server:initProtocolMetaInfo(org.apache.hadoop.conf.Configuration),1200,1209,"/**
* Sets up RPC configuration for protocol metadata.
* @param conf Configuration object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKeyProvider,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:createKeyProvider(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",231,242,"/**
* Creates a KeyProvider using a token or configuration.
* @param token authentication token
* @param conf configuration settings
* @return KeyProvider instance or null if URI is invalid
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)",100,140,"/**
 * Initializes a load-balanced KMS client provider.
 * @param uri the token service URI
 * @param providers array of underlying KMS client providers
 * @param seed random seed for shuffling providers (0 for deterministic tests)
 * @param conf configuration settings
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,decodeHarURI,"org.apache.hadoop.fs.HarFileSystem:decodeHarURI(java.net.URI,org.apache.hadoop.conf.Configuration)",218,254,"/**
 * Masks a given URI based on configuration.
 * @param rawURI the original URI to be masked
 * @param conf the configuration settings
 * @return masked URI or default FileSystem URI if no authority is found
 * @throws IOException if URI format is invalid
 */","* decode the raw URI to get the underlying URI
   * @param rawURI raw Har URI
   * @return filtered URI of the underlying fileSystem",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration),288,290,"/**
 * Creates a FileSystem instance.
 * @param conf configuration settings
 * @return FileSystem object
 * @throws IOException if an I/O error occurs
 */","* Returns the configured FileSystem implementation.
   * @param conf the configuration to use
   * @return FileSystem.
   * @throws IOException If an I/O error occurred.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,initialize,"org.apache.hadoop.fs.FileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",339,350,"/**
* Initializes file system configuration.
* @param name URI representing the file system path
* @param conf Configuration object for file system settings
*/","* Initialize a FileSystem.
   *
   * Called after the new FileSystem instance is constructed, and before it
   * is ready for use.
   *
   * FileSystem implementations overriding this method MUST forward it to
   * their superclass, though the order in which it is done, and whether
   * to alter the configuration before the invocation are options of the
   * subclass.
   * @param name a URI whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration
   * @throws IOException on any failure to initialize this instance.
   * @throws IllegalArgumentException if the URI is considered invalid.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,org.apache.hadoop.fs.FileSystem:newInstance(org.apache.hadoop.conf.Configuration),621,623,"/**
 * Retrieves FileSystem instance using configuration.
 * @param conf Hadoop configuration object
 * @return FileSystem object
 * @throws IOException if an I/O error occurs
 */","* Returns a unique configured FileSystem implementation for the default
   * filesystem of the supplied configuration.
   * This always returns a new FileSystem object.
   * @param conf the configuration to use
   * @return the new FS instance
   * @throws IOException FS creation or initialization failure.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,checkPath,org.apache.hadoop.fs.FileSystem:checkPath(org.apache.hadoop.fs.Path),792,825,"/**
 * Masks a given file path based on URI scheme and authority.
 * @param path the file path to be masked
 */","* Check that a Path belongs to this FileSystem.
   *
   * The base implementation performs case insensitive equality checks
   * of the URIs' schemes and authorities. Subclasses may implement slightly
   * different checks.
   * @param path to check
   * @throws IllegalArgumentException if the path is not considered to be
   * part of this FileSystem.
   *",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getSocketAddr,"org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)",2539,2556,"/**
 * Determines bind address using properties.
 * @param hostProperty property key for host
 * @param addressProperty property key for address
 * @param defaultAddressValue default address if not found
 * @param defaultPort default port number
 * @return InetSocketAddress with resolved host or null
 */","* Get the socket address for <code>hostProperty</code> as a
   * <code>InetSocketAddress</code>. If <code>hostProperty</code> is
   * <code>null</code>, <code>addressProperty</code> will be used. This
   * is useful for cases where we want to differentiate between host
   * bind address and address clients should use to establish connection.
   *
   * @param hostProperty bind host property name.
   * @param addressProperty address property name.
   * @param defaultAddressValue the default value
   * @param defaultPort the default port
   * @return InetSocketAddress",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FutureDataInputStreamBuilder.java,build,org.apache.hadoop.fs.FutureDataInputStreamBuilder:build(),47,50,"/**
 * Returns a CompletableFuture for an FSDataInputStream.
 * @throws IllegalArgumentException if invalid arguments provided
 * @throws UnsupportedOperationException if operation not supported
 * @throws IOException if I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,open,"org.apache.hadoop.fs.sftp.SFTPFileSystem:open(org.apache.hadoop.fs.Path,int)",507,539,"/**
* Opens a file input stream for reading.
* @param f file path
* @param bufferSize buffer size for the input stream
* @return FSDataInputStream object
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,create,"org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",545,589,"/**
 * Opens a file for writing with specified permissions and replication settings.
 * @param f file path to open
 * @param permission file permissions
 * @param overwrite flag to overwrite existing file
 * @param bufferSize buffer size for I/O operations
 * @param replication desired replication factor
 * @param blockSize block size for the file
 * @param progress progress callback
 * @return FSDataOutputStream for writing to the file
 * @throws IOException if an error occurs during file operation
 */","* A stream obtained via this call must be closed before using other APIs of
   * this class or else the invocation will block.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,rename,"org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",603,612,"/**
* Transfers a file from source to destination using SFTP.
* @param src source file path
* @param dst destination file path
* @return true if transfer is successful, false otherwise
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,delete,"org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",614,623,"/**
 * Uploads a file or directory to an SFTP server.
 * @param f the path of the local file or directory to upload
 * @param recursive true if directories should be uploaded recursively
 * @return true if the upload is successful, false otherwise
 * @throws IOException if an I/O error occurs during the operation
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,listStatus,org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(org.apache.hadoop.fs.Path),625,634,"/**
* Retrieves file status for a given path.
* @param f file path
* @return array of FileStatus objects
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory(),657,673,"/**
* Retrieves SFTP channel's home directory.
* @return Path object representing home directory or null on error
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",688,697,"/**
* Applies file permissions on a remote path.
* @param f remote file path
* @param permission desired file permissions
* @return true if operation succeeds, false otherwise
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path),699,708,"/**
 * Retrieves file status from SFTP server.
 * @param f file path
 * @return FileStatus object
 * @throws IOException if I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,read,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:read(long,byte[],int,int)",230,246,"/**
 * Reads data from a specified position into a byte array.
 * @param position starting position in the file
 * @param b destination byte array
 * @param off offset within the byte array
 * @param len number of bytes to read
 * @return number of bytes actually read
 * @throws IOException if an I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",91,96,"/**
* Initializes a new instance with the specified filesystem and path.
* @param fileSystem the filesystem to use
* @param path the path within the filesystem
*/","* Constructor.
   * @param fileSystem owner FS.
   * @param path path",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",103,108,"/**
 * Initializes a new instance with specified file system and path handle.
 * @param fileSystem the file system to use
 * @param pathHandle the path handle to initialize from
 */","* Constructor with PathHandle.
   * @param fileSystem owner FS.
   * @param pathHandle path handle",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,openFileOnInstance,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFileOnInstance(org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)",472,498,"/**
* Opens a file using specified policies.
* @param instance DynamicWrappedIO instance
* @param fs FileSystem object
* @param status FileStatus of the file to open
* @param readPolicies policies for reading the file
* @return FSDataInputStream for the opened file
* @throws IOException if an I/O error occurs
*/","* Open a file.
   * <p>
   * If the WrappedIO class is found, uses
   * {@link #fileSystem_openFile(FileSystem, Path, String, FileStatus, Long, Map)} with
   * {@link #PARQUET_READ_POLICIES} as the list of read policies and passing down
   * the file status.
   * <p>
   * If not, falls back to the classic {@code fs.open(Path)} call.
   * @param instance dynamic wrapped IO instance.
   * @param fs filesystem
   * @param status file status
   * @param readPolicies read policy to use
   * @return the input stream
   * @throws IOException any IO failure.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,getInputStreamForFile,org.apache.hadoop.security.alias.KeyStoreProvider:getInputStreamForFile(),63,66,"/**
 * Retrieves an input stream from file system operations.
 * @return InputStream from file system methods
 * @throws IOException if file access fails
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,loadFromPath,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadFromPath(org.apache.hadoop.fs.Path,char[])",291,298,"/**
* Applies file permissions mask using a password.
* @param p file path
* @param password array of characters for decryption
* @return FsPermission object after applying mask
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkAppend,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkAppend(org.apache.hadoop.fs.FileSystem),483,495,"/**
* Checks if file system supports appending.
* @param fs the FileSystem to check
* @return true if append is supported, false otherwise
*/","* Test whether the file system supports append and return the answer.
   *
   * @param fs the target file system",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)",2934,2937,"/**
* Constructs a Sorter with specified FileSystem and Comparator.
* @param fs file system to use for sorting
* @param comparator custom comparator for key comparison
* @param keyClass class of the sort keys
* @param valClass class of the values
* @param conf configuration settings for sorting
*/","* Sort and merge using an arbitrary {@link RawComparator}.
     * @param fs input FileSystem.
     * @param comparator input RawComparator.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(org.apache.hadoop.conf.Configuration),72,76,"/**
* Initializes Bzip2Compressor with configuration settings.
* @param conf Configuration object containing compression parameters
*/","* Creates a new compressor, taking settings from the configuration.
   * @param conf configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,reinit,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reinit(org.apache.hadoop.conf.Configuration),108,122,"/**
 * Reinitializes the compressor with a new configuration.
 * @param conf Configuration object for compression settings
 */","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's block size and
   * and work factor.
   * 
   * @param conf Configuration storing new settings",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createCompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",279,281,"/**
 * Applies compression to an output stream.
 * @param downStream original output stream
 * @param compressor compression algorithm to use
 * @param downStreamBufferSize buffer size for the output stream
 * @return compressed OutputStream
 * @throws IOException if an I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,init,org.apache.hadoop.io.SequenceFile$Reader:init(boolean),2022,2161,"/**
 * Initializes a SequenceFile reader.
 * @param tempReader flag indicating temporary reader status
 * @throws IOException if file version mismatch or other I/O errors occur
 */","* Initialize the {@link Reader}
     * @param tmpReader <code>true</code> if we are constructing a temporary
     *                  reader {@link SequenceFile.Sorter.cloneFileAttributes}, 
     *                  and hence do not initialize every component; 
     *                  <code>false</code> otherwise.
     * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createDecompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",275,277,"/**
 * Masks input stream data using a decompressor.
 * @param downStream source input stream
 * @param decompressor used to process the stream
 * @param downStreamBufferSize buffer size for reading from the stream
 * @return InputStream with masked data
 * @throws IOException if an I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,reinit,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reinit(org.apache.hadoop.conf.Configuration),112,120,"/**
* Initializes compressor with new configuration.
* @param conf Configuration object containing settings
*/","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's compression level
   * and compression strategy.
   *
   * @param conf Configuration storing new settings",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressionBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getCompressionBufferSize(org.apache.hadoop.conf.Configuration),94,99,"/**
* Returns buffer size from configuration or default value.
* @param conf Configuration object
* @return Buffer size, default if zero
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getDecompressionBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getDecompressionBufferSize(org.apache.hadoop.conf.Configuration),101,106,"/**
* Retrieves buffer size from configuration or default value.
* @param conf Configuration object
* @return Buffer size, using default if configured size is zero
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,writeStreamToFile,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:writeStreamToFile(java.io.InputStream,org.apache.hadoop.fs.shell.PathData,boolean,boolean)",499,512,"/**
 * Masks input stream content to target path.
 * @param in InputStream containing data to mask
 * @param target PathData specifying the output location
 * @param lazyPersist Flag for lazy persistence
 * @param direct Direct mode flag
 * @throws IOException if I/O operations fail
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,printToStdout,org.apache.hadoop.fs.shell.Display$Cat:printToStdout(java.io.InputStream),98,104,"/**
 * Masks input stream content.
 * @param in InputStream to be processed
 * @throws IOException if an I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareAppendValue,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendValue(int),554,575,"/**
* Creates a DataOutputStream for writing values.
* @param length expected value length; if negative, uses buffer
* @return DataOutputStream for writing value data
* @throws IOException if I/O error occurs or incorrect state
*/","* Obtain an output stream for writing a value into TFile. This may only be
     * called right after a key appending operation (the key append stream must
     * be closed).
     * 
     * @param length
     *          The expected length of the value. If length of the value is not
     *          known, set length = -1. Otherwise, the application must write
     *          exactly as many bytes as specified here before calling close on
     *          the returned output stream. Advertising the value size up-front
     *          guarantees that the value is encoded in one chunk, and avoids
     *          intermediate chunk buffering.
     * @throws IOException raised on errors performing I/O.
     * @return DataOutputStream.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion,org.apache.hadoop.conf.Configuration)",496,513,"/**
 * Initializes a block state for reading compressed data.
 * @param compressionAlgo algorithm used for decompression
 * @param fsin input stream from the file system
 * @param region defines the block's offset and size
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs during initialization
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.conf.Configuration)",119,139,"/**
* Initializes a WBlockState for writing compressed data.
* @param compressionAlgo algorithm for data compression
* @param fsOut file system output stream
* @param fsOutputBuffer buffer for file system output
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/","* @param compressionAlgo
       *          The compression algorithm to be used to for compression.
       * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,setConf,org.apache.hadoop.net.ScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration),135,139,"/**
 * Calls superclass and nested method with configuration.
 * @param conf Configuration object to be passed
 */","* {@inheritDoc}.
   * <p>
   * This will get called in the superclass constructor, so a check is needed
   * to ensure that the raw mapping is defined before trying to relaying a null
   * configuration.
   * </p>
   * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,setConf,org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration),130,138,"/**
* Overrides the base method to set configuration.
* @param conf Configuration object containing settings
*/","* Set the configuration and extract the configuration parameters of interest
     * @param conf the new configuration",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createHttpsChannelConnector,"org.apache.hadoop.http.HttpServer2$Builder:createHttpsChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)",583,632,"/**
* Configures and returns a secure ServerConnector.
* @param server the server instance to configure
* @param httpConfig HTTP configuration settings
* @return configured ServerConnector with SSL support
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getDelegationToken(java.lang.String),251,264,"/**
* Renews a token using the specified renewer.
* @param renewer user or entity renewing the token
* @return renewed Token object
* @throws IOException if an I/O error occurs during renewal
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,renewDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token),266,274,"/**
* Executes a secure operation using the provided token.
* @param token security token for authentication
* @return result of the operation as a long value
* @throws IOException if an I/O error occurs during execution
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,cancelDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token),276,285,"/**
* Executes a callable with a token using a KMS client provider.
* @param token authentication token
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,generateEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:generateEncryptedKey(java.lang.String),326,344,"/**
* Retrieves encrypted key version by name.
* @param encryptionKeyName name of the encryption key
* @return EncryptedKeyVersion object
* @throws IOException if an I/O error occurs
* @throws GeneralSecurityException if a security error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),346,364,"/**
* Decrypts an encrypted key version.
* @param encryptedKeyVersion the encrypted key to decrypt
* @return KeyVersion object representing the decrypted key
* @throws IOException if an I/O error occurs
* @throws GeneralSecurityException if a security error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),366,384,"/**
 * Processes an encrypted key version.
 * @param ekv EncryptedKeyVersion object to process
 * @return Processed EncryptedKeyVersion object
 * @throws IOException if IO error occurs
 * @throws GeneralSecurityException if security error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKeys(java.util.List),386,404,"/**
* Calls provider's m2 method with encrypted key versions.
* @param ekvs list of EncryptedKeyVersion objects
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersion(java.lang.String),406,414,"/**
* Retrieves key version by name.
* @param versionName unique version identifier
* @return KeyVersion object or null if not found
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeys(),416,424,"/**
* Calls m1 on a KMS client provider.
* @return list of strings from the provider's m1 method
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeysMetadata,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeysMetadata(java.lang.String[]),426,434,"/**
* Fetches metadata for given names.
* @param names variable number of name strings
* @return array of Metadata objects
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersions(java.lang.String),436,445,"/**
 * Fetches key versions by name.
 * @param name key identifier
 * @return list of KeyVersion objects
 * @throws IOException if an I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCurrentKey(java.lang.String),447,455,"/**
* Retrieves key version by name.
* @param name key identifier
* @return KeyVersion object or null if not found
* @throws IOException on I/O errors
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getMetadata,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getMetadata(java.lang.String),457,465,"/**
* Retrieves metadata for a given name.
* @param name resource identifier
* @return Metadata object
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",467,476,"/**
* Creates a key version with specified name and material.
* @param name key identifier
* @param material key material data
* @param options additional configuration options
* @return KeyVersion object representing the created key version
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",478,495,"/**
* Retrieves key version by name with specified options.
* @param name key identifier
* @param options configuration for key retrieval
* @return KeyVersion object
* @throws NoSuchAlgorithmException if algorithm is not found
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,options,org.apache.hadoop.crypto.key.KeyProvider:options(org.apache.hadoop.conf.Configuration),433,435,"/**
 * Creates an Options object from configuration.
 * @param conf Configuration settings
 * @return Options object initialized with given config
 */","* A helper function to create an options object.
   * @param conf the configuration to use
   * @return a new options object",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",142,145,"/**
 * Initializes a CryptoInputStream with specified parameters.
 * @param in input stream to encrypt
 * @param codec encryption codec to use
 * @param key encryption key
 * @param iv initialization vector
 * @throws IOException if an I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long,boolean)",130,135,"/**
* Initializes a CryptoOutputStream with specified parameters.
* @param out underlying output stream
* @param codec encryption codec to use
* @param key encryption key
* @param iv initialization vector
* @param streamOffset initial offset for the stream
* @param closeOutputStream whether to close the underlying stream on close
* @throws IOException if an I/O error occurs during initialization
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$ConnectionId:<init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)",1679,1709,"/**
 * Initializes a new ConnectionId with specified parameters.
 * @param address socket address for the connection
 * @param protocol class representing the communication protocol
 * @param ticket user group information for authentication
 * @param rpcTimeout timeout for RPC calls
 * @param connectionRetryPolicy policy for retrying connections
 * @param conf configuration settings for various connection properties
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getTimeout,org.apache.hadoop.ipc.Client:getTimeout(org.apache.hadoop.conf.Configuration),202,213,"/**
* Deprecated function to determine a mask value based on configuration.
* @param conf Configuration object
* @return Timeout value or -1 if not applicable
*/","* The time after which a RPC will timeout.
   * If ping is not enabled (via ipc.client.ping), then the timeout value is the 
   * same as the pingInterval.
   * If ping is enabled, then there is no timeout value.
   * 
   * @param conf Configuration
   * @return the timeout period in milliseconds. -1 if no timeout value is set
   * @deprecated use {@link #getRpcTimeout(Configuration)} instead",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,parseMetaData,org.apache.hadoop.fs.HarFileSystem$HarMetaData:parseMetaData(),1166,1236,"/**
* Reads and processes index files for version validation and store/archive data.
* Throws IOException if any I/O error occurs.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,<init>,"org.apache.hadoop.ha.FailoverController:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceProtocol$RequestSource)",61,80,"/**
 * Initializes FailoverController with configuration and request source.
 * Configures graceful fence settings based on provided configuration.
 * @param conf Configuration object for controller setup
 * @param source RequestSource for handling requests
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,connect,org.apache.hadoop.fs.ftp.FTPFileSystem:connect(),141,167,"/**
* Establishes an FTP connection using configuration settings.
* @return connected FTPClient instance
* @throws IOException if connection fails
*/","* Connect to the FTP server using configuration parameters *
   * 
   * @return An FTPClient instance
   * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FSBuilderSupport.java,getPositiveLong,"org.apache.hadoop.fs.impl.FSBuilderSupport:getPositiveLong(java.lang.String,long)",61,69,"/**
* Retrieves a long value for a given key with a default.
* @param key configuration key
* @param defVal default value if retrieval fails or value is negative
* @return retrieved long value or default if negative
*/","* Get a long value with resilience to unparseable values.
   * Negative values are replaced with the default.
   * @param key key to log
   * @param defVal default value
   * @return long value",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(),1282,1286,"/**
 * Calls deprecated method fs.m1().
 * @return Result of fs.m1()
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getServerDefaults,org.apache.hadoop.fs.FileSystem:getServerDefaults(),939,954,"/**
* Creates FsServerDefaults with default configuration.
* @throws IOException if an I/O error occurs
*/","* Return a set of server default configuration values.
   * @return server default configuration values
   * @throws IOException IO failure
   * @deprecated use {@link #getServerDefaults(Path)} instead",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),2766,2768,"/**
 * Calls another m1 method without parameters.
 * @param f file path (unused in this implementation)
 * @return result from the parameterless m1 method
 */","* Return the number of bytes that large input files should be optimally
   * be split into to minimize I/O time.  The given path will be used to
   * locate the actual filesystem.  The full path does not have to exist.
   * @param f path of file
   * @return the default block size for the path's filesystem",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize(),426,429,"/**
 * Delegates to fs.m1() to get a value.
 * @return result from fs.m1()
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,reportChecksumFailure,"org.apache.hadoop.fs.LocalFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)",99,149,"/**
* Moves a bad file to a designated directory.
* @param p path to the file
* @param in input stream of the file
* @param sums input stream for checksums
* @return always false
*/","* Moves files to a bad file directory on the same device, so that their
   * storage will not be reused.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,<init>,org.apache.hadoop.fs.DU:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),43,48,"/**
* Constructs a DU instance using a builder.
* @param builder configuration builder for DU
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,<init>,org.apache.hadoop.fs.CachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),60,66,"/**
* Constructs a new CachingGetSpaceUsed instance.
* @param builder builder object containing configuration settings
* @throws IOException if an I/O error occurs during initialization
*/","* This is the constructor used by the builder.
   * All overriding classes should implement this.
   *
   * @param builder builder.
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/WindowsGetSpaceUsed.java,<init>,org.apache.hadoop.fs.WindowsGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),34,40,"/**
* Initializes space usage monitoring with specified parameters.
* @param builder configuration for space usage monitoring
* @throws IOException if an I/O error occurs during initialization
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getOwner,org.apache.hadoop.io.nativeio.NativeIO:getOwner(java.io.FileDescriptor),934,954,"/**
 * Retrieves file owner's username.
 * @param fd FileDescriptor object representing the file
 * @return Username of the file owner or null if not found
 * @throws IOException if an I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration),135,137,"/**
 * Constructs a ShellBasedIdMapping with default behavior.
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initHM,org.apache.hadoop.ha.ZKFailoverController:initHM(),323,328,"/**
* Initializes and configures a HealthMonitor instance.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,<init>,"org.apache.hadoop.fs.TrashPolicyDefault:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)",79,82,"/**
* Initializes TrashPolicy with FileSystem and Configuration.
* @param fs FileSystem instance
* @param conf Configuration settings
* @throws IOException if initialization fails
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,close,org.apache.hadoop.fs.viewfs.ViewFileSystem:close(),1986,2007,"/**
* Overrides and extends base method functionality.
* Conditionally interacts with cache or file systems based on enableInnerCache.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.DataOutputBuffer),2565,2584,"/**
* Writes compressed data to buffer.
* @param buffer output buffer for data
* @return key length or -1 if end of file
* @throws IOException on I/O errors
*/","@deprecated Call {@link #nextRaw(DataOutputBuffer,SequenceFile.ValueBytes)}.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,getDefaultRetryPolicy,"org.apache.hadoop.io.retry.RetryUtils:getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String,java.lang.String)",59,85,"/**
 * Creates a retry policy based on configuration.
 * @param conf configuration object
 * @param retryPolicyEnabledKey key for enabling retry policy
 * @param defaultRetryPolicyEnabled default value for retry policy enabled
 * @param retryPolicySpecKey key for retry policy specification
 * @param defaultRetryPolicySpec default retry policy specification
 * @param remoteExceptionToRetry exception class to retry on
 * @return configured RetryPolicy or TRY_ONCE_THEN_FAIL if none set
 */","* Return the default retry policy set in conf.
   * 
   * If the value retryPolicyEnabledKey is set to false in conf,
   * use TRY_ONCE_THEN_FAIL.
   * 
   * Otherwise, get the MultipleLinearRandomRetry policy specified in the conf
   * and then
   * (1) use multipleLinearRandomRetry for
   *     - remoteExceptionToRetry, or
   *     - IOException other than RemoteException, or
   *     - ServiceException; and
   * (2) use TRY_ONCE_THEN_FAIL for
   *     - non-remoteExceptionToRetry RemoteException, or
   *     - non-IOException.
   *     
   *
   * @param conf configuration.
   * @param retryPolicyEnabledKey     conf property key for enabling retry
   * @param defaultRetryPolicyEnabled default retryPolicyEnabledKey conf value 
   * @param retryPolicySpecKey        conf property key for retry policy spec
   * @param defaultRetryPolicySpec    default retryPolicySpecKey conf value
   * @param remoteExceptionToRetry    The particular RemoteException to retry
   * @return the default retry policy.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCodec,org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:getCodec(),273,273,"/**
* Returns a compression codec based on function mask.
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createDecompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",275,277,"/**
 * Masks input stream data using a decompressor.
 * @param downStream original input stream
 * @param decompressor tool to decompress data
 * @param downStreamBufferSize buffer size for the input stream
 * @return InputStream with masked data
 * @throws IOException if an I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createCompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",279,281,"/**
 * Creates an OutputStream that applies masking.
 * @param downStream original output stream
 * @param compressor compression algorithm to use
 * @param downStreamBufferSize buffer size for the downstream stream
 * @return OutputStream with masking applied
 * @throws IOException if I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,init,"org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:init(java.lang.String,java.lang.String)",59,64,"/**
* Retrieves JVM metrics instance.
* @param processName name of the process
* @param sessionId session identifier
* @return JvmMetrics object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)",1349,1351,"/**
* Constructs a Client with specified value class and configuration.
* @param valueClass type of writable objects to handle
* @param conf Hadoop configuration settings
*/","* Construct an IPC client with the default SocketFactory.
   * @param valueClass input valueClass.
   * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,"org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,java.lang.Class)",50,68,"/**
* Retrieves or creates a client instance.
* @param conf configuration settings
* @param factory socket factory for creating connections
* @param valueClass class type for writable values
* @return Client object
*/","* Construct &amp; cache an IPC client with the user-provided SocketFactory
   * if no cached client exists.
   * 
   * @param conf Configuration
   * @param factory SocketFactory for client socket
   * @param valueClass Class of the expected response
   * @return an IPC client",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>(org.apache.hadoop.conf.Configuration),73,85,"/**
* Initializes FsUrlStreamHandlerFactory with configuration.
* @param conf Configuration object to be used
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,createURLStreamHandler,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler(java.lang.String),87,111,"/**
* Creates a URLStreamHandler for a given protocol.
* @param protocol the protocol name
* @return URLStreamHandler instance or null if unknown
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,excludeIncompatibleCredentialProviders,"org.apache.hadoop.security.ProviderUtils:excludeIncompatibleCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.Class)",141,199,"/**
* Filters credential providers to avoid recursive dependencies.
* @param config Configuration object containing provider paths
* @param fileSystemClass Class representing the filesystem type
* @return New Configuration object with filtered provider paths
*/","* There are certain integrations of the credential provider API in
   * which a recursive dependency between the provider and the hadoop
   * filesystem abstraction causes a problem. These integration points
   * need to leverage this utility method to remove problematic provider
   * types from the existing provider path within the configuration.
   *
   * @param config the existing configuration with provider path
   * @param fileSystemClass the class which providers must be compatible
   * @return Configuration clone with new provider path
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,get,"org.apache.hadoop.fs.AbstractFileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)",263,266,"/**
 * Creates an abstract file system instance.
 * @param uri file system URI
 * @param conf configuration settings
 * @return AbstractFileSystem object
 * @throws UnsupportedFileSystemException if the URI is unsupported
 */","* The main factory method for creating a file system. Get a file system for
   * the URI's scheme and authority. The scheme of the <code>uri</code>
   * determines a configuration property name,
   * <tt>fs.AbstractFileSystem.<i>scheme</i>.impl</tt> whose value names the
   * AbstractFileSystem class.
   * 
   * The entire URI and conf is passed to the AbstractFileSystem factory method.
   * 
   * @param uri for the file system to be created.
   * @param conf which is passed to the file system impl.
   * 
   * @return file system for the given URI.
   * 
   * @throws UnsupportedFileSystemException if the file system for
   *           <code>uri</code> is not supported.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,setConf,org.apache.hadoop.security.CompositeGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),138,145,"/**
* Updates configuration and initializes combined status.
* @param conf new configuration settings
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.RpcClientUtil:getProtocolMetaInfoProxy(java.lang.Object,org.apache.hadoop.conf.Configuration)",179,187,"/**
* Retrieves ProtocolMetaInfoPB using proxy and configuration.
* @param proxy the proxy object
* @param conf the configuration settings
* @return ProtocolMetaInfoPB instance
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,build,org.apache.hadoop.ipc.RPC$Builder:build(),975,991,"/**
 * Initializes and returns a Server instance.
 * @throws IOException if an I/O error occurs
 * @throws HadoopIllegalArgumentException if required fields are not set
 */","* @return Build the RPC Server.
     * @throws IOException on error
     * @throws HadoopIllegalArgumentException when mandatory fields are not set",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicy.java,getInstance,"org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",139,146,"/**
* Creates and configures a trash policy instance.
* @param conf configuration object for the file system
* @param fs file system where trash is applied
* @param home home directory path
* @return configured TrashPolicy instance
*/","* Get an instance of the configured TrashPolicy based on the value
   * of the configuration parameter fs.trash.classname.
   *
   * @param conf the configuration to be used
   * @param fs the file system to be used
   * @param home the home directory
   * @return an instance of TrashPolicy
   * @deprecated Use {@link #getInstance(Configuration, FileSystem)} instead.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicy.java,getInstance,"org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",156,162,"/**
* Creates and initializes a TrashPolicy instance.
* @param conf configuration settings
* @param fs file system reference
* @return configured TrashPolicy object
*/","* Get an instance of the configured TrashPolicy based on the value
   * of the configuration parameter fs.trash.classname.
   *
   * @param conf the configuration to be used
   * @param fs the file system to be used
   * @return an instance of TrashPolicy",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getMountTableConfigLoader,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountTableConfigLoader(org.apache.hadoop.conf.Configuration),181,203,"/**
* Loads mount table configuration.
* @param conf Configuration object
* @return MountTableConfigLoader instance
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getKlass,org.apache.hadoop.fs.GetSpaceUsed$Builder:getKlass(),74,89,"/**
* Returns the class for space used calculation.
* Uses WindowsGetSpaceUsed for Windows, DU otherwise.
* @return Class of type GetSpaceUsed or configured alternative
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getInstance,"org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.net.InnerNode$Factory)",77,83,"/**
* Creates a network topology instance.
* @param conf configuration object
* @param factory InnerNode factory
* @return configured NetworkTopology object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String)",68,75,"/**
* Creates a DomainNameResolver instance from configuration.
* @param conf Configuration object containing settings
* @param configKey Key for resolver class in configuration
* @return DomainNameResolver instance or default if not specified
*/","* This function gets the instance based on the config.
   *
   * @param conf Configuration
   * @param configKey config key name.
   * @return Domain name resolver.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getInstance,org.apache.hadoop.security.SaslPropertiesResolver:getInstance(org.apache.hadoop.conf.Configuration),52,58,"/**
* Creates a SaslPropertiesResolver instance from configuration.
* @param conf Configuration object containing SASL properties
* @return SaslPropertiesResolver instance configured by the provided settings
*/","* Returns an instance of SaslPropertiesResolver.
   * Looks up the configuration to see if there is custom class specified.
   * Constructs the instance by passing the configuration directly to the
   * constructor to achieve thread safety using final fields.
   * @param conf configuration.
   * @return SaslPropertiesResolver",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,<init>,"org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)",104,153,"/**
* Initializes group mapping with configuration and timer.
* @param conf Hadoop configuration object
* @param timer Timer for cache operations
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateSasl,org.apache.hadoop.security.KDiag:validateSasl(java.lang.String),733,748,"/**
* Resolves SASL properties using a specified resolver key.
* @param saslPropsResolverKey key for the SASL property resolver
*/","* Try to load the SASL resolver.
   * @param saslPropsResolverKey key for the SASL resolver",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getInstance,org.apache.hadoop.security.authorize.ProxyUsers:getInstance(org.apache.hadoop.conf.Configuration),47,53,"/**
* Retrieves and instantiates the impersonation provider class.
* @param conf configuration object
* @return instantiated ImpersonationProvider or default if not specified
*/","* Returns an instance of ImpersonationProvider.
   * Looks up the configuration to see if there is custom class specified.
   * @param conf
   * @return ImpersonationProvider",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),84,100,"/**
* Configures the random number generator based on configuration.
* @param conf Configuration object containing security settings
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,<init>,"org.apache.hadoop.util.ShutdownHookManager$HookEntry:<init>(java.lang.Runnable,int)",205,209,"/**
 * Constructs a HookEntry with specified hook and priority.
 * @param hook the Runnable task to be executed
 * @param priority execution priority of the hook
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,shutdownExecutor,org.apache.hadoop.util.ShutdownHookManager:shutdownExecutor(org.apache.hadoop.conf.Configuration),142,162,"/**
* Handles graceful shutdown of the executor.
* @param conf configuration settings
*/","* Shutdown the executor thread itself.
   * @param conf the configuration containing the shutdown timeout setting.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPasswordFromCredentialProviders,"org.apache.hadoop.security.LdapGroupsMapping:getPasswordFromCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",893,906,"/**
* Retrieves password from configuration by alias.
* @param config Configuration object
* @param alias Password alias
* @param defaultPass Default password if retrieval fails
* @return Retrieved or default password
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPassword,org.apache.hadoop.conf.Configuration:getPassword(java.lang.String),2418,2428,"/**
 * Masks a given name to produce a character array.
 * @param name the input string to be masked
 * @return char[] representing the masked password or null if masking fails
 * @throws IOException if an I/O error occurs during masking
 */","* Get the value for a known password configuration element.
   * In order to enable the elimination of clear text passwords in config,
   * this method attempts to resolve the property name as an alias through
   * the CredentialProvider API and conditionally fallsback to config.
   * @param name property name
   * @return password
   * @throws IOException when error in fetching password",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawEncoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",131,137,"/**
* Creates an encoder with mask functionality.
* @param conf configuration settings
* @param codec erasure codec name
* @param coderOptions options for the erasure coder
* @return RawErasureEncoder instance
*/","* Create RS raw encoder according to configuration.
   * @param conf configuration
   * @param coderOptions coder options that's used to create the coder
   * @param codec the codec to use. If null, will use the default codec
   * @return raw encoder",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawDecoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",146,152,"/**
* Creates a raw erasure decoder.
* @param conf configuration settings
* @param codec codec identifier
* @param coderOptions options for the erasure coder
* @return RawErasureDecoder instance
*/","* Create RS raw decoder according to configuration.
   * @param conf configuration
   * @param coderOptions coder options that's used to create the coder
   * @param codec the codec to use. If null, will use the default codec
   * @return raw decoder",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,tryFence,"org.apache.hadoop.ha.SshFenceByTcpPort:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",80,115,"/**
 * Performs fencing operation on a target service.
 * @param target HAServiceTarget object representing the target service
 * @param argsStr arguments string for the fencing operation
 * @return true if fencing is successful, false otherwise
 * @throws BadFencingConfigurationException if configuration is invalid
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,isProxyServer,org.apache.hadoop.security.authorize.ProxyServers:isProxyServer(java.lang.String),47,52,"/**
* Checks if given IP is a proxy server.
* @param remoteAddr IP address to check
* @return true if IP is a proxy, false otherwise
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,<init>,"org.apache.hadoop.ipc.CallQueueManager:<init>(java.lang.Class,java.lang.Class,boolean,int,java.lang.String,org.apache.hadoop.conf.Configuration)",78,96,"/**
* Initializes a CallQueueManager with specified configurations.
* @param backingClass Class for the blocking queue implementation
* @param schedulerClass Class for the RPC scheduler
* @param clientBackOffEnabled Flag to enable client back-off
* @param maxQueueSize Maximum size of the call queue
* @param namespace Configuration namespace
* @param conf Configuration object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,swapQueue,"org.apache.hadoop.ipc.CallQueueManager:swapQueue(java.lang.Class,java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration)",464,495,"/**
* Initializes a new scheduler and queue for RPC operations.
* @param schedulerClass Class of the RpcScheduler to use
* @param queueClassToUse Class of the BlockingQueue implementation
* @param maxSize Maximum size of the queue
* @param ns Namespace identifier
* @param conf Configuration settings
*/","* Replaces active queue with the newly requested one and transfers
   * all calls to the newQ before returning.
   *
   * @param schedulerClass input schedulerClass.
   * @param queueClassToUse input queueClassToUse.
   * @param maxSize input maxSize.
   * @param ns input ns.
   * @param conf input configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,create,"org.apache.hadoop.ipc.metrics.RpcMetrics:create(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)",115,118,"/**
* Creates and registers RPC metrics.
* @param server the server instance
* @param conf configuration settings
* @return RpcMetrics object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,int[],boolean,org.apache.hadoop.conf.Configuration)",119,154,"/**
* Initializes a FairCallQueue with specified priority levels and capacity.
* @param priorityLevels number of priority queues
* @param capacity total call capacity
* @param ns namespace for logging
* @param capacityWeights weights for each queue's capacity
* @param serverFailOverEnabled flag to enable failover
* @param conf configuration settings
*/","* Create a FairCallQueue.
   * @param priorityLevels the total size of all multi-level queue
   *                       priority policies
   * @param capacity the total size of all sub-queues
   * @param ns the prefix to use for configuration
   * @param capacityWeights the weights array for capacity allocation
   *                        among subqueues
   * @param serverFailOverEnabled whether or not to enable callqueue overflow trigger failover
   *                              for stateless servers when RPC call queue is filled
   * @param conf the configuration to read from
   * Notes: Each sub-queue has a capacity of `capacity / numSubqueues`.
   * The first or the highest priority sub-queue has an excess capacity
   * of `capacity % numSubqueues`",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,initializeWebServer,"org.apache.hadoop.http.HttpServer2:initializeWebServer(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String[])",726,795,"/**
* Configures and starts a web server with specified settings.
* @param name application name
* @param hostName server bind address
* @param conf configuration settings
* @param pathSpecs URL path specifications
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseCostProvider,"org.apache.hadoop.ipc.DecayRpcScheduler:parseCostProvider(java.lang.String,org.apache.hadoop.conf.Configuration)",281,309,"/**
 * Retrieves a CostProvider for the given namespace.
 * @param ns namespace string
 * @param conf configuration object
 * @return CostProvider instance or default if none found
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseIdentityProvider,"org.apache.hadoop.ipc.DecayRpcScheduler:parseIdentityProvider(java.lang.String,org.apache.hadoop.conf.Configuration)",312,337,"/**
 * Retrieves an IdentityProvider from configuration.
 * @param ns namespace string
 * @param conf Configuration object
 * @return IdentityProvider instance or default UserIdentityProvider if none found
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,store,"org.apache.hadoop.io.DefaultStringifier:store(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.String)",110,117,"/**
 * Masks an item in configuration by serializing it.
 * @param conf Configuration object
 * @param item Item to be masked
 * @param keyName Key for storing the serialized item
 */","* Stores the item in the configuration with the given keyName.
   * 
   * @param <K>  the class of the item
   * @param conf the configuration to store
   * @param item the object to be stored
   * @param keyName the name of the key to use
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,load,"org.apache.hadoop.io.DefaultStringifier:load(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)",130,140,"/**
* Retrieves and deserializes a configuration value.
* @param conf Configuration object
* @param keyName key for the configuration value
* @param itemClass class of the expected return type
* @return deserialized object or null if not found
* @throws IOException if an I/O error occurs
*/","* Restores the object from the configuration.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use
   * @param keyName the name of the key to use
   * @param itemClass the class of the item
   * @return restored object
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,storeArray,"org.apache.hadoop.io.DefaultStringifier:storeArray(org.apache.hadoop.conf.Configuration,java.lang.Object[],java.lang.String)",153,171,"/**
* Masks configuration with serialized items.
* @param conf Configuration object
* @param items Array of items to serialize
* @param keyName Key for storing the masked value in config
* @throws IOException if serialization fails
*/","* Stores the array of items in the configuration with the given keyName.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use 
   * @param items the objects to be stored
   * @param keyName the name of the key to use
   * @throws IndexOutOfBoundsException if the items array is empty
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,loadArray,"org.apache.hadoop.io.DefaultStringifier:loadArray(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)",184,203,"/**
* Parses configuration key into an array of items.
* @param conf Configuration object
* @param keyName key to fetch from configuration
* @param itemClass class type of the items in the array
* @return array of parsed items or empty if key not found
*/","* Restores the array of objects from the configuration.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use
   * @param keyName the name of the key to use
   * @param itemClass the class of the item
   * @return restored object
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",1257,1266,"/**
 * Initializes a Writer with specified configuration and file system parameters.
 * @param fs FileSystem instance for file operations
 * @param conf Configuration settings
 * @param name Path to the file to be written
 * @param keyClass Class of keys in the data
 * @param valClass Class of values in the data
 * @param bufferSize Size of the buffer used for writing
 * @param replication Number of replicas for the file
 * @param blockSize Block size for the file
 * @param progress Progressable instance to report progress
 * @param metadata Metadata associated with the file
 * @throws IOException if an I/O error occurs
 */","* Create the named file with write-progress reporter.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param bufferSize input bufferSize.
     * @param replication input replication.
     * @param blockSize input blockSize.
     * @param progress input progress.
     * @param metadata input metadata.
     * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,copy,"org.apache.hadoop.util.ReflectionUtils:copy(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.Object)",346,361,"/**
 * Masks a source object by serializing and deserializing it.
 * @param conf configuration settings
 * @param src source object to mask
 * @param dst destination object for masked data
 * @return masked destination object
 */","* Make a copy of the writable object using serialization to a buffer.
   * @param src the object to copy from
   * @param dst the object to copy into, which is destroyed
   * @param <T> Generics Type.
   * @param conf configuration.
   * @return dst param (the copy)
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,propagateOptions,"org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",140,149,"/**
* Deprecated method to configure and build an FS object.
* @param builder FSBuilder instance for configuration
* @param conf Configuration settings
* @param optionalPrefix Optional prefix string
* @param mandatoryPrefix Mandatory prefix string
* @return Configured FSBuilder instance
*/","* Propagate options to any builder.
   * {@link FutureIO#propagateOptions(FSBuilder, Configuration, String, String)}
   * @param builder builder to modify
   * @param conf configuration to read
   * @param optionalPrefix prefix for optional settings
   * @param mandatoryPrefix prefix for mandatory settings
   * @param <T> type of result
   * @param <U> type of builder
   * @return the builder passed in.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,run,org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:run(),116,162,"/**
* Reconfigures system settings with new configuration.
* Logs changes and handles exceptions during reconfiguration.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,doGetGroups,"org.apache.hadoop.security.LdapGroupsMapping:doGetGroups(java.lang.String,int)",509,557,"/**
 * Retrieves groups for a given user.
 * @param user username to search for
 * @param goUpHierarchy number of levels to traverse up the hierarchy
 * @return set of group names or empty if none found
 * @throws NamingException if LDAP operations fail
 */","* Perform LDAP queries to get group names of a user.
   *
   * Perform the first LDAP query to get the user object using the user's name.
   * If one-query is enabled, retrieve the group names from the user object.
   * If one-query is disabled, or if it failed, perform the second query to
   * get the groups.
   *
   * @param user user name
   * @return a list of group names for the user. If the user can not be found,
   * return an empty string array.
   * @throws NamingException if unable to get group names",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,generateEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(java.lang.String),285,305,"/**
* Generates a new encrypted key version.
* @param encryptionKeyName name of the encryption key
* @return EncryptedKeyVersion object
* @throws IOException if I/O error occurs
* @throws GeneralSecurityException if security error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKeys(java.util.List),356,408,"/**
 * Masks encrypted key versions using the current key version.
 * @param ekvs list of EncryptedKeyVersion objects to be masked
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),433,457,"/**
* Decrypts an EncryptedKeyVersion using the corresponding KeyVersion.
* @param encryptedKeyVersion the encrypted key version to be decrypted
* @return the decrypted KeyVersion object
* @throws IOException if an I/O error occurs during decryption
* @throws GeneralSecurityException if a security exception occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,org.apache.hadoop.conf.Configuration:writeXml(java.io.Writer),3593,3595,"/**
 * Calls overloaded method with null as first argument.
 * @param out Writer to output data
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,writeResponse,"org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String,java.lang.String)",95,105,"/**
 * Writes configuration property to output in specified format.
 * @param conf Configuration object
 * @param out Writer for output
 * @param format Output format (JSON or XML)
 * @param propertyName Property name to write
 * @throws IOException if I/O error occurs
 * @throws IllegalArgumentException if invalid arguments are provided
 * @throws BadFormatException if unsupported format is specified
 */",* Guts of the servlet - extracted for easy testing.,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSources,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSources(),539,543,"/**
* Initializes filtering and configuration settings.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)",555,568,"/**
 * Creates a file output stream for writing.
 * @param f file path
 * @param overwrite flag to overwrite existing file
 * @param createParent flag to create parent directories if needed
 * @param bufferSize buffer size for the output stream
 * @param replication replication factor for the file
 * @param blockSize block size for the file
 * @param progress progress callback for file creation
 * @param permission file permissions
 * @return FSDataOutputStream for writing
 * @throws IOException if an I/O error occurs or file already exists when not overwriting
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createSymlink,"org.apache.hadoop.fs.RawLocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1179,1202,"/**
 * Creates a symbolic link.
 * @param target the path where the symlink points to
 * @param link the path of the symlink to create
 * @param createParent if true, creates parent directories for the symlink
 * @throws IOException if an I/O error occurs or symlinks are not supported
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getFilterProperties,"org.apache.hadoop.http.HttpServer2:getFilterProperties(org.apache.hadoop.conf.Configuration,java.util.List)",872,886,"/**
* Masks configuration properties based on prefixes.
* @param conf Configuration object
* @param prefixes List of prefix strings to filter properties
* @return Properties object with masked configurations
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java,createFilterConfig,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:createFilterConfig(org.apache.hadoop.conf.Configuration),42,51,"/**
* Masks configuration settings.
* @param conf Configuration object
* @return Filtered configuration map
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java,initFilter,"org.apache.hadoop.security.AuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",57,64,"/**
* Masks filters in container based on configuration.
* @param container holds filter configurations
* @param conf application configuration settings
*/","* Initializes hadoop-auth AuthenticationFilter.
   * <p>
   * Propagates to hadoop-auth AuthenticationFilter configuration all Hadoop
   * configuration properties prefixed with ""hadoop.http.authentication.""
   *
   * @param container The filter container
   * @param conf Configuration for run-time parameters",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration),159,183,"/**
 * Initializes ZKDelegationTokenSecretManager with configuration settings.
 * @param conf Configuration object containing token management parameters
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,newZooKeeper,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean,org.apache.zookeeper.client.ZKClientConfig)",555,576,"/**
* Creates a ZooKeeper instance with specified configurations.
* @param connectString comma-separated list of host:port pairs
* @param sessionTimeout session timeout in milliseconds
* @param watcher object to receive notifications
* @param canBeReadOnly allows read-only connections if true
* @param zkClientConfig client configuration settings
* @return ZooKeeper instance
* @throws Exception if connection fails
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,createSaslClient,org.apache.hadoop.security.SaslRpcClient:createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),211,270,"/**
* Creates a SASL client for authentication.
* @param authType authentication type details
* @return SaslClient instance or null if unsupported method
* @throws SaslException on SASL creation error
* @throws IOException on I/O issues
*/","* Try to create a SaslClient for an authentication type.  May return
   * null if the type isn't supported or the client lacks the required
   * credentials.
   * 
   * @param authType - the requested authentication method
   * @return SaslClient for the authType or null
   * @throws SaslException - error instantiating client
   * @throws IOException - misc errors",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,renew,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",189,208,"/**
* Renews a delegation token.
* @param token the token to renew
* @param conf configuration settings
* @return expiration time of the renewed token
* @throws IOException if renewal fails or key provider is incompatible
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,cancel,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",210,229,"/**
* Cancels a delegation token using the provided configuration.
* @param token the token to be canceled
* @param conf the configuration settings
* @throws IOException if cancellation fails or token cannot be handled
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],org.apache.hadoop.conf.Configuration)",89,92,"/**
* Constructs a LoadBalancingKMSClientProvider.
* @param providerUri URI of the key management service provider
* @param providers array of KMSClientProvider instances
* @param conf configuration settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)",94,98,"/**
* Constructs a KMSClientProvider for testing.
* @param providers array of KMS client providers
* @param seed random seed for load balancing
* @param conf configuration settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getFS,org.apache.hadoop.fs.FsShell:getFS(),79,84,"/**
 * Retrieves or initializes the file system.
 * @return the FileSystem instance
 * @throws IOException if an I/O error occurs during initialization
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,initialize,"org.apache.hadoop.fs.sftp.SFTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",493,500,"/**
* Overrides and extends base method functionality.
* @param uriInfo URI information for the request
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,<init>,"org.apache.hadoop.fs.DelegateToFileSystem:<init>(java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",49,57,"/**
* Initializes a file system delegate.
* @param theUri URI of the file system
* @param theFsImpl FileSystem implementation
* @param conf Configuration settings
* @param supportedScheme Supported URI scheme
* @param authorityRequired Flag indicating if authority is required
* @throws IOException on I/O error
* @throws URISyntaxException on invalid URI syntax
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
* Initializes resource with URI and configuration.
* @param name unique resource identifier
* @param conf configuration settings for the resource
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,initialize,"org.apache.hadoop.fs.ftp.FTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",102,133,"/**
* Configures FTP settings from URI and configuration.
* @param uri FTP server URI
* @param conf Configuration object
* @throws IOException if invalid host or credentials are specified
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,initialize,"org.apache.hadoop.fs.RawLocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",130,135,"/**
* Overrides method to initialize with URI and configuration.
* @param uri the resource location
* @param conf the configuration settings
* @throws IOException if an I/O error occurs
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createFileSystem,"org.apache.hadoop.fs.FileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",3604,3629,"/**
 * Creates a FileSystem instance for the given URI.
 * @param uri URI of the file system
 * @param conf configuration settings
 * @return FileSystem object initialized with the provided URI and configuration
 * @throws IOException if initialization fails
 */","* Create and initialize a new instance of a FileSystem.
   * @param uri URI containing the FS schema and FS details
   * @param conf configuration to use to look for the FS instance declaration
   * and to pass to the {@link FileSystem#initialize(URI, Configuration)}.
   * @return the initialized filesystem.
   * @throws IOException problems loading or initializing the FileSystem",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,initialize,"org.apache.hadoop.fs.viewfs.ViewFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",310,384,"/**
 * Initializes ViewFileSystem with given URI and configuration.
 * @param theUri file system URI
 * @param conf configuration settings
 * @throws IOException on initialization failure
 */","* Called after a new FileSystem instance is constructed.
   * @param theUri a uri whose authority section names the host, port, etc. for
   *        this FileSystem
   * @param conf the configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,createFileSystem,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",277,291,"/**
 * Creates a FileSystem instance for the given URI.
 * @param uri the target URI
 * @param conf configuration settings
 * @return FileSystem object configured for the URI
 * @throws IOException if an I/O error occurs or unsupported file system
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,initialize,"org.apache.hadoop.fs.FilterFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",92,104,"/**
* Overrides m1 to handle URI and Configuration.
* @param name the URI to process
* @param conf the configuration settings
*/","Called after a new FileSystem instance is constructed.
   * @param name a uri whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,initialize,"org.apache.hadoop.fs.LocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",44,53,"/**
 * Checks and sets the file system scheme.
 * @param name URI of the resource
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,checkPath,org.apache.hadoop.fs.HarFileSystem:checkPath(org.apache.hadoop.fs.Path),338,341,"/**
 * Delegates file system operation to underlying implementation.
 * @param path file path to operate on
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,makeQualified,org.apache.hadoop.fs.FileSystem:makeQualified(org.apache.hadoop.fs.Path),682,685,"/**
* Modifies and returns the input path.
* @param path initial file path
* @return modified path after applying transformations
*/","* Qualify a path to one which uses this FileSystem and, if relative,
   * made absolute.
   * @param path to qualify.
   * @return this path if it contains a scheme and authority and is absolute, or
   * a new path that includes a path and authority and is fully qualified
   * @see Path#makeQualified(URI, Path)
   * @throws IllegalArgumentException if the path has a schema/URI different
   * from this FileSystem.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,resolvePath,org.apache.hadoop.fs.FileSystem:resolvePath(org.apache.hadoop.fs.Path),975,978,"/**
 * Applies mask to file path.
 * @param p input file path
 * @return modified file path after masking
 * @throws IOException if an I/O error occurs
 */","* Return the fully-qualified path of path, resolving the path
   * through any symlinks or mount point.
   * @param p path to be resolved
   * @return fully qualified path
   * @throws FileNotFoundException if the path is not present
   * @throws IOException for any other error",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,checkPath,org.apache.hadoop.fs.FilterFileSystem:checkPath(org.apache.hadoop.fs.Path),146,149,"/**
 * Delegates file system operation to underlying implementation.
 * @param path file path to operate on
 */",Check that a Path belongs to this FileSystem.,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,<init>,"org.apache.hadoop.fs.AvroFSInput:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",55,63,"/**
 * Initializes AvroFSInput with file context and path.
 * @param fc FileContext for accessing the file system
 * @param p Path to the file
 * @throws IOException if an I/O error occurs
 */","Construct given a {@link FileContext} and a {@link Path}.
   * @param fc filecontext.
   * @param p the path.
   * @throws IOException If an I/O error occurred.
   *",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,copy,"org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)",2208,2248,"/**
 * Recursively copies files from source to destination.
 * @param src source path
 * @param dst destination path
 * @param deleteSource flag to delete source after copy
 * @param overwrite flag to overwrite existing files
 * @return true if operation successful, else false
 * @throws various exceptions for file access issues
 */","* Copy from src to dst, optionally deleting src and overwriting dst.
     * @param src src.
     * @param dst dst.
     * @param deleteSource - delete src if true
     * @param overwrite  overwrite dst if true; throw IOException if dst exists
     *         and overwrite is false.
     *
     * @return true if copy is successful
     *
     * @throws AccessControlException If access is denied
     * @throws FileAlreadyExistsException If <code>dst</code> already exists
     * @throws FileNotFoundException If <code>src</code> does not exist
     * @throws ParentNotDirectoryException If parent of <code>dst</code> is not
     *           a directory
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>src</code> or <code>dst</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server
     * 
     * RuntimeExceptions:
     * @throws InvalidPathException If path <code>dst</code> is invalid",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory(),641,645,"/**
 * Returns the path from function m1.
 * @return Path object representing the function's path
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4913,4917,"/**
 * Constructs an FSDataInputStreamBuilder.
 * @param fileSystem the FileSystem to use
 * @param path the Path of the file to read from
 */","* Path Constructor.
     * @param fileSystem owner
     * @param path path to open.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",4924,4928,"/**
* Constructs an FSDataInputStreamBuilder.
* @param fileSystem the file system to use
* @param pathHandle the path handle for the input stream
*/","* Construct from a path handle.
     * @param fileSystem owner
     * @param pathHandle path handle of file to open.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,openFile,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)",449,454,"/**
* Opens file input stream with specified policies.
* @param fs FileSystem instance
* @param status FileStatus object
* @param readPolicies read policies as a string
* @return FSDataInputStream for reading the file
* @throws IOException if an I/O error occurs
*/","* Open a file.
   * <p>
   * If the WrappedIO class is found, use it.
   * <p>
   * If not, falls back to the classic {@code fs.open(Path)} call.
   * @param fs filesystem
   * @param status file status
   * @param readPolicies read policy to use
   * @return the input stream
   * @throws IOException any IO failure.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,tryLoadFromPath,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadFromPath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",188,216,"/**
* Retrieves file permissions using a primary and backup path.
* @param path primary file path
* @param backupPath fallback file path
* @return FsPermission object or null if both paths fail
* @throws NoSuchAlgorithmException if algorithm is not found
* @throws CertificateException if certificate error occurs
* @throws IOException if I/O error occurs
*/","* Try loading from the user specified path, else load from the backup
   * path in case Exception is not due to bad/wrong password.
   * @param path Actual path to load from
   * @param backupPath Backup path (_OLD)
   * @return The permissions of the loaded file
   * @throws NoSuchAlgorithmException
   * @throws CertificateException
   * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,loadAndReturnPerm,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadAndReturnPerm(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",252,272,"/**
* Loads keystore and deletes a file.
* @param pathToLoad path to the keystore file
* @param pathToDelete path to the file to delete
* @return FsPermission object from loaded keystore
* @throws NoSuchAlgorithmException if algorithm is not found
* @throws CertificateException if certificate error occurs
* @throws IOException if I/O error happens
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,resetKeyStoreState,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:resetKeyStoreState(org.apache.hadoop.fs.Path),586,598,"/**
* Resets Keystore to a previous state.
* @param path Path to the Keystore file
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)",2921,2924,"/**
* Initializes a Sorter with specified FileSystem and configuration.
* @param fs the FileSystem to use
* @param keyClass the class of keys to sort by
* @param valClass the class of values associated with keys
* @param conf the Configuration object for settings
*/","* Sort and merge files containing the named classes.
     * @param fs input FileSystem.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2Compressor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Compressor(org.apache.hadoop.conf.Configuration),98,101,"/**
* Creates a compressor based on configuration.
* @param conf configuration settings
* @return Bzip2Compressor if condition met, else BZip2DummyCompressor
*/","* Return the appropriate implementation of the bzip2 compressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the bzip2 compressor.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,initialize,"org.apache.hadoop.io.SequenceFile$Reader:initialize(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,long,org.apache.hadoop.conf.Configuration,boolean)",1965,1989,"/**
 * Masks a file by reading and processing its content.
 * @param filename the path to the file
 * @param in input stream for the file
 * @param start starting position in the file
 * @param length number of bytes to mask
 * @param conf configuration settings
 * @param tempReader flag indicating if temporary reader is used
 * @throws IOException if an I/O error occurs
 */",Common work of the constructors.,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createOutputStream,"org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",137,144,"/**
* Creates a compression output stream.
* @param out target output stream
* @param compressor used for compression
* @return CompressionOutputStream for writing compressed data
* @throws IOException if an I/O error occurs
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have compressed
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createCompressor,org.apache.hadoop.io.compress.ZStandardCodec:createCompressor(),162,167,"/**
* Initializes and returns a ZStandard compressor.
* @return ZStandardCompressor instance configured with settings from conf
*/","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createInputStream,"org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",194,201,"/**
* Wraps input stream with decompression.
* @param in input stream to be decompressed
* @param decompressor decompressor for the stream
* @return CompressionInputStream for reading decompressed data
* @throws IOException if an I/O error occurs
*/","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createDecompressor,org.apache.hadoop.io.compress.ZStandardCodec:createDecompressor(),220,224,"/**
 * Initializes and returns a ZStandard decompressor.
 * @return ZStandardDecompressor instance configured with settings from m2(conf)
 */","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.ZStandardCodec:createDirectDecompressor(),236,241,"/**
* Creates a direct decompressor using ZStandard.
* @return DirectDecompressor instance configured with m1(conf)
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,createReader,"org.apache.hadoop.io.file.tfile.BCFile$Reader:createReader(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)",730,734,"/**
 * Creates a BlockReader for the specified algorithm and region.
 * @param compressAlgo compression algorithm to use
 * @param region block region to read
 * @return BlockReader instance
 * @throws IOException if an I/O error occurs
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,org.apache.hadoop.io.file.tfile.Compression$Algorithm)",345,363,"/**
 * Creates a new MetaBlock with the given name and compression algorithm.
 * @param name unique identifier for the block
 * @param compressAlgo algorithm used for compression
 * @return BlockAppender instance for writing to the block
 * @throws IOException if I/O error occurs
 * @throws MetaBlockAlreadyExists if a block with the same name already exists
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareDataBlock,org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareDataBlock(),417,436,"/**
* Creates a new data block.
* @throws IOException if an I/O error occurs
* @return BlockAppender instance for the new block
*/","* Create a Data Block and obtain an output stream for adding data into the
     * block. There can only be one BlockAppender stream active at any time.
     * Data Blocks may not be created after the first Meta Blocks. The caller
     * must call BlockAppender.close() to conclude the block creation.
     * 
     * @return The BlockAppender stream
     * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.conf.Configuration),103,106,"/**
 * Constructs a ScriptBasedMapping with the given configuration.
 * @param conf Configuration object to be set
 */","* Create an instance from the given configuration
   * @param conf configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,setConf,org.apache.hadoop.net.ScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration),85,89,"/**
 * Calls superclass and nested method with configuration.
 * @param conf Configuration object to apply
 */","* {@inheritDoc}.
   * <p>
   * This will get called in the superclass constructor, so a check is needed
   * to ensure that the raw mapping is defined before trying to relaying a null
   * configuration.
   * </p>
   * @param conf input Configuration.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,init,org.apache.hadoop.crypto.key.KeyShell:init(java.lang.String[]),80,168,"/**
* Parses command-line arguments for key management operations.
* @param args array of command-line arguments
* @return exit code (0 for success, 1 for error)
*/","* Parse the command line arguments and initialize the data.
   * <pre>
   * % hadoop key create keyName [-size size] [-cipher algorithm]
   *    [-provider providerPath]
   * % hadoop key roll keyName [-provider providerPath]
   * % hadoop key list [-provider providerPath]
   * % hadoop key delete keyName [-provider providerPath] [-i]
   * % hadoop key invalidateCache keyName [-provider providerPath]
   * </pre>
   * @param args Command line arguments.
   * @return 0 on success, 1 on failure.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataInputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",33,36,"/**
* Initializes a CryptoFSDataInputStream with encryption.
* @param in the underlying input stream
* @param codec the cryptographic codec to use
* @param key the encryption key
* @param iv the initialization vector
* @throws IOException if an I/O error occurs
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long)",125,128,"/**
* Initializes CryptoOutputStream with encryption parameters.
* @param out underlying output stream
* @param codec encryption codec to use
* @param key encryption key
* @param iv initialization vector
* @param streamOffset initial offset for the stream
* @throws IOException if an I/O error occurs
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getConnectionId,"org.apache.hadoop.ipc.Client$ConnectionId:getConnectionId(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)",1798,1817,"/**
* Creates a ConnectionId with specified parameters.
* @param addr InetSocketAddress for the connection
* @param protocol Class representing the protocol
* @param ticket UserGroupInformation for authentication
* @param rpcTimeout timeout for RPC calls in milliseconds
* @param connectionRetryPolicy policy for retrying connections, defaults if null
* @param conf configuration settings
* @return ConnectionId object configured with given parameters
*/","* Returns a ConnectionId object. 
     * @param addr Remote address for the connection.
     * @param protocol Protocol for RPC.
     * @param ticket UGI
     * @param rpcTimeout timeout
     * @param conf Configuration object
     * @return A ConnectionId instance
     * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,open,"org.apache.hadoop.fs.ftp.FTPFileSystem:open(org.apache.hadoop.fs.Path,int)",276,306,"/**
 * Opens a file for reading using FTP.
 * @param file path of the file to read
 * @param bufferSize size of the buffer for data transfer
 * @return FSDataInputStream for reading the file
 * @throws IOException if an I/O error occurs or file is not found
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,create,"org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",312,375,"/**
 * Creates a file in the specified path with given permissions.
 * @param file target file path
 * @param permission file permissions
 * @param overwrite flag to overwrite existing file
 * @param bufferSize buffer size for data transfer
 * @param replication replication factor (not used)
 * @param blockSize block size (not used)
 * @param progress progress callback (not used)
 * @return FSDataOutputStream for writing to the file
 * @throws IOException if an I/O error occurs
 */","* A stream obtained via this call must be closed before using other APIs of
   * this class or else the invocation will block.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,delete,"org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",400,409,"/**
* Uploads a file to an FTP server.
* @param file the local file to upload
* @param recursive true if directories should be uploaded recursively
* @return true if the upload is successful, false otherwise
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,listStatus,org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.hadoop.fs.Path),468,477,"/**
 * Retrieves file status from an FTP server.
 * @param file path to the file
 * @return array of FileStatus objects
 * @throws IOException if an I/O error occurs
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path),500,509,"/**
* Retrieves file status from an FTP server.
* @param file path to the file on the FTP server
* @return FileStatus object representing the file's status
* @throws IOException if an I/O error occurs during the operation
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",574,583,"/**
* Sets file permissions on an FTP server.
* @param file the path to the file
* @param permission the desired file permissions
* @return true if operation is successful, false otherwise
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,rename,"org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",631,640,"/**
 * Transfers a file from source to destination using FTP.
 * @param src source file path
 * @param dst destination file path
 * @return true if transfer is successful, false otherwise
 * @throws IOException if I/O error occurs during the process
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.ftp.FTPFileSystem:getHomeDirectory(),713,729,"/**
* Retrieves the home directory path from an FTP server.
* @return Path object representing the home directory
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getServerDefaults,org.apache.hadoop.fs.HarFileSystem:getServerDefaults(),1260,1264,"/**
 * Retrieves server defaults.
 * @return FsServerDefaults object
 * @throws IOException if an I/O error occurs
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getServerDefaults,org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults(),158,162,"/**
 * Deprecated method to get server defaults.
 * @return FsServerDefaults object
 * @throws IOException if an I/O error occurs
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getServerDefaults,org.apache.hadoop.fs.FileSystem:getServerDefaults(org.apache.hadoop.fs.Path),963,965,"/**
 * Retrieves default server settings from a specified path.
 * @param p file system path to configuration
 * @return FsServerDefaults object
 * @throws IOException if an I/O error occurs
 */","* Return a set of server default configuration values.
   * @param p path is used to identify an FS since an FS could have
   *          another FS that it could be delegating the call to
   * @return server default configuration values
   * @throws IOException IO failure",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getServerDefaults,org.apache.hadoop.fs.FilterFileSystem:getServerDefaults(),436,439,"/**
 * Retrieves server defaults from file system.
 * @throws IOException if an I/O error occurs
 * @return FsServerDefaults object containing default settings
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),1288,1292,"/**
 * Returns file size.
 * @param f file path
 * @return size of the file in bytes
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean)",1089,1096,"/**
 * Opens a file for writing with specified buffer size and options.
 * @param f file path to write to
 * @param overwrite flag to indicate if existing file should be overwritten
 * @return FSDataOutputStream for writing data
 * @throws IOException if an I/O error occurs
 */","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file to create
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an exception will be thrown.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)",1107,1114,"/**
* Opens a file for writing with specified buffer size and progress tracking.
* @param f file path
* @param progress progress tracker
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * Files are overwritten by default.
   * @param f the file to create
   * @param progress to report progress
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short)",1124,1131,"/**
* Creates a file output stream with specified replication.
* @param f file path
* @param replication desired replication factor
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/","* Create an FSDataOutputStream at the indicated Path.
   * Files are overwritten by default.
   * @param f the file to create
   * @param replication the replication factor
   * @throws IOException IO failure
   * @return output stream1",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)",1143,1149,"/**
* Opens a file for writing with specified parameters.
* @param f file path
* @param replication desired replication factor
* @param progress progress monitor
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * Files are overwritten by default.
   * @param f the file to create
   * @param replication the replication factor
   * @param progress to report progress
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int)",1161,1168,"/**
* Creates or opens an FSDataOutputStream.
* @param f file path
* @param overwrite flag to overwrite existing file
* @param bufferSize size of buffer
* @return FSDataOutputStream for writing
* @throws IOException if I/O error occurs
*/","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file to create
   * @param overwrite if a path with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)",1183,1191,"/**
* Opens an output stream to write data to a file.
* @param f file path
* @param overwrite flag to overwrite existing file
* @param bufferSize size of the buffer
* @param progress callback for progress updates
* @return FSDataOutputStream for writing
* @throws IOException if an I/O error occurs
*/","* Create an {@link FSDataOutputStream} at the indicated Path
   * with write-progress reporting.
   *
   * The frequency of callbacks is implementation-specific; it may be ""none"".
   * @param f the path of the file to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param progress to report progress.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),976,988,"/**
 * Retrieves the default block size for a given file path.
 * @param f file path
 * @return default block size as a long
 * @throws NotInMountpointException if file not found in any mount point
 * @throws RuntimeException for other I/O errors
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),442,445,"/**
 * Delegates to file system to get file size.
 * @param f file path
 * @return size of the file in bytes
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,<init>,"org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",130,139,"/**
 * Initializes a FSDataOutputStreamBuilder with the given FileSystem and Path.
 * @param fileSystem the FileSystem to use for output operations
 * @param p the Path where data will be written
 */","* Constructor.
   *
   * @param fileSystem file system.
   * @param p the path.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DFCachingGetSpaceUsed.java,<init>,org.apache.hadoop.fs.DFCachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),39,42,"/**
 * Constructs a DFCachingGetSpaceUsed instance.
 * @param builder configuration builder
 * @throws IOException if an I/O error occurs
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable),2462,2506,"/**
 * Reads and validates a key from the input stream.
 * @param key Writable object representing the key to be read
 * @return true if key is successfully read, false otherwise
 * @throws IOException if an I/O error occurs or key class mismatch
 */","* @return Read the next key in the file into <code>key</code>, skipping its
     * value.True if another entry exists, and false at end of file.
     *
     * @param key key.
     * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(java.lang.Object),2707,2752,"/**
* Retrieves and validates an object by key.
* @param key unique identifier for the object
* @return retrieved object or null if not found
* @throws IOException if key validation fails or I/O error occurs
*/","* Read the next key in the file, skipping its
     * value.
     *
     * @param key input Object key.
     * @throws IOException raised on errors performing I/O.
     * @return Return null at end of file.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,initSingleton,"org.apache.hadoop.metrics2.source.JvmMetrics:initSingleton(java.lang.String,java.lang.String)",129,131,"/**
 * Retrieves JVM metrics for a specific process and session.
 * @param processName name of the process
 * @param sessionId unique session identifier
 * @return JvmMetrics object containing metrics data
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",162,170,"/**
 * Initializes an Invoker for a specified protocol and connection.
 * @param protocol the remote service interface class
 * @param connId unique identifier for the client connection
 * @param conf configuration settings for the RPC client
 * @param factory socket factory for creating connections
 * @param alignmentContext context for alignment purposes
 */","* This constructor takes a connectionId, instead of creating a new one.
     * @param protocol input protocol.
     * @param connId input connId.
     * @param conf input Configuration.
     * @param factory input factory.
     * @param alignmentContext Alignment context",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration),77,79,"/**
 * Creates a client with default socket factory and object writable class.
 * @param conf configuration settings
 * @return Client instance
 */","* Construct &amp; cache an IPC client with the default SocketFactory
   * and default valueClass if no cached client exists. 
   * 
   * @param conf Configuration
   * @return an IPC client",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,"org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",89,91,"/**
 * Creates a client with specified configuration and socket factory.
 * @param conf configuration settings
 * @param factory socket factory for network communication
 * @return Client instance initialized with given parameters
 */","* Construct &amp; cache an IPC client with the user-provided SocketFactory
   * if no cached client exists. Default response type is ObjectWritable.
   * 
   * @param conf Configuration
   * @param factory SocketFactory for client socket
   * @return an IPC client",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getClient,org.apache.hadoop.ipc.ProtobufRpcEngine2:getClient(org.apache.hadoop.conf.Configuration),370,376,"/**
* Creates a client instance with specified configuration.
* @param conf Configuration object for client setup
* @return Client object initialized with config and factory settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",170,178,"/**
* Initializes an Invoker with protocol and connection details.
* @param protocol the class of the remote service interface
* @param connId unique connection identifier
* @param conf configuration settings for the client
* @param factory socket factory for creating connections
* @param alignmentContext context for data alignment
*/","* This constructor takes a connectionId, instead of creating a new one.
     *
     * @param protocol input protocol.
     * @param connId input connId.
     * @param conf input Configuration.
     * @param factory input factory.
     * @param alignmentContext Alignment context",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getClient,org.apache.hadoop.ipc.ProtobufRpcEngine:getClient(org.apache.hadoop.conf.Configuration),360,366,"/**
* Creates a client instance with specified configuration.
* @param conf Configuration object for client setup
* @return Client object configured with given settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>(),69,71,"/**
 * Constructs a FsUrlStreamHandlerFactory with a default configuration.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,isMethodSupported,"org.apache.hadoop.ipc.RpcClientUtil:isMethodSupported(java.lang.Object,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcKind,long,java.lang.String)",108,147,"/**
* Checks if a method with the given name exists in the specified protocol version.
* @param rpcProxy RPC proxy object
* @param protocol class representing the protocol
* @param rpcKind type of RPC kind
* @param version protocol version
* @param methodName name of the method to check
* @return true if method exists and matches the version, false otherwise
*/","* Returns whether the given method is supported or not.
   * The protocol signatures are fetched and cached. The connection id for the
   * proxy provided is re-used.
   * @param rpcProxy Proxy which provides an existing connection id.
   * @param protocol Protocol for which the method check is required.
   * @param rpcKind The RpcKind for which the method check is required.
   * @param version The version at the client.
   * @param methodName Name of the method.
   * @return true if the method is supported, false otherwise.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,<init>,"org.apache.hadoop.ha.ZKFCRpcServer:<init>(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,org.apache.hadoop.ha.ZKFailoverController,org.apache.hadoop.security.authorize.PolicyProvider)",47,76,"/**
* Initializes ZKFC RPC server.
* @param conf configuration settings
* @param bindAddr address to bind the server
* @param zkfc ZKFailoverController instance
* @param policy security policy provider
* @throws IOException if initialization fails
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,<init>,"org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)",60,63,"/**
* Initializes Trash with given FileSystem and configuration.
* @param fs the FileSystem to use
* @param conf the Configuration settings
* @throws IOException if an I/O error occurs
*/","* Construct a trash can accessor for the FileSystem provided.
   * @param fs the FileSystem
   * @param conf a Configuration
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,build,org.apache.hadoop.fs.GetSpaceUsed$Builder:build(),144,176,"/**
* Creates and configures a space usage object.
* @return GetSpaceUsed instance configured for the current OS
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getInstance,org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration),73,75,"/**
 * Creates network topology using configuration and default factory.
 * @param conf Configuration object
 * @return NetworkTopology instance
 */","* Get an instance of NetworkTopology based on the value of the configuration
   * parameter net.topology.impl.
   * 
   * @param conf the configuration to be used
   * @return an instance of NetworkTopology",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",55,59,"/**
* Resolves domain name using configuration.
* @param conf Configuration object
* @param host Hostname for resolution
* @param configKey Base configuration key
* @return DomainNameResolver instance
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setConfigurationInternal,org.apache.hadoop.security.SecurityUtil:setConfigurationInternal(org.apache.hadoop.conf.Configuration),105,125,"/**
* Configures DNS settings based on provided configuration.
* @param conf Configuration object containing security settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,<init>,"org.apache.hadoop.security.SaslRpcClient:<init>(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",118,125,"/**
* Initializes a new SaslRpcClient.
* @param ugi user group information
* @param protocol RPC protocol class
* @param serverAddr server address to connect to
* @param conf configuration settings
*/","* Create a SaslRpcClient that can be used by a RPC client to negotiate
   * SASL authentication with a RPC server
   * @param ugi - connecting user
   * @param protocol - RPC protocol
   * @param serverAddr - InetSocketAddress of remote server
   * @param conf - Configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,<init>,org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration),100,102,"/**
* Constructs a Groups instance with the given configuration.
* @param conf Configuration object containing settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,"org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String)",70,80,"/**
* Configures impersonation settings for a given prefix.
* @param conf Configuration object
* @param proxyUserPrefix Prefix for proxy user
*/","* Refreshes configuration using the specified Proxy user prefix for
   * properties.
   *
   * @param conf configuration
   * @param proxyUserPrefix proxy user configuration prefix",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),55,59,"/**
* Overrides base method to configure OpenSSL engine.
* @param conf configuration object with security settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,addShutdownHook,"org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int)",294,305,"/**
 * Adds a shutdown hook with a specified priority.
 * @param shutdownHook the Runnable to execute on shutdown
 * @param priority the priority of the shutdown hook
 */","* Adds a shutdownHook with a priority, the higher the priority
   * the earlier will run. ShutdownHooks with same priority run
   * in a non-deterministic order.
   *
   * @param shutdownHook shutdownHook <code>Runnable</code>
   * @param priority priority of the shutdownHook.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getPasswordString,"org.apache.hadoop.http.HttpServer2$Builder:getPasswordString(org.apache.hadoop.conf.Configuration,java.lang.String)",443,450,"/**
* Masks a configuration value by name.
* @param conf Configuration object
* @param name Key for the configuration value
* @return Masked password as a string or null if not found
*/","* A wrapper of {@link Configuration#getPassword(String)}. It returns
     * <code>String</code> instead of <code>char[]</code>.
     *
     * @param conf the configuration
     * @param name the property name
     * @return the password string or null",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPassword,"org.apache.hadoop.security.LdapGroupsMapping:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",914,927,"/**
* Retrieves password from configuration.
* @param conf Configuration object
* @param alias password identifier
* @param defaultPass default password if retrieval fails
* @return retrieved password or default password
*/","* Passwords should not be stored in configuration. Use
   * {@link #getPasswordFromCredentialProviders(
   *            Configuration, String, String)}
   * to avoid reading passwords from a configuration file.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,getPassword,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",302,315,"/**
* Retrieves password from configuration.
* @param conf Configuration object
* @param alias Key for the password
* @param defaultPass Default password if retrieval fails
* @return Password as String, or default password if not found
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getZKAuthInfos,"org.apache.hadoop.security.SecurityUtil:getZKAuthInfos(org.apache.hadoop.conf.Configuration,java.lang.String)",775,791,"/**
* Masks and parses ZooKeeper authentication info.
* @param conf configuration object
* @param configKey key for auth configuration
* @return list of ZKAuthInfo or empty list if none found
* @throws IOException if reading fails
*/","* Utility method to fetch ZK auth info from the configuration.
   *
   * @param conf configuration.
   * @param configKey config key.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @throws ZKUtil.BadAuthFormatException if the auth format is invalid
   * @return ZKAuthInfo List.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,checkCreateRSRawEncoder,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:checkCreateRSRawEncoder(),52,59,"/**
 * Returns a raw erasure encoder.
 * Initializes the encoder if not already created.
 * @return RawErasureEncoder instance
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,checkCreateRSRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateRSRawEncoder(),61,67,"/**
* Initializes and returns a raw erasure encoder.
* @return RawErasureEncoder instance
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,checkCreateXorRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateXorRawEncoder(),69,76,"/**
* Initializes and returns XOR raw erasure encoder.
* @return RawErasureEncoder instance for XOR codec
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,checkCreateXorRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateXorRawEncoder(),75,81,"/**
 * Returns a raw erasure encoder with XOR codec.
 * Initializes if not already created.
 * @return RawErasureEncoder instance
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),40,50,"/**
 * Creates an erasure coding step for masking.
 * @param blockGroup group of blocks to encode
 * @return ErasureCodingStep configured for masking
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),40,51,"/**
* Creates an erasure decoding step for a block group.
* @param blockGroup the group of blocks to decode
* @return an ErasureCodingStep configured for decoding
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,checkCreateRSRawDecoder,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:checkCreateRSRawDecoder(),52,58,"/**
* Returns the RS raw decoder instance.
* Initializes it if not already created.
* @return RawErasureDecoder object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,checkCreateRSRawDecoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateRSRawDecoder(),67,73,"/**
* Returns the RS raw erasure decoder.
* Initializes it if not already created.
* @return RawErasureDecoder instance
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshCallQueue,org.apache.hadoop.ipc.Server:refreshCallQueue(org.apache.hadoop.conf.Configuration),903,915,"/**
* Configures and initializes the call queue with settings from configuration.
* @param conf Configuration object containing server settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,org.apache.hadoop.conf.Configuration)",88,94,"/**
* Initializes a FairCallQueue with default weights.
* @param priorityLevels number of priority levels
* @param capacity total queue capacity
* @param ns namespace for the queue
* @param conf configuration settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,boolean,org.apache.hadoop.conf.Configuration)",96,102,"/**
* Constructs a FairCallQueue with specified parameters.
* @param priorityLevels number of priority levels
* @param capacity total queue capacity
* @param ns namespace identifier
* @param serverFailOverEnabled flag for server failover
* @param conf configuration settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)",236,279,"/**
* Initializes a DecayRpcScheduler with specified parameters.
* @param numLevels number of priority levels
* @param ns namespace string
* @param conf configuration object
*/","* Create a decay scheduler.
   * @param numLevels number of priority levels
   * @param ns config prefix, so that we can configure multiple schedulers
   *           in a single instance.
   * @param conf configuration to use.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,clone,"org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)",218,227,"/**
* Creates a masked copy of the original object.
* @param orig original Writable object to be copied
* @param conf configuration settings for cloning
* @return T new instance of the same type as orig, with data masked
*/","* Make a copy of a writable object using serialization to a buffer.
   *
   * @param <T> Generics Type T.
   * @param orig The object to copy
   * @param conf input Configuration.
   * @return The copied object",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.LdapGroupsMapping:getGroupsSet(java.lang.String),726,760,"/**
* Masks user data with groups.
* @param user username to mask
* @return set of masked group names or empty set on failure
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),327,354,"/**
 * Masks an encrypted key version.
 * @param ekv the EncryptedKeyVersion to mask
 * @return masked EncryptedKeyVersion
 * @throws IOException if I/O error occurs
 * @throws GeneralSecurityException if security error occurs
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,org.apache.hadoop.conf.Configuration:writeXml(java.io.OutputStream),3589,3591,"/**
 * Writes to output stream using UTF-8 encoding.
 * @param out OutputStream to write to
 */","* Write out the non-default properties in this configuration to the given
   * {@link OutputStream} using UTF-8 encoding.
   *
   * @param out the output stream to write to.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,writeResponse,"org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String)",107,110,"/**
 * Calls overloaded method with default null value.
 * @param conf Configuration object
 * @param out Writer to output data
 * @param format Data format string
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configure,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configure(java.lang.String),481,486,"/**
 * Updates configuration and invokes metrics methods.
 * @param prefix string used to configure metrics
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",547,553,"/**
* Opens a file for writing with specified options.
* @param f file path
* @param overwrite flag to overwrite existing file
* @param bufferSize buffer size in bytes
* @param replication replication factor
* @param blockSize block size in bytes
* @param progress progress callback
* @return FSDataOutputStream for writing
* @throws IOException if an I/O error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",592,600,"/**
* Creates a file output stream with specified permissions.
* @param f file path
* @param permission file permissions
* @param overwrite flag to overwrite existing files
* @param bufferSize buffer size for data transfer
* @param replication number of replicas
* @param blockSize block size in bytes
* @param progress progressable object
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",602,610,"/**
* Creates a file output stream for writing to a path.
* @param f file path
* @param permission file permissions
* @param overwrite flag to overwrite existing file
* @param bufferSize buffer size in bytes
* @param replication number of block replicas
* @param blockSize block size in bytes
* @param progress progress tracker
* @return FSDataOutputStream for writing
* @throws IOException if an I/O error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,constructSecretProvider,"org.apache.hadoop.http.HttpServer2:constructSecretProvider(org.apache.hadoop.http.HttpServer2$Builder,javax.servlet.ServletContext)",862,870,"/**
* Provides a SignerSecretProvider using configuration.
* @param b Builder object containing configuration and prefixes
* @param ctx ServletContext for authentication filter
* @return SignerSecretProvider instance
* @throws Exception if configuration fails
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java,initFilter,"org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",53,58,"/**
* Configures a proxy user authentication filter.
* @param container Filter configuration container
* @param conf Configuration settings
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",98,101,"/**
* Initializes ZKSecretManager with configuration and token kind.
* @param conf Configuration object
* @param tokenKind Token kind as Text
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,newZooKeeper,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean)",547,553,"/**
* Creates a ZooKeeper instance with specified configuration.
* @param connectString comma-separated list of servers to connect to
* @param sessionTimeout session timeout in milliseconds
* @param watcher object for monitoring changes
* @param canBeReadOnly indicates if the client can operate in read-only mode
* @return ZooKeeper instance
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,selectSaslClient,org.apache.hadoop.security.SaslRpcClient:selectSaslClient(java.util.List),154,187,"/**
* Selects appropriate SASL auth type from list.
* @param authTypes available authentication types
* @return chosen SaslAuth or null if none suitable
*/","* Instantiate a sasl client for the first supported auth type in the
   * given list.  The auth type must be defined, enabled, and the user
   * must possess the required credentials, else the next auth is tried.
   * 
   * @param authTypes to attempt in the given order
   * @return SaslAuth of instantiated client
   * @throws AccessControlException - client doesn't support any of the auths
   * @throws IOException - misc errors",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,<init>,"org.apache.hadoop.fs.ftp.FtpFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",50,53,"/**
* Initializes an FTP file system with a given URI and configuration.
* @param theUri FTP URI for the file system
* @param conf configuration settings for the file system
* @throws IOException if an I/O error occurs
* @throws URISyntaxException if the URI is invalid
*/","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFs.java,<init>,"org.apache.hadoop.fs.HarFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",28,31,"/**
 * Constructs a Hadoop Archive File System.
 * @param theUri archive file system URI
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs
 * @throws URISyntaxException if the URI is invalid
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,<init>,"org.apache.hadoop.fs.local.RawLocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",55,59,"/**
 * Constructs a RawLocalFs instance.
 * @param theUri file system URI
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs
 * @throws URISyntaxException if the URI is invalid
 */","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.HttpsFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
* Overrides base method to set URI and configuration.
* @param name file URI
* @param conf system configuration
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.HttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
* Initializes URI and configuration.
* @param name target URI
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,initialize,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",150,179,"/**
 * Initializes ViewFileSystemOverloadScheme with given URI and configuration.
 * @param theUri URI to initialize the scheme with
 * @param conf Configuration object containing settings for initialization
 * @throws IOException if an I/O error occurs during initialization
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,initialize,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",135,140,"/**
* Calls superclass method and additional processing.
* @param name URI of the resource
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/","* Called after a new FileSystem instance is constructed.
   * @param name a uri whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkDependencies,"org.apache.hadoop.fs.FileUtil:checkDependencies(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",336,353,"/**
 * Checks for invalid file copying scenarios between source and destination filesystems.
 * @param srcFS source filesystem
 * @param src source path
 * @param dstFS destination filesystem
 * @param dst destination path
 * @throws IOException if copy is to itself or its subdirectory
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",95,104,"/**
* Initializes a MultipartUploaderBuilderImpl with given filesystem and path.
* @param fileSystem the FileSystem to use
* @param p the Path for operations
*/","* Constructor.
   *
   * @param fileSystem fileSystem.
   * @param p path.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.FileStatus)",156,166,"/**
 * Initializes a new PathData instance.
 * @param fs FileSystem object
 * @param pathString string representation of the path
 * @param stat FileStatus object for the path
 * @throws IOException if an I/O error occurs
 */","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it.
   * @param fs the FileSystem
   * @param pathString a String of the path
   * @param stat the FileStatus (may be null if the path doesn't exist)",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hasPathCapability,"org.apache.hadoop.fs.FileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",3487,3501,"/**
* Checks if a path has specified capabilities.
* @param path the file system path to check
* @param capability the capability to verify
* @return true if the path supports the capability, false otherwise
*/","* The base FileSystem implementation generally has no knowledge
   * of the capabilities of actual implementations.
   * Unless it has a way to explicitly determine the capabilities,
   * this method returns false.
   * {@inheritDoc}",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.FileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),4973,4978,"/**
* Applies mask to given path and returns root path.
* @param path input file path
* @return root path after applying mask
* @throws IOException if operation fails
*/","* Return path of the enclosing root for a given path.
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   *
   * Call makeQualified on the param path to ensure its part of the correct filesystem.
   *
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,makeQualified,org.apache.hadoop.fs.FilterFileSystem:makeQualified(org.apache.hadoop.fs.Path),124,139,"/**
 * Modifies the path's scheme if swapScheme is not null.
 * @param path original file path
 * @return modified path with swapped scheme or original path
 */",Make sure that a path specifies a FileSystem.,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,resolvePath,org.apache.hadoop.fs.HarFileSystem:resolvePath(org.apache.hadoop.fs.Path),343,346,"/**
 * Delegates file processing to underlying filesystem.
 * @param p file path to process
 * @return processed file path
 * @throws IOException if an I/O error occurs
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,getFileStatus,"org.apache.hadoop.fs.shell.find.BaseExpression:getFileStatus(org.apache.hadoop.fs.shell.PathData,int)",279,290,"/**
* Retrieves file status, resolving symbolic links if enabled.
* @param item path data of the file
* @param depth current recursion depth
* @return FileStatus object with resolved link or original if not a link
*/","* Returns the {@link FileStatus} from the {@link PathData} item. If the
   * current options require links to be followed then the returned file status
   * is that of the linked file.
   *
   * @param item
   *          PathData
   * @param depth
   *          current depth in the process directories
   * @return FileStatus
   * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,resolvePath,org.apache.hadoop.fs.viewfs.ViewFileSystem:resolvePath(org.apache.hadoop.fs.Path),412,420,"/**
 * Resolves and returns the correct path in the file system.
 * @param f initial file path to resolve
 * @return resolved Path object
 * @throws IOException if an I/O error occurs during resolution
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,resolvePath,org.apache.hadoop.fs.FilterFileSystem:resolvePath(org.apache.hadoop.fs.Path),157,160,"/**
 * Delegates file processing to another filesystem.
 * @param p path to process
 * @return processed path
 * @throws IOException if an I/O error occurs
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,fullPath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:fullPath(org.apache.hadoop.fs.Path),91,97,"/**
* Applies mask to given path.
* @param path original file path
* @return masked file path
*/","* @param path
   * @return  full path including the chroot",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,copy,"org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2173,2178,"/**
 * Moves a file from source to destination.
 * @param src source file path
 * @param dst destination file path
 * @return true if move is successful, false otherwise
 * @throws various exceptions for file access issues
 */","* Copy file from src to dest. See
     * {@link #copy(Path, Path, boolean, boolean)}
     *
     * @param src src.
     * @param dst dst.
     * @throws AccessControlException If access is denied.
     * @throws FileAlreadyExistsException If file <code>src</code> already exists.
     * @throws FileNotFoundException if next file does not exist any more.
     * @throws ParentNotDirectoryException If parent of <code>src</code> is not a
     * directory.
     * @throws UnsupportedFileSystemException If file system for
     * <code>src/dst</code> is not supported.
     * @throws IOException If an I/O error occurred.
     * @return if success copy true, not false.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createDataInputStreamBuilder,"org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4877,4883,"/**
 * Creates a builder for reading files from the specified filesystem.
 * @param fileSystem the filesystem to read from
 * @param path the path of the file to read
 * @return FSDataInputStreamBuilder instance
 */","* Create instance of the standard {@link FSDataInputStreamBuilder} for the
   * given filesystem and path.
   * @param fileSystem owner
   * @param path path to read
   * @return a builder.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createDataInputStreamBuilder,"org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",4892,4898,"/**
* Creates a builder for FSDataInputStream.
* @param fileSystem the file system to use
* @param pathHandle the path handle for the input stream
* @return FSDataInputStreamBuilder instance
*/","* Create instance of the standard {@link FSDataInputStreamBuilder} for the
   * given filesystem and path handle.
   * @param fileSystem owner
   * @param pathHandle path handle of file to open.
   * @return a builder.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,tryLoadIncompleteFlush,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadIncompleteFlush(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",229,250,"/**
 * Determines file permissions based on paths.
 * @param oldPath original file path
 * @param newPath new file path
 * @return FsPermission object, defaulting to ""600"" if unable to determine
 * @throws IOException if I/O error occurs
 * @throws NoSuchAlgorithmException if algorithm is unavailable
 * @throws CertificateException if certificate handling fails
 */","* The KeyStore might have gone down during a flush, In which case either the
   * _NEW or _OLD files might exists. This method tries to load the KeyStore
   * from one of these intermediate files.
   * @param oldPath the _OLD file created during flush
   * @param newPath the _NEW file created during flush
   * @return The permissions of the loaded file
   * @throws IOException
   * @throws NoSuchAlgorithmException
   * @throws CertificateException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createCompressor,org.apache.hadoop.io.compress.BZip2Codec:createCompressor(),147,150,"/**
 * Returns a Bzip2 compressor configured with the given settings.
 * @return Compressor instance for Bzip2 compression
 */","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getMetaBlock,org.apache.hadoop.io.file.tfile.BCFile$Reader:getMetaBlock(java.lang.String),701,710,"/**
 * Reads block data by name.
 * @param name block identifier
 * @return BlockReader instance
 * @throws IOException if I/O error occurs
 * @throws MetaBlockDoesNotExist if block does not exist
 */","* Stream access to a Meta Block.
     * 
     * @param name
     *          meta block name
     * @return BlockReader input stream for reading the meta block.
     * @throws IOException
     * @throws MetaBlockDoesNotExist
     *           The Meta Block with the given name does not exist.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDataBlock,org.apache.hadoop.io.file.tfile.BCFile$Reader:getDataBlock(int),720,728,"/**
* Retrieves a BlockReader for the specified block index.
* @param blockIndex index of the block to read
* @return BlockReader instance for the block
* @throws IOException if an I/O error occurs
*/","* Stream access to a Data Block.
     * 
     * @param blockIndex
     *          0-based data block index.
     * @return BlockReader input stream for reading the data block.
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Writer:close(),303,339,"/**
* Closes the resource, writing necessary metadata if not already in error state.
* Throws IllegalStateException if close is called with an active block appender.
*/","* Close the BCFile Writer. Attempting to use the Writer after calling
     * <code>close</code> is not allowed and may lead to undetermined results.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)",381,385,"/**
 * Creates a block appender with specified name and compression.
 * @param name unique identifier for the block
 * @param compressionName name of the compression algorithm
 * @return BlockAppender instance
 * @throws IOException if an I/O error occurs
 * @throws MetaBlockAlreadyExists if the block already exists
 */","* Create a Meta Block and obtain an output stream for adding data into the
     * block. There can only be one BlockAppender stream active at any time.
     * Regular Blocks may not be created after the first Meta Blocks. The caller
     * must call BlockAppender.close() to conclude the block creation.
     * 
     * @param name
     *          The name of the Meta Block. The name must not conflict with
     *          existing Meta Blocks.
     * @param compressionName
     *          The name of the compression algorithm to be used.
     * @return The BlockAppender stream
     * @throws IOException
     * @throws MetaBlockAlreadyExists
     *           If the meta block with the name already exists.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String),403,406,"/**
 * Appends a block with a given name using default settings.
 * @param name block name to append
 * @return BlockAppender instance for further operations
 * @throws IOException if an I/O error occurs
 * @throws MetaBlockAlreadyExists if the block already exists
 */","* Create a Meta Block and obtain an output stream for adding data into the
     * block. The Meta Block will be compressed with the same compression
     * algorithm as data blocks. There can only be one BlockAppender stream
     * active at any time. Regular Blocks may not be created after the first
     * Meta Blocks. The caller must call BlockAppender.close() to conclude the
     * block creation.
     * 
     * @param name
     *          The name of the Meta Block. The name must not conflict with
     *          existing Meta Blocks.
     * @return The BlockAppender stream
     * @throws MetaBlockAlreadyExists
     *           If the meta block with the name already exists.
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,initDataBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:initDataBlock(),639,644,"/**
* Initializes block appender if not already set.
* @throws IOException if an I/O error occurs during initialization
*/","* Check if we need to start a new data block.
     * 
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",42,47,"/**
* Initializes a new CryptoFSDataOutputStream.
* @param out underlying FSDataOutputStream
* @param codec encryption codec to use
* @param key encryption key
* @param iv initialization vector
* @throws IOException if an I/O error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",120,123,"/**
* Creates a CryptoOutputStream with specified parameters.
* @param out underlying OutputStream for data output
* @param codec encryption/decryption codec to use
* @param key encryption key
* @param iv initialization vector
* @throws IOException if an I/O error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.ftp.FTPFileSystem:getWorkingDirectory(),707,711,"/**
 * Returns the masked path.
 * @return Path object representing the masked path
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getServerDefaults,org.apache.hadoop.fs.HarFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),1266,1269,"/**
 * Retrieves server defaults for a given file path.
 * @param f file path to check
 * @return FsServerDefaults object
 * @throws IOException if an I/O error occurs
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getServerDefaults,org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),164,167,"/**
 * Retrieves default settings for a file system server.
 * @param f file path to check
 * @return FsServerDefaults object with server configuration
 * @throws IOException if an I/O error occurs
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),1004,1013,"/**
 * Retrieves server defaults for a given file path.
 * @param f file path to resolve
 * @return FsServerDefaults object
 * @throws IOException if an I/O error occurs
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getServerDefaults,org.apache.hadoop.fs.FilterFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),452,455,"/**
 * Retrieves default settings for a file system server.
 * @param f file path
 * @return FsServerDefaults object
 * @throws IOException if an I/O error occurs
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processArguments(java.util.LinkedList),392,424,"/**
* Writes data to a destination file.
* @param args list of source paths
* @throws IOException if an I/O error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path),1077,1079,"/**
 * Opens an output stream to write data to a file.
 * @param f path to the file
 * @return FSDataOutputStream for writing
 * @throws IOException if an I/O error occurs
 */","* Create an FSDataOutputStream at the indicated Path.
   * Files are overwritten by default.
   * @param f the file to create
   * @throws IOException IO failure
   * @return output stream.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,close,org.apache.hadoop.io.BloomMapFile$Writer:close(),192,204,"/**
* Writes bloom filter data to disk.
* @throws IOException if I/O operations fail
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,createLogFile,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createLogFile(org.apache.hadoop.fs.Path),688,716,"/**
 * Attempts to open a file for writing, appending a numeric suffix if necessary.
 * @param initial the initial file path
 * @throws IOException if an I/O error occurs
 */","* Create a new log file and return the {@link FSDataOutputStream}. If a
   * file with the specified path already exists, add a suffix, starting with 1
   * and try again. Keep incrementing the suffix until a nonexistent target
   * path is found.
   *
   * Once the file is open, update {@link #currentFSOutStream},
   * {@link #currentOutStream}, and {@#link #currentFilePath} are set
   * appropriately.
   *
   * @param initial the target path
   * @throws IOException thrown if the call to see if the exists fails",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,createOrAppendLogFile,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createOrAppendLogFile(org.apache.hadoop.fs.Path),789,820,"/**
 * Opens a file for writing with UTF-8 encoding.
 * @param targetFile the file to write to
 * @throws IOException if an I/O error occurs
 */","* Create a new log file and return the {@link FSDataOutputStream}. If a
   * file with the specified path already exists, open the file for append
   * instead.
   *
   * Once the file is open, update {@link #currentFSOutStream},
   * {@link #currentOutStream}, and {@#link #currentFilePath}.
   *
   * @param initial the target path
   * @throws IOException thrown if the call to see the append operation fails.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,save,"org.apache.hadoop.util.JsonSerialization:save(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Object,boolean)",293,297,"/**
* Masks file or directory.
* @param fs FileSystem object
* @param path target Path
* @param instance T instance for masking
* @param overwrite flag to allow overwriting existing files
*/","* Save to a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten
   * @param instance instance
   * @throws IOException IO exception.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",1231,1238,"/**
 * Initializes a Writer with specified configuration and file system parameters.
 * @param fs File system where the data will be written
 * @param conf Configuration settings for the writer
 * @param name Path to the output file
 * @param keyClass Class of the key objects
 * @param valClass Class of the value objects
 * @param progress Progressable object for tracking progress
 * @param metadata Metadata associated with the data being written
 * @throws IOException if an I/O error occurs during initialization
 */","* Create the named file with write-progress reporter.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param progress input progress.
     * @param metadata input metadata.
     * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNewFile,org.apache.hadoop.fs.FileSystem:createNewFile(org.apache.hadoop.fs.Path),1495,1503,"/**
 * Masks a file by applying specific operations.
 * @param f Path to the file to be masked
 * @return true if masking is successful, false otherwise
 */","* Creates the given Path as a brand-new zero-length file.  If
   * create fails, or if it already existed, return false.
   * <i>Important: the default implementation is not atomic</i>
   * @param f path to use for create
   * @throws IOException IO failure
   * @return if create new file success true,not false.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4690,4692,"/**
 * Constructs a new FileSystemDataOutputStreamBuilder.
 * @param fileSystem the file system to operate on
 * @param p the path to the file
 */","* Constructor.
     * @param fileSystem owner
     * @param p path to create",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,"org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",2518,2530,"/**
* Validates and processes key-value pair.
* @param key the key to process
* @param val the value to validate
* @return true if processing continues, false otherwise
* @throws IOException if value class does not match expected class
*/","* Read the next key/value pair in the file into <code>key</code> and
     * <code>val</code>.
     * @return Returns true if such a pair exists and false when at
     * end of file.
     *
     * @param key input key.
     * @param val input val.
     * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,read,org.apache.hadoop.fs.shell.Display$TextRecordInputStream:read(),236,257,"/**
* Processes input buffer and writes key-value pair to output buffer.
* @return processed value from input buffer or -1 if no data is available
* @throws IOException if an I/O error occurs during processing
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",143,152,"/**
* Constructs an Invoker for making RPC calls.
* @param protocol the interface of the remote service
* @param addr address of the remote server
* @param ticket user authentication information
* @param conf configuration settings
* @param factory socket factory to use for connections
* @param rpcTimeout timeout for RPC operations
* @param connectionRetryPolicy policy for retrying failed connections
* @param fallbackToSimpleAuth flag indicating if simple auth should be used as a fallback
* @param alignmentContext context for alignment purposes
* @throws IOException if an I/O error occurs during setup
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",80,88,"/**
* Creates a protocol proxy for client communication.
* @param protocol the interface class of the protocol
* @param clientVersion version of the client
* @param connId unique connection identifier
* @param conf configuration settings
* @param factory socket factory for network connections
* @param alignmentContext context for alignment purposes
* @return ProtocolProxy instance for communication
* @throws IOException if an I/O error occurs during proxy creation
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",121,130,"/**
* Creates a proxy for the ProtocolMetaInfoPB interface.
* @param connId connection identifier
* @param conf configuration settings
* @param factory socket factory
* @return ProtocolProxy instance for ProtocolMetaInfoPB
* @throws IOException if an I/O error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getClient,org.apache.hadoop.ipc.WritableRpcEngine:getClient(org.apache.hadoop.conf.Configuration),279,283,"/**
* Retrieves a client instance using configuration.
* @param conf Configuration object
* @return Client instance
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",219,230,"/**
* Initializes a new Invoker instance.
* @param protocol the RPC protocol class
* @param address the server address
* @param ticket user authentication information
* @param conf configuration settings
* @param factory socket factory for connection
* @param rpcTimeout timeout for RPC calls
* @param fallbackToSimpleAuth flag to allow simple auth fallback
* @param alignmentContext context for data alignment
* @throws IOException if an I/O error occurs during initialization
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",103,111,"/**
* Creates a protocol proxy for the given protocol class.
* @param <T> protocol type
* @param protocol protocol class to create proxy for
* @param clientVersion version of the client
* @param connId connection ID
* @param conf configuration settings
* @param factory socket factory
* @param alignmentContext context for alignment
* @return ProtocolProxy instance
* @throws IOException if an I/O error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",128,137,"/**
* Creates a proxy for the ProtocolMetaInfoPB protocol.
* @param connId connection identifier
* @param conf configuration settings
* @param factory socket factory
* @return ProtocolProxy object for communication
* @throws IOException if an I/O error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",150,159,"/**
* Initializes an Invoker with specified parameters.
* @param protocol RPC protocol class
* @param addr socket address for the connection
* @param ticket user group information for authentication
* @param conf configuration settings
* @param factory socket factory for creating connections
* @param rpcTimeout timeout for RPC operations
* @param connectionRetryPolicy policy for retrying failed connections
* @param fallbackToSimpleAuth flag to allow fallback to simple auth
* @param alignmentContext context for alignment purposes
* @throws IOException if an I/O error occurs during initialization
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),60,67,"/**
* Invokes a method on the RPC proxy.
* @param methodName name of the method to invoke
* @return true if successful, false otherwise
* @throws IOException if an I/O error occurs
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),71,78,"/**
* Invokes an RPC method to refresh user mappings.
* @param methodName name of the method to invoke
* @return true if successful, false otherwise
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),106,113,"/**
* Calls remote procedure for method execution.
* @param methodName name of the method to execute remotely
* @return true if successful, false otherwise
* @throws IOException if communication fails
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),60,67,"/**
* Invokes a remote method on the RPC client.
* @param methodName name of the method to invoke
* @return boolean result of the method invocation
* @throws IOException if an I/O error occurs during the call
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),61,66,"/**
* Invokes an RPC to check method availability.
* @param methodName name of the method to check
* @return true if method is available, false otherwise
* @throws IOException if communication fails
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initRPC,org.apache.hadoop.ha.ZKFailoverController:initRPC(),330,334,"/**
* Binds and initializes the ZKFC RpcServer.
* @throws IOException if binding fails
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,<init>,org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.conf.Configuration),50,52,"/**
 * Initializes a Trash instance.
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs
 */","* Construct a trash can accessor.
   * @param conf a Configuration
   * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,main,org.apache.hadoop.fs.DU:main(java.lang.String[]),91,102,"/**
* Calculates disk usage for a specified path.
* @param args optional array where first element is the directory path
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.String)",50,53,"/**
 * Resolves domain name using configuration and URI.
 * @param conf configuration object
 * @param uri target URI
 * @param configKey key for configuration setting
 * @return DomainNameResolver instance
 */","* Create a domain name resolver to convert the domain name in the config to
   * the actual IP addresses of the Namenode/Router/RM.
   *
   * @param conf Configuration to get the resolver from.
   * @param uri the url that the resolver will be used against
   * @param configKey The config key name suffixed with
   *                  the nameservice/yarnservice.
   * @return Domain name resolver.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setConfiguration,org.apache.hadoop.security.SecurityUtil:setConfiguration(org.apache.hadoop.conf.Configuration),98,103,"/**
* Updates configuration settings.
* @param conf Configuration object to be updated
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,<init>,org.apache.hadoop.security.UserGroupInformation$TestingGroups:<init>(org.apache.hadoop.security.Groups),1575,1578,"/**
 * Initializes a new instance of TestingGroups.
 * @param underlyingImplementation the Groups implementation to be used
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingService,org.apache.hadoop.security.Groups:getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration),471,481,"/**
* Returns singleton Groups instance.
* @param conf configuration settings
* @return Groups object initialized with given config
*/","* Get the groups being used to map user-to-groups.
   * @param conf configuration.
   * @return the groups being used to map user-to-groups.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingServiceWithLoadedConfiguration,org.apache.hadoop.security.Groups:getUserToGroupsMappingServiceWithLoadedConfiguration(org.apache.hadoop.conf.Configuration),488,495,"/**
 * Initializes and returns a singleton Groups instance.
 * @param conf configuration settings for group initialization
 * @return Groups object initialized with provided configuration
 */","* Create new groups used to map user-to-groups with loaded configuration.
   * @param conf configuration.
   * @return the groups being used to map user-to-groups.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,init,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:init(javax.servlet.FilterConfig),53,58,"/**
* Initializes filter configuration and sets up proxy users.
* @param filterConfig the filter configuration to be used
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:init(javax.servlet.FilterConfig),177,202,"/**
 * Initializes the filter with configuration and authentication handlers.
 * @param filterConfig configuration for the filter
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration),86,88,"/**
 * Calls m1 with default proxy user configuration.
 * @param conf Configuration object to be used
 */","* Refreshes configuration using the default Proxy user prefix for properties.
   * @param conf configuration",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,deleteOnExit,org.apache.hadoop.fs.FileContext:deleteOnExit(org.apache.hadoop.fs.Path),1706,1724,"/**
* Marks a file for deletion on exit.
* @param f the Path of the file to mark
* @return true if marking is successful, false otherwise
*/","* Mark a path to be deleted on JVM shutdown.
   * 
   * @param f the existing path to delete.
   *
   * @return  true if deleteOnExit is successful, otherwise false.
   *
   * @throws AccessControlException If access is denied
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,register,org.apache.hadoop.service.launcher.ServiceShutdownHook:register(int),61,64,"/**
 * Masks functionality with given priority.
 * @param priority execution priority level
 */","* Register the service for shutdown with Hadoop's
   * {@link ShutdownHookManager}.
   * @param priority shutdown hook priority",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,startupShutdownMessage,"org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.slf4j.Logger)",805,828,"/**
 * Masks a function with logging and shutdown hooks.
 * @param clazz class to mask
 * @param args arguments for masking
 * @param log logger instance
 */","* Print a log message for starting up and shutting down
   * @param clazz the class of the server
   * @param args arguments
   * @param log the target log object",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,loadSSLConfiguration,org.apache.hadoop.http.HttpServer2$Builder:loadSSLConfiguration(),455,483,"/**
* Configures SSL settings from configuration.
* @throws IOException if required properties are missing or invalid
*/",* Load SSL properties from the SSL configuration.,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,loadSslConf,org.apache.hadoop.security.LdapGroupsMapping:loadSslConf(org.apache.hadoop.conf.Configuration),874,891,"/**
 * Configures SSL settings from provided configuration.
 * @param sslConf Configuration object containing SSL settings
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPasswordForBindUser,org.apache.hadoop.security.LdapGroupsMapping:getPasswordForBindUser(java.lang.String),977,991,"/**
* Masks a password using configuration settings.
* @param keyPrefix prefix for configuration keys
* @return masked password or default if not found
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,createTrustManagersFromConfiguration,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createTrustManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,java.lang.String,long)",105,150,"/**
 * Initializes SSL trust manager with specified settings.
 * @param mode SSL factory mode
 * @param truststoreType type of truststore
 * @param truststoreLocation location of the truststore
 * @param storesReloadInterval interval for reloading truststores in milliseconds
 * @throws IOException if an I/O error occurs
 * @throws GeneralSecurityException if a security exception occurs
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,createKeyManagersFromConfiguration,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createKeyManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,long)",160,205,"/**
* Configures SSL key management with specified mode and reload interval.
* @param mode SSL configuration mode
* @param keystoreType type of the keystore (e.g., JKS, PKCS12)
* @param storesReloadInterval interval in milliseconds for reloading keystores
* @throws GeneralSecurityException if keystore properties are missing or invalid
* @throws IOException on I/O errors during keystore configuration
*/","* Implements logic of initializing the KeyManagers with the options
   * to reload keystores.
   * @param mode client or server
   * @param keystoreType The keystore type.
   * @param storesReloadInterval The interval to check if the keystore certificates
   *                             file has changed.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getZKAuths,org.apache.hadoop.util.curator.ZKCuratorManager:getZKAuths(org.apache.hadoop.conf.Configuration),120,123,"/**
* Retrieves ZooKeeper authentication information.
* @param conf configuration object
* @return list of ZKAuthInfo objects
*/","* Utility method to fetch ZK auth info from the configuration.
   *
   * @param conf configuration.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @throws ZKUtil.BadAuthFormatException if the auth format is invalid
   * @return ZKAuthInfo List.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initZK,org.apache.hadoop.ha.ZKFailoverController:initZK(),341,382,"/**
 * Initializes ZooKeeper configuration and creates an ActiveStandbyElector.
 * @throws HadoopIllegalArgumentException if required configurations are missing or invalid
 * @throws IOException for I/O errors during initialization
 * @throws KeeperException for issues with ZooKeeper operations
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),41,50,"/**
 * Creates an erasure encoding step for a block group.
 * @param blockGroup the block group to process
 * @return ErasureCodingStep configured with input blocks and encoder
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),48,59,"/**
 * Creates an erasure coding step for masking.
 * @param blockGroup group of blocks to encode
 * @return ErasureCodingStep configured with input blocks and encoders
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),41,50,"/**
* Implements erasure decoding step.
* @param blockGroup group of EC blocks for processing
* @return ErasureCodingStep configured for decoding
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),49,65,"/**
* Implements erasure coding decoding step.
* @param blockGroup group of EC blocks to process
* @return ErasureCodingStep for decoding
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroupsSet(java.lang.String),92,105,"/**
* Modifies user group names based on rule.
* @param user the username
* @return modified set of group names or original if no change applies
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getGroups,org.apache.hadoop.security.LdapGroupsMapping:getGroups(java.lang.String),357,360,"/**
 * Masks sensitive information in the user string.
 * @param user input string containing user data
 * @return list of masked strings
 */","* Returns list of groups for a user.
   * 
   * The LdapCtx which underlies the DirContext object is not thread-safe, so
   * we need to block around this whole method. The caching infrastructure will
   * ensure that performance stays in an acceptable range.
   *
   * @param user get groups for this user
   * @return list of groups for a given user",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,main,org.apache.hadoop.conf.Configuration:main(java.lang.String[]),3945,3947,"/**
* Configures and executes system setup.
* @param args command line arguments (not used)
*/","For debugging.  List non-default properties to the terminal and exit.
   * @param args the argument to be parsed.
   * @throws Exception exception.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,start,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:start(),178,194,"/**
* Starts the metrics system, ensuring it's not already running.
* @throws MetricsException if the metrics system is illegally restarted
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,<init>,org.apache.hadoop.http.HttpServer2:<init>(org.apache.hadoop.http.HttpServer2$Builder),699,724,"/**
* Initializes the HttpServer2 with configuration from Builder.
* @param b builder containing server configuration
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",118,125,"/**
* Initializes a DelegationTokenManager.
* @param conf configuration settings
* @param tokenKind type of delegation token
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,saslConnect,org.apache.hadoop.security.SaslRpcClient:saslConnect(org.apache.hadoop.ipc.Client$IpcStreams),365,455,"/**
 * Negotiates authentication method using IPC streams.
 * @param ipcStreams communication streams for IPC
 * @return negotiated AuthMethod
 * @throws IOException if an I/O error occurs
 */","* Do client side SASL authentication with server via the given IpcStreams.
   *
   * @param ipcStreams ipcStreams.
   * @return AuthMethod used to negotiate the connection
   * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,<init>,org.apache.hadoop.fs.local.RawLocalFs:<init>(org.apache.hadoop.conf.Configuration),42,44,"/**
 * Constructs a new instance of RawLocalFs.
 * @param conf configuration settings for the filesystem
 * @throws IOException if an I/O error occurs
 * @throws URISyntaxException if the URI is invalid
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,<init>,"org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",37,41,"/**
* Constructs a new FileSystemMultipartUploaderBuilder.
* @param fileSystem the file system to use
* @param path the path within the file system
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String)",111,113,"/**
* Initializes PathData with FileSystem and path string.
* @param fs the file system to use
* @param pathString the path as a string
* @throws IOException if an I/O error occurs
*/","* Looks up the file status for a path.  If the path
   * doesn't exist, then the status will be null
   * @param fs the FileSystem for the path
   * @param pathString a string for a path 
   * @throws IOException if anything goes wrong",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getDirectoryContents,org.apache.hadoop.fs.shell.PathData:getDirectoryContents(),274,285,"/**
* Fetches and sorts directory contents.
* @return Array of PathData objects representing directory items
* @throws IOException if an I/O error occurs
*/","* Returns a list of PathData objects of the items contained in the given
   * directory.
   * @return list of PathData objects for its children
   * @throws IOException if anything else goes wrong...",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,maybeIgnoreMissingDirectory,"org.apache.hadoop.fs.FileUtil:maybeIgnoreMissingDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.FileNotFoundException)",2094,2110,"/**
* Handles directory inconsistency by throwing an exception or logging.
* @param fs FileSystem instance
* @param path Path to the directory
* @param e FileNotFoundException to throw if necessary
* @throws FileNotFoundException if directory is inconsistent and not ignored
*/","* Method to call after a FNFE has been raised on a treewalk, so as to
   * decide whether to throw the exception (default), or, if the FS
   * supports inconsistent directory listings, to log and ignore it.
   * If this returns then the caller should ignore the failure and continue.
   * @param fs filesystem
   * @param path path
   * @param e exception caught
   * @throws FileNotFoundException the exception passed in, if rethrown.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.DelegateToFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",287,292,"/**
 * Checks file system capability for given path.
 * @param path file path to check
 * @param capability capability to verify
 * @return true if capability is supported, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
* Checks if a file system is read-only.
* @param path the path to check
* @param capability the required capability
* @return true if read-only, otherwise delegates to superclass
*/","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.RawLocalFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1315,1332,"/**
* Checks if a path supports specified capability.
* @param path file system path to check
* @param capability capability to verify
* @return true if supported, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.viewfs.ViewFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1350,1371,"/**
* Checks if a path has a given capability.
* @param path the file system path to check
* @param capability the capability to verify
* @return true if the path has the capability, false otherwise
* @throws IOException if an I/O error occurs
*/","* Reject the concat operation; forward the rest to the viewed FS.
   * @param path path to query the capability of.
   * @param capability string to query the stream support for.
   * @return the capability
   * @throws IOException if there is no resolved FS, or it raises an IOE.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),1373,1389,"/**
 * Resolves and returns the path with the closest mount point.
 * @param path input file system path
 * @return resolved path within the closest mount point
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path),1941,1958,"/**
 * Resolves the enclosing path of a given file system path.
 * @param path input file system path
 * @return resolved enclosing path or original path if not applicable
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.FilterFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),735,738,"/**
 * Delegates file system operation to underlying implementation.
 * @param path file path to operate on
 * @return result of the file system operation
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,fileSystem_getEnclosingRoot,"org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_getEnclosingRoot(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",202,204,"/**
 * Applies a mask to a file path.
 * @param fs FileSystem instance
 * @param path original file path
 * @return modified file path after applying the mask
 * @throws IOException if an I/O error occurs
 */","* Return path of the enclosing root for a given path.
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   * @param fs filesystem
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.FilterFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",740,753,"/**
* Checks if a path supports a given capability.
* @param path file system path to check
* @param capability required capability
* @return true if supported, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ViewFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",694,760,"/**
 * Renames a file from source to destination path.
 * @param src source file path
 * @param dst destination file path
 * @return true if rename is successful, false otherwise
 * @throws IOException if an I/O error occurs during the process
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",193,199,"/**
* Overrides method to create a file with specified permissions and settings.
* @param f file path
* @param permission file permissions
* @param overwrite flag to overwrite existing file
* @param bufferSize buffer size for I/O operations
* @param replication replication factor
* @param blockSize block size
* @param progress progress monitor
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",201,208,"/**
 * Overrides method to create a file with specified parameters.
 * @param f file path
 * @param permission file permissions
 * @param flags creation flags
 * @param bufferSize buffer size for I/O operations
 * @param replication number of block replicas
 * @param blockSize size of each block
 * @param progress progress monitor
 * @return FSDataOutputStream for writing to the file
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",210,214,"/**
* Overrides to process file path recursively.
* @param f file path to process
* @param recursive flag indicating if processing should be recursive
* @return true if processed successfully, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",223,228,"/**
* Retrieves block locations for a file status.
* @param fs FileStatus object representing the file
* @param start starting offset of the block
* @param len length of the block
* @return array of BlockLocation objects
* @throws IOException if an I/O error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),230,234,"/**
 * Computes checksum for a file.
 * @param f file path
 * @return FileChecksum object
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",236,240,"/**
 * Computes checksum for a file.
 * @param f path to the file
 * @param length file length
 * @return FileChecksum object
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileStatus(org.apache.hadoop.fs.Path),242,246,"/**
 * Checks file status by path.
 * @param f file path to check
 * @return FileStatus object
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),248,251,"/**
 * Processes file path by applying two transformations.
 * @param f input file path
 * @return transformed file path
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStatus(org.apache.hadoop.fs.Path),259,262,"/**
 * Calls m1 on path and passes result to super.m2.
 * @param p file system path
 * @return FsStatus from super.m2
 * @throws IOException if I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatus(org.apache.hadoop.fs.Path),264,268,"/**
 * Retrieves file status by processing path.
 * @param f file path to process
 * @return array of FileStatus objects
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),270,274,"/**
 * Retrieves file status iterator.
 * @param f file path
 * @return iterator of LocatedFileStatus
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",276,280,"/**
 * Sets file permissions.
 * @param f file path
 * @param permission desired file permissions
 * @return true if successful, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,mkdirs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path),282,285,"/**
 * Overrides method to process file path.
 * @param f file path to be processed
 * @return result of processing
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:open(org.apache.hadoop.fs.Path,int)",287,291,"/**
 * Opens an input stream for the given file path.
 * @param f file path to open
 * @param bufferSize size of the buffer
 * @return FSDataInputStream for reading the file
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",293,297,"/**
* Opens an output stream for writing to a file.
* @param f file path
* @param bufferSize buffer size for writing
* @param progress progress callback
* @return FSDataOutputStream for writing to the specified file
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",299,304,"/**
 * Checks if source path is less than destination path.
 * @param src source file path
 * @param dst destination file path
 * @return true if src is less than dst, false otherwise
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",306,311,"/**
* Calls superclass method with modified file path and user details.
* @param f original file path
* @param username user's name
* @param groupname group's name
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",313,317,"/**
 * Calls superclass method with modified path and given permission.
 * @param f file path to be processed
 * @param permission file system permissions
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",319,323,"/**
* Calls superclass method with processed file path and replication factor.
* @param f file path to process
* @param replication replication factor
* @return result of superclass method call
* @throws IOException if an I/O error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",325,329,"/**
 * Calls superclass method with modified file path and timestamps.
 * @param f file path
 * @param mtime modification time in milliseconds
 * @param atime access time in milliseconds
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",331,335,"/**
* Overrides method to process file path and ACL entries.
* @param path file system path
* @param aclSpec list of access control entries
* @throws IOException if an I/O error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",337,341,"/**
 * Calls superclass method with transformed path and ACL spec.
 * @param path file system path to process
 * @param aclSpec list of access control entries
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),343,346,"/**
 * Calls m1 with given path and passes result to super's m2.
 * @param path file system path
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAcl(org.apache.hadoop.fs.Path),348,351,"/**
 * Calls superclass method with processed file path.
 * @param path file path to process
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",353,356,"/**
* Overrides base method to modify ACLs.
* @param path file system path
* @param aclSpec list of access control entries
* @throws IOException if operation fails
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getAclStatus(org.apache.hadoop.fs.Path),358,361,"/**
 * Retrieves ACL status for a given path.
 * @param path file system path
 * @return AclStatus object representing access control list
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",363,367,"/**
 * Overrides m2 to modify file attributes.
 * @param path file path
 * @param name attribute name
 * @param value attribute value
 * @param flag modification flags
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",369,372,"/**
 * Processes file and returns bytes.
 * @param path file path
 * @param name file name
 * @return byte array of processed file
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path),374,377,"/**
 * Reads and processes files from a given path.
 * @param path directory containing files to process
 * @return map of file names to their byte content
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",379,383,"/**
* Reads files and returns their contents.
* @param path directory containing files
* @param names list of file names to read
* @return map of file names to byte arrays or null if not found
* @throws IOException if an I/O error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:truncate(org.apache.hadoop.fs.Path,long)",385,388,"/**
* Sets file length to newLength.
* @param path file path
* @param newLength desired length of the file
* @return true if successful, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listXAttrs(org.apache.hadoop.fs.Path),390,393,"/**
 * Reads file content and processes it.
 * @param path file path to read from
 * @return list of processed strings
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",395,398,"/**
 * Calls superclass method with processed file path and name.
 * @param path original file path
 * @param name file name
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",400,403,"/**
 * Modifies and returns a file path.
 * @param path original file path
 * @param name new file name
 * @return modified file path
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",405,409,"/**
* Overrides method to rename a file snapshot.
* @param path path to the file
* @param snapshotOldName old name of the snapshot
* @param snapshotNewName new name for the snapshot
* @throws IOException if an I/O error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",411,415,"/**
* Processes a snapshot file.
* @param snapshotDir directory containing snapshots
* @param snapshotName name of the snapshot to process
* @throws IOException if an I/O error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,resolvePath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:resolvePath(org.apache.hadoop.fs.Path),417,420,"/**
 * Calls m1 on input path and passes result to superclass's m2.
 * @param p input file path
 * @return processed path from superclass method
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getContentSummary(org.apache.hadoop.fs.Path),422,425,"/**
 * Retrieves content summary for a file.
 * @param f file path
 * @return ContentSummary object
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),427,430,"/**
 * Retrieves quota usage for a file.
 * @param f file path
 * @return QuotaUsage object
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),439,442,"/**
 * Calls m1 with file path and passes result to super.m2.
 * @param f file path
 * @return result from super.m2
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),449,452,"/**
 * Calls m1 on file path and passes result to superclass's m2.
 * @param f file path
 * @return short value from superclass's m2
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),464,467,"/**
 * Retrieves block storage policy for a source path.
 * @param src source file path
 * @return BlockStoragePolicySpi object
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),469,472,"/**
 * Calls superclass method with modified path.
 * @param src source file path
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",474,477,"/**
 * Calls superclass method with processed source path and policy name.
 * @param src source file path
 * @param policyName security policy name
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),479,482,"/**
 * Processes file at given path.
 * @param src source file path
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createFile,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createFile(org.apache.hadoop.fs.Path),484,487,"/**
 * Creates an output stream builder for the specified path.
 * @param path file system path
 * @return FSDataOutputStreamBuilder instance
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFile,org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.Path),4761,4765,"/**
* Builds a FutureDataInputStream from a file path.
* @param path file path to read data from
* @return FutureDataInputStreamBuilder instance
* @throws IOException if an I/O error occurs
* @throws UnsupportedOperationException if operation is not supported
*/","* Open a file for reading through a builder API.
   * Ultimately calls {@link #open(Path, int)} unless a subclass
   * executes the open command differently.
   *
   * The semantics of this call are therefore the same as that of
   * {@link #open(Path, int)} with one special point: it is in
   * {@code FSDataInputStreamBuilder.build()} in which the open operation
   * takes place -it is there where all preconditions to the operation
   * are checked.
   * @param path file path
   * @return a FSDataInputStreamBuilder object to build the input stream
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFile,org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.PathHandle),4780,4785,"/**
* Builds a data input stream from a file path.
* @param pathHandle file path handle
* @return FutureDataInputStreamBuilder for further configuration
* @throws IOException if an I/O error occurs
* @throws UnsupportedOperationException if operation is not supported
*/","* Open a file for reading through a builder API.
   * Ultimately calls {@link #open(PathHandle, int)} unless a subclass
   * executes the open command differently.
   *
   * If PathHandles are unsupported, this may fail in the
   * {@code FSDataInputStreamBuilder.build()}  command,
   * rather than in this {@code openFile()} operation.
   * @param pathHandle path handle.
   * @return a FSDataInputStreamBuilder object to build the input stream
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,locateKeystore,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:locateKeystore(),145,176,"/**
 * Loads and masks the keystore with a password.
 * Handles file permissions and throws exceptions on errors.
 */","* Open up and initialize the keyStore.
   * @throws IOException If there is a problem reading the password file
   * or a problem reading the keystore.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,checkTFileDataIndex,org.apache.hadoop.io.file.tfile.TFile$Reader:checkTFileDataIndex(),882,893,"/**
* Loads TFileIndex if not already loaded.
* @throws IOException if an I/O error occurs
*/","* Lazily loading the TFile index.
     * 
     * @throws IOException",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getMetaBlock,org.apache.hadoop.io.file.tfile.TFile$Reader:getMetaBlock(java.lang.String),967,970,"/**
 * Retrieves data input stream by name.
 * @param name resource name
 * @return DataInputStream object
 * @throws IOException if an I/O error occurs
 * @throws MetaBlockDoesNotExist if the meta block does not exist
 */","* Stream access to a meta block.``
     * 
     * @param name
     *          The name of the meta block.
     * @return The input stream.
     * @throws IOException
     *           on I/O error.
     * @throws MetaBlockDoesNotExist
     *           If the meta block with the name does not exist.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)",617,645,"/**
 * Initializes a BCFile Reader.
 * @param fin file input stream of the BCFile
 * @param fileLength length of the file
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs
 */","* Constructor
     * 
     * @param fin
     *          FS input stream.
     * @param fileLength
     *          Length of the corresponding file
     * @throws IOException",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockReader,org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockReader(int),2035,2037,"/**
 * Reads block data using BCF reader.
 * @param blockIndex index of the block to read
 * @return BlockReader object
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)",595,606,"/**
* Starts a meta block with compression.
* @param name block name
* @param compressName compression algorithm name
* @return DataOutputStream for writing data
* @throws IOException if I/O error occurs
* @throws MetaBlockAlreadyExists if block already exists
*/","* Obtain an output stream for creating a meta block. This function may not
     * be called when there is a key append stream or value append stream
     * active. No more key-value insertion is allowed after a meta data block
     * has been added to TFile.
     * 
     * @param name
     *          Name of the meta block.
     * @param compressName
     *          Name of the compression algorithm to be used. Must be one of the
     *          strings returned by
     *          {@link TFile#getSupportedCompressionAlgorithms()}.
     * @return A DataOutputStream that can be used to write Meta Block data.
     *         Closing the stream would signal the ending of the block.
     * @throws IOException raised on errors performing I/O.
     * @throws MetaBlockAlreadyExists
     *           the Meta Block with the same name already exists.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer:close(),300,343,"/**
* Closes the TFile if not already closed.
* Throws IllegalStateException if closing in the middle of key-value insertion.
*/","* Close the Writer. Resources will be released regardless of the exceptions
     * being thrown. Future close calls will have no effect.
     * 
     * The underlying FSDataOutputStream is not closed.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareMetaBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String),623,632,"/**
* Starts a new Meta Block with the given name.
* @param name the name of the Meta Block
* @return DataOutputStream for writing to the block
* @throws IOException if an I/O error occurs
* @throws MetaBlockAlreadyExists if the block already exists
*/","* Obtain an output stream for creating a meta block. This function may not
     * be called when there is a key append stream or value append stream
     * active. No more key-value insertion is allowed after a meta data block
     * has been added to TFile. Data will be compressed using the default
     * compressor as defined in Writer's constructor.
     * 
     * @param name
     *          Name of the meta block.
     * @return A DataOutputStream that can be used to write Meta Block data.
     *         Closing the stream would signal the ending of the block.
     * @throws IOException raised on errors performing I/O.
     * @throws MetaBlockAlreadyExists
     *           the Meta Block with the same name already exists.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareAppendKey,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendKey(int),527,537,"/**
* Initializes and returns a new DataOutputStream for key registration.
* @param length the length of the key to register
* @return DataOutputStream for writing key data
* @throws IOException if an I/O error occurs or state is incorrect
*/","* Obtain an output stream for writing a key into TFile. This may only be
     * called when there is no active Key appending stream or value appending
     * stream.
     * 
     * @param length
     *          The expected length of the key. If length of the key is not
     *          known, set length = -1. Otherwise, the application must write
     *          exactly as many bytes as specified here before calling close on
     *          the returned output stream.
     * @return The key appending output stream.
     * @throws IOException raised on errors performing I/O.
     *",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),459,462,"/**
 * Retrieves server defaults from a file path.
 * @param f file path to read settings from
 * @return FsServerDefaults object containing server configuration
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)",517,557,"/**
* Recursively copies a file or directory to a destination.
* @param src source file or directory
* @param dstFS destination filesystem
* @param dst destination path
* @param deleteSource whether to delete the source after copying
* @param conf configuration settings
* @return true if operation successful, false otherwise
*/","* Copy local files to a FileSystem.
   *
   * @param src src.
   * @param dstFS dstFs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,innerComplete,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerComplete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)",195,243,"/**
* Processes multipart upload and combines parts into a final file.
* @param multipartUploadId unique identifier for the multipart upload
* @param filePath destination path for the combined file
* @param handleMap mapping of part indices to their handles
* @return PathHandle representing the processed file
* @throws IOException if an I/O error occurs during processing
*/","* The upload complete operation.
   * @param multipartUploadId the ID of the upload
   * @param filePath path
   * @param handleMap map of handles
   * @return the path handle
   * @throws IOException failure",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,touch,org.apache.hadoop.fs.shell.TouchCommands$Touch:touch(org.apache.hadoop.fs.shell.PathData),162,175,"/**
* Masks a file path, creating it if necessary.
* @param item PathData object containing file information
* @throws IOException if an I/O error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,touchz,org.apache.hadoop.fs.shell.TouchCommands$Touchz:touchz(org.apache.hadoop.fs.shell.PathData),88,90,"/**
 * Masks file by applying specified operations.
 * @param item PathData object containing file path and filesystem reference
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",742,749,"/**
* Creates and returns a new file output stream with specified permissions.
* @param fs FileSystem instance
* @param file Path to the file
* @param permission File system permissions
* @return FSDataOutputStream for writing to the file
* @throws IOException if an I/O error occurs
*/","* Create a file with the provided permission.
   *
   * The permission of the file is set to be the provided permission as in
   * setPermission, not permission{@literal &~}umask
   *
   * The HDFS implementation is implemented using two RPCs.
   * It is understood that it is inefficient,
   * but the implementation is thread-safe. The other option is to change the
   * value of umask in configuration to be 0, but it is not thread-safe.
   *
   * @param fs FileSystem
   * @param file the name of the file to be created
   * @param permission the permission of the file
   * @return an output stream
   * @throws IOException IO failure",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)",1209,1215,"/**
* Initializes a Writer for writing data to a file in Hadoop's FileSystem.
* @param fs the FileSystem to write to
* @param conf Configuration object containing settings
* @param name Path of the file to create
* @param keyClass Class type for keys
* @param valClass Class type for values
* @throws IOException if an I/O error occurs
*/","* Create the named file.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,rollLogDir,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDir(),661,673,"/**
* Creates a log file with a masked name and writes to it.
* @throws IOException if file operations fail
*/","* Create a new directory based on the current interval and a new log file in
   * that directory.
   *
   * @throws IOException thrown if an error occurs while creating the
   * new directory or new log file",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,readIndex,org.apache.hadoop.io.MapFile$Reader:readIndex(),577,631,"/**
 * Masks keys and positions from an index.
 * Handles IOExceptions and ensures index closure.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,next,"org.apache.hadoop.io.MapFile$Reader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",811,814,"/**
 * Delegates call to data's m1 method.
 * @param key key object for comparison
 * @param val value object
 * @return result of data.m1
 * @throws IOException if an I/O error occurs
 */","* Read the next key/value pair in the map into <code>key</code> and
     * <code>val</code>.  Returns true if such a pair exists and false when at
     * the end of the map.
     *
     * @param key WritableComparable.
     * @param val Writable.
     * @return if such a pair exists true,not false.
     * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",106,119,"/**
 * Creates a ProtocolProxy for the specified protocol.
 * @param <T> type of the protocol
 * @param protocol class representing the protocol
 * @param clientVersion version of the client
 * @param addr address of the server
 * @param ticket user group information for authentication
 * @param conf configuration settings
 * @param factory socket factory for creating sockets
 * @param rpcTimeout timeout for RPC calls
 * @param connectionRetryPolicy policy for retrying connections
 * @param fallbackToSimpleAuth flag to indicate if simple auth should be used as a fallback
 * @param alignmentContext context for alignment
 * @return ProtocolProxy instance
 * @throws IOException if an I/O error occurs
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",348,367,"/**
* Creates a protocol proxy with specified parameters.
* @param protocol the interface class of the protocol
* @param clientVersion version of the client
* @param addr socket address for connection
* @param ticket user group information for authentication
* @param conf configuration settings
* @param factory socket factory for creating sockets
* @param rpcTimeout timeout for RPC operations
* @param connectionRetryPolicy retry policy for connections (not supported)
* @param fallbackToSimpleAuth flag to fall back to simple auth
* @param alignmentContext context for alignment
* @return ProtocolProxy instance
* @throws IOException if an I/O error occurs
*/","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input ticket.
   * @param conf input configuration.
   * @param factory input factory.
   * @param rpcTimeout input rpcTimeout.
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @param fallbackToSimpleAuth input fallbackToSimpleAuth.
   * @param alignmentContext input alignmentContext.
   * @return ProtocolProxy.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",113,126,"/**
* Creates a proxy for a given protocol.
* @param protocol the interface of the remote service
* @param clientVersion version of the client
* @param addr address of the RPC server
* @param ticket user authentication information
* @param conf configuration settings
* @param factory socket factory for creating connections
* @param rpcTimeout timeout for RPC calls
* @param connectionRetryPolicy policy for retrying failed connections
* @param fallbackToSimpleAuth flag to enable simple auth if complex fails
* @param alignmentContext context for alignment purposes
* @return ProtocolProxy instance for the specified protocol
* @throws IOException if an I/O error occurs during proxy creation
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processArguments,org.apache.hadoop.fs.shell.Delete$Expunge:processArguments(java.util.LinkedList),244,273,"/**
* Handles file system trash operations.
* @param args list of PathData arguments
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getTrash,org.apache.hadoop.fs.FsShell:getTrash(),86,91,"/**
* Returns the trash instance.
* @return Trash object, creating it if necessary
* @throws IOException if an error occurs during creation
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingService,org.apache.hadoop.security.Groups:getUserToGroupsMappingService(),462,464,"/**
 * Retrieves default groups configuration.
 * @return Groups object configured with default settings
 */","* Get the groups being used to map user-to-groups.
   * @return the groups being used to map user-to-groups.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,initialize,"org.apache.hadoop.security.UserGroupInformation:initialize(org.apache.hadoop.conf.Configuration,boolean)",309,354,"/**
 * Configures Kerberos authentication and related settings.
 * @param conf Configuration object containing security settings
 * @param overrideNameRules Flag to override name rules
 */","* Initialize UGI and related classes.
   * @param conf the configuration to use",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,org.apache.hadoop.security.authorize.AccessControlList:<init>(),73,74,"/**
 * Constructs an empty AccessControlList instance.
 */",* This constructor exists primarily for AccessControlList to be Writable.,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String),85,87,"/**
 * Constructs an AccessControlList from a string.
 * @param aclString space-separated string representing ACL rules
 */","* Construct a new ACL from a String representation of the same.
   * 
   * The String is a a comma separated list of users and groups.
   * The user list comes first and is separated by a space followed 
   * by the group list. For e.g. ""user1,user2 group1,group2""
   * 
   * @param aclString String representation of the ACL",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,"org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String,java.lang.String)",97,99,"/**
 * Constructs an AccessControlList with specified users and groups.
 * @param users comma-separated list of user names
 * @param groups comma-separated list of group names
 */","* Construct a new ACL from String representation of users and groups
   * 
   * The arguments are comma separated lists
   * 
   * @param users comma separated list of users
   * @param groups comma separated list of groups",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(),58,61,"/**
 * Calls m1 with a new default Configuration.
 */",* refresh Impersonation rules,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,coreServiceLaunch,"org.apache.hadoop.service.launcher.ServiceLauncher:coreServiceLaunch(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)",571,647,"/**
* Initializes and executes a service.
* @param conf configuration settings
* @param instance service instance
* @param processedArgs list of processed arguments
* @param addShutdownHook flag to add shutdown hook
* @param execute flag to execute the service
* @return exit code of the service execution
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,build,org.apache.hadoop.http.HttpServer2$Builder:build(),485,564,"/**
* Initializes and configures an HttpServer2 instance.
* @param name server name, must be set
* @return configured HttpServer2 instance
* @throws IOException if configuration fails
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,initializeBindUsers,org.apache.hadoop.security.LdapGroupsMapping:initializeBindUsers(),950,975,"/**
* Configures and initializes bind users for LDAP.
* Populates `bindUsers` with user credentials from configuration.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,init,org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:init(org.apache.hadoop.security.ssl.SSLFactory$Mode),252,300,"/**
* Configures SSL settings based on mode.
* @param mode SSL configuration mode
* @throws IOException if I/O error occurs
* @throws GeneralSecurityException if security error occurs
*/","* Initializes the keystores of the factory.
   *
   * @param mode if the keystores are to be used in client or server mode.
   * @throws IOException thrown if the keystores could not be initialized due
   * to an IO error.
   * @throws GeneralSecurityException thrown if the keystores could not be
   * initialized due to a security error.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,"org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List,boolean)",149,193,"/**
* Initializes a CuratorFramework client with given auth info and SSL settings.
* @param authInfos list of authentication information
* @param sslEnabled flag to enable SSL
* @throws IOException if ZK_ADDRESS is not configured or other I/O errors occur
*/","* Start the connection to the ZooKeeper ensemble.
   *
   * @param authInfos  List of authentication keys.
   * @param sslEnabled If the connection should be SSL/TLS encrypted.
   * @throws IOException            If the connection cannot be started.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doRun,org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[]),202,270,"/**
* Initializes and starts the failover controller.
* @param args command-line arguments for configuration
* @return error code or 0 on success
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,getGroups,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroups(java.lang.String),76,90,"/**
 * Transforms user group names based on a rule.
 * @param user the user identifier
 * @return list of transformed or original group names
 */","* Returns list of groups for a user.
     * This calls {@link LdapGroupsMapping}'s getGroups and applies the
     * configured rules on group names before returning.
     *
     * @param user get groups for this user
     * @return list of groups for a given user",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,init,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:init(java.lang.String),148,176,"/**
* Initializes and configures the MetricsSystem.
* @param prefix identifier for the metrics system
* @return the configured MetricsSystem instance
*/","* Initialized the metrics system with a prefix.
   * @param prefix  the system will look for configs with the prefix
   * @return the metrics system object itself",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,initTokenManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:initTokenManager(java.util.Properties),148,163,"/**
* Initializes token manager with configuration.
* @param config properties containing configuration settings
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupSaslConnection,org.apache.hadoop.ipc.Client$Connection:setupSaslConnection(org.apache.hadoop.ipc.Client$IpcStreams),571,579,"/**
* Initializes SASL RPC client and authenticates.
* @param streams IpcStreams for communication
* @return AuthMethod representing the authentication method used
* @throws IOException if an I/O error occurs during initialization
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalFs.java,<init>,org.apache.hadoop.fs.local.LocalFs:<init>(org.apache.hadoop.conf.Configuration),36,38,"/**
 * Constructs a LocalFs instance.
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs
 * @throws URISyntaxException if the URI syntax is invalid
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,suffix,org.apache.hadoop.fs.shell.PathData:suffix(java.lang.String),241,243,"/**
 * Creates a PathData object with an appended file extension.
 * @param extension file extension to append
 * @return PathData object representing the path with the new extension
 * @throws IOException if an I/O error occurs
 */","* Returns a new PathData with the given extension.
   * @param extension for the suffix
   * @return PathData
   * @throws IOException shouldn't happen",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getPathDataForChild,org.apache.hadoop.fs.shell.PathData:getPathDataForChild(org.apache.hadoop.fs.shell.PathData),307,310,"/**
* Applies mask to child path data.
* @param child input path data
* @return modified PathData object
* @throws IOException if file system operation fails
*/","* Creates a new object for a child entry in this directory
   * @param child the basename will be appended to this object's path
   * @return PathData for the child
   * @throws IOException if this object does not exist or is not a directory",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,recursePath,org.apache.hadoop.fs.shell.Command:recursePath(org.apache.hadoop.fs.shell.PathData),449,466,"/**
* Processes PathData item recursively.
* @param item the PathData to process
* @throws IOException if an I/O error occurs
*/","*  Gets the directory listing for a path and invokes
   *  {@link #processPaths(PathData, PathData...)}
   *  @param item {@link PathData} for directory to recurse into
   *  @throws IOException if anything goes wrong...",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.HttpsFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
* Checks if path has read-only access.
* @param path file system path to check
* @param capability required capability
* @return true if read-only, otherwise delegates to superclass
*/","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.HttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
* Checks if a path has the specified capability.
* @param path file system path to check
* @param capability required capability
* @return true if path is read-only, otherwise delegates to superclass method
*/","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",495,499,"/**
 * Checks file capability based on path.
 * @param path file path to check
 * @param capability required capability string
 * @return true if capability is met, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.ChecksumFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1128,1140,"/**
* Checks path capability excluding append and concat.
* @param path file system path to check
* @param capability capability to verify
* @return true if capability is not append or concat, else false
*/","* Disable those operations which the checksummed FS blocks.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path),217,221,"/**
 * Calls overloaded m1 with default force flag.
 * @param f file path to process
 * @return result of operation
 * @throws IOException if an I/O error occurs
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,updateFileStatus,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:updateFileStatus(org.apache.hadoop.fs.Path),131,136,"/**
* Updates file status; retrieves or creates if necessary.
* @param f file path to update
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,listStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:listStatus(org.apache.hadoop.fs.Path),804,845,"/**
* Retrieves file status from multiple nodes.
* @param f path to the file
* @return FileStatus array or throws exception if not found
*/","* Returns the closest non-failing destination's result.
   *
   * @param f given path
   * @return array of file statuses according to nfly modes
   * @throws FileNotFoundException
   * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.NflyFSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",866,873,"/**
* Applies file permissions to a path across multiple nodes.
* @param f the file path
* @param permission the file permissions to apply
* @return true if all operations succeed, false otherwise
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,rename,"org.apache.hadoop.fs.viewfs.NflyFSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",739,765,"/**
* Renames a file from source to destination across multiple nodes.
* @param src source file path
* @param dst destination file path
* @return true if all renamings are successful, false otherwise
* @throws IOException if an I/O error occurs during renaming
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize(),434,437,"/**
* Calls m2 with result of m1 on rootPath.
* @return long value from m2
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication(),444,447,"/**
 * Calls m2 with result from m1 using rootPath.
 * @return Short value from m2
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",462,504,"/**
 * Copies a file or directory from source to destination.
 * @param srcFS source filesystem
 * @param srcStatus status of the source file/directory
 * @param dstFS destination filesystem
 * @param dst destination path
 * @param deleteSource flag to delete source after copy
 * @param overwrite flag to overwrite existing files
 * @param conf configuration settings
 * @return true if operation successful, false otherwise
 * @throws IOException if an I/O error occurs
 */","* Copy a file/directory tree within/between filesystems.
   * <p>
   * returns true if the operation succeeded. When deleteSource is true,
   * this means ""after the copy, delete(source) returned true""
   * If the destination is a directory, and mkdirs (dest) fails,
   * the operation will return false rather than raise any exception.
   * </p>
   * The overwrite flag is about overwriting files; it has no effect about
   * handing an attempt to copy a file atop a directory (expect an IOException),
   * or a directory over a path which contains a file (mkdir will fail, so
   * ""false"").
   * <p>
   * The operation is recursive, and the deleteSource operation takes place
   * as each subdirectory is copied. Therefore, if an operation fails partway
   * through, the source tree may be partially deleted.
   * </p>
   * @param srcFS source filesystem
   * @param srcStatus status of source
   * @param dstFS destination filesystem
   * @param dst path of source
   * @param deleteSource delete the source?
   * @param overwrite overwrite files at destination?
   * @param conf configuration to use when opening files
   * @return true if the operation succeeded.
   * @throws IOException failure",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.io.File,boolean,org.apache.hadoop.conf.Configuration)",578,605,"/**
 * Recursively copies files from source to destination.
 * @param srcFS source file system
 * @param srcStatus status of the source file or directory
 * @param dst destination file
 * @param deleteSource flag to delete source after copy
 * @param conf configuration settings
 * @return true if operation successful, false otherwise
 */",Copy FileSystem files to local files.,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,openFile,org.apache.hadoop.fs.shell.PathData:openFile(java.lang.String),632,639,"/**
 * Opens a file stream for reading with specified policy.
 * @param policy the read policy to apply
 * @return FSDataInputStream for reading the file
 * @throws IOException if an I/O error occurs
 */","* Open a file.
   * @param policy fadvise policy.
   * @return an input stream
   * @throws IOException failure",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFile,org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.Path),709,713,"/**
 * Retrieves a FutureDataInputStreamBuilder for the specified path.
 * @param path file system path
 * @return FutureDataInputStreamBuilder instance
 * @throws IOException if an I/O error occurs
 * @throws UnsupportedOperationException if operation is not supported
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,openFile,"org.apache.hadoop.io.SequenceFile$Reader:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int,long)",2002,2012,"/**
* Opens a file for reading with specified buffer size and optional length.
* @param fs FileSystem instance
* @param file Path to the file
* @param bufferSize Buffer size for reading
* @param length Length of data to read, -1 for full file
* @return FSDataInputStream for reading the file
* @throws IOException if an I/O error occurs
*/","* Override this method to specialize the type of
     * {@link FSDataInputStream} returned.
     * @param fs The file system used to open the file.
     * @param file The file being read.
     * @param bufferSize The buffer size used to read the file.
     * @param length The length being read if it is {@literal >=} 0.
     *               Otherwise, the length is not available.
     * @return The opened stream.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,load,"org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus)",264,283,"/**
 * Reads and processes a file from the given filesystem.
 * @param fs FileSystem instance
 * @param path Path to the file
 * @param status Optional FileStatus object
 * @return Processed data of type T
 * @throws IOException if an I/O error occurs
 */","* Load from a Hadoop filesystem.
   * If a file status is supplied, it's passed in to the openFile()
   * call so that FS implementations can optimize their opening.
   * @param fs filesystem
   * @param path path
   * @param status status of the file to open.
   * @return a loaded object
   * @throws PathIOException JSON parse problem
   * @throws EOFException file status references an empty file
   * @throws IOException IO problems",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFile,org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.PathHandle),715,719,"/**
* Creates a FutureDataInputStreamBuilder from a PathHandle.
* @param pathHandle file path handle
* @return FutureDataInputStreamBuilder for the given path
* @throws IOException if an I/O error occurs
* @throws UnsupportedOperationException if operation is not supported
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFirstKey,org.apache.hadoop.io.file.tfile.TFile$Reader:getFirstKey(),901,904,"/**
 * Calls m1 and retrieves data from tfileIndex.
 * @return RawComparable object
 * @throws IOException if an I/O error occurs
 */","* Get the first key in the TFile.
     * 
     * @return The first key in the TFile.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLastKey,org.apache.hadoop.io.file.tfile.TFile$Reader:getLastKey(),912,915,"/**
 * Calls m1 and returns result from tfileIndex's m2.
 * @return RawComparable object from tfileIndex
 * @throws IOException if an I/O error occurs
 */","* Get the last key in the TFile.
     * 
     * @return The last key in the TFile.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockContainsKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockContainsKey(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",985,995,"/**
* Finds location of key in TFile.
* @param key the search key
* @param greater true to find first >= key, false for <= key
* @return Location object or end if not found
* @throws IOException on I/O error
*/","* if greater is true then returns the beginning location of the block
     * containing the key strictly greater than input key. if greater is false
     * then returns the beginning location of the block greater than equal to
     * the input key
     * 
     * @param key
     *          the input key
     * @param greater
     *          boolean flag
     * @return
     * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationByRecordNum,org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationByRecordNum(long),997,1000,"/**
 * Retrieves location data by record number.
 * @param recNum record number of the location
 * @return Location object
 * @throws IOException if an I/O error occurs
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumByLocation,org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),1002,1005,"/**
 * Processes location data.
 * @param location the location to process
 * @return processed result as a long value
 * @throws IOException if an I/O error occurs
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKeyNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getKeyNear(long),1063,1068,"/**
* Retrieves data from a file using an offset.
* @param offset the position in the file to read from
* @return RawComparable object or null if not found
* @throws IOException if an I/O error occurs
*/","* Get a sample key that is within a block whose starting offset is greater
     * than or equal to the specified offset.
     * 
     * @param offset
     *          The file offset.
     * @return the key that fits the requirement; or null if no such key exists
     *         (which could happen if the offset is close to the end of the
     *         TFile).
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)",802,818,"/**
 * Initializes a TFile reader with given input stream and configuration.
 * @param fsdis file system data input stream
 * @param fileLength length of the file to read
 * @param conf Hadoop configuration
 * @throws IOException if an I/O error occurs
 */","* Constructor
     * 
     * @param fsdis
     *          FS input stream of the TFile.
     * @param fileLength
     *          The length of TFile. This is required because we have no easy
     *          way of knowing the actual size of the input file through the
     *          File input stream.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,initBlock,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:initBlock(int),1548,1559,"/**
* Masks a block and updates the reader.
* @param blockIndex index of the block to mask
*/","* Load a compressed block for reading. Expecting blockIndex is valid.
       * 
       * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,append,"org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],int,int,byte[],int,int)",380,413,"/**
* Masks key and value buffers.
* @param key buffer containing the key data
* @param koff offset in the key buffer
* @param klen length of the key data
* @param value buffer containing the value data
* @param voff offset in the value buffer
* @param vlen length of the value data
* @throws IOException if an I/O error occurs
*/","* Adding a new key-value pair to TFile.
     * 
     * @param key
     *          buffer for key.
     * @param koff
     *          offset in key buffer.
     * @param klen
     *          length of key.
     * @param value
     *          buffer for value.
     * @param voff
     *          offset in value buffer.
     * @param vlen
     *          length of value.
     * @throws IOException
     *           Upon IO errors.
     *           <p>
     *           If an exception is thrown, the TFile will be in an inconsistent
     *           state. The only legitimate call after that would be close",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults(),454,457,"/**
 * Retrieves server defaults using root path.
 * @return FsServerDefaults object
 * @throws IOException if an I/O error occurs
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processPath,org.apache.hadoop.fs.shell.TouchCommands$Touch:processPath(org.apache.hadoop.fs.shell.PathData),148,151,"/**
 * Applies masking to a PathData item.
 * @param item PathData object to be masked
 * @throws IOException if an I/O error occurs during processing
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processNonexistentPath,org.apache.hadoop.fs.shell.TouchCommands$Touch:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),153,160,"/**
 * Masks a path data item.
 * @param item the PathData object to mask
 * @throws IOException if masking fails or path is not found
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processPath,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processPath(org.apache.hadoop.fs.shell.PathData),67,77,"/**
* Masks file by checking and processing its properties.
* @param item PathData object representing the file to be masked
* @throws IOException if directory or non-zero length file is encountered
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processNonexistentPath,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),79,86,"/**
 * Masks a path data item.
 * @param item the PathData object to be masked
 * @throws IOException if path not found or other I/O error occurs
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,getOutputStreamForKeystore,org.apache.hadoop.security.alias.KeyStoreProvider:getOutputStreamForKeystore(),52,56,"/**
* Creates an output stream for file masking.
* @return OutputStream for writing masked data
* @throws IOException if file system operations fail
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,writeToNew,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:writeToNew(org.apache.hadoop.fs.Path),607,620,"/**
 * Stores a keystore to the specified path.
 * @param newPath destination path for the keystore
 * @throws IOException if an I/O error occurs or unsupported algorithm/certificate issues arise
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,midKey,org.apache.hadoop.io.MapFile$Reader:midKey(),649,657,"/**
* Retrieves and removes the maximum element.
* @return the maximum element or null if empty
* @throws IOException if an I/O error occurs
*/","* Get the key at approximately the middle of the file. Or null if the
     *  file is empty.
     *
     * @throws IOException raised on errors performing I/O.
     * @return WritableComparable.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,finalKey,org.apache.hadoop.io.MapFile$Reader:finalKey(org.apache.hadoop.io.WritableComparable),665,681,"/**
* Masks data for a given key.
* @param key the key to mask data by
* @throws IOException if an I/O error occurs
*/","* Reads the final key from the file.
     *
     * @param key key to read into
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seekInternal,"org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable,boolean)",721,780,"/**
 * Searches for a key in a sorted dataset.
 * @param key the key to search for
 * @param before flag indicating whether to find the closest key less than or equal to the given key
 * @return comparison result or status code
 * @throws IOException if an I/O error occurs during search
 */","* Positions the reader at the named key, or if none such exists, at the
     * key that falls just before or just after dependent on how the
     * <code>before</code> parameter is set.
     * 
     * @param before - IF true, and <code>key</code> does not exist, position
     * file at entry that falls just before <code>key</code>.  Otherwise,
     * position file at record that sorts just after.
     * @return  0   - exact match found
     *          < 0 - positioned at next record
     *          1   - no more records in file",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,mergePass,org.apache.hadoop.io.MapFile$Merger:mergePass(),1101,1146,"/**
 * Merges and writes sorted data from multiple input readers to an output writer.
 * @throws IOException if I/O operations fail
 */","* Merge all input files to output map file.<br>
     * 1. Read first key/value from all input files to keys/values array. <br>
     * 2. Select the least key and corresponding value. <br>
     * 3. Write the selected key and value to output file. <br>
     * 4. Replace the already written key/value in keys/values arrays with the
     * next key/value from the selected input <br>
     * 5. Repeat step 2-4 till all keys are read. <br>",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",97,104,"/**
* Creates a ProtocolProxy with default additional parameters.
* @param protocol the interface class of the remote protocol
* @param clientVersion version of the client
* @param addr address of the server
* @param ticket user authentication information
* @param conf configuration settings
* @param factory socket factory for creating connections
* @param rpcTimeout timeout for RPC calls
* @param connectionRetryPolicy policy for retrying failed connections
* @return ProtocolProxy instance
* @throws IOException if an I/O error occurs
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",299,307,"/**
* Creates a ProtocolProxy instance with default additional parameters.
* @param protocol interface class of the remote service
* @param clientVersion version of the client
* @param addr address of the RPC server
* @param ticket user authentication information
* @param conf configuration settings
* @param factory socket factory for creating connections
* @param rpcTimeout timeout in milliseconds for RPC calls
* @param connectionRetryPolicy policy for retrying failed connections
* @return ProtocolProxy instance
* @throws IOException if an I/O error occurs
*/","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type T
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input ticket.
   * @param conf input configuration.
   * @param factory input factory.
   * @param rpcTimeout input rpcTimeout.
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",322,330,"/**
* Creates a ProtocolProxy instance.
* @param <T> type of protocol
* @param protocol class representing the protocol
* @param clientVersion version of the client
* @param connId connection identifier
* @param conf configuration settings
* @param factory socket factory for creating connections
* @param alignmentContext context for alignment
* @return ProtocolProxy instance
* @throws IOException if an I/O error occurs
*/","* Construct a client-side proxy object with a ConnectionId.
   *
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param connId input ConnectionId.
   * @param conf input Configuration.
   * @param factory input factory.
   * @param alignmentContext Alignment context
   * @throws IOException raised on errors performing I/O.
   * @return ProtocolProxy.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",93,101,"/**
 * Creates a protocol proxy with specified parameters.
 * @param <T> type of the protocol
 * @param protocol class representing the protocol
 * @param clientVersion version of the client
 * @param addr address of the server
 * @param ticket user group information for authentication
 * @param conf configuration settings
 * @param factory socket factory for creating sockets
 * @param rpcTimeout timeout for RPC calls
 * @param connectionRetryPolicy policy for retrying failed connections
 * @return ProtocolProxy instance
 * @throws IOException if an I/O error occurs
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getCurrentTrashDir,org.apache.hadoop.fs.FsShell:getCurrentTrashDir(),125,127,"/**
 * Retrieves a file path.
 * @throws IOException if an I/O error occurs
 * @return Path object representing the file location
 */","* Returns the Trash object associated with this shell.
   * @return Path to the trash
   * @throws IOException upon error",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getCurrentTrashDir,org.apache.hadoop.fs.FsShell:getCurrentTrashDir(org.apache.hadoop.fs.Path),135,137,"/**
 * Processes a file path.
 * @param path file path to process
 * @return processed Path object
 * @throws IOException if an I/O error occurs
 */","* Returns the current trash location for the path specified
   * @param path to be deleted
   * @return path to the trash
   * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,ensureInitialized,org.apache.hadoop.security.UserGroupInformation:ensureInitialized(),295,303,"/**
* Ensures configuration is set; synchronizes access.
*/","* A method to initialize the fields that depend on a configuration.
   * Must be called before useKerberos or groups is used.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setConfiguration,org.apache.hadoop.security.UserGroupInformation:setConfiguration(org.apache.hadoop.conf.Configuration),362,366,"/**
* Applies mask configuration.
* @param conf Configuration object to modify
*/","* Set the static configuration for UGI.
   * In particular, set the security authentication mechanism and the
   * group look up service.
   * @param conf the configuration to use",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,refreshWithLoadedConfiguration,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refreshWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",152,200,"/**
 * Initializes access control lists and machine lists for services.
 * @param conf Configuration object containing security settings
 * @param provider PolicyProvider for retrieving service definitions
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,init,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:init(java.lang.String),68,101,"/**
* Applies configuration masks based on prefix.
* @param configurationPrefix base prefix for configuration keys
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getSip,org.apache.hadoop.security.authorize.ProxyUsers:getSip(),116,124,"/**
* Returns an ImpersonationProvider instance.
* Initializes it if not already created.
* @return ImpersonationProvider object
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchService,"org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)",485,539,"/**
* Handles service launch and exceptions.
* @param conf configuration object
* @param instance service instance
* @param processedArgs list of processed arguments
* @param addShutdownHook flag to add shutdown hook
* @param execute flag to execute the service
* @return ExitException with details of success or failure
*/","* Launch a service catching all exceptions and downgrading them to exit codes
   * after logging.
   *
   * Sets {@link #serviceException} to this value.
   * @param conf configuration to use
   * @param instance optional instance of the service.
   * @param processedArgs command line after the launcher-specific arguments
   * have been stripped out.
   * @param addShutdownHook should a shutdown hook be added to terminate
   * this service on shutdown. Tests should set this to false.
   * @param execute execute/wait for the service to stop.
   * @return an exit exception, which will have a status code of 0 if it worked",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,setConf,org.apache.hadoop.security.LdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),767,864,"/**
* Configures the LDAP settings from the provided configuration.
* @param conf Configuration object containing LDAP settings
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List),138,140,"/**
 * Calls m1 with default override flag.
 * @param authInfos list of authentication information
 */","* Start the connection to the ZooKeeper ensemble.
   * @param authInfos List of authentication keys.
   * @throws IOException If the connection cannot be started.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:init(java.util.Properties),127,132,"/**
* Initializes handlers with configuration properties.
* @param config application configuration settings
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalFs.java,<init>,"org.apache.hadoop.fs.local.LocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",49,52,"/**
 * Constructs a LocalFs instance.
 * @param theUri file system URI
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs
 * @throws URISyntaxException if the URI syntax is invalid
 */","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,copyStreamToTarget,"org.apache.hadoop.fs.shell.CommandWithDestination:copyStreamToTarget(java.io.InputStream,org.apache.hadoop.fs.shell.PathData)",418,434,"/**
 * Copies input stream to target path with optional checksum and overwrite checks.
 * @param in source input stream
 * @param target destination path data
 * @throws IOException if an I/O error occurs
 */","* If direct write is disabled ,copies the stream contents to a temporary
   * file ""target._COPYING_"". If the copy is successful, the temporary file
   * will be renamed to the real path, else the temporary file will be deleted.
   * if direct write is enabled , then creation temporary file is skipped.
   *
   * @param in     the input stream for the copy
   * @param target where to store the contents of the stream
   * @throws IOException if copy fails",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getTargetPath,org.apache.hadoop.fs.shell.CommandWithDestination:getTargetPath(org.apache.hadoop.fs.shell.PathData),330,342,"/**
* Applies mask to source PathData.
* @param src source PathData object
* @return modified or original PathData based on conditions
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,cleanupAllTmpFiles,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:cleanupAllTmpFiles(),399,407,"/**
 * Masks files by calling fs.m2 on each node's file system.
 * Handles exceptions by calling m1 for cleanup.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,commit,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:commit(),409,444,"/**
* Masks files by committing changes and setting timestamps.
* Throws IOException if replication is below minimum or other issues occur.
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,delete,"org.apache.hadoop.fs.viewfs.NflyFSystem:delete(org.apache.hadoop.fs.Path,boolean)",768,793,"/**
 * Processes file deletion across multiple nodes.
 * @param f Path to the file to delete
 * @param recursive whether to perform a recursive delete
 * @return true if all deletions are successful, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",426,433,"/**
 * Copies a file from source to destination.
 * @param srcFS source filesystem
 * @param src source path
 * @param dstFS destination filesystem
 * @param dst destination path
 * @param deleteSource true to delete source after copy
 * @param overwrite true to overwrite destination if it exists
 * @param conf configuration settings
 * @return true if copy is successful, false otherwise
 */","* Copy files between FileSystems.
   *
   * @param srcFS srcFs.
   * @param src src.
   * @param dstFS dstFs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param overwrite overwrite.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,repairAndOpen,"org.apache.hadoop.fs.viewfs.NflyFSystem:repairAndOpen(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode[],org.apache.hadoop.fs.Path,int)",636,713,"/**
 * Repairs and opens a file stream from the most recent node.
 * @param mrNodes array of MRNflyNode objects representing nodes
 * @param f Path to the file
 * @param bufferSize size of buffer for reading operations
 * @return FSDataInputStream if successful, null otherwise
 */","* Iterate all available nodes in the proximity order to attempt repair of all
   * FileNotFound nodes.
   *
   * @param mrNodes work set copy of nodes
   * @param f path to repair and open
   * @param bufferSize buffer size for read RPC
   * @return the closest/most recent replica stream AFTER repair",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.File,boolean,org.apache.hadoop.conf.Configuration)",570,575,"/**
* Copies a file from source to destination.
* @param srcFS source FileSystem
* @param src source Path
* @param dst destination File
* @param deleteSource true if source should be deleted after copy
* @param conf configuration for the operation
* @return true if copy is successful, false otherwise
*/","* Copy FileSystem files to local files.
   *
   * @param srcFS srcFs.
   * @param src src.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,openForSequentialIO,org.apache.hadoop.fs.shell.PathData:openForSequentialIO(),621,624,"/**
 * Opens file with sequential read policy.
 * @return FSDataInputStream for reading file
 * @throws IOException if file cannot be opened
 */","* Open a file for sequential IO.
   * <p>
   * This uses FileSystem.openFile() to request sequential IO;
   * the file status is also passed in.
   * Filesystems may use to optimize their IO.
   * </p>
   * @return an input stream
   * @throws IOException failure",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,dumpToOffset,org.apache.hadoop.fs.shell.Head:dumpToOffset(org.apache.hadoop.fs.shell.PathData),72,77,"/**
 * Reads and prints file content from a given PathData.
 * @param item the PathData object representing the file to read
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,dumpFromOffset,"org.apache.hadoop.fs.shell.Tail:dumpFromOffset(org.apache.hadoop.fs.shell.PathData,long)",105,121,"/**
 * Reads and processes data from a file at a specified offset.
 * @param item PathData object representing the file
 * @param offset starting position in the file
 * @return final offset after processing or total file size if offset exceeds it
 * @throws IOException if an I/O error occurs
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,openFile,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:openFile(org.apache.hadoop.fs.Path),489,493,"/**
 * Builds a FutureDataInputStream from a file path.
 * @param path file path to read from
 * @return FutureDataInputStreamBuilder instance
 * @throws IOException if an I/O error occurs
 * @throws UnsupportedOperationException if operation is not supported
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,load,"org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",248,250,"/**
 * Calls overloaded method with default null value.
 * @param fs FileSystem instance
 * @param path file path
 * @return result of the overloaded method
 * @throws IOException if an I/O error occurs
 */","* Load from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return a loaded object
   * @throws PathIOException JSON parse problem
   * @throws IOException IO problems",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumNear(long),1048,1050,"/**
 * Applies mask functions to an offset.
 * @param offset input value to be masked
 * @return masked result as a long
 * @throws IOException if an I/O error occurs
 */","* Get the RecordNum for the first key-value pair in a compressed block
     * whose byte offset in the TFile is greater than or equal to the specified
     * offset.
     * 
     * @param offset
     *          the user supplied offset.
     * @return the RecordNum to the corresponding entry. If no such entry
     *         exists, it returns the total entry count.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNum,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:getRecordNum(),1629,1631,"/**
 * Reads and returns a masked value from the current location.
 * @return long value representing the masked data
 * @throws IOException if an I/O error occurs during reading
 */","* Get the RecordNum corresponding to the entry pointed by the cursor.
       * @return The RecordNum corresponding to the entry pointed by the cursor.
       * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.TFile$Reader$Location,org.apache.hadoop.io.file.tfile.TFile$Reader$Location)",1281,1303,"/**
* Initializes a Scanner with specified reader and location range.
* @param reader source of data to scan
* @param begin start location for scanning
* @param end end location for scanning
* @throws IOException if an I/O error occurs
*/","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param begin
       *          Begin location of the scan.
       * @param end
       *          End location of the scan.
       * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),1396,1429,"/**
* Masks a location within specified bounds.
* @param l the Location to mask
* @throws IOException if an I/O error occurs
*/","* Move the cursor to the new location. The entry returned by the previous
       * entry() call will be invalid.
       * 
       * @param l
       *          new cursor location. It must fall between the begin and end
       *          location of the scanner.
       * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,advance,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:advance(),1521,1541,"/**
* Masks the current location if conditions are met.
* @return true if masking is successful, false otherwise
*/","* Move the cursor to the next key-value pair. The entry returned by the
       * previous entry() call will be invalid.
       * 
       * @return true if the cursor successfully moves. False when cursor is
       *         already at the end location and cannot be advanced.
       * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,append,"org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],byte[])",355,357,"/**
 * Calls overloaded method with full array ranges.
 * @param key   byte array containing the key
 * @param value byte array containing the value
 * @throws IOException if an I/O error occurs
 */","* Adding a new key-value pair to the TFile. This is synonymous to
     * append(key, 0, key.length, value, 0, value.length)
     * 
     * @param key
     *          Buffer for key.
     * @param value
     *          Buffer for value.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,flush,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:flush(),529,584,"/**
 * Handles file operations and metadata updates.
 * @throws IOException if an I/O error occurs
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seekInternal,org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable),704,707,"/**
 * Calls m1 with default value.
 * @param key input key
 * @return result of m1 call
 * @throws IOException if an I/O error occurs
 */","* Positions the reader at the named key, or if none such exists, at the
     * first entry after the named key.
     *
     * @return  0   - exact match found
     *          < 0 - positioned at next record
     *          1   - no more records in file",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getClosest,"org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,boolean)",859,875,"/**
 * Masks key based on comparison and updates value.
 * @param key the key to compare
 * @param val the value to update if condition met
 * @param before flag indicating comparison direction
 * @return masked key or null if condition not met
 * @throws IOException if an I/O error occurs
 */","* Finds the record that is the closest match to the specified key.
     * 
     * @param key       - key that we're trying to find
     * @param val       - data value if key is found
     * @param before    - IF true, and <code>key</code> does not exist, return
     * the first entry that falls just before the <code>key</code>.  Otherwise,
     * return the record that sorts just after.
     * @return          - the key that was the closest match or null if eof.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",90,95,"/**
 * Creates a ProtocolProxy instance.
 * @param protocol the interface class of the protocol
 * @param clientVersion version of the client
 * @param addr address of the server
 * @param ticket user group information for authentication
 * @param conf configuration settings
 * @param factory socket factory for creating sockets
 * @param rpcTimeout timeout for RPC calls
 * @return ProtocolProxy instance or throws IOException
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",86,91,"/**
* Creates a ProtocolProxy instance.
* @param protocol the interface class of the protocol
* @param clientVersion version of the client
* @param addr address of the server
* @param ticket user group information for authentication
* @param conf configuration settings
* @param factory socket factory for creating sockets
* @param rpcTimeout timeout for RPC calls
* @return ProtocolProxy instance
* @throws IOException if an I/O error occurs
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isAuthenticationMethodEnabled,org.apache.hadoop.security.UserGroupInformation:isAuthenticationMethodEnabled(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod),391,396,"/**
* Checks if given authentication method matches the current one.
* @param method Authentication method to compare against
* @return true if methods match, false otherwise
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isKerberosKeyTabLoginRenewalEnabled,org.apache.hadoop.security.UserGroupInformation:isKerberosKeyTabLoginRenewalEnabled(),398,404,"/**
* Checks Kerberos keytab login renewal status.
* @return true if enabled, false otherwise
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKerberosLoginRenewalExecutor,org.apache.hadoop.security.UserGroupInformation:getKerberosLoginRenewalExecutor(),406,412,"/**
* Retrieves Kerberos login renewal executor.
* @return Optional containing ExecutorService or empty if not initialized
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createUserForTesting,"org.apache.hadoop.security.UserGroupInformation:createUserForTesting(java.lang.String,java.lang.String[])",1607,1620,"/**
* Creates UserGroupInformation for a user with specified groups.
* @param user username
* @param userGroups array of group names
* @return UserGroupInformation object configured with the given user and groups
*/","* Create a UGI for testing HDFS and MapReduce
   * @param user the full user principal name
   * @param userGroups the names of the groups that the user belongs to
   * @return a fake user for running unit tests",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createProxyUserForTesting,"org.apache.hadoop.security.UserGroupInformation:createProxyUserForTesting(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.String[])",1634,1645,"/**
* Masks user information with specified groups.
* @param user username string
* @param realUser UserGroupInformation object for the actual user
* @param userGroups array of group names
* @return masked UserGroupInformation object
*/","* Create a proxy user UGI for testing HDFS and MapReduce
   * 
   * @param user
   *          the full user principal name for effective user
   * @param realUser
   *          UGI of the real user
   * @param userGroups
   *          the names of the groups that the user belongs to
   * @return a fake user for running unit tests",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroups,org.apache.hadoop.security.UserGroupInformation:getGroups(),1790,1799,"/**
* Deprecated method, does not perform any meaningful operation.
* @deprecated Use a different approach instead.
* @return Empty list of strings
*/","* Get the group names for this user. {@link #getGroupsSet()} is less
   * expensive alternative when checking for a contained element.
   * @return the list of users with the primary group first. If the command
   *    fails, it returns an empty list.
   * @deprecated Use {@link #getGroupsSet()} instead.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupsSet,org.apache.hadoop.security.UserGroupInformation:getGroupsSet(),1806,1814,"/**
* Retrieves groups for a user.
* @return Set of group names or empty set if an error occurs
*/","* Get the groups names for the user as a Set.
   * @return the set of users with the primary group first. If the command
   *     fails, it returns an empty set.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,doSubjectLogin,"org.apache.hadoop.security.UserGroupInformation:doSubjectLogin(javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$LoginParams)",2042,2073,"/**
* Authenticates user and returns UserGroupInformation.
* @param subject optional security subject
* @param params login parameters
* @return authenticated UserGroupInformation object
* @throws IOException if authentication fails
*/","* Login a subject with the given parameters.  If the subject is null,
   * the login context used to create the subject will be attached.
   * @param subject to login, null for new subject.
   * @param params for login, null for externally managed ugi.
   * @return UserGroupInformation for subject
   * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,init,org.apache.hadoop.fs.FsShell:init(),100,109,"/**
* Initializes command factory and sets up help and usage commands.
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshServiceAclWithLoadedConfiguration,"org.apache.hadoop.ipc.Server:refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",778,782,"/**
* Masks configuration and policy.
* @param conf Configuration object to be masked
* @param provider PolicyProvider object for masking policies
*/","* Refresh the service authorization ACL for the service handled by this server
   * using the specified Configuration.
   *
   * @param conf input Configuration.
   * @param provider input provider.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getTestProvider,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getTestProvider(),52,59,"/**
* Returns a singleton instance of DefaultImpersonationProvider.
* Initializes provider if not already created.
* @return the singleton DefaultImpersonationProvider instance
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",99,102,"/**
* Delegates authorization check to another method.
* @param user UserGroupInformation object representing the user
* @param remoteAddress IP address of the remote client
* @throws AuthorizationException if authorization fails
*/","* Authorize the superuser which is doing doAs.
   * {@link #authorize(UserGroupInformation, InetAddress)} should be preferred
   * to avoid possibly re-resolving the ip address.
   *
   * @param user ugi of the effective or proxy user which contains a real user
   * @param remoteAddress the ip address of client
   * @throws AuthorizationException Authorization Exception.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)",111,114,"/**
* Delegates authorization check to another method.
* @param user user group information
* @param remoteAddress IP address of the client
* @throws AuthorizationException if authorization fails
*/","* Authorize the superuser which is doing doAs.
   *
   * @param user ugi of the effective or proxy user which contains a real user
   * @param remoteAddress the inet address of client
   * @throws AuthorizationException Authorization Exception.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getDefaultImpersonationProvider,org.apache.hadoop.security.authorize.ProxyUsers:getDefaultImpersonationProvider(),140,143,"/**
 * Returns a DefaultImpersonationProvider instance.
 * @return DefaultImpersonationProvider object
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchService,"org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)",464,469,"/**
* Calls overloaded m1 with default null logger.
* @param conf configuration settings
* @param processedArgs list of processed arguments
* @param addShutdownHook flag to add shutdown hook
* @param execute flag to execute action
* @return ExitException indicating result
*/","* Launch a service catching all exceptions and downgrading them to exit codes
   * after logging.
   *
   * Sets {@link #serviceException} to this value.
   * @param conf configuration to use
   * @param processedArgs command line after the launcher-specific arguments
   * have been stripped out.
   * @param addShutdownHook should a shutdown hook be added to terminate
   * this service on shutdown. Tests should set this to false.
   * @param execute execute/wait for the service to stop.
   * @return an exit exception, which will have a status code of 0 if it worked",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,setConf,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),56,66,"/**
* Overrides m1 to configure conversion rule.
* @param conf Configuration object containing settings
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,org.apache.hadoop.util.curator.ZKCuratorManager:start(),129,131,"/**
 * Calls overloaded method m1 with an empty list.
 * Throws IOException if an I/O error occurs.
 */","* Start the connection to the ZooKeeper ensemble.
   * @throws IOException If the connection cannot be started.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,init,org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:init(java.util.Properties),99,126,"/**
* Initializes authentication schemes from configuration properties.
* @param config application configuration settings
* @throws ServletException if initialization fails
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,copyFileToTarget,"org.apache.hadoop.fs.shell.CommandWithDestination:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",350,367,"/**
 * Masks data from source to target path.
 * @param src source PathData object
 * @param target target PathData object
 * @throws IOException if an I/O error occurs
 */","* Copies the source file to the target.
   * @param src item to copy
   * @param target where to copy the item
   * @throws IOException if copy fails",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPathArgument,org.apache.hadoop.fs.shell.CommandWithDestination:processPathArgument(org.apache.hadoop.fs.shell.PathData),247,274,"/**
* Checks and processes file paths for copying.
* @param src source file path data
* @throws IOException if paths are identical or nested incorrectly
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,recursePath,org.apache.hadoop.fs.shell.CommandWithDestination:recursePath(org.apache.hadoop.fs.shell.PathData),299,328,"/**
* Processes source path data by copying or creating destination directory.
* @param src source path data to process
* @throws IOException if an I/O error occurs
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,close,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:close(),378,397,"/**
* Closes output streams and checks replication status.
* Throws IOException if replication is insufficient.
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)",366,371,"/**
 * Copies file from source to destination.
 * @param srcFS source filesystem
 * @param src source path
 * @param dstFS destination filesystem
 * @param dst destination path
 * @param deleteSource flag to delete source after copy
 * @param conf configuration settings
 * @return true if operation successful, false otherwise
 * @throws IOException on I/O errors
 */","* Copy files between FileSystems.
   * @param srcFS src fs.
   * @param src src.
   * @param dstFS dst fs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @return if copy success true, not false.
   * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",373,411,"/**
* Copies multiple files from source to destination directory.
* @param srcFS source file system
* @param srcs array of source paths
* @param dstFS destination file system
* @param dst destination path (must be a directory)
* @param deleteSource true if source should be deleted after copy
* @param overwrite true if existing files in destination should be overwritten
* @param conf configuration settings
* @return true if all copies are successful, false otherwise
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,open,"org.apache.hadoop.fs.viewfs.NflyFSystem:open(org.apache.hadoop.fs.Path,int)",579,621,"/**
 * Opens a file input stream for the given path.
 * @param f file path to open
 * @param bufferSize buffer size for reading
 * @return FSDataInputStream object or throws exception if failed
 * @throws IOException if an I/O error occurs
 */","* Category: READ.
   *
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * @return input stream according to nfly flags (closest, most recent)
   * @throws IOException
   * @throws FileNotFoundException iff all destinations generate this exception",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$Merge:processArguments(java.util.LinkedList),91,114,"/**
* Copies files from source to destination.
* @param items list of PathData objects representing the files to copy
* @throws IOException if an I/O error occurs during the process
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,getInputStream,org.apache.hadoop.fs.shell.Display$Cat:getInputStream(org.apache.hadoop.fs.shell.PathData),106,109,"/**
 * Retrieves input stream from PathData.
 * @param item PathData object containing file path
 * @return InputStream of the file or throws IOException if an error occurs
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,processPath,org.apache.hadoop.fs.shell.Head:processPath(org.apache.hadoop.fs.shell.PathData),63,70,"/**
 * Masks a file path.
 * @param item PathData object representing the file
 * @throws IOException if an I/O error occurs or if the path is a directory
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,processPath,org.apache.hadoop.fs.shell.Tail:processPath(org.apache.hadoop.fs.shell.PathData),88,103,"/**
 * Masks a file by modifying its content.
 * @param item PathData representing the file to mask
 * @throws IOException if an I/O error occurs or path is a directory
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(),1077,1079,"/**
 * Creates a Scanner to read from this input source.
 * @param begin starting index of the range
 * @param end ending index of the range (exclusive)
 * @return Scanner object for the specified range
 * @throws IOException if an I/O error occurs
 */","* Get a scanner than can scan the whole TFile.
     * 
     * @return The scanner object. A valid Scanner is always returned even if
     *         the TFile is empty.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByRecordNum,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByRecordNum(long,long)",1194,1202,"/**
* Creates a Scanner for records within a specified range.
* @param beginRecNum starting record number (inclusive)
* @param endRecNum ending record number (exclusive)
* @return Scanner object for the specified range
* @throws IOException if an I/O error occurs
*/","* Create a scanner that covers a range of records.
     * 
     * @param beginRecNum
     *          The RecordNum for the first record (inclusive).
     * @param endRecNum
     *          The RecordNum for the last record (exclusive). To scan the whole
     *          file, either specify endRecNum==-1 or endRecNum==getEntryCount().
     * @return The TFile scanner that covers the specified range of records.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,long,long)",1264,1268,"/**
* Initializes scanner with specific range.
* @param reader source of characters
* @param offBegin offset to start reading from
* @param offEnd offset to stop reading at
* @throws IOException if an I/O error occurs
*/","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param offBegin
       *          Begin byte-offset of the scan.
       * @param offEnd
       *          End byte-offset of the scan.
       * @throws IOException
       * 
       *           The offsets will be rounded to the beginning of a compressed
       *           block whose offset is greater than or equal to the specified
       *           offset.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",1366,1385,"/**
 * Searches for key in range and updates location.
 * @param key the RawComparable key to search for
 * @param beyond flag indicating if search should go beyond current bounds
 * @return true if key is found within valid range, false otherwise
 * @throws IOException if an I/O error occurs during search
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,rewind,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:rewind(),1437,1439,"/**
 * Begins masking operation at specified location.
 * @param beginLocation starting point for masking
 * @throws IOException if an I/O error occurs during masking
 */","* Rewind to the first entry in the scanner. The entry returned by the
       * previous entry() call will be invalid.
       * 
       * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seek,org.apache.hadoop.io.MapFile$Reader:seek(org.apache.hadoop.io.WritableComparable),692,694,"/**
 * Checks if the given key is masked.
 * @param key the key to check
 * @return true if the key is masked, false otherwise
 */","* Positions the reader at the named key, or if none such exists, at the
     * first entry after the named key.  Returns true iff the named key exists
     * in this map.
     *
     * @param key key.
     * @throws IOException raised on errors performing I/O.
     * @return if the named key exists in this map true, not false.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getClosest,"org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",842,846,"/**
* Calls m1 with default append flag.
* @param key input key
* @param val input value
* @return result from m1 method
* @throws IOException if an I/O error occurs
*/","* Finds the record that is the closest match to the specified key.
     * Returns <code>key</code> or if it does not exist, at the first entry
     * after the named key.
     * 
     * @param key key that we're trying to find.
     * @param val data value if key is found.
     * @return the key that was the closest match or null if eof.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isSecurityEnabled,org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled(),387,389,"/**
 * Checks if SIMPLE authentication is masked.
 * @return true if SIMPLE auth is masked, false otherwise
 */","* Determine if UserGroupInformation is using Kerberos to determine
   * user identities or is relying on simple authentication
   * 
   * @return true if UGI is working in a secure environment",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logoutUserFromKeytab,org.apache.hadoop.security.UserGroupInformation:logoutUserFromKeytab(),1158,1189,"/**
 * Initiates logout process for a user.
 * @throws IOException if an I/O error occurs during logout
 */","* Log the current user out who previously logged in using keytab.
   * This method assumes that the user logged in by calling
   * {@link #loginUserFromKeytab(String, String)}.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if a failure occurred in logout,
   * or if the user did not log in by invoking loginUserFromKeyTab() before.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,checkStat,"org.apache.hadoop.io.SecureIOUtils:checkStat(java.io.File,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",282,303,"/**
 * Checks and enforces file ownership.
 * @param f the file to check
 * @param owner current owner of the file
 * @param group current group of the file
 * @param expectedOwner expected owner of the file
 * @param expectedGroup expected group of the file
 * @throws IOException if owner does not match expected owner
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getPrimaryGroupName,org.apache.hadoop.security.UserGroupInformation:getPrimaryGroupName(),1655,1661,"/**
* Masks the function with a string.
* @return masked string representation
* @throws IOException if primary group not found
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupNames,org.apache.hadoop.security.UserGroupInformation:getGroupNames(),1778,1781,"/**
* Retrieves and masks user groups.
* @return Array of masked group names
*/","* Get the group names for this user. {@link #getGroupsSet()} is less
   * expensive alternative when checking for a contained element.
   * @return the list of users with the primary group first. If the command
   *    fails, it returns an empty list.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,isUserInList,org.apache.hadoop.security.authorize.AccessControlList:isUserInList(org.apache.hadoop.security.UserGroupInformation),235,249,"/**
* Checks if user or group has permission.
* @param ugi user group information
* @return true if allowed, false otherwise
*/","* Checks if a user represented by the provided {@link UserGroupInformation}
   * is a member of the Access Control List. If user was proxied and
   * USE_REAL_ACLS + the real user name is in the control list, then treat this
   * case as if user were in the ACL list.
   * @param ugi UserGroupInformation to check if contained in the ACL
   * @return true if ugi is member of the list or if USE_REAL_ACLS + real user
   * is in the list",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUGIFromSubject,org.apache.hadoop.security.UserGroupInformation:getUGIFromSubject(javax.security.auth.Subject),651,664,"/**
* Retrieves UserGroupInformation from Subject.
* @param subject security context for authentication
* @return UserGroupInformation object
* @throws IOException if authentication fails
*/","* Create a UserGroupInformation from a Subject with Kerberos principal.
   *
   * @param subject             The KerberosPrincipal to use in UGI.
   *                            The creator of subject is responsible for
   *                            renewing credentials.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if the kerberos login fails
   * @return UserGroupInformation",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createLoginUser,org.apache.hadoop.security.UserGroupInformation:createLoginUser(javax.security.auth.Subject),731,803,"/**
* Loads user credentials from token files and base64 tokens.
* @param subject security subject
* @return UserGroupInformation object with loaded credentials
* @throws IOException if credential loading fails
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,doFilter,"org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",60,105,"/**
* Handles proxy user authentication for HTTP requests.
* @param filterChain the filter chain to continue processing with
* @param request the HttpServletRequest object
* @param response the HttpServletResponse object
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,doFilter,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",243,308,"/**
* Handles authentication and user group information setup for requests.
* @param filterChain the filter chain to process the request
* @param request the HTTP servlet request
* @param response the HTTP servlet response
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,managementOperation,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",220,355,"/**
* Handles Kerberos delegation token operations.
* @param token authentication token
* @param request HTTP request object
* @param response HTTP response object
* @return true if request processing continues, false otherwise
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.conf.Configuration)",134,138,"/**
* Deprecated. Use m1 with fewer parameters instead.
* @param user UserGroupInformation object representing the user
* @param remoteAddress Remote address string
* @param conf Configuration object (not used in this method)
*/","* This function is kept to provide backward compatibility.
   * @param user user.
   * @param remoteAddress remote address.
   * @param conf configuration.
   * @throws AuthorizationException Authorization Exception.
   * @deprecated use {@link #authorize(UserGroupInformation, String)} instead.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,authorizeConnection,org.apache.hadoop.ipc.Server$Connection:authorizeConnection(),3018,3039,"/**
* Handles authorization and logs success or failure.
* @throws RpcServerException if authorization fails
*/","* Authorize proxy users to access this server
     * @throws RpcServerException - user is not allowed to proxy",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,copyFileToTarget,"org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",140,154,"/**
* Copies file data from source to target.
* Uses executor if available, otherwise calls superclass method directly.
* @param src source PathData object
* @param target target PathData object
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPath,"org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",287,297,"/**
 * Masks path data from source to destination.
 * @param src source path data
 * @param dst destination path data
 * @throws IOException if operation fails
 */","* Called with a source and target destination pair
   * @param src for the operation
   * @param dst for the operation
   * @throws IOException if anything goes wrong",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.HarFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",845,849,"/**
 * Masks and copies a file.
 * @param delSrc true to delete source file after copying
 * @param src source file path
 * @param dst destination file path
 * @throws IOException if an I/O error occurs
 */",* copies the file in the har filesystem to a local file.,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,rename,"org.apache.hadoop.fs.RawLocalFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",624,644,"/**
 * Masks a file by copying it.
 * @param src source file path
 * @param dst destination file path
 * @return true if masking successful, false otherwise
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",991,996,"/**
 * Moves or copies a file from source to destination.
 * @param delSrc true to delete the source file after copying
 * @param src source file path
 * @param dst destination file path
 * @throws IOException if an I/O error occurs
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1002,1007,"/**
 * Masks a file by moving it to a new location.
 * @param delSrc flag to delete the source file after copying
 * @param src source file path
 * @param dst destination file path
 * @throws IOException if an I/O error occurs
 */","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.LocalFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,87,"/**
 * Moves or copies files from source to destination.
 * @param delSrc true to delete the source file after copying
 * @param src source file path
 * @param dst destination file path
 * @throws IOException if an I/O error occurs during the operation
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.LocalFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",89,93,"/**
 * Masks and moves a file.
 * @param delSrc true to delete source file after moving
 * @param src source file path
 * @param dst destination file path
 * @throws IOException if an I/O error occurs
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processPath,org.apache.hadoop.fs.shell.Display$Cat:processPath(org.apache.hadoop.fs.shell.PathData),88,96,"/**
* Processes file path data, verifying checksum and handling directories.
* @param item PathData object containing file information
* @throws IOException if an I/O error occurs or item is a directory
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByByteRange,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByByteRange(long,long)",1094,1096,"/**
 * Creates a Scanner from a specified range in the input source.
 * @param offset starting position in bytes
 * @param length number of bytes to scan
 * @return Scanner object for the specified range
 * @throws IOException if an I/O error occurs
 */","* Get a scanner that covers a portion of TFile based on byte offsets.
     * 
     * @param offset
     *          The beginning byte offset in the TFile.
     * @param length
     *          The length of the region.
     * @return The actual coverage of the returned scanner tries to match the
     *         specified byte-region but always round up to the compression
     *         block boundaries. It is possible that the returned scanner
     *         contains zero key-value pairs even if length is positive.
     * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1318,1331,"/**
 * Initializes Scanner with specified reader and key range.
 * @param reader data source
 * @param beginKey start of the key range (inclusive)
 * @param endKey end of the key range (exclusive)
 * @throws IOException if an I/O error occurs
 */","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param beginKey
       *          Begin key of the scan. If null, scan from the first
       *          &lt;K, V&gt; entry of the TFile.
       * @param endKey
       *          End key of the scan. If null, scan up to the last &lt;K, V&gt;
       *          entry of the TFile.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[],int,int)",1361,1364,"/**
* Validates byte array using specified offset and length.
* @param key byte array containing data to validate
* @param keyOffset starting index in the byte array
* @param keyLen number of bytes to validate from the offset
* @return true if validation passes, false otherwise
*/","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. The entry returned by the previous entry() call will
       * be invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @return true if we find an equal key; false otherwise.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[],int,int)",1477,1480,"/**
* Masks data using specified key.
* @param key byte array containing the mask key
* @param keyOffset starting offset in the key array
* @param keyLen length of the key to use
*/","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. The entry returned by the previous entry() call will
       * be invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[],int,int)",1508,1511,"/**
* Masks data using provided key.
* @param key byte array containing the mask key
* @param keyOffset starting index of the key in the array
* @param keyLen length of the key to use
*/","* Move the cursor to the first entry whose key is strictly greater than
       * the input key. The entry returned by the previous entry() call will be
       * invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,seek,org.apache.hadoop.io.SetFile$Reader:seek(org.apache.hadoop.io.WritableComparable),130,134,"/**
 * Overrides the default implementation to call the superclass method.
 * @param key the key to process
 * @return true if processing is successful, false otherwise
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,get,"org.apache.hadoop.io.MapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",823,830,"/**
* Applies mask operation to input value if condition is met.
* @param key unique identifier for the writable object
* @param val writable object to be masked
* @return masked value or null if condition fails
*/","* Return the value for the named key, or null if none exists.
     * @param key key.
     * @param val val.
     * @return Writable if such a pair exists true,not false.
     * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,commit,org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit(),189,235,"/**
 * Performs Hadoop login and sets up the user subject.
 * @throws LoginException if user cannot be authenticated
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",579,587,"/**
* Creates a ProtocolProxy for specified protocol.
* @param <T> type of the protocol
* @param protocol class of the protocol
* @param clientVersion version of the client
* @param connId connection identifier
* @param conf configuration settings
* @param factory socket factory for connections
* @param alignmentContext context for alignment
* @return ProtocolProxy instance
* @throws IOException if an I/O error occurs
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T
   * @param protocol protocol class
   * @param clientVersion client's version
   * @param connId client connection identifier
   * @param conf configuration
   * @param factory socket factory
   * @param alignmentContext StateID alignment context
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean)",661,677,"/**
 * Creates a ProtocolProxy for a given protocol class.
 * @param <T> type of the protocol
 * @param protocol class representing the protocol
 * @param clientVersion version of the client
 * @param addr address of the server
 * @param ticket user group information for authentication
 * @param conf configuration settings
 * @param factory socket factory for creating connections
 * @param rpcTimeout timeout for RPC operations
 * @param connectionRetryPolicy retry policy for connection attempts
 * @param fallbackToSimpleAuth flag to allow falling back to simple auth
 * @return ProtocolProxy instance
 * @throws IOException if an I/O error occurs
 */","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @param fallbackToSimpleAuth set to true or false during calls to indicate if
   *   a secure client falls back to simple auth
   * @return the proxy
   * @throws IOException if any error occurs",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",698,715,"/**
* Creates a ProtocolProxy for the given protocol.
* @param <T> type of the protocol
* @param protocol class of the protocol
* @param clientVersion version of the client
* @param addr address of the server
* @param ticket user authentication information
* @param conf configuration settings
* @param factory socket factory for creating connections
* @param rpcTimeout timeout for RPC calls
* @param connectionRetryPolicy policy for retrying failed connections
* @param fallbackToSimpleAuth flag to allow simple authentication fallback
* @param alignmentContext context for alignment purposes
* @return ProtocolProxy instance
* @throws IOException if an I/O error occurs
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @param fallbackToSimpleAuth set to true or false during calls to indicate
   *   if a secure client falls back to simple auth
   * @param alignmentContext state alignment context
   * @param <T> Generics Type T.
   * @return the proxy
   * @throws IOException if any error occurs",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setFallBackToSimpleAuth,org.apache.hadoop.ipc.Client$Connection:setFallBackToSimpleAuth(java.util.concurrent.atomic.AtomicBoolean),858,890,"/**
* Handles authentication method checks and fallbacks.
* @param fallbackToSimpleAuth flag to enable SIMPLE auth fallback
* @throws AccessControlException if secure connection is required but not allowed
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenForRandomRead,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)",126,143,"/**
* Opens a file with specified mode and checks ownership.
* @param f the file to open
* @param mode file access mode (""r"", ""rw"", etc.)
* @param expectedOwner expected owner of the file
* @param expectedGroup expected group of the file
* @return RandomAccessFile object if successful, throws IOException otherwise
*/","* @return Same as openForRandomRead except that it will run even if security is off.
   * This is used by unit tests.
   *
   * @param f input f.
   * @param mode input mode.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenFSDataInputStream,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenFSDataInputStream(java.io.File,java.lang.String,java.lang.String)",174,192,"/**
* Opens a file for reading and verifies its owner and group.
* @param file the file to open
* @param expectedOwner expected owner of the file
* @param expectedGroup expected group of the file
* @return FSDataInputStream if verification succeeds, throws IOException otherwise
*/","* Same as openFSDataInputStream except that it will run even if security is
   * off. This is used by unit tests.
   *
   * @param file input file.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.
   * @return FSDataInputStream.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenForRead,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRead(java.io.File,java.lang.String,java.lang.String)",224,241,"/**
* Opens a file input stream and checks file ownership.
* @param f the file to open
* @param expectedOwner expected owner of the file
* @param expectedGroup expected group of the file
* @return FileInputStream if successful, throws IOException otherwise
*/","* @return Same as openForRead() except that it will run even if security is off.
   * This is used by unit tests.
   * @param f input f.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path),1080,1087,"/**
 * Masks a file and returns its status.
 * @param f the file path to mask
 * @return FileStatus object representing the masked file
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path),1089,1128,"/**
 * Retrieves file status for a given path.
 * @param f the path to check
 * @return FileStatus object representing the file or directory
 * @throws IOException if an I/O error occurs
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path),1406,1413,"/**
* Retrieves ACL status for a path.
* @param path file system path
* @return AclStatus object with specified permissions
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path),1544,1551,"/**
 * Creates a masked file status for the given path.
 * @param f the file path
 * @return FileStatus object with specific permissions and metadata
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path),1554,1620,"/**
* Retrieves file statuses for a given path.
* @param f file path
* @return array of FileStatus objects
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if file not found
* @throws IOException if I/O error occurs
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path),1832,1839,"/**
* Retrieves ACL status for a path.
* @param path file system path
* @return AclStatus object with masked permissions
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpUGI,"org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",666,690,"/**
* Logs user group information details.
* @param title descriptive title for the log
* @param ugi UserGroupInformation object to log
*/","* Dump a UGI.
   *
   * @param title title of this section
   * @param ugi UGI to dump
   * @throws IOException",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,print,org.apache.hadoop.security.UserGroupInformation:print(),2022,2032,"/**
* Prints user information and group details.
* @throws IOException if an I/O error occurs
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,isUserAllowed,org.apache.hadoop.security.authorize.AccessControlList:isUserAllowed(org.apache.hadoop.security.UserGroupInformation),251,253,"/**
 * Checks if the given UserGroupInformation is masked.
 * @param ugi user group information object
 * @return true if masked, false otherwise
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getLoginUser,org.apache.hadoop.security.UserGroupInformation:getLoginUser(),673,698,"/**
* Retrieves the current login user information.
* @return UserGroupInformation object representing the logged-in user
* @throws IOException if an I/O error occurs during retrieval
*/","* Get the currently logged in user.  If no explicit login has occurred,
   * the user will automatically be logged in with either kerberos credentials
   * if available, or as the local OS user, based on security settings.
   * @return the logged in user
   * @throws IOException if login fails",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromSubject,org.apache.hadoop.security.UserGroupInformation:loginUserFromSubject(javax.security.auth.Subject),725,729,"/**
* Masks sensitive information in the given subject.
* @param subject the subject containing sensitive data
*/","* Log in a user using the given subject
   * @param subject the subject to use when logging in a user, or null to
   * create a new subject.
   *
   * If subject is not null, the creator of subject is responsible for renewing
   * credentials.
   *
   * @throws IOException if login fails",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processConnectionContext,org.apache.hadoop.ipc.Server$Connection:processConnectionContext(org.apache.hadoop.ipc.RpcWritable$Buffer),2678,2724,"/**
 * Processes RPC connection context and user authentication.
 * @param buffer input buffer containing connection details
 * @throws RpcServerException on invalid or unauthorized access
 */","Reads the connection context following the connection header
     * @throws RpcServerException - if the header cannot be
     *         deserialized, or the user is not authorized",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPath,org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData),276,279,"/**
 * Calls overloaded m2 with source path and result from m1.
 * @param src source PathData object
 */",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.RawLocalFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",877,880,"/**
 * Masks a file by moving it from source to destination.
 * @param src source file path
 * @param dst destination file path
 * @throws IOException if an I/O error occurs
 */",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1174,1181,"/**
* Creates a Scanner with specified key range.
* @param beginKey start of the key range (inclusive)
* @param endKey end of the key range (exclusive)
* @return Scanner object for the given key range
* @throws IOException if an I/O error occurs
*/","* Get a scanner that covers a specific key range.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[]),1343,1345,"/**
 * Checks if a byte array contains valid data.
 * @param key byte array to check
 * @return true if valid, false otherwise
 * @throws IOException if an I/O error occurs
 */","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. Synonymous to seekTo(key, 0, key.length). The entry
       * returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @return true if we find an equal key.
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[]),1460,1462,"/**
 * Calls overloaded method with full byte array.
 * @param key byte array to process
 */","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. Synonymous to lowerBound(key, 0, key.length). The
       * entry returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[]),1491,1493,"/**
 * Calls overloaded method with full byte array.
 * @param key byte array to process
 */","* Move the cursor to the first entry whose key is strictly greater than
       * the input key. Synonymous to upperBound(key, 0, key.length). The entry
       * returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,get,org.apache.hadoop.io.SetFile$Reader:get(org.apache.hadoop.io.WritableComparable),156,163,"/**
* Applies mask to key if condition met.
* @param key input key to process
* @return masked key or null if condition fails
*/","* Read the matching key from a set into <code>key</code>.
     *
     * @param key input key.
     * @return Returns <code>key</code>, or null if no match exists.
     * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,get,"org.apache.hadoop.io.BloomMapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",281,288,"/**
* Overrides base method to conditionally process writable data.
* @param key unique identifier for the data
* @param val data value to be processed
* @return processed Writable or null if condition not met
*/","* Fast version of the
     * {@link MapFile.Reader#get(WritableComparable, Writable)} method. First
     * it checks the Bloom filter for the existence of the key, and only if
     * present it performs the real get operation. This yields significant
     * performance improvements for get operations on sparsely populated files.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",558,563,"/**
* Creates a ProtocolProxy instance.
* @param <T> type of the protocol
* @param protocol Class representing the protocol
* @param clientVersion version of the client
* @param connId unique connection identifier
* @param conf configuration settings
* @param factory SocketFactory for creating sockets
* @return ProtocolProxy object
* @throws IOException if an I/O error occurs
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T
   * @param protocol protocol class
   * @param clientVersion client's version
   * @param connId client connection identifier
   * @param conf configuration
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",631,641,"/**
* Creates a ProtocolProxy with default retry handler.
* @param protocol interface class of the protocol
* @param clientVersion version of the client
* @param addr address of the server
* @param ticket user group information for authentication
* @param conf configuration settings
* @param factory socket factory for creating sockets
* @param rpcTimeout timeout for RPC calls
* @param connectionRetryPolicy policy for retrying connections
* @return ProtocolProxy instance
* @throws IOException if an I/O error occurs
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @return the proxy
   * @throws IOException if any error occurs",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupIOstreams,org.apache.hadoop.ipc.Client$Connection:setupIOstreams(java.util.concurrent.atomic.AtomicBoolean),764,856,"/**
 * Establishes an IPC connection with a server using SASL authentication.
 * @param fallbackToSimpleAuth flag to switch to simple auth if necessary
 */","Connect to the server and set up the I/O streams. It then sends
     * a header to the server and starts
     * the connection thread that waits for responses.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openForRandomRead,"org.apache.hadoop.io.SecureIOUtils:openForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)",107,114,"/**
* Opens a file with specified mode and ownership checks.
* @param f the file to open
* @param mode the access mode (""r"", ""rw"", etc.)
* @param expectedOwner the expected owner of the file
* @param expectedGroup the expected group of the file
* @return RandomAccessFile object
* @throws IOException if file cannot be opened or checks fail
*/","* @return Open the given File for random read access, verifying the expected user/
   * group constraints if security is enabled.
   * 
   * Note that this function provides no additional security checks if hadoop
   * security is disabled, since doing the checks would be too expensive when
   * native libraries are not available.
   * 
   * @param f file that we are trying to open
   * @param mode mode in which we want to open the random access file
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO error occurred or if the user/group does
   * not match when security is enabled.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openFSDataInputStream,"org.apache.hadoop.io.SecureIOUtils:openFSDataInputStream(java.io.File,java.lang.String,java.lang.String)",156,162,"/**
 * Checks file ownership and returns input stream.
 * @param file the target file
 * @param expectedOwner expected owner of the file
 * @param expectedGroup expected group of the file
 * @return FSDataInputStream if conditions are met, otherwise throws IOException
 */","* Opens the {@link FSDataInputStream} on the requested file on local file
   * system, verifying the expected user/group constraints if security is
   * enabled.
   * @param file absolute path of the file
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO Error occurred or the user/group does not
   * match if security is enabled
   * @return FSDataInputStream.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openForRead,"org.apache.hadoop.io.SecureIOUtils:openForRead(java.io.File,java.lang.String,java.lang.String)",208,214,"/**
 * Opens a file input stream with ownership checks.
 * @param f the file to open
 * @param expectedOwner the expected owner of the file
 * @param expectedGroup the expected group of the file
 * @return FileInputStream if permissions match, otherwise throws IOException
 */","* Open the given File for read access, verifying the expected user/group
   * constraints if security is enabled.
   *
   * @return Note that this function provides no additional checks if Hadoop
   * security is disabled, since doing the checks would be too expensive
   * when native libraries are not available.
   *
   * @param f the file that we are trying to open
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO Error occurred, or security is enabled and
   * the user/group does not match",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getLinkTarget(org.apache.hadoop.fs.Path),1334,1338,"/**
 * Applies masking operations to a file path.
 * @param f input file path
 * @return modified file path after masking
 * @throws FileNotFoundException if the file is not found
 * @throws IOException if an I/O error occurs
 */",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getContentSummary(org.apache.hadoop.fs.Path),1659,1678,"/**
* Computes content summary for a directory.
* @param f directory path
* @return ContentSummary object with total size, file count, and directory count
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStatus(org.apache.hadoop.fs.Path),1680,1694,"/**
* Aggregates file system status for a directory.
* @param p path to the directory
* @return aggregated FsStatus object
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,userHasAdministratorAccess,"org.apache.hadoop.http.HttpServer2:userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)",1727,1734,"/**
* Checks if the remote user is an admin.
* @param servletContext application context
* @param remoteUser username of the user to check
* @return true if user is an admin, false otherwise
*/","* Get the admin ACLs from the given ServletContext and check if the given
   * user is in the ACL.
   *
   * @param servletContext the context containing the admin ACL.
   * @param remoteUser the remote user to check for.
   * @return true if the user is present in the ACL, false if no ACL is set or
   *         the user is not present",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,authorize,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,org.apache.hadoop.conf.Configuration,java.net.InetAddress)",88,138,"/**
* Authorizes a user for a specific protocol and address.
* @param user the UserGroupInformation object representing the user
* @param protocol the Class<?> object representing the protocol
* @param conf the Configuration object containing configuration settings
* @param addr the InetAddress object representing the client's address
*/","* Authorize the user to access the protocol being used.
   * 
   * @param user user accessing the service 
   * @param protocol service being accessed
   * @param conf configuration to use
   * @param addr InetAddress of the client
   * @throws AuthorizationException on authorization failure",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,authorize,"org.apache.hadoop.security.authorize.DefaultImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)",108,135,"/**
 * Checks authorization for impersonation and proxy access.
 * @param user the UserGroupInformation of the user attempting to act as another
 * @param remoteAddress the InetAddress of the connection
 * @throws AuthorizationException if impersonation is not allowed or IP is unauthorized
 */",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCurrentUser,org.apache.hadoop.security.UserGroupInformation:getCurrentUser(),583,594,"/**
* Retrieves user group information.
* @return UserGroupInformation object or default if null or empty subject
* @throws IOException on I/O errors during retrieval
*/","* Return the current user, including any doAs in the current stack.
   * @return the current user
   * @throws IOException if login fails",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginKeytabBased,org.apache.hadoop.security.UserGroupInformation:isLoginKeytabBased(),1417,1421,"/**
* Checks a condition using nested methods.
* @return boolean result of the condition check
* @throws IOException if an I/O error occurs during the process
*/","* Did the login happen via keytab.
   * @return true or false
   * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginTicketBased,org.apache.hadoop.security.UserGroupInformation:isLoginTicketBased(),1428,1430,"/**
 * Checks a condition using nested methods.
 * @throws IOException if an I/O error occurs
 * @return result of the condition check
 */","* Did the login happen via ticket cache.
   * @return true or false
   * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsLoginUserOrFatal,org.apache.hadoop.security.SecurityUtil:doAsLoginUserOrFatal(java.security.PrivilegedAction),508,522,"/**
 * Executes an action with privileged permissions.
 * @param action the action to execute
 * @return result of the action
 */","* Perform the given action as the daemon's login user. If the login
   * user cannot be determined, this will log a FATAL error and exit
   * the whole JVM.
   *
   * @param action action.
   * @param <T> generic type T.
   * @return generic type T.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsLoginUser,org.apache.hadoop.security.SecurityUtil:doAsLoginUser(java.security.PrivilegedExceptionAction),533,536,"/**
* Executes an action with user privileges.
* @param action privileged action to execute
* @return result of the action or throws IOException
*/","* Perform the given action as the daemon's login user. If an
   * InterruptedException is thrown, it is converted to an IOException.
   *
   * @param action the action to perform
   * @param <T> Generics Type T.
   * @return the result of the action
   * @throws IOException in the event of error",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,cedeActive,org.apache.hadoop.ha.ZKFailoverController:cedeActive(int),575,588,"/**
* Executes a privileged action with a specified delay.
* @param millisToCede milliseconds to delay execution
* @throws AccessControlException if access is denied
* @throws ServiceFailedException if service fails
* @throws IOException if an I/O error occurs
*/","* Request from graceful failover to cede active role. Causes
   * this ZKFC to transition its local node to standby, then quit
   * the election for the specified period of time, after which it
   * will rejoin iff it is healthy.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,gracefulFailoverToYou,org.apache.hadoop.ha.ZKFailoverController:gracefulFailoverToYou(),630,643,"/**
* Executes a privileged action with user group information.
* Throws ServiceFailedException or IOException on failure.
*/","* Coordinate a graceful failover to this node.
   * @throws ServiceFailedException if the node fails to become active
   * @throws IOException some other error occurs",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(byte[],byte[])",1132,1137,"/**
* Creates a Scanner for a range of byte arrays.
* @param beginKey start of the range or null for no limit
* @param endKey end of the range or null for no limit
* @return Scanner object configured for the specified range
* @throws IOException if an I/O error occurs
*/","* Get a scanner that covers a portion of TFile based on keys.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1155,1159,"/**
* @deprecated Use m1 instead.
* Returns a Scanner for keys between beginKey and endKey.
* @param beginKey start of the key range (inclusive)
* @param endKey end of the key range (exclusive)
* @return Scanner object for iterating over the range
* @throws IOException if an I/O error occurs
*/","* Get a scanner that covers a specific key range.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.
     * 
     * @deprecated Use {@link #createScannerByKey(RawComparable, RawComparable)}
     *             instead.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",535,543,"/**
* Creates a ProtocolProxy instance.
* @param protocol class of the protocol to proxy
* @param clientVersion version of the client
* @param addr address of the server
* @param ticket user group information for authentication
* @param conf configuration settings
* @param factory socket factory for creating sockets
* @return ProtocolProxy object
* @throws IOException if an I/O error occurs
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param ticket user group information
   * @param conf configuration to use
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",604,613,"/**
* Invokes method m2 on a remote service.
* @param <T> type of the protocol class
* @param protocol interface class for remote service
* @param clientVersion version of the client
* @param addr address of the remote server
* @param ticket user authentication information
* @param conf configuration settings
* @param factory socket factory for creating connections
* @param rpcTimeout timeout for RPC calls
* @return result of m2 method call
* @throws IOException if communication fails
*/","* Construct a client-side proxy that implements the named protocol,
   * talking to a server at the named address.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @return the proxy
   * @throws IOException if any error occurs",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,hasAdministratorAccess,"org.apache.hadoop.http.HttpServer2:hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1686,1716,"/**
* Checks user authorization based on configuration and ACLs.
* @param servletContext current ServletContext
* @param request incoming HttpServletRequest
* @param response outgoing HttpServletResponse
* @return true if authorized, false otherwise
*/","* Does the user sending the HttpServletRequest has the administrator ACLs? If
   * it isn't the case, response will be modified to send an error to the user.
   *
   * @param servletContext servletContext.
   * @param request request.
   * @param response used to send the error response if user does not have admin access.
   * @return true if admin-authorized, false otherwise
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,authorize,"org.apache.hadoop.ipc.Server:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.net.InetAddress)",3806,3821,"/**
* Authorizes user access based on protocol and address.
* @param user user information
* @param protocolName name of the communication protocol
* @param addr IP address of the client
* @throws AuthorizationException if authorization fails
*/","* Authorize the incoming client connection.
   * 
   * @param user client user
   * @param protocolName - the protocol
   * @param addr InetAddress of incoming connection
   * @throws AuthorizationException when the client isn't authorized to talk the protocol",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getHomeDirectory,org.apache.hadoop.fs.FileSystem:getHomeDirectory(),2445,2456,"/**
 * Retrieves the user's home directory path.
 * @return Path object representing the user's home directory
 */","Return the current user's home directory in this FileSystem.
   * The default implementation returns {@code ""/user/$USER/""}.
   * @return the path.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,checkAccessPermissions,"org.apache.hadoop.fs.FileSystem:checkAccessPermissions(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction)",2855,2877,"/**
* Checks file permissions for a given user.
* @param stat FileStatus object representing the file
* @param mode FsAction representing the required permission
* @throws AccessControlException if permission is denied
* @throws IOException on I/O errors
*/","* This method provides the default implementation of
   * {@link #access(Path, FsAction)}.
   *
   * @param stat FileStatus to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws IOException for any error",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(),280,283,"/**
 * Initializes the FileSystem view.
 * @throws IOException if an I/O error occurs during initialization
 */","* This is the  constructor with the signature needed by
   * {@link FileSystem#createFileSystem(URI, Configuration)}
   *
   * After this constructor is called initialize() is called.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",227,290,"/**
 * Initializes a ViewFs file system.
 * @param theUri URI of the file system
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs
 * @throws URISyntaxException if URI syntax is invalid
 */","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   *
   * @param theUri which must be that of ViewFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree:<init>(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI,boolean)",617,770,"/**
 * Initializes an InodeTree with configuration and URI.
 * @param config Configuration object
 * @param viewName Name of the view
 * @param theUri Target URI
 * @param initingUriAsFallbackOnNoMounts Flag to initialize URI as fallback
 * @throws UnsupportedFileSystemException, URISyntaxException,
 *         FileAlreadyExistsException, IOException on errors
 */","* Create Inode Tree from the specified mount-table specified in Config.
   *
   * @param config the mount table keys are prefixed with
   *               FsConstants.CONFIG_VIEWFS_PREFIX.
   * @param viewName the name of the mount table
   *                 if null use defaultMT name.
   * @param theUri heUri.
   * @param initingUriAsFallbackOnNoMounts initingUriAsFallbackOnNoMounts.
   * @throws UnsupportedFileSystemException file system for <code>uri</code> is
   *                                        not found.
   * @throws URISyntaxException if the URI does not have an authority
   *                            it is badly formed.
   * @throws FileAlreadyExistsException there is a file at the path specified
   *                                    or is discovered on one of its ancestors.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration,long)",3881,3889,"/**
 * Initializes a Key with URI, configuration, and a unique identifier.
 * @param uri the URI associated with the key
 * @param conf the Configuration object
 * @param unique a unique long identifier for the key
 * @throws IOException if an I/O error occurs while retrieving current user information
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.AbstractFileSystem:getHomeDirectory(),461,472,"/**
 * Returns a masked path for the current user.
 * @return Path object representing the user's directory
 */","* Return the current user's home directory in this file system.
   * The default implementation returns ""/user/$USER/"".
   * 
   * @return current user's home directory.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",283,335,"/**
 * Establishes an HTTP connection with authentication and optional delegation.
 * @param url target URL for the connection
 * @param token authentication token
 * @param doAs user to impersonate (optional)
 * @return HttpURLConnection instance
 * @throws IOException if connection fails
 * @throws AuthenticationException if authentication fails
 */","* Returns an authenticated {@link HttpURLConnection}. If the Delegation
   * Token is present, it will be used taking precedence over the configured
   * <code>Authenticator</code>. If the <code>doAs</code> parameter is not NULL,
   * the request will be done on behalf of the specified <code>doAs</code> user.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @param doAs user to do the the request on behalf of, if NULL the request is
   * as self.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,authenticate,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",134,153,"/**
* Authenticates a URL using a token or delegates to authenticator.
* @param url the URL to authenticate
* @param token the authentication token
* @throws IOException if an I/O error occurs
* @throws AuthenticationException if authentication fails
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getBestUGI,"org.apache.hadoop.security.UserGroupInformation:getBestUGI(java.lang.String,java.lang.String)",606,615,"/**
* Retrieves UserGroupInformation based on ticket cache and user.
* @param ticketCachePath path to the ticket cache file
* @param user username for authentication
* @return UserGroupInformation object or null if not found
*/","* Find the most appropriate UserGroupInformation to use
   *
   * @param ticketCachePath    The Kerberos ticket cache path, or NULL
   *                           if none is specfied
   * @param user               The user name, or NULL if none is specified.
   *
   * @return                   The most appropriate UserGroupInformation
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromKeytabAndReturnUGI,"org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytabAndReturnUGI(java.lang.String,java.lang.String)",1388,1399,"/**
* Retrieves UserGroupInformation using Kerberos credentials.
* @param user principal name for authentication
* @param path path to the keytab file
* @return UserGroupInformation object or null if not initialized
* @throws IOException if an I/O error occurs during the process
*/","* Log a user in from a keytab file. Loads a user identity from a keytab
   * file and login them in. This new user does not affect the currently
   * logged-in user.
   * @param user the principal name to load from the keytab
   * @param path the path to the keytab file
   * @throws IOException if the keytab file can't be read
   * @return UserGroupInformation.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logAllUserInfo,"org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.slf4j.Logger,org.apache.hadoop.security.UserGroupInformation)",1999,2010,"/**
* Logs user information for security auditing.
* @param log Logger instance for logging
* @param ugi UserGroupInformation object containing user details
*/","* Log all (current, real, login) UGI and token info into specified log.
   * @param ugi - UGI
   * @param log - log.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,<init>,org.apache.hadoop.security.alias.UserProvider:<init>(),44,47,"/**
 * Initializes the UserProvider with current user's credentials.
 * @throws IOException if an I/O error occurs while fetching user information
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsCurrentUser,org.apache.hadoop.security.SecurityUtil:doAsCurrentUser(java.security.PrivilegedExceptionAction),547,550,"/**
* Executes an action with user privileges.
* @param action privileged action to execute
* @return result of the action or throws IOException
*/","* Perform the given action as the daemon's current user. If an
   * InterruptedException is thrown, it is converted to an IOException.
   *
   * @param action the action to perform
   * @param <T> generic type T.
   * @return the result of the action
   * @throws IOException in the event of error",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,<init>,org.apache.hadoop.security.SaslRpcServer:<init>(org.apache.hadoop.security.SaslRpcServer$AuthMethod),89,120,"/**
* Initializes a SaslRpcServer with the specified authentication method.
* @param authMethod the authentication method to use
* @throws IOException if an I/O error occurs
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,create,"org.apache.hadoop.security.SaslRpcServer:create(org.apache.hadoop.ipc.Server$Connection,java.util.Map,org.apache.hadoop.security.token.SecretManager)",122,173,"/**
* Creates a SASL server using specified authentication method and properties.
* @param connection network connection object
* @param saslProperties configuration properties for SASL
* @param secretManager manages secrets for token-based auth
* @return SaslServer instance or throws exception if creation fails
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,<init>,org.apache.hadoop.crypto.key.UserProvider:<init>(org.apache.hadoop.conf.Configuration),47,51,"/**
* Initializes UserProvider with configuration.
* @param conf application configuration
* @throws IOException if an I/O error occurs
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDoAsUser,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDoAsUser(),1129,1134,"/**
* Retrieves proxy user if authentication method is PROXY.
* @return proxy user string or null if not PROXY
* @throws IOException if an I/O error occurs
*/","* Get the doAs user name.
   *
   * 'actualUGI' is the UGI of the user creating the client
   * It is possible that the creator of the KMSClientProvier
   * calls this method on behalf of a proxyUser (the doAsUser).
   * In which case this call has to be made as the proxy user.
   *
   * @return the doAs user name.
   * @throws IOException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.io.retry.RetryPolicy,long)",411,453,"/**
* Creates a protocol proxy with retries.
* @param <T> type of the protocol
* @param protocol class representing the protocol
* @param clientVersion version of the client
* @param addr server address
* @param conf configuration settings
* @param rpcTimeout timeout for RPC calls
* @param connectionRetryPolicy retry policy for connections
* @param timeout total timeout for proxy creation
* @return ProtocolProxy instance
* @throws IOException if an I/O error occurs or timeout exceeds
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param rpcTimeout timeout for each RPC
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @param timeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,shouldAuthenticateOverKrb,org.apache.hadoop.ipc.Client$Connection:shouldAuthenticateOverKrb(),556,569,"/**
 * Checks Kerberos authentication for the current user.
 * @return true if authenticated, false otherwise
 * @throws IOException if an I/O error occurs during authentication
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRestrictParserDefault,org.apache.hadoop.conf.Configuration$Resource:getRestrictParserDefault(java.lang.Object),288,299,"/**
* Checks if resource masking is applicable.
* @param resource the object to be checked
* @return true if masking is not applicable, false otherwise
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection$1:run(),1082,1109,"/**
* Executes a function with RPC requests and handles exceptions.
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doKerberosRelogin,org.apache.hadoop.ipc.Server:doKerberosRelogin(),3407,3425,"/**
 * Handles re-login logic based on user group information.
 * Throws IOException if an I/O error occurs during login process.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,run,org.apache.hadoop.ha.ZKFailoverController:run(java.lang.String[]),173,199,"/**
* Executes a secure operation with failover checks.
* @param args input arguments for the operation
* @return result of the operation or error code if fails
* @throws Exception if operation fails
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,cedeActive,org.apache.hadoop.ha.ZKFCRpcServer:cedeActive(int),91,96,"/**
 * Calls methods to manage ZooKeeper failover control.
 * @param millisToCede time in milliseconds to cede control
 * @throws IOException if an I/O error occurs
 * @throws AccessControlException if access is denied
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,gracefulFailover,org.apache.hadoop.ha.ZKFCRpcServer:gracefulFailover(),98,102,"/**
 * Executes masked functions m1 and m2.
 * @throws IOException if an I/O error occurs during execution
 * @throws AccessControlException if access is denied
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(byte[],byte[])",1113,1117,"/**
* Masks data between two keys.
* @param beginKey starting key for masking
* @param endKey ending key for masking
* @return Scanner object for processed data
*/","* Get a scanner that covers a portion of TFile based on keys.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.
     * 
     * @deprecated Use {@link #createScannerByKey(byte[], byte[])} instead.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",488,494,"/**
 * Creates a ProtocolProxy instance.
 * @param protocol the interface class of the protocol
 * @param clientVersion version of the client
 * @param addr address of the server
 * @param conf configuration settings
 * @param factory socket factory for creating connections
 * @return ProtocolProxy object
 * @throws IOException if an I/O error occurs
 */","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",511,519,"/**
* Invokes method m2 on a proxy object created by m1.
* @param protocol interface class for the proxy
* @param clientVersion version of the client
* @param addr address of the server
* @param ticket user group information for authentication
* @param conf configuration settings
* @param factory socket factory for creating connections
* @return result of calling m2 on the proxy object
* @throws IOException if an I/O error occurs
*/","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   *
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input tocket.
   * @param conf input conf.
   * @param factory input factory.
   * @return the protocol proxy.
   * @throws IOException raised on errors performing I/O.
   *",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",46,54,"/**
* Initializes ZKFCProtocolClientSideTranslatorPB with given parameters.
* @param addr server address
* @param conf configuration settings
* @param socketFactory socket factory for connections
* @param timeout connection timeout in milliseconds
* @throws IOException if initialization fails
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",74,82,"/**
* Initializes HAServiceProtocol client with specified address and configuration.
* @param addr server address
* @param conf configuration settings
* @param socketFactory socket factory for connections
* @param timeout connection timeout in milliseconds
* @throws IOException if initialization fails
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doGet,"org.apache.hadoop.log.LogLevel$Servlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",325,360,"/**
 * Handles log level configuration via HTTP request.
 * @param request HTTP request containing log details
 * @param response HTTP response to send results
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/AdminAuthorizedServlet.java,doGet,"org.apache.hadoop.http.AdminAuthorizedServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",36,45,"/**
* Handles HTTP requests by delegating to another method.
* @param request incoming HTTP request
* @param response outgoing HTTP response
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,isInstrumentationAccessAllowed,"org.apache.hadoop.http.HttpServer2:isInstrumentationAccessAllowed(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1660,1674,"/**
* Checks user access based on configuration.
* @param servletContext application context
* @param request HTTP request
* @param response HTTP response
* @return true if access is granted, false otherwise
*/","* Checks the user has privileges to access to instrumentation servlets.
   * <p>
   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to FALSE
   * (default value) it always returns TRUE.
   * <p>
   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to TRUE
   * it will check that if the current user is in the admin ACLS. If the user is
   * in the admin ACLs it returns TRUE, otherwise it returns FALSE.
   *
   * @param servletContext the servlet context.
   * @param request the servlet request.
   * @param response the servlet response.
   * @return TRUE/FALSE based on the logic decribed above.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.DelegateToFileSystem:getHomeDirectory(),169,172,"/**
 * Delegates to file system implementation.
 * @return Path result from underlying FS
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getTrashRoot,org.apache.hadoop.fs.FileSystem:getTrashRoot(org.apache.hadoop.fs.Path),3439,3442,"/**
* Masks a given file path by appending a trash prefix.
* @param path original file path
* @return modified path with trash prefix
*/","* Get the root directory of Trash for current user when the path specified
   * is deleted.
   *
   * @param path the trash root of the path to be determined.
   * @return the default implementation returns {@code /user/$USER/.Trash}",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getTrashRoots,org.apache.hadoop.fs.FileSystem:getTrashRoots(boolean),3452,3478,"/**
* Retrieves file statuses from trash directories.
* @param allUsers if true, checks all users' trash; otherwise, checks current user's trash
* @return list of FileStatus objects representing files in trash
*/","* Get all the trash roots for current user or all users.
   *
   * @param allUsers return trash roots for all users if true.
   * @return all the trash root directories.
   *         Default FileSystem returns .Trash under users' home directories if
   *         {@code /user/$USER/.Trash} exists.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.FilterFileSystem:getHomeDirectory(),297,300,"/**
 * Returns a path from the file system.
 * @return Path object representing a file or directory
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,access,"org.apache.hadoop.fs.FileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",2840,2844,"/**
* Masks file permissions at a given path.
* @param path the file path to mask permissions for
* @param mode the file access permissions to apply
* @throws AccessControlException if permission check fails
* @throws FileNotFoundException if the file does not exist
* @throws IOException if an I/O error occurs
*/","* Checks if the user can access a path.  The mode specifies which access
   * checks to perform.  If the requested permissions are granted, then the
   * method returns normally.  If access is denied, then the method throws an
   * {@link AccessControlException}.
   * <p>
   * The default implementation calls {@link #getFileStatus(Path)}
   * and checks the returned permissions against the requested permissions.
   *
   * Note that the {@link #getFileStatus(Path)} call will be subject to
   * authorization checks.
   * Typically, this requires search (execute) permissions on each directory in
   * the path's prefix, but this is implementation-defined.  Any file system
   * that provides a richer authorization model (such as ACLs) may override the
   * default implementation so that it checks against that model instead.
   * <p>
   * In general, applications should avoid using this method, due to the risk of
   * time-of-check/time-of-use race conditions.  The permissions on a file may
   * change immediately after the access call returns.  Most applications should
   * prefer running specific file system actions as the desired user represented
   * by a {@link UserGroupInformation}.
   *
   * @param path Path to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws FileNotFoundException if the path does not exist
   * @throws IOException see specific implementation",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,access,"org.apache.hadoop.fs.AbstractFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",1046,1050,"/**
* Masks file permissions for a given path.
* @param path the file system path
* @param mode the access control mode to apply
* @throws AccessControlException if permission is denied
* @throws FileNotFoundException if file does not exist
* @throws UnresolvedLinkException if symbolic link cannot be resolved
* @throws IOException for other I/O errors
*/","* The specification of this method matches that of
   * {@link FileContext#access(Path, FsAction)}
   * except that an UnresolvedLinkException may be thrown if a symlink is
   * encountered in the path.
   *
   * @param path the path.
   * @param mode fsaction mode.
   * @throws AccessControlException access control exception.
   * @throws FileNotFoundException file not found exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",392,396,"/**
 * Initializes file system view with given URI and configuration.
 * @param theUri file system URI
 * @param conf configuration settings
 * @throws IOException if initialization fails
 */","* Convenience Constructor for apps to call directly.
   * @param theUri which must be that of ViewFileSystem
   * @param conf conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:<init>(),123,125,"/**
 * Constructs a new instance of ViewFileSystemOverloadScheme.
 * @throws IOException if an I/O error occurs during initialization
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,org.apache.hadoop.fs.viewfs.ViewFs:<init>(org.apache.hadoop.conf.Configuration),213,216,"/**
* Constructs a ViewFs instance with default URI.
* @param conf configuration settings
* @throws IOException if an I/O error occurs
* @throws URISyntaxException if the URI is invalid
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",3877,3879,"/**
 * Constructs a new Key instance.
 * @param uri URI of the key
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUnique,"org.apache.hadoop.fs.FileSystem$Cache:getUnique(java.net.URI,org.apache.hadoop.conf.Configuration)",3671,3674,"/**
* Initializes and returns a FileSystem instance.
* @param uri file system URI
* @param conf configuration settings
* @return FileSystem object
* @throws IOException if initialization fails
*/",The objects inserted into the cache using this method are all unique.,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ChRootedFs:getHomeDirectory(),151,154,"/**
* Returns the path by delegating to myFs.
* @return Path object from myFs
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,<init>,"org.apache.hadoop.fs.FileContext:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)",243,271,"/**
* Initializes a FileContext with a default file system and configuration.
* @param defFs default file system to use
* @param aConf configuration settings
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getHomeDirectory,org.apache.hadoop.fs.FileContext:getHomeDirectory(),577,579,"/**
 * Retrieves a file system path.
 * @return Path object from the default file system
 */","* Return the current user's home directory in this file system.
   * The default implementation returns ""/user/$USER/"".
   * @return the home directory",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getHomeDirectory,org.apache.hadoop.fs.FilterFs:getHomeDirectory(),83,86,"/**
 * Retrieves a path from the file system.
 * @return Path object representing a file or directory
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",230,235,"/**
* Handles URL connection with authentication.
* @param url target URL for the connection
* @param token authentication token
* @return HttpURLConnection instance
* @throws IOException if an I/O error occurs
* @throws AuthenticationException if authentication fails
*/","* Returns an authenticated {@link HttpURLConnection}, it uses a Delegation
   * Token only if the given auth token is an instance of {@link Token} and
   * it contains a Delegation Token, otherwise use the configured
   * {@link DelegationTokenAuthenticator} to authenticate the connection.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)",268,280,"/**
* Creates a FileSystem instance with Kerberos authentication.
* @param uri Hadoop file system URI
* @param conf configuration settings
* @param user username for authentication
* @return FileSystem object authenticated via Kerberos
* @throws IOException if an I/O error occurs
* @throws InterruptedException if the operation is interrupted
*/","* Get a FileSystem instance based on the uri, the passed in
   * configuration and the user.
   * @param uri of the filesystem
   * @param conf the configuration to use
   * @param user to perform the get as
   * @return the filesystem instance
   * @throws IOException failure to load
   * @throws InterruptedException If the {@code UGI.doAs()} call was
   * somehow interrupted.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,"org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)",571,583,"/**
 * Retrieves a FileSystem instance for the given URI and configuration.
 * @param uri Hadoop file system URI
 * @param conf Hadoop configuration object
 * @param user Kerberos principal name
 * @return FileSystem object
 * @throws IOException if an I/O error occurs
 * @throws InterruptedException if the operation is interrupted
 */","* Returns the FileSystem for this URI's scheme and authority and the
   * given user. Internally invokes {@link #newInstance(URI, Configuration)}
   * @param uri uri of the filesystem.
   * @param conf the configuration to use
   * @param user to perform the get as
   * @return filesystem instance
   * @throws IOException if the FileSystem cannot be instantiated.
   * @throws InterruptedException If the {@code UGI.doAs()} call was
   *         somehow interrupted.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUGIFromTicketCache,"org.apache.hadoop.security.UserGroupInformation:getUGIFromTicketCache(java.lang.String,java.lang.String)",627,638,"/**
* Creates UserGroupInformation for a given Kerberos ticket and user.
* @param ticketCache path to the Kerberos ticket cache
* @param user username for authentication
* @return UserGroupInformation object or null if Kerberos is not enabled
* @throws IOException on login failure
*/","* Create a UserGroupInformation from a Kerberos ticket cache.
   * 
   * @param user                The principal name to load from the ticket
   *                            cache
   * @param ticketCache     the path to the ticket cache file
   *
   * @throws IOException        if the kerberos login fails
   * @return UserGroupInformation.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,loginFromKeytab,org.apache.hadoop.security.KDiag:loginFromKeytab(),628,657,"/**
* Masks user identity using Kerberos or current user.
* @throws IOException if login fails
*/","* Log in from a keytab, dump the UGI, validate it, then try and log in again.
   *
   * That second-time login catches JVM/Hadoop compatibility problems.
   * @throws IOException Keytab loading problems",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromKeytab,"org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytab(java.lang.String,java.lang.String)",1127,1147,"/**
 * Masks a user with specified path.
 * @param user username
 * @param path keytab file path
 * @throws IOException if an I/O error occurs
 */","* Log a user in from a keytab file. Loads a user identity from a keytab
   * file and logs them in. They become the currently logged-in user.
   * @param user the principal name to load from the keytab
   * @param path the path to the keytab file
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if it's a kerberos login exception.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logAllUserInfo,org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.apache.hadoop.security.UserGroupInformation),2017,2020,"/**
 * Logs in using UserGroupInformation.
 * @param ugi user group information object
 */","* Log all (current, real, login) UGI and token info into UGI debug log.
   * @param ugi - UGI
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getActualUgi,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getActualUgi(),1167,1190,"/**
* Retrieves the effective user group information.
* @return UserGroupInformation object representing the current user or login user if conditions are met
* @throws IOException if an I/O error occurs during retrieval
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,buildNegotiateResponse,org.apache.hadoop.ipc.Server:buildNegotiateResponse(java.util.List),3445,3467,"/**
 * Processes authentication methods and builds an RPC SASL protocol message.
 * @param authMethods list of available authentication methods
 * @return RpcSaslProto object representing the negotiation status
 * @throws IOException if an I/O error occurs during processing
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,createSaslServer,org.apache.hadoop.ipc.Server$Connection:createSaslServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod),2621,2626,"/**
* Creates a SASL server for the specified authentication method.
* @param authMethod the authentication method to use
* @return a configured SaslServer instance
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)",366,372,"/**
* Creates a ProtocolProxy instance.
* @param <T> generic type for the protocol
* @param protocol class of the protocol interface
* @param clientVersion version of the client
* @param addr server address
* @param conf configuration settings
* @param connTimeout connection timeout in milliseconds
* @return ProtocolProxy object
* @throws IOException if an I/O error occurs
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param connTimeout time in milliseconds before giving up
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,long)",387,394,"/**
* Masks a function call with specified parameters.
* @param protocol class type of the protocol
* @param clientVersion version of the client
* @param addr address of the socket
* @param conf configuration settings
* @param rpcTimeout timeout for RPC calls
* @param timeout general timeout
* @return result of the masked function call
* @throws IOException if an I/O error occurs
*/","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param rpcTimeout timeout for each RPC
   * @param timeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,java.lang.String)",261,263,"/**
 * Constructs a Resource with a default parser.
 * @param resource the underlying resource object
 * @param name the name of the resource
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",467,473,"/**
* Invokes m1 with given parameters and calls m2 on the result.
* @param <T> generic type for protocol class
* @param protocol class of the protocol to use
* @param clientVersion version of the client
* @param addr address of the socket
* @param conf configuration settings
* @param factory socket factory for creating sockets
* @return result of calling m2 on the object returned by m1
* @throws IOException if an I/O error occurs
*/","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param conf input Configuration.
   * @param factory input factory.
   * @throws IOException raised on errors performing I/O.
   * @return proxy.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",773,780,"/**
* Creates a ProtocolProxy instance.
* @param protocol class type of the protocol
* @param clientVersion version of the client
* @param addr address of the server
* @param conf configuration settings
* @return ProtocolProxy object for communication
* @throws IOException if an I/O error occurs
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   * 
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param conf input configuration.
   * @param <T> Generics Type T.
   * @return a protocol proxy
   * @throws IOException if the thread is interrupted.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,getUgmProtocol,org.apache.hadoop.tools.GetGroupsBase:getUgmProtocol(),97,105,"/**
* Creates and returns a GetUserMappingsProtocol instance.
* @return GetUserMappingsProtocol object for user group mapping
*/","* Get a client of the {@link GetUserMappingsProtocol}.
   * @return A {@link GetUserMappingsProtocol} client proxy.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getZKFCProxy,"org.apache.hadoop.ha.HAServiceTarget:getZKFCProxy(org.apache.hadoop.conf.Configuration,int)",164,173,"/**
* Creates a ZKFCProtocol instance.
* @param conf configuration settings
* @param timeoutMs connection timeout in milliseconds
* @return ZKFCProtocol client side translator
* @throws IOException if an I/O error occurs
*/","* @return a proxy to the ZKFC which is associated with this HA service.
   * @param conf configuration.
   * @param timeoutMs timeout in milliseconds.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxyForAddress,"org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,int,java.net.InetSocketAddress)",146,156,"/**
* Creates an HAServiceProtocol client with specified configuration and parameters.
* @param conf configuration settings for the client
* @param timeoutMs connection timeout in milliseconds
* @param retries number of retry attempts on failure
* @param addr address of the service endpoint
* @return HAServiceProtocol client instance
* @throws IOException if an I/O error occurs
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,isInstrumentationAccessAllowed,"org.apache.hadoop.jmx.JMXJsonServlet:isInstrumentationAccessAllowed(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",152,156,"/**
 * Handles HTTP request by delegating to another method.
 * @param request  incoming HTTP request
 * @param response outgoing HTTP response
 * @return true if processed successfully, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,doGet,"org.apache.hadoop.http.HttpServer2$StackServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1745,1758,"/**
* Handles HTTP request and response.
* @param request incoming HttpServletRequest object
* @param response outgoing HttpServletResponse object
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileOutputServlet.java,doGet,"org.apache.hadoop.http.ProfileOutputServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",48,77,"/**
 * Handles HTTP requests, checks authorization, and manages file access.
 * @param req HttpServletRequest object containing request details
 * @param resp HttpServletResponse object for sending response
 * @throws ServletException if an error occurs during servlet processing
 * @throws IOException if an I/O error occurs
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,doGet,"org.apache.hadoop.http.ProfileServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",187,327,"/**
 * Handles profiling requests, ensuring proper authorization and configuration.
 * @param req HTTP request containing profiling parameters
 * @param resp HTTP response for sending results or errors
 * @throws IOException if an I/O error occurs during processing
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,doGet,"org.apache.hadoop.conf.ConfServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",57,83,"/**
 * Handles request for masked data in XML or JSON format.
 * @param request HTTP request object
 * @param response HTTP response object
 * @throws ServletException if servlet processing fails
 * @throws IOException if an I/O error occurs
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,moveToTrash,org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash(org.apache.hadoop.fs.Path),129,204,"/**
 * Moves a file to the trash.
 * @param path the file path to be moved
 * @return true if successful, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,getCurrentTrashDir,org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(),241,244,"/**
 * Returns a path with current directory.
 * @return Path object representing the current directory
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,getCurrentTrashDir,org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(org.apache.hadoop.fs.Path),246,249,"/**
 * Masks a given file path.
 * @param path original file path
 * @return masked file path
 * @throws IOException if an I/O error occurs
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getTrashRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoot(org.apache.hadoop.fs.Path),1180,1220,"/**
 * Computes the trash root path for a given file system path.
 * @param path the input file system path
 * @return the computed trash root path
 */","* Get the trash root directory for current user when the path
   * specified is deleted.
   *
   * If FORCE_INSIDE_MOUNT_POINT flag is not set, return the default trash root
   * from targetFS.
   *
   * When FORCE_INSIDE_MOUNT_POINT is set to true,
   * <ol>
   *   <li>
   *     If the trash root for path p is in the same mount point as path p,
   *       and one of:
   *       <ol>
   *         <li>The mount point isn't at the top of the target fs.</li>
   *         <li>The resolved path of path is root (in fallback FS).</li>
   *         <li>The trash isn't in user's target fs home directory
   *            get the corresponding viewFS path for the trash root and return
   *            it.
   *         </li>
   *       </ol>
   *   </li>
   *   <li>
   *     else, return the trash root under the root of the mount point
   *     (/{mntpoint}/.Trash/{user}).
   *   </li>
   * </ol>
   *
   * These conditions handle several different important cases:
   * <ul>
   *   <li>File systems may need to have more local trash roots, such as
   *         encryption zones or snapshot roots.</li>
   *   <li>The fallback mount should use the user's home directory.</li>
   *   <li>Cloud storage systems should not use trash in an implicity defined
   *        home directory, per a container, unless it is the fallback fs.</li>
   * </ul>
   *
   * @param path the trash root of the path to be determined.
   * @return the trash root path.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getTrashRoot,org.apache.hadoop.fs.FilterFileSystem:getTrashRoot(org.apache.hadoop.fs.Path),689,692,"/**
 * Delegates file system operation to underlying implementation.
 * @param path file path to operate on
 * @return result of the file system operation
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,run,org.apache.hadoop.fs.TrashPolicyDefault$Emptier:run(),278,320,"/**
* Periodically empties the trash directory.
* @param emptierInterval interval between emptying operations in milliseconds
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(java.util.Date),212,220,"/**
* Creates a checkpoint for each trash root.
* @param date the date to use for checkpoint creation
* @throws IOException if an I/O error occurs
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(boolean),232,239,"/**
* Deletes checkpoint files from trash roots.
* @param deleteImmediately if true, deletes immediately; otherwise, schedules deletion
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getTrashRoots,org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoots(boolean),1231,1297,"/**
* Retrieves file statuses from trash across multiple filesystems.
* @param allUsers flag to include files from all users
* @return collection of FileStatus objects representing trash contents
*/","* Get all the trash roots for current user or all users.
   *
   * When FORCE_INSIDE_MOUNT_POINT is set to true, we also return trash roots
   * under the root of each mount point, with their viewFS paths.
   *
   * @param allUsers return trash roots for all users if true.
   * @return all Trash root directories.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getTrashRoots,org.apache.hadoop.fs.FilterFileSystem:getTrashRoots(boolean),694,697,"/**
 * Retrieves file statuses.
 * @param allUsers flag to include all users' files
 * @return collection of FileStatus objects
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,testAccess,"org.apache.hadoop.fs.shell.Test:testAccess(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.permission.FsAction)",110,118,"/**
* Checks if an action is permitted on a file system item.
* @param item PathData object representing the file or directory
* @param action FsAction specifying the operation to check
* @return true if allowed, false otherwise
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,access,"org.apache.hadoop.fs.viewfs.ViewFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",576,582,"/**
* Applies file access control to a specified path.
* @param path the file system path
* @param mode the file access permissions
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if the path does not exist
* @throws IOException for other I/O errors
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,access,"org.apache.hadoop.fs.FilterFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",470,474,"/**
* Checks access to a file system path.
* @param path file system path
* @param mode required access permissions
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,access,"org.apache.hadoop.fs.viewfs.ChRootedFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",208,211,"/**
* Applies file access control to a specified path.
* @param path file or directory path
* @param mode access permissions
* @throws AccessControlException if access is denied
* @throws FileNotFoundException if the file does not exist
* @throws UnresolvedLinkException if a symlink cannot be resolved
* @throws IOException for other I/O errors
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,access,"org.apache.hadoop.fs.viewfs.ViewFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",428,434,"/**
 * Sets file permissions.
 * @param path file path
 * @param mode access permissions
 * @throws AccessControlException if permission denied
 * @throws FileNotFoundException if file not found
 * @throws UnresolvedLinkException if unresolved link
 * @throws IOException for other I/O errors
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,access,"org.apache.hadoop.fs.FilterFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",132,137,"/**
 * Applies file system action to given path.
 * @param path the target file or directory path
 * @param mode the file access permissions
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if file does not exist
 * @throws UnresolvedLinkException if symbolic link cannot be resolved
 * @throws IOException for other I/O errors
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(org.apache.hadoop.conf.Configuration),403,405,"/**
 * Constructs a ViewFileSystem with default URI and configuration.
 * @param conf configuration settings for file system
 * @throws IOException if an I/O error occurs during initialization
 */","* Convenience Constructor for apps to call directly.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,addFileSystemForTesting,"org.apache.hadoop.fs.FileSystem:addFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",240,244,"/**
* Caches filesystem for given URI and configuration.
* @param uri target URI
* @param conf configuration settings
* @param fs FileSystem object to cache
*/","* This method adds a FileSystem instance to the cache so that it can
   * be retrieved later. It is only for testing.
   * @param uri the uri to store it under
   * @param conf the configuration to store it under
   * @param fs the FileSystem to store
   * @throws IOException if the current user cannot be determined.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,removeFileSystemForTesting,"org.apache.hadoop.fs.FileSystem:removeFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",246,250,"/**
* Maps URI to FileSystem in cache.
* @param uri target URI
* @param conf configuration settings
* @param fs FileSystem instance
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem$Cache:get(java.net.URI,org.apache.hadoop.conf.Configuration)",3665,3668,"/**
 * Masks file system operations.
 * @param uri target URI for file system operation
 * @param conf configuration settings
 * @return FileSystem object with masked operations
 * @throws IOException if an I/O error occurs
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,"org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",594,611,"/**
* Retrieves FileSystem instance based on URI and configuration.
* @param uri target URI for file system access
* @param config configuration settings for the file system
* @return FileSystem object or throws IOException if failed
*/","* Returns the FileSystem for this URI's scheme and authority.
   * The entire URI is passed to the FileSystem instance's initialize method.
   * This always returns a new FileSystem object.
   * @param uri FS URI
   * @param config configuration to use
   * @return the new FS instance
   * @throws IOException FS creation or initialization failure.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,"org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)",373,376,"/**
* Creates and returns a FileContext instance.
* @param defFS default file system implementation
* @param aConf configuration settings
* @return FileContext object initialized with the provided filesystem and configuration
*/","* Create a FileContext with specified FS as default using the specified
   * config.
   * 
   * @param defFS default fs.
   * @param aConf configutration.
   * @return new FileContext with specified FS as default.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",230,235,"/**
* Creates an HTTP connection using a URL and authentication token.
* @param url the target URL for the connection
* @param token the authentication token
* @return HttpURLConnection instance
* @throws IOException if an I/O error occurs
* @throws AuthenticationException if authentication fails
*/","* Returns an authenticated {@link HttpURLConnection}, it uses a Delegation
   * Token only if the given auth token is an instance of {@link Token} and
   * it contains a Delegation Token, otherwise use the configured
   * {@link DelegationTokenAuthenticator} to authenticate the connection.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,execute,org.apache.hadoop.security.KDiag:execute(),282,420,"/**
 * Performs Kerberos diagnostics and logs configuration details.
 * @return true if diagnostics complete successfully
 * @throws Exception if any error occurs during diagnostics
 */","* Execute diagnostics.
   * <p>
   * Things it would be nice if UGI made accessible
   * <ol>
   *   <li>A way to enable JAAS debug programatically</li>
   *   <li>Access to the TGT</li>
   * </ol>
   * @return true if security was enabled and all probes were successful
   * @throws KerberosDiagsFailure explicitly raised failure
   * @throws Exception other security problems",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,maybeDoLoginFromKeytabAndPrincipal,org.apache.hadoop.security.token.DtUtilShell:maybeDoLoginFromKeytabAndPrincipal(java.lang.String[]),82,106,"/**
* Processes arguments for Kerberos authentication.
* @param args input arguments array
* @return processed arguments or original if no changes
* @throws IOException on processing errors
*/","* Parse arguments looking for Kerberos keytab/principal.
   * If both are found: remove both from the argument list and attempt login.
   * If only one of the two is found: remove it from argument list, log warning
   * and do not attempt login.
   * If neither is found: return original args array, doing nothing.
   * Return the pruned args array if either flag is present.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,main,org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[]),2300,2318,"/**
 * Masks user information and handles keytab authentication.
 * @param args command-line arguments (username and keytab path if provided)
 */","* A test method to print out the current user's UGI.
   * @param args if there are two arguments, read the user from the keytab
   * and print it out.
   * @throws Exception Exception.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,login,"org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)",309,329,"/**
* Masks user credentials in secure mode.
* @param conf Configuration object
* @param keytabFileKey Key for the keytab file path
* @param userNameKey Key for the username
* @param hostname Hostname to generate principal name
* @throws IOException if keytab file is missing or invalid
*/","* Login as a principal specified in config. Substitute $host in user's Kerberos principal 
   * name with hostname. If non-secure mode - return. If no keytab available -
   * bail out with an exception
   * 
   * @param conf
   *          conf to use
   * @param keytabFileKey
   *          the key to look for keytab file in conf
   * @param userNameKey
   *          the key to look for user's Kerberos principal name in conf
   * @param hostname
   *          hostname to use for substitution
   * @throws IOException if the config doesn't specify a keytab",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createConnection,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createConnection(java.net.URL,java.lang.String)",502,537,"/**
 * Establishes an authenticated HTTP connection.
 * @param url target URL for the connection
 * @param method HTTP method (e.g., GET, POST)
 * @return configured HttpURLConnection object
 * @throws IOException if connection fails or is timed out
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDelegationToken(java.lang.String),1025,1059,"/**
 * Retrieves a delegation token for the specified renewer.
 * @param renewer user allowed to renew the token
 * @return Token object or throws IOException if fails
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,renewDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token),1061,1087,"/**
* Renews a delegation token using an authenticated URL.
* @param dToken the delegation token to renew
* @return the renewal status as a long value
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,cancelDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token),1089,1116,"/**
* Cancels a delegation token.
* @param dToken the token to cancel
* @throws IOException if an I/O error occurs
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)",3307,3405,"/**
* Initializes a Server instance with specified configurations and parameters.
* @param bindAddress address to bind the server
* @param port port number for the server
* @param rpcRequestClass class of RPC request
* @param handlerCount number of handler threads
* @param numReaders number of reader threads
* @param queueSizePerHandler max queue size per handler
* @param conf configuration settings
* @param serverName name of the server
* @param secretManager secret manager for authentication
* @param portRangeConfig port range configuration
* @throws IOException if an I/O error occurs during initialization
*/","* Constructs a server listening on the named port and address.  Parameters passed must
   * be of the named class.  The <code>handlerCount</code> determines
   * the number of handler threads that will be used to process calls.
   * If queueSizePerHandler or numReaders are not -1 they will be used instead of parameters
   * from configuration. Otherwise the configuration will be picked up.
   * 
   * If rpcRequestClass is null then the rpcRequestClass must have been 
   * registered via {@link #registerProtocolEngine(RPC.RpcKind,
   *  Class, RPC.RpcInvoker)}
   * This parameter has been retained for compatibility with existing tests
   * and usage.
   *
   * @param bindAddress input bindAddress.
   * @param port input port.
   * @param rpcRequestClass input rpcRequestClass.
   * @param handlerCount input handlerCount.
   * @param numReaders input numReaders.
   * @param queueSizePerHandler input queueSizePerHandler.
   * @param conf input Configuration.
   * @param serverName input serverName.
   * @param secretManager input secretManager.
   * @param portRangeConfig input portRangeConfig.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,buildSaslNegotiateResponse,org.apache.hadoop.ipc.Server$Connection:buildSaslNegotiateResponse(),2603,2619,"/**
* Handles SASL negotiation for RPC.
* @return RpcSaslProto with negotiated challenge or response
* @throws InterruptedException, SaslException, IOException on failure
*/","* Process the Sasl's Negotiate request, including the optimization of 
     * accelerating token negotiation.
     * @return the response to Negotiate request - the list of enabled 
     *         authMethods and challenge if the TOKENS are supported. 
     * @throws SaslException - if attempt to generate challenge fails.
     * @throws IOException - if it fails to create the SASL server for Tokens",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",326,332,"/**
* Creates a ProtocolProxy instance with default timeout.
* @param <T> the type of protocol
* @param protocol Class representing the protocol
* @param clientVersion version of the client
* @param addr address for the proxy
* @param conf configuration settings
* @return ProtocolProxy instance
* @throws IOException if an I/O error occurs
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)",346,351,"/**
* Masks a function call with specified parameters.
* @param <T> type of the protocol class
* @param protocol class representing the protocol
* @param clientVersion version of the client
* @param addr address of the socket
* @param conf configuration settings
* @param connTimeout connection timeout in milliseconds
* @return result of the masked function call
* @throws IOException if an I/O error occurs
*/","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param connTimeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String)",998,1000,"/**
* Masks input stream resource.
* @param in input stream to be masked
* @param name name of the resource
*/","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param in InputStream to deserialize the object from.
   * @param name the name of the resource because InputStream.toString is not
   * very descriptive some times.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object),253,255,"/**
 * Constructs a Resource with an object and its string representation.
 * @param resource the underlying resource object
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",728,734,"/**
* Invokes m1 with given parameters and calls m2 on the result.
* @param <T> type of the protocol class
* @param protocol Protocol class to use
* @param clientVersion Version of the client
* @param addr Address of the socket
* @param conf Configuration settings
* @return Result of calling m2 on the object returned by m1
* @throws IOException if an I/O error occurs
*/","* Construct a client-side proxy object with the default SocketFactory.
    *
    * @param <T> Generics Type T.
    * @param protocol input protocol.
    * @param clientVersion input clientVersion.
    * @param addr input addr.
    * @param conf input Configuration.
    * @return a proxy instance
    * @throws IOException  if the thread is interrupted.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,run,org.apache.hadoop.tools.GetGroupsBase:run(java.lang.String[]),62,79,"/**
* Masks user groups from input arguments.
* @param args usernames to process, defaults to current user if empty
* @return 0 indicating successful execution
*/","* Get the groups for the users given and print formatted output to the
   * {@link PrintStream} configured earlier.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,gracefulFailoverThroughZKFCs,org.apache.hadoop.ha.HAAdmin:gracefulFailoverThroughZKFCs(org.apache.hadoop.ha.HAServiceTarget),276,290,"/**
* Initiates failover to a specified node.
* @param toNode target HAServiceTarget for failover
* @return 0 if successful, -1 if failover fails
*/","* Initiate a graceful failover by talking to the target node's ZKFC.
   * This sends an RPC to the ZKFC, which coordinates the failover.
   *
   * @param toNode the node to fail to
   * @return status code (0 for success)
   * @throws IOException if failover does not succeed",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,cedeRemoteActive,"org.apache.hadoop.ha.ZKFailoverController:cedeRemoteActive(org.apache.hadoop.ha.HAServiceTarget,int)",749,756,"/**
* Requests a remote service to relinquish its active status temporarily.
* @param remote HAServiceTarget instance representing the remote service
* @param timeout duration in milliseconds for which the service should step down
* @return ZKFCProtocol instance of the old active service
* @throws IOException if communication with the remote service fails
*/","* Ask the remote zkfc to cede its active status and wait for the specified
   * timeout before attempting to claim leader status.
   * @param remote node to ask
   * @param timeout amount of time to cede
   * @return the {@link ZKFCProtocol} used to talk to the ndoe
   * @throws IOException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getHealthMonitorProxy,"org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int,int)",131,138,"/**
* Initializes HA service protocol.
* @param conf configuration settings
* @param timeoutMs operation timeout in milliseconds
* @param retries number of retry attempts
* @return HAServiceProtocol instance
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxyForAddress,"org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,java.net.InetSocketAddress)",140,144,"/**
* Establishes connection to service.
* @param conf configuration settings
* @param timeoutMs timeout in milliseconds
* @param addr address of the service
* @return HAServiceProtocol instance
* @throws IOException if connection fails
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,doGet,"org.apache.hadoop.jmx.JMXJsonServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",175,232,"/**
* Handles JMX requests, processes query parameters, and returns JSON responses.
* @param request HTTP servlet request containing query parameters
* @param response HTTP servlet response for sending JSON data
*/","* Process a GET request for the specified resource.
   * 
   * @param request
   *          The servlet request we are processing
   * @param response
   *          The servlet response we are creating",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(),206,210,"/**
 * Calls overloaded method m1 with current date.
 * @throws IOException if an I/O error occurs
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(),222,225,"/**
 * Calls overloaded method with false flag.
 * @throws IOException if an I/O error occurs
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpointsImmediately,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpointsImmediately(),227,230,"/**
 * Calls m1 with true to mask functionality.
 * @throws IOException if an I/O error occurs
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,processPath,org.apache.hadoop.fs.shell.Test:processPath(org.apache.hadoop.fs.shell.PathData),77,108,"/**
* Applies a mask to PathData based on flag.
* @param item PathData object to be masked
* @throws IOException if an I/O error occurs
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,access,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",253,257,"/**
 * Calls superclass method with modified path and given file action.
 * @param path original file path
 * @param mode file access permissions
 * @throws AccessControlException if access is denied
 * @throws FileNotFoundException if file does not exist
 * @throws IOException for other I/O errors
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)",536,558,"/**
* Creates a FileSystem instance from URI and configuration.
* @param uri file system URI
* @param conf configuration settings
* @return FileSystem object
* @throws IOException if an I/O error occurs
*/","* Get a FileSystem for this URI's scheme and authority.
   * <ol>
   * <li>
   *   If the configuration has the property
   *   {@code ""fs.$SCHEME.impl.disable.cache""} set to true,
   *   a new instance will be created, initialized with the supplied URI and
   *   configuration, then returned without being cached.
   * </li>
   * <li>
   *   If the there is a cached FS instance matching the same URI, it will
   *   be returned.
   * </li>
   * <li>
   *   Otherwise: a new FS instance will be created, initialized with the
   *   configuration and URI, cached and returned to the caller.
   * </li>
   * </ol>
   * @param uri uri of the filesystem.
   * @param conf configrution.
   * @return filesystem instance.
   * @throws IOException if the FileSystem cannot be instantiated.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstanceLocal,org.apache.hadoop.fs.FileSystem:newInstanceLocal(org.apache.hadoop.conf.Configuration),631,634,"/**
 * Retrieves the local file system instance.
 * @param conf configuration settings
 * @return LocalFileSystem object
 * @throws IOException if an I/O error occurs
 */","* Get a unique local FileSystem object.
   * @param conf the configuration to configure the FileSystem with
   * @return a new LocalFileSystem object.
   * @throws IOException FS creation or initialization failure.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/FsGetter.java,getNewInstance,"org.apache.hadoop.fs.viewfs.FsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",42,45,"/**
 * Creates and returns a FileSystem instance.
 * @param uri file system URI
 * @param conf configuration settings
 * @return FileSystem object
 * @throws IOException if an I/O error occurs
 */","* Gets new file system instance of given uri.
   * @param uri uri.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return file system.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getNewInstance,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",234,250,"/**
 * Initializes file system based on URI scheme.
 * @param uri target URI for the file system
 * @param conf configuration settings
 * @return initialized FileSystem object
 * @throws IOException if an I/O error occurs
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatusForFallbackLink,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatusForFallbackLink(),1244,1265,"/**
 * Retrieves file statuses from a linked filesystem.
 * @return Array of FileStatus objects or empty array if none found
 * @throws IOException if an I/O error occurs
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem),385,388,"/**
 * Creates a FileContext using the provided FileSystem and default configuration.
 * @param defaultFS the default FileSystem to use
 * @return a FileContext instance
 */","* Create a FileContext for specified file system using the default config.
   * 
   * @param defaultFS default fs.
   * @return a FileContext with the specified AbstractFileSystem
   *                 as the default FS.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,"org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI,org.apache.hadoop.conf.Configuration)",456,473,"/**
 * Creates FileContext using default file system URI and configuration.
 * @param defaultFsUri URI of the default file system
 * @param aConf Hadoop configuration
 * @return FileContext object
 * @throws UnsupportedFileSystemException if file system is unsupported
 */","* Create a FileContext for specified default URI using the specified config.
   * 
   * @param defaultFsUri defaultFsUri.
   * @param aConf configrution.
   * @return new FileContext for specified uri
   * @throws UnsupportedFileSystemException If the file system with specified is
   *           not supported
   * @throws RuntimeException If the file system specified is supported but
   *         could not be instantiated, or if login fails.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,run,org.apache.hadoop.security.KDiag:run(java.lang.String[]),197,244,"/**
* Parses and processes command-line arguments for configuration.
* @param argv array of command-line arguments
* @return exit code, 0 if successful, -1 if errors occur
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,init,org.apache.hadoop.security.token.DtUtilShell:init(java.lang.String[]),116,177,"/**
* Parses command-line arguments for token file operations.
* @param args command-line arguments array
* @return exit code: 0 on success, 1 on error
*/","* Parse the command line arguments and initialize subcommand.
   * Also will attempt to perform Kerberos login if both -principal and -keytab
   * flags are passed in args array.
   * @param args args.
   * @return 0 if the argument(s) were recognized, 1 otherwise
   * @throws Exception Exception.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,login,"org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",287,292,"/**
* Calls another method with configuration and keys.
* @param conf configuration object
* @param keytabFileKey key for the keytab file path
* @param userNameKey key for the user name
*/","* Login as a principal specified in config. Substitute $host in
   * user's Kerberos principal name with a dynamically looked-up fully-qualified
   * domain name of the current host.
   * 
   * @param conf
   *          conf to use
   * @param keytabFileKey
   *          the key to look for keytab file in conf
   * @param userNameKey
   *          the key to look for user's Kerberos principal name in conf
   * @throws IOException if login fails",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,call,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class,int)",544,608,"/**
* Executes HTTP request and processes JSON response.
* @param conn HttpURLConnection instance
* @param jsonOutput JSON data to send
* @param expectedResponse expected HTTP response code
* @param klass class type for deserialization
* @param authRetryCount number of authentication retries
* @return deserialized object or null if not applicable
* @throws IOException on network error
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,<init>,"org.apache.hadoop.ipc.RPC$Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)",1189,1198,"/**
* Initializes a new Server instance.
* @param bindAddress address to bind the server
* @param port port number for the server
* @param paramClass class type of writable objects
* @param handlerCount number of handlers
* @param numReaders number of readers
* @param queueSizePerHandler size of queue per handler
* @param conf configuration settings
* @param serverName name of the server
* @param secretManager secret manager for authentication
* @param portRangeConfig port range configuration
* @throws IOException if an I/O error occurs during initialization
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,org.apache.hadoop.conf.Configuration)",3264,3271,"/**
* Constructs a Server instance.
* @param bindAddress address to bind the server
* @param port port number for the server
* @param paramClass class type for parameter handling
* @param handlerCount number of handlers
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager)",3273,3280,"/**
* Initializes a new Server instance with specified parameters.
* @param bindAddress address to bind the server
* @param port port number for the server
* @param rpcRequestClass class of RPC request
* @param handlerCount number of handlers for processing requests
* @param numReaders number of reader threads
* @param queueSizePerHandler size of the queue per handler
* @param conf configuration settings
* @param serverName name of the server
* @param secretManager secret manager for security tokens
* @throws IOException if there is an I/O error during initialization
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processSaslMessage,org.apache.hadoop.ipc.Server$Connection:processSaslMessage(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2327,2385,"/**
 * Processes SASL messages and returns a response.
 * @param saslMessage incoming SASL message
 * @return processed SASL response or null
 * @throws SaslException, IOException, AccessControlException, InterruptedException on error
 */","* Process a saslMessge.
     * @param saslMessage received SASL message
     * @return the sasl response to send back to client
     * @throws SaslException if authentication or generating response fails, 
     *                       or SASL protocol mixup
     * @throws IOException if a SaslServer cannot be created
     * @throws AccessControlException if the requested authentication type 
     *         is not supported or trying to re-attempt negotiation.
     * @throws InterruptedException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",305,312,"/**
* Masks and returns an object of specified protocol type.
* @param protocol Class type of the protocol
* @param clientVersion version of the client
* @param addr InetSocketAddress for communication
* @param conf Configuration settings
* @return Object of type T or throws IOException
*/","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.lang.String),923,925,"/**
 * Masks a resource by name.
 * @param name the name of the resource to mask
 */","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param name resource to be added, the classpath is examined for a file 
   *             with that name.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.net.URL),941,943,"/**
 * Masks a URL resource.
 * @param url the URL to be masked
 */","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param url url of the resource to be added, the local filesystem is 
   *            examined directly to find the resource, without referring to 
   *            the classpath.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path),959,961,"/**
 * Masks a file by applying a resource transformation.
 * @param file the path to the file to be masked
 */","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param file file-path of resource to be added, the local filesystem is
   *             examined directly to find the resource, without referring to 
   *             the classpath.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream),980,982,"/**
 * Processes input stream through resource handling.
 * @param in input stream to be processed
 */","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * WARNING: The contents of the InputStream will be cached, by this method. 
   * So use this sparingly because it does increase the memory consumption.
   * 
   * @param in InputStream to deserialize the object from. In will be read from
   * when a get or set is called next.  After it is read the stream will be
   * closed.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",66,72,"/**
* Initializes the HAServiceProtocol client.
* @param addr server address
* @param conf configuration settings
* @throws IOException if connection fails
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doGracefulFailover,org.apache.hadoop.ha.ZKFailoverController:doGracefulFailover(),660,739,"/**
* Initiates a failover process to make the local node active.
* Throws exceptions if failover fails or service becomes unhealthy.
*/","* Coordinate a graceful failover. This proceeds in several phases:
   * 1) Pre-flight checks: ensure that the local node is healthy, and
   * thus a candidate for failover.
   * 2a) Determine the current active node. If it is the local node, no
   * need to failover - return success.
   * 2b) Get the other nodes
   * 3a) Ask the other nodes to yield from election for a number of seconds
   * 3b) Ask the active node to yield from the election for a number of seconds.
   * 4) Allow the normal election path to run in other threads. Wait until
   * we either become unhealthy or we see an election attempt recorded by
   * the normal code path.
   * 5) Allow the old active to rejoin the election, so a future
   * failback is possible.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,createProxy,org.apache.hadoop.ha.HealthMonitor:createProxy(),191,193,"/**
 * Retrieves service protocol from the target monitor.
 * @return HAServiceProtocol instance
 * @throws IOException if communication fails
 */","* Connect to the service to be monitored. Stubbed out for easier testing.
   *
   * @throws IOException raised on errors performing I/O.
   * @return HAServiceProtocol.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getHealthMonitorProxy,"org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int)",126,129,"/**
* Initializes HAServiceProtocol with configuration and timeout.
* @param conf configuration settings
* @param timeoutMs timeout in milliseconds
* @return HAServiceProtocol instance
*/","* Returns a proxy to connect to the target HA service for health monitoring.
   * If {@link #getHealthMonitorAddress()} is implemented to return a non-null
   * address, then this proxy will connect to that address.  Otherwise, the
   * returned proxy defaults to using {@link #getAddress()}, which means this
   * method's behavior is identical to {@link #getProxy(Configuration, int)}.
   *
   * @param conf configuration.
   * @param timeoutMs timeout in milliseconds
   * @return a proxy to connect to the target HA service for health monitoring
   * @throws IOException if there is an error",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxy,"org.apache.hadoop.ha.HAServiceTarget:getProxy(org.apache.hadoop.conf.Configuration,int)",100,103,"/**
* Initializes and returns HA service protocol.
* @param conf configuration settings
* @param timeoutMs operation timeout in milliseconds
* @return HAServiceProtocol instance
* @throws IOException if initialization fails
*/","* @return a proxy to connect to the target HA Service.
   * @param timeoutMs timeout in milliseconds.
   * @param conf Configuration.
   * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,initialize,"org.apache.hadoop.fs.HarFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",128,175,"/**
* Initializes and configures the Har Filesystem.
* @param name URI of the filesystem
* @param conf configuration settings
* @throws IOException if initialization fails
*/","* Initialize a Har filesystem per har archive. The 
   * archive home directory is the top level directory
   * in the filesystem that contains the HAR archive.
   * Be careful with this method, you do not want to go 
   * on creating new Filesystem instances per call to 
   * path.getFileSystem().
   * the uri of Har is 
   * har://underlyingfsscheme-host:port/archivepath.
   * or 
   * har:///archivepath. This assumes the underlying filesystem
   * to be used in case not specified.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,moveToAppropriateTrash,"org.apache.hadoop.fs.Trash:moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",77,122,"/**
 * Moves a file to the trash.
 * @param fs FileSystem instance
 * @param p Path of the file to be moved
 * @param conf Configuration settings
 * @return true if successfully moved to trash, false otherwise
 */","* In case of the symlinks or mount points, one has to move the appropriate
   * trashbin in the actual volume of the path p being deleted.
   *
   * Hence we get the file system of the fully-qualified resolved-path and
   * then move the path p to the trashbin in that volume,
   * @param fs - the filesystem of path p
   * @param p - the path being deleted - to be moved to trash
   * @param conf - configuration
   * @return false if the item is already in the trash or trash is disabled
   * @throws IOException on error",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,connect,org.apache.hadoop.fs.FsUrlConnection:connect(),55,75,"/**
 * Establishes a connection to the specified URL.
 * @throws IOException if an I/O error occurs or if already connected
 */",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",88,90,"/**
* Initializes PathData with given path and configuration.
* @param pathString file system path as a string
* @param conf Hadoop configuration object
* @throws IOException if an I/O error occurs
*/","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it
   * @param pathString a string for a path
   * @param conf the configuration file
   * @throws IOException if anything goes wrong...",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFSofPath,"org.apache.hadoop.fs.FileSystem:getFSofPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",427,435,"/**
 * Masks a file system path.
 * @param absOrFqPath absolute or fully qualified path to mask
 * @param conf configuration settings for the operation
 * @return FileSystem object after masking operations
 * @throws UnsupportedFileSystemException if file system is unsupported
 * @throws IOException if an I/O error occurs during operations
 */",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getNamed,"org.apache.hadoop.fs.FileSystem:getNamed(java.lang.String,org.apache.hadoop.conf.Configuration)",477,481,"/**
* Creates a FileSystem instance with specified URI and configuration.
* @param name file system name
* @param conf configuration settings
* @return FileSystem object
* @deprecated Use alternative method instead
*/","* @deprecated call {@link #get(URI, Configuration)} instead.
   *
   * @param name name.
   * @param conf configuration.
   * @return file system.
   * @throws IOException If an I/O error occurred.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLocal,org.apache.hadoop.fs.FileSystem:getLocal(org.apache.hadoop.conf.Configuration),508,511,"/**
 * Retrieves local file system instance.
 * @param conf configuration settings
 * @return LocalFileSystem object
 * @throws IOException if an I/O error occurs
 */","* Get the local FileSystem.
   * @param conf the configuration to configure the FileSystem with
   * if it is newly instantiated.
   * @return a LocalFileSystem
   * @throws IOException if somehow the local FS cannot be instantiated.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",124,127,"/**
 * Constructs a ChRootedFileSystem with a given URI and configuration.
 * @param uri file system URI
 * @param conf Hadoop configuration
 * @throws IOException if an I/O error occurs
 */","* Constructor.
   * @param uri base file system
   * @param conf configuration
   * @throws IOException",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/FsGetter.java,get,"org.apache.hadoop.fs.viewfs.FsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)",55,57,"/**
 * Creates a FileSystem instance for the given URI and configuration.
 * @param uri the uniform resource identifier
 * @param conf the configuration settings
 * @return a FileSystem object
 * @throws IOException if an I/O error occurs
 */","* Gets file system instance of given uri.
   *
   * @param uri uri.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return FileSystem.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,get,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)",258,275,"/**
 * Initializes file system based on URI scheme.
 * @param uri target URI for the file system
 * @param conf configuration settings
 * @return initialized FileSystem object
 * @throws IOException if an I/O error occurs
 */","* When ViewFileSystemOverloadScheme scheme and target uri scheme are
     * matching, it will not take advantage of FileSystem cache as it will
     * create instance directly. For caching needs please set
     * ""fs.viewfs.enable.inner.cache"" to true.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getFileSystem,org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration),365,367,"/**
 * Creates a FileSystem instance.
 * @param conf configuration settings
 * @return FileSystem object
 * @throws IOException if an I/O error occurs
 */","* Return the FileSystem that owns this Path.
   *
   * @param conf the configuration to use when resolving the FileSystem
   * @return the FileSystem that owns this Path
   * @throws java.io.IOException thrown if there's an issue resolving the
   * FileSystem",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getFileSystem,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getFileSystem(),458,476,"/**
 * Retrieves the FileSystem instance.
 * @return FileSystem object, either supplied or created from base path
 * @throws MetricsException if URI is invalid or connection fails
 */","* Return the supplied file system for testing or otherwise get a new file
   * system.
   *
   * @return the file system to use
   * @throws MetricsException thrown if the file system could not be retrieved",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,get,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:get(java.net.URI,org.apache.hadoop.conf.Configuration)",129,153,"/**
 * Retrieves or creates a FileSystem for the given URI.
 * @param uri file system URI
 * @param config configuration settings
 * @return FileSystem object
 * @throws IOException if an I/O error occurs
 */",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path),1159,1226,"/**
 * Retrieves file statuses for a given path.
 * @param f the input path
 * @return array of FileStatus objects
 * @throws IOException if an I/O error occurs
 */","* {@inheritDoc}
     *
     * Note: listStatus on root(""/"") considers listing from fallbackLink if
     * available. If the same directory name is present in configured mount
     * path as well as in fallback link, then only the configured mount path
     * will be listed in the returned result.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI),440,443,"/**
 * Creates a FileContext using the specified default file system URI and default configuration.
 * @param defaultFsUri URI of the default file system
 * @return FileContext instance
 * @throws UnsupportedFileSystemException if the file system is not supported
 */","* Create a FileContext for specified URI using the default config.
   * 
   * @param defaultFsUri defaultFsUri.
   * @return a FileContext with the specified URI as the default FS.
   * 
   * @throws UnsupportedFileSystemException If the file system for
   *           <code>defaultFsUri</code> is not supported",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.conf.Configuration),485,496,"/**
* Creates FileContext from Configuration.
* @param aConf configuration object
* @return FileContext instance
* @throws UnsupportedFileSystemException if URI has no valid scheme
*/","* Create a FileContext using the passed config. Generally it is better to use
   * {@link #getFileContext(URI, Configuration)} instead of this one.
   * 
   * 
   * @param aConf configration.
   * @return new FileContext
   * @throws UnsupportedFileSystemException If file system in the config
   *           is not supported",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLocalFSFileContext,org.apache.hadoop.fs.FileContext:getLocalFSFileContext(org.apache.hadoop.conf.Configuration),506,509,"/**
 * Creates a FileContext for the local file system.
 * @param aConf configuration settings
 * @return FileContext object for local file system
 * @throws UnsupportedFileSystemException if local file system is not supported
 */","* @param aConf - from which the FileContext is configured
   * @return a FileContext for the local file system using the specified config.
   * 
   * @throws UnsupportedFileSystemException If default file system in the config
   *           is not supported
   *",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,init,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:init(org.apache.commons.configuration2.SubsetConfiguration),233,265,"/**
* Configures the system with metrics settings.
* @param metrics2Properties configuration properties for metrics
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,call,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class)",539,542,"/**
* Makes an HTTP request with JSON output and retries on auth failure.
* @param <T> response type
* @param conn HttpURLConnection instance
* @param jsonOutput JSON data to send
* @param expectedResponse expected HTTP response code
* @param klass class of the response object
* @return deserialized response object
* @throws IOException if request fails
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",479,493,"/**
* Constructs a new Server instance.
* @param protocolClass the RPC protocol class
* @param protocolImpl the implementation of the protocol
* @param conf configuration settings
* @param bindAddress address to bind the server to
* @param port port number for the server
* @param numHandlers number of handler threads
* @param numReaders number of reader threads
* @param queueSizePerHandler size of the request queue per handler
* @param verbose enable verbose logging
* @param secretManager secret manager for security tokens
* @param portRangeConfig configuration for port range
* @param alignmentContext context for data alignment
* @throws IOException if an I/O error occurs during server setup
*/","* Construct an RPC server.
     *
     * @param protocolClass the class of protocol
     * @param protocolImpl the protocolImpl whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param numReaders number of read threads
     * @param queueSizePerHandler the size of the queue contained
     *                            in each Handler
     * @param verbose whether each call should be logged
     * @param secretManager the server-side secret manager for each token type
     * @param portRangeConfig A config parameter that can be used to restrict
     * the range of ports used when port is 0 (an ephemeral port)
     * @param alignmentContext provides server state info on client responses
     * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",495,535,"/**
* Initializes a Server with specified parameters.
* @param protocolClass interface class for the protocol
* @param protocolImpl implementation of the protocol
* @param conf configuration settings
* @param bindAddress address to bind the server
* @param port port number for the server
* @param numHandlers number of handler threads
* @param numReaders number of reader threads
* @param queueSizePerHandler size of the queue per handler
* @param verbose enables verbose logging
* @param secretManager secret manager for security tokens
* @param portRangeConfig configuration for port range
* @param alignmentContext context for alignment settings
* @throws IOException if initialization fails
*/","* Construct an RPC server.
     * @param protocolClass - the protocol being registered
     *     can be null for compatibility with old usage (see below for details)
     * @param protocolImpl the protocol impl that will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param alignmentContext provides server state info on client responses
     * @param numReaders input numReaders.
     * @param portRangeConfig input portRangeConfig.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param secretManager input secretManager.
     * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,saslProcess,org.apache.hadoop.ipc.Server$Connection:saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2242,2314,"/**
 * Handles SASL authentication for incoming RPC messages.
 * @param saslMessage the SASL message from the client
 * @throws RpcServerException if server-side error occurs
 * @throws IOException if I/O error occurs
 * @throws InterruptedException if thread is interrupted
 */","* Process saslMessage and send saslResponse back
     * @param saslMessage received SASL message
     * @throws RpcServerException setup failed due to SASL negotiation
     *         failure, premature or invalid connection context, or other state 
     *         errors. This exception needs to be sent to the client. This 
     *         exception will wrap {@link RetriableException}, 
     *         {@link InvalidToken}, {@link StandbyException} or 
     *         {@link SaslException}.
     * @throws IOException if sending reply fails
     * @throws InterruptedException",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,readSSLConfiguration,"org.apache.hadoop.security.ssl.SSLFactory:readSSLConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.ssl.SSLFactory$Mode)",162,184,"/**
* Adjusts SSL configuration based on mode.
* @param conf original Configuration object
* @param mode operation mode (CLIENT or SERVER)
* @return modified Configuration object with SSL settings
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,refresh,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refresh(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",140,150,"/**
* Applies security policies from a configuration file.
* @param conf main configuration object
* @param provider policy provider instance
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/HCFSMountTableConfigLoader.java,load,"org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:load(java.lang.String,org.apache.hadoop.conf.Configuration)",57,113,"/**
 * Loads the latest version of the mount table configuration.
 * @param mountTableConfigPath path to the mount table configuration directory
 * @param conf Hadoop configuration object
 * @throws IOException if an I/O error occurs
 */","* Loads the mount-table configuration from hadoop compatible file system and
   * add the configuration items to given configuration. Mount-table
   * configuration format should be suffixed with version number.
   * Format: {@literal mount-table.<versionNumber>.xml}
   * Example: mount-table.1.xml
   * When user wants to update mount-table, the expectation is to upload new
   * mount-table configuration file with monotonically increasing integer as
   * version number. This API loads the highest version number file. We can
   * also configure single file path directly.
   *
   * @param mountTableConfigPath : A directory path where mount-table files
   *          stored or a mount-table file path. We recommend to configure
   *          directory with the mount-table version files.
   * @param conf : to add the mount table as resource.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,tryConnect,org.apache.hadoop.ha.HealthMonitor:tryConnect(),170,183,"/**
* Initializes proxy connection, handling exceptions by logging and setting state.
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,isOtherTargetNodeActive,"org.apache.hadoop.ha.HAAdmin:isOtherTargetNodeActive(java.lang.String,boolean)",187,215,"/**
* Activates a target node if not already active.
* @param targetNodeToActivate the node to activate
* @param forceActive flag to force activation on error
* @return true if activation is successful or skipped, false otherwise
*/","* Checks whether other target node is active or not
   * @param targetNodeToActivate
   * @return true if other target node is active or some other exception 
   * occurred and forceActive was set otherwise false
   * @throws IOException",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,transitionToStandby,org.apache.hadoop.ha.HAAdmin:transitionToStandby(org.apache.commons.cli.CommandLine),217,234,"/**
* Transitions a service to standby mode.
* @param cmd command line interface for input arguments
* @return 0 on success, -1 on failure
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkHealth,org.apache.hadoop.ha.HAAdmin:checkHealth(org.apache.commons.cli.CommandLine),292,309,"/**
* Executes health check for a service.
* @param cmd command line interface object
* @return 0 if successful, -1 on error
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,getServiceState,org.apache.hadoop.ha.HAAdmin:getServiceState(org.apache.commons.cli.CommandLine),311,324,"/**
* Masks function for command line.
* @param cmd command line object
* @return 0 on success, -1 on error
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,getAllServiceState,org.apache.hadoop.ha.HAAdmin:getAllServiceState(),443,464,"/**
* Logs service addresses and statuses.
* @return 0 if successful, -1 if failed to get service IDs
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,becomeActive,org.apache.hadoop.ha.ZKFailoverController:becomeActive(),408,443,"/**
* Attempts to activate the local target. Throws exception on failure.
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,becomeStandby,org.apache.hadoop.ha.ZKFailoverController:becomeStandby(),514,529,"/**
* Transitions the local target to standby mode based on ZK election.
* Logs success or failure of the transition.
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doCedeActive,org.apache.hadoop.ha.ZKFailoverController:doCedeActive(int),590,623,"/**
* Cedes active role for a specified duration.
* @param millisToCede milliseconds to delay joining
* @throws AccessControlException if access is denied
* @throws ServiceFailedException if service fails
* @throws IOException if I/O error occurs
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,preFailoverChecks,"org.apache.hadoop.ha.FailoverController:preFailoverChecks(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean)",109,156,"/**
* Initiates a failover from one HAServiceTarget to another.
* @param from the source HAServiceTarget
* @param target the destination HAServiceTarget
* @param forceActive flag to force active state if not ready
* @throws FailoverFailedException if failover fails for any reason
*/","* Perform pre-failover checks on the given service we plan to
   * failover to, eg to prevent failing over to a service (eg due
   * to it being inaccessible, already active, not healthy, etc).
   *
   * An option to ignore toSvc if it claims it is not ready to
   * become active is provided in case performing a failover will
   * allow it to become active, eg because it triggers a log roll
   * so the standby can learn about new blocks and leave safemode.
   *
   * @param from currently active service
   * @param target service to make active
   * @param forceActive ignore toSvc if it reports that it is not ready
   * @throws FailoverFailedException if we should avoid failover",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,tryGracefulFence,org.apache.hadoop.ha.FailoverController:tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget),168,186,"/**
* Attempts to gracefully put a service target into standby mode.
* @param svc the HAServiceTarget to be modified
* @return true if successful, false otherwise
*/","* Try to get the HA state of the node at the given address. This
   * function is guaranteed to be ""quick"" -- ie it has a short timeout
   * and no retries. Its only purpose is to avoid fencing a node that
   * has already restarted.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,moveToTrash,org.apache.hadoop.fs.shell.Delete$Rm:moveToTrash(org.apache.hadoop.fs.shell.PathData),151,167,"/**
* Moves item to trash.
* @param item PathData object representing the file or directory
* @return true if successful, false otherwise
* @throws IOException if an I/O error occurs
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,getInputStream,org.apache.hadoop.fs.FsUrlConnection:getInputStream(),77,83,"/**
 * Returns input stream, initializing if necessary.
 * @return InputStream object
 * @throws IOException if an I/O error occurs
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,recursePath,org.apache.hadoop.fs.shell.find.Find:recursePath(org.apache.hadoop.fs.shell.PathData),345,371,"/**
 * Processes a PathData item, handling links and infinite loops.
 * @param item the PathData to process
 * @throws IOException if an I/O error occurs
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isPathRecursable,org.apache.hadoop.fs.shell.find.Find:isPathRecursable(org.apache.hadoop.fs.shell.PathData),373,392,"/**
* Checks file permissions and linked item status.
* @param item PathData object representing the file
* @return true if conditions are met, otherwise false
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,expandArgument,org.apache.hadoop.fs.shell.Head:expandArgument(java.lang.String),56,61,"/**
* Creates a list containing path data.
* @param arg file path argument
* @return List of PathData objects
* @throws IOException if an I/O error occurs
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,expandArgument,org.apache.hadoop.fs.shell.Tail:expandArgument(java.lang.String),81,86,"/**
* Creates a list with a single PathData item.
* @param arg path string argument
* @return list containing one PathData object
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemLinkResolver.java,resolve,"org.apache.hadoop.fs.FileSystemLinkResolver:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",71,111,"/**
 * Resolves and opens a file path, handling symlinks.
 * @param filesys the FileSystem instance
 * @param path the Path to resolve
 * @return an open file handle or null if not found
 * @throws IOException if an I/O error occurs
 */","* Attempt calling overridden {@link #doCall(Path)} method with
   * specified {@link FileSystem} and {@link Path}. If the call fails with an
   * UnresolvedLinkException, it will try to resolve the path and retry the call
   * by calling {@link #next(FileSystem, Path)}.
   * @param filesys FileSystem with which to try call
   * @param path Path with which to try call
   * @return Generic type determined by implementation
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",100,102,"/**
 * Constructs PathData with local file system and path.
 * @param localPath URI representing the local path
 * @param conf Hadoop configuration
 * @throws IOException if an I/O error occurs
 */","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it
   * @param localPath a local URI
   * @param conf the configuration file
   * @throws IOException if anything goes wrong...",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",2571,2576,"/**
 * Copies files or directories to a destination.
 * @param delSrc true if source files should be deleted after copying
 * @param overwrite true if existing destination files should be overwritten
 * @param srcs array of source paths to copy from
 * @param dst destination path to copy to
 * @throws IOException if an I/O error occurs during the operation
 */","* The src files are on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param overwrite whether to overwrite an existing file
   * @param srcs array of paths which are source
   * @param dst path
   * @throws IOException IO failure",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2588,2593,"/**
* Moves or copies a file from source to destination.
* @param delSrc true if source should be deleted after move
* @param overwrite true if destination can be overwritten
* @param src source file path
* @param dst destination file path
* @throws IOException if an I/O error occurs
*/","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param overwrite whether to overwrite an existing file
   * @param src path
   * @param dst path
   * @throws IOException IO failure",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",2648,2658,"/**
 * Masks a file by moving it to a destination path.
 * @param delSrc true if source should be deleted after masking
 * @param src source file path
 * @param dst destination file path
 * @param useRawLocalFileSystem flag to use raw local file system
 * @throws IOException if an I/O error occurs
 */","* The src file is under this filesystem, and the dst is on the local disk.
   * Copy it from the remote filesystem to the local dst name.
   * delSrc indicates if the src will be removed
   * or not. useRawLocalFileSystem indicates whether to use RawLocalFileSystem
   * as the local file system or not. RawLocalFileSystem is non checksumming,
   * So, It will not create any crc files at local.
   *
   * @param delSrc
   *          whether to delete the src
   * @param src
   *          path
   * @param dst
   *          path
   * @param useRawLocalFileSystem
   *          whether to use RawLocalFileSystem as local file system or not.
   *
   * @throws IOException for any IO error",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,confChanged,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:confChanged(org.apache.hadoop.conf.Configuration),309,360,"/**
* Initializes context with local directories from configuration.
* @param conf Configuration object
* @return Context object initialized with local directories
* @throws IOException if directory configuration is invalid or inaccessible
*/","This method gets called everytime before any read/write to make sure
     * that any change to localDirs is reflected immediately.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLocalPath,"org.apache.hadoop.conf.Configuration:getLocalPath(java.lang.String,java.lang.String)",2828,2848,"/**
 * Finds a suitable directory for a file path.
 * @param dirsProp property containing directory paths
 * @param path file path to be stored
 * @return Path object of the chosen directory or throws IOException if none found
 */","* Get a local file under a directory named by <i>dirsProp</i> with
   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,
   * then one is chosen based on <i>path</i>'s hash code.  If the selected
   * directory does not exist, an attempt is made to create it.
   *
   * @param dirsProp directory in which to locate the file.
   * @param path file-path.
   * @return local file under the directory with the given path.
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,java.net.URI,org.apache.hadoop.conf.Configuration)",94,97,"/**
* Initializes an NflyNode with specified host, rack, and configuration.
* @param hostName hostname of the node
* @param rackName name of the rack
* @param uri URI for the file system
* @param conf configuration settings
* @throws IOException if initialization fails
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getRawFileSystem,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getRawFileSystem(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",328,340,"/**
* Resolves and retrieves FileSystem for a given path.
* @param path file system path
* @param conf configuration settings
* @return FileSystem object or throws exception if not found
*/","* This is an admin only API to give access to its child raw file system, if
   * the path is link. If the given path is an internal directory(path is from
   * mount paths tree), it will initialize the file system of given path uri
   * directly. If path cannot be resolved to any internal directory or link, it
   * will throw NotInMountpointException. Please note, this API will not return
   * chrooted file system. Instead, this API will get actual raw file system
   * instances.
   *
   * @param path - fs uri path
   * @param conf - configuration
   * @throws IOException raised on errors performing I/O.
   * @return file system.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getMountPathInfo,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountPathInfo(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",351,372,"/**
* Resolves mount point information for a given path.
* @param path file system path to resolve
* @param conf configuration settings
* @return MountPathInfo containing resolved file system and remaining path
* @throws IOException if an I/O error occurs
*/","* Gets the mount path info, which contains the target file system and
   * remaining path to pass to the target file system.
   *
   * @param path the path.
   * @param conf configuration.
   * @return mount path info.
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,expandAsGlob,"org.apache.hadoop.fs.shell.PathData:expandAsGlob(java.lang.String,org.apache.hadoop.conf.Configuration)",344,396,"/**
 * Parses a pattern and returns matching path data.
 * @param pattern the file pattern to match
 * @param conf configuration settings
 * @return array of PathData objects or null if no matches found
 * @throws IOException on I/O errors during file system operations
 */","* Expand the given path as a glob pattern.  Non-existent paths do not
   * throw an exception because creation commands like touch and mkdir need
   * to create them.  The ""stat"" field will be null if the path does not
   * exist.
   * @param pattern the pattern to expand as a glob
   * @param conf the hadoop configuration
   * @return list of {@link PathData} objects.  if the pattern is not a glob,
   * and does not exist, the list will contain a single PathData with a null
   * stat 
   * @throws IOException anything else goes wrong...",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",1894,1932,"/**
* Initializes a Reader with given configuration and options.
* @param conf Hadoop Configuration object
* @param opts variable number of Option objects for configuration
* @throws IOException if initialization fails
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,initBloomFilter,"org.apache.hadoop.io.BloomMapFile$Reader:initBloomFilter(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",237,254,"/**
* Loads a Bloom filter from the specified directory.
* @param dirName path to the directory containing the Bloom filter file
* @param conf configuration settings for file system operations
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFileDumper.java,dumpInfo,"org.apache.hadoop.io.file.tfile.TFileDumper:dumpInfo(java.lang.String,java.io.PrintStream,org.apache.hadoop.conf.Configuration)",96,295,"/**
 * Analyzes and prints properties of a TFile.
 * @param file path to the TFile
 * @param out PrintStream for output
 * @param conf Configuration settings
 * @throws IOException if an I/O error occurs
 */","* Dump information about TFile.
   * 
   * @param file
   *          Path string of the TFile
   * @param out
   *          PrintStream to output the information.
   * @param conf
   *          The configuration object.
   * @throws IOException",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageFile,"org.apache.hadoop.security.Credentials:readTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",225,241,"/**
* Reads credentials from a file.
* @param filename path to the credential file
* @param conf configuration settings
* @return Credentials object containing read data
* @throws IOException if an I/O error occurs
*/","* Convenience method for reading a token storage file and loading its Tokens.
   * @param filename filename.
   * @param conf configuration.
   * @throws IOException  raised on errors performing I/O.
   * @return Credentials.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageFile,"org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials$SerializedFormat)",341,347,"/**
 * Writes data to a file using specified configuration and format.
 * @param filename path to the target file
 * @param conf configuration settings for file operations
 * @param format serialized format of the data
 * @throws IOException if an I/O error occurs during writing
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.KeyStoreProvider:initFileSystem(java.net.URI),81,85,"/**
 * Calls superclass method and initializes file system.
 * @param uri resource location
 * @throws IOException if an I/O error occurs
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,<init>,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",129,138,"/**
 * Initializes a JavaKeyStoreProvider with a URI and configuration.
 * @param uri the location of the keystore
 * @param conf the configuration settings
 * @throws IOException if an I/O error occurs
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,getLibJars,org.apache.hadoop.util.GenericOptionsParser:getLibJars(org.apache.hadoop.conf.Configuration),372,389,"/**
* Parses configuration for JAR files and returns URLs.
* @param conf Configuration object
* @return Array of JAR file URLs or null if none found
*/","* If libjars are set in the conf, parse the libjars.
   * @param conf input Configuration.
   * @return libjar urls
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,initFs,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:initFs(),271,304,"/**
* Initializes file system and sets up flushing timer.
* @return true if initialization is successful, false otherwise
*/","* Initialize the connection to HDFS and create the base directory. Also
   * launch the flush thread.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLocalFSFileContext,org.apache.hadoop.fs.FileContext:getLocalFSFileContext(),426,429,"/**
 * Retrieves local file system context.
 * @return FileContext for local file system
 * @throws UnsupportedFileSystemException if local file system is not supported
 */","* @return a FileContext for the local file system using the default config.
   * @throws UnsupportedFileSystemException If the file system for
   *           {@link FsConstants#LOCAL_FS_URI} is not supported.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,<init>,org.apache.hadoop.fs.shell.Display$AvroFileInputStream:<init>(org.apache.hadoop.fs.FileStatus),278,289,"/**
* Initializes an AvroFileInputStream for reading Avro files.
* @param status FileStatus object representing the file to be read
* @throws IOException if an I/O error occurs during initialization
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(),416,419,"/**
 * Creates a FileContext instance using default configuration.
 * @return FileContext object configured with default settings
 * @throws UnsupportedFileSystemException if file system is unsupported
 */","* Create a FileContext using the default config read from the
   * $HADOOP_CONFIG/core.xml, Unspecified key-values for config are defaulted
   * from core-defaults.xml in the release jar.
   * 
   * @throws UnsupportedFileSystemException If the file system from the default
   *           configuration is not supported
   * @return file context.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersion(java.lang.String),616,624,"/**
 * Fetches key version by name.
 * @param versionName unique identifier for the key version
 * @return KeyVersion object representing the fetched version
 * @throws IOException if an I/O error occurs during the operation
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCurrentKey(java.lang.String),626,634,"/**
 * Fetches the current version of a key by name.
 * @param name unique key identifier
 * @return KeyVersion object representing the current version
 * @throws IOException if an I/O error occurs during the operation
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeys(),636,644,"/**
* Fetches key names from KMS.
* @return List of key names or empty list if none found
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeysMetadata,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeysMetadata(java.lang.String[]),675,694,"/**
* Fetches metadata for given key names.
* @param keyNames variable number of key names
* @return array of Metadata objects
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKeyInternal,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKeyInternal(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",696,722,"/**
* Creates a new key version.
* @param name key name
* @param material optional key material
* @param options configuration options for the key
* @return KeyVersion object representing the created key version
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,invalidateCache,org.apache.hadoop.crypto.key.kms.KMSClientProvider:invalidateCache(java.lang.String),741,750,"/**
* Masks a resource by name.
* @param name the name of the resource to mask
* @throws IOException if an I/O error occurs
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),801,835,"/**
* Decrypts an encrypted key version.
* @param encryptedKeyVersion the EncryptedKeyVersion to decrypt
* @return KeyVersion object representing the decrypted key
* @throws IOException if a network error occurs
* @throws GeneralSecurityException if security operations fail
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),837,864,"/**
 * Reencrypts an encrypted key version.
 * @param ekv EncryptedKeyVersion object to reencrypt
 * @return New EncryptedKeyVersion object after reencryption
 * @throws IOException if there's an I/O error
 * @throws GeneralSecurityException if there's a security issue
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKeys(java.util.List),866,906,"/**
* Masks encrypted key versions by reencrypting them.
* @param ekvs list of encrypted key versions to mask
* @throws IOException if an I/O error occurs
* @throws GeneralSecurityException if a security exception occurs
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersions(java.lang.String),908,923,"/**
* Fetches key versions by name.
* @param name key identifier
* @return List of KeyVersion objects or null if none found
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getMetadata,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getMetadata(java.lang.String),925,933,"/**
* Fetches metadata for a given resource.
* @param name resource identifier
* @return Metadata object
* @throws IOException if connection fails
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,deleteKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:deleteKey(java.lang.String),935,941,"/**
 * Masks a resource by name.
 * @param name unique resource identifier
 * @throws IOException if an I/O error occurs during the operation
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getServer,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",380,390,"/**
* Creates and returns a new RPC server.
* @param protocol the interface class for the protocol
* @param protocolImpl implementation of the protocol
* @param bindAddress address to bind the server to
* @param port port number for the server
* @param numHandlers number of handler threads
* @param numReaders number of reader threads
* @param queueSizePerHandler size of the request queue per handler
* @param verbose flag to enable verbose logging
* @param conf configuration settings
* @param secretManager secret manager for secure communication
* @param portRangeConfig port range configuration
* @param alignmentContext alignment context for server operations
* @return new RPC Server instance
* @throws IOException if an I/O error occurs during server setup
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",446,455,"/**
* Initializes a new Server instance.
* @param protocolClass the class of the protocol
* @param protocolImpl implementation of the protocol
* @param conf configuration settings for the server
* @param bindAddress address to bind the server to
* @param port port number for the server
* @param numHandlers number of handler threads
* @param numReaders number of reader threads
* @param queueSizePerHandler size of the request queue per handler
* @param verbose enables detailed logging if true
* @param secretManager secret manager for secure communication
* @param portRangeConfig configuration for port range
* @param alignmentContext context for data alignment
* @throws IOException if an I/O error occurs during initialization
*/","* Construct an RPC server.
     * 
     * @param protocolClass the class of protocol
     * @param protocolImpl the protocolImpl whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param portRangeConfig A config parameter that can be used to restrict
     * the range of ports used when port is 0 (an ephemeral port)
     * @param alignmentContext provides server state info on client responses
     * @param secretManager input secretManager.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param numReaders input numReaders.
     * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getServer,"org.apache.hadoop.ipc.WritableRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",371,382,"/**
* Initializes and returns a new RPC server.
* @param protocolClass interface class for the protocol
* @param protocolImpl implementation of the protocol
* @param bindAddress address to bind the server
* @param port port number for the server
* @param numHandlers number of handler threads
* @param numReaders number of reader threads
* @param queueSizePerHandler size of the request queue per handler
* @param verbose enable verbose logging
* @param conf configuration settings
* @param secretManager secret manager for secure communication
* @param portRangeConfig port range configuration
* @param alignmentContext context for alignment
* @return RPC.Server instance
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String)",466,476,"/**
* Deprecated constructor for setting up a server.
* @param protocolClass unused parameter
* @param protocolImpl implementation of the protocol
* @param conf configuration settings
* @param bindAddress address to bind the server
* @param port port number to listen on
* @param numHandlers number of handler threads
* @param numReaders number of reader threads
* @param queueSizePerHandler size of the request queue per handler
* @param verbose enable verbose logging
* @param secretManager secret manager for security tokens
* @param portRangeConfig unused parameter
* @throws IOException if an I/O error occurs during setup
*/","* Construct an RPC server.
     * @param protocolClass - the protocol being registered
     *     can be null for compatibility with old usage (see below for details)
     * @param protocolImpl the protocol impl that will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param secretManager input secretManager.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param portRangeConfig input portRangeConfig.
     * @param numReaders input numReaders.
     *
     * @deprecated use Server#Server(Class, Object,
     *      Configuration, String, int, int, int, int, boolean, SecretManager)
     * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,saslReadAndProcess,org.apache.hadoop.ipc.Server$Connection:saslReadAndProcess(org.apache.hadoop.ipc.RpcWritable$Buffer),2178,2196,"/**
* Handles SASL message processing.
* @param buffer input buffer containing SASL message
* @throws RpcServerException if server error occurs
* @throws IOException on I/O errors
* @throws InterruptedException if operation is interrupted
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,<init>,"org.apache.hadoop.security.ssl.SSLFactory:<init>(org.apache.hadoop.security.ssl.SSLFactory$Mode,org.apache.hadoop.conf.Configuration)",136,160,"/**
 * Initializes SSLFactory with given mode and configuration.
 * @param mode SSL operation mode
 * @param conf SSL configuration settings
 */","* Creates an SSLFactory.
   *
   * @param mode SSLFactory mode, client or server.
   * @param conf Hadoop configuration from where the SSLFactory configuration
   * will be read.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshServiceAcl,"org.apache.hadoop.ipc.Server:refreshServiceAcl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",767,769,"/**
 * Masks configuration and policy using specified provider.
 * @param conf Configuration object to be masked
 * @param provider PolicyProvider used for masking
 */","* Refresh the service authorization ACL for the service handled by this server.
   *
   * @param conf input Configuration.
   * @param provider input PolicyProvider.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,loopUntilConnected,org.apache.hadoop.ha.HealthMonitor:loopUntilConnected(),161,168,"/**
 * Initializes proxy until it is not null.
 * @throws InterruptedException if thread is interrupted during sleep
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,transitionToActive,org.apache.hadoop.ha.HAAdmin:transitionToActive(org.apache.commons.cli.CommandLine),155,178,"/**
 * Transitions a service to active state.
 * @param cmd command line interface for input arguments
 * @return 0 on success, -1 on failure
 * @throws IOException if I/O error occurs
 * @throws ServiceFailedException if service operation fails
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doFence,org.apache.hadoop.ha.ZKFailoverController:doFence(org.apache.hadoop.ha.HAServiceTarget),543,566,"/**
 * Masks a service target by attempting graceful transition; falls back to fencing.
 * @param target HAServiceTarget to be masked
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,failover,"org.apache.hadoop.ha.FailoverController:failover(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean,boolean)",198,260,"/**
 * Initiates failover from one service target to another.
 * @param fromSvc current active service target
 * @param toSvc target service for failover
 * @param forceFence flag to force fencing operation
 * @param forceActive flag to force activation of the target service
 * @throws FailoverFailedException if failover fails
 */","* Failover from service 1 to service 2. If the failover fails
   * then try to failback.
   *
   * @param fromSvc currently active service
   * @param toSvc service to make active
   * @param forceFence to fence fromSvc even if not strictly necessary
   * @param forceActive try to make toSvc active even if it is not ready
   * @throws FailoverFailedException if the failover fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processPath,org.apache.hadoop.fs.shell.Delete$Rm:processPath(org.apache.hadoop.fs.shell.PathData),110,127,"/**
 * Deletes a file or directory.
 * @param item the PathData object representing the file/directory to delete
 * @throws IOException if deletion fails
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,expandArgument,org.apache.hadoop.fs.shell.CopyCommands$Put:expandArgument(java.lang.String),295,304,"/**
* Processes path data from input string.
* @param arg input path as a string
* @return list of processed PathData objects
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Merge:processOptions(java.util.LinkedList),71,89,"/**
 * Parses and initializes parameters for a file operation.
 * @param args command arguments list
 * @throws IOException if an I/O error occurs or invalid URI syntax
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,expandArgument,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:expandArgument(java.lang.String),357,375,"/**
 * Parses input argument to create a list of PathData.
 * @param arg input string representing path or URI
 * @return List of PathData objects
 * @throws IOException if invalid URI and not on Windows
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getLocalDestination,org.apache.hadoop.fs.shell.CommandWithDestination:getLocalDestination(java.util.LinkedList),182,195,"/**
 * Initializes destination path data.
 * @param args list of arguments containing path information
 * @throws IOException if URI syntax is invalid and not on Windows
 */","*  The last arg is expected to be a local path, if only one argument is
   *  given then the destination will be the current directory 
   *  @param args is the list of arguments
   * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",2530,2533,"/**
 * Masks multiple source files to a destination path.
 * @param srcs array of source file paths
 * @param dst destination file path
 * @throws IOException if an I/O error occurs
 */","* The src files is on the local disk.  Add it to filesystem at
   * the given dst name, removing the source afterwards.
   * @param srcs source paths
   * @param dst path
   * @throws IOException IO failure",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",360,365,"/**
 * Moves files from source paths to destination.
 * @param delSrc true if source files should be deleted after move
 * @param overwrite true if existing destination files should be overwritten
 * @param srcs array of source file paths
 * @param dst destination directory path
 * @throws IOException if an I/O error occurs
 */","* The src files are on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2556,2559,"/**
* Calls overloaded version of m1 with copy flag set to true.
* @param delSrc flag indicating whether to delete source after copying
* @param src source file path
* @param dst destination file path
*/","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param src path
   * @param dst path
   * @throws IOException IO failure.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",372,377,"/**
 * Copies or moves files from source to destination.
 * @param delSrc true if source should be deleted after copy/move
 * @param overwrite true if existing files in destination should be overwritten
 * @param src source file path
 * @param dst destination file path
 * @throws IOException if an I/O error occurs
 */","* The src file is on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2624,2627,"/**
* Copies file from source to destination.
* @param delSrc true if source should be deleted after copy
* @param src source file path
* @param dst destination file path
*/","* Copy it a file from a remote filesystem to the local one.
   * delSrc indicates if the src will be removed or not.
   * @param delSrc whether to delete the src
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)",394,492,"/**
 * Finds a suitable local directory for file storage.
 * @param pathStr base path string
 * @param size required size or SIZE_UNKNOWN
 * @param conf configuration settings
 * @param checkWrite whether to check write permissions
 * @return Path object of the selected directory
 * @throws IOException if no valid directory is found
 */","Get a path from the local FS. If size is known, we go
     *  round-robin over the set of disks (via the configured dirs) and return
     *  the first complete path which has enough space.
     *  
     *  If size is not known, use roulette selection -- pick directories
     *  with probability proportional to their available space.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathToRead,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",518,539,"/**
 * Searches for a file in configured local directories.
 * @param pathStr relative file path to search for
 * @param conf configuration settings
 * @return Path to the found file or throws DiskErrorException if not found
 */","Get a path from the local FS for reading. We search through all the
     *  configured dirs for the file's existence and return the complete
     *  path to the file when we find one",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAllLocalPathsToRead,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",603,610,"/**
 * Iterates over paths matching a given string pattern.
 * @param pathStr the path string pattern
 * @param conf configuration settings
 * @return Iterable of matching Path objects
 * @throws IOException if an I/O error occurs
 */","* Get all of the paths that currently exist in the working directories.
     * @param pathStr the path underneath the roots
     * @param conf the configuration to look up the roots in
     * @return all of the paths that exist under any of the roots
     * @throws IOException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet,org.apache.hadoop.fs.viewfs.FsGetter)",228,275,"/**
 * Initializes NflyFSystem with URIs and configuration.
 * @param uris array of URI destinations
 * @param conf configuration settings
 * @param minReplication minimum number of replicas required
 * @param nflyFlags flags for Nfly system behavior
 * @param fsGetter factory for file systems
 * @throws IOException if initialization fails
 */","* Creates a new Nfly instance.
   *
   * @param uris the list of uris in the mount point
   * @param conf configuration object
   * @param minReplication minimum copies to commit a write op
   * @param nflyFlags modes such readMostRecent
   * @param fsGetter to get the file system instance with the given uri
   * @throws IOException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,runAll,org.apache.hadoop.fs.shell.Command:runAll(),128,142,"/**
* Processes source files and handles exceptions.
* @return 0 if successful, -1 if an IOException occurs
*/","* For each source path, execute the command
   * 
   * @return 0 if it runs successfully; -1 if it fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,expandArgument,org.apache.hadoop.fs.shell.Command:expandArgument(java.lang.String),264,271,"/**
* Masks paths based on given argument.
* @param arg input string for masking
* @return List of PathData objects
* @throws IOException if path not found
*/","* Expand the given argument into a list of {@link PathData} objects.
   * The default behavior is to expand globs.  Commands may override to
   * perform other expansions on an argument.
   * @param arg string pattern to expand
   * @return list of {@link PathData} objects
   * @throws IOException if anything goes wrong...",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getRemoteDestination,org.apache.hadoop.fs.shell.CommandWithDestination:getRemoteDestination(java.util.LinkedList),203,221,"/**
 * Sets destination path based on arguments.
 * @param args list of command-line arguments
 * @throws IOException if path operations fail
 */","*  The last arg is expected to be a remote path, if only one argument is
   *  given then the destination will be the remote user's directory 
   *  @param args is the list of arguments
   *  @throws PathIOException if path doesn't exist or matches too many times",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",1942,1946,"/**
* Constructs a new Reader instance.
* @param fs FileSystem object
* @param file Path to the file
* @param conf Configuration object
* @throws IOException if an I/O error occurs
*/","* Construct a reader by opening a file from the given file system.
     * @param fs The file system used to open the file.
     * @param file The file being read.
     * @param conf Configuration
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Reader(Configuration, Option...) instead.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,int,long,long,org.apache.hadoop.conf.Configuration)",1958,1962,"/**
* Constructs a Reader with specified parameters.
* @param in input stream from file system
* @param bufferSize size of buffer for reading
* @param start starting position in the file
* @param length length of data to read
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/","* Construct a reader by the given input stream.
     * @param in An input stream.
     * @param buffersize unused
     * @param start The starting position.
     * @param length The length being read.
     * @param conf Configuration
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Reader(Configuration, Reader.Option...) instead.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,createDataFileReader,"org.apache.hadoop.io.MapFile$Reader:createDataFileReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",568,575,"/**
* Creates a SequenceFile reader with specified options.
* @param dataFile path to the SequenceFile
* @param conf Hadoop configuration
* @param options additional reader options
* @return SequenceFile.Reader instance
* @throws IOException if an I/O error occurs
*/","* Override this method to specialize the type of
     * {@link SequenceFile.Reader} returned.
     *
     * @param dataFile data file.
     * @param conf configuration.
     * @param options options.
     * @throws IOException raised on errors performing I/O.
     * @return SequenceFile.Reader.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawKey,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawKey(),3832,3857,"/**
* Reads next key from input.
* @return true if key is successfully read, false otherwise
* @throws IOException if an I/O error occurs or class mismatch
*/","* Fills up the rawKey object with the key returned by the Reader.
       * @return true if there is a key returned; false, otherwise
       * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1071,1195,"/**
 * Constructs a Writer with specified configuration and options.
 * @param conf Hadoop configuration
 * @param opts variable number of option objects
 * @throws IOException if an I/O error occurs
 */","* Construct a uncompressed writer from a set of options.
     * @param conf the configuration to use
     * @param opts the options used when creating the writer
     * @throws IOException if it fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,main,org.apache.hadoop.io.file.tfile.TFile:main(java.lang.String[]),2348,2366,"/**
 * Dumps TFile content.
 * @param args array of TFile paths
 */","* Dumping the TFile information.
   * 
   * @param args
   *          A list of TFile paths.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageFile,"org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",335,339,"/**
 * Calls overloaded m1 with default format.
 * @param filename path to file
 * @param conf configuration settings
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,doFormattedWrite,"org.apache.hadoop.security.token.DtFileOperations:doFormattedWrite(java.io.File,java.lang.String,org.apache.hadoop.security.Credentials,org.apache.hadoop.conf.Configuration)",104,114,"/**
* Masks a file with given credentials format.
* @param f the file to be masked
* @param format the serialization format for credentials
* @param creds the credentials to use for masking
* @param conf configuration settings for the operation
*/","Write out a Credentials object as a local file.
   *  @param f a local File object.
   *  @param format a string equal to FORMAT_PB or FORMAT_JAVA.
   *  @param creds the Credentials object to be written out.
   *  @param conf a Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,rollLogDirIfNeeded,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDirIfNeeded(),504,542,"/**
* Handles log flushing based on time and force flush flag.
* @throws MetricsException if an error occurs during log handling
*/","* Check the current directory against the time stamp.  If they're not
   * the same, create a new directory and a new log file in that directory.
   *
   * @throws MetricsException thrown if an error occurs while creating the
   * new directory or new log file",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,getJarsInDirectory,"org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String,boolean)",1777,1796,"/**
 * Finds JAR files matching the given path.
 * @param path directory or file pattern to search
 * @param useLocal flag to use local filesystem context
 * @return list of Paths to JAR files
 */","* Returns all jars that are in the directory. It is useful in expanding a
   * wildcard path to return all jars from the directory to use in a classpath.
   *
   * @param path the path to the directory. The path may include the wildcard.
   * @param useLocal use local.
   * @return the list of jars as URLs, or an empty list if there are no jars, or
   * the directory does not exist",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,getInputStream,org.apache.hadoop.fs.shell.Display$Text:getInputStream(org.apache.hadoop.fs.shell.PathData),123,173,"/**
* Processes file input stream based on lead bytes.
* @param item PathData object representing the file
* @return InputStream with appropriate compression or format handling
* @throws IOException if an I/O error occurs
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",724,728,"/**
* Generates a masked key version.
* @param name key identifier
* @param options configuration options
* @return KeyVersion object with masking applied
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",730,739,"/**
* Generates a key version with masking.
* @param name key identifier
* @param material raw key material
* @param options additional configuration options
* @return KeyVersion object
* @throws IOException if an I/O error occurs
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersionInternal,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersionInternal(java.lang.String,byte[])",752,768,"/**
 * Creates a new key version with the specified name and material.
 * @param name unique identifier for the key
 * @param material cryptographic material for the key
 * @return KeyVersion object representing the created key version
 * @throws NoSuchAlgorithmException if algorithm is not supported
 * @throws IOException if I/O error occurs during request
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,invalidateCache,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:invalidateCache(java.lang.String),319,324,"/**
* Delegates the m1 operation to all registered KMSClientProviders.
* @param keyName name of the key to operate on
* @throws IOException if an I/O error occurs during the operation
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getServer,"org.apache.hadoop.ipc.ProtobufRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",368,378,"/**
* Creates and returns a new RPC server.
* @param protocol service interface class
* @param protocolImpl implementation of the service interface
* @param bindAddress address to bind the server
* @param port port number for the server
* @param numHandlers number of handler threads
* @param numReaders number of reader threads
* @param queueSizePerHandler size of request queue per handler
* @param verbose enable verbose logging
* @param conf configuration settings
* @param secretManager security token manager
* @param portRangeConfig port range configuration
* @param alignmentContext context for alignment
* @return RPC server instance
* @throws IOException if an I/O error occurs
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)",413,418,"/**
 * Constructs a Server instance.
 * @param protocolClass Protocol class type
 * @param protocolImpl Implementation of the protocol
 * @param conf Configuration settings
 * @param bindAddress Address to bind server
 * @param port Port number for the server
 * @throws IOException if an I/O error occurs during initialization
 */","Construct an RPC server.
     * @param protocolClass class
     * @param protocolImpl the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager)",436,445,"/**
 * Deprecated constructor for Server.
 * @param protocolImpl implementation of the protocol
 * @param conf configuration settings
 * @param bindAddress address to bind to
 * @param port port number
 * @param numHandlers number of handler threads
 * @param numReaders number of reader threads
 * @param queueSizePerHandler queue size per handler
 * @param verbose flag for verbose output
 * @param secretManager secret manager for security tokens
 * @throws IOException if an I/O error occurs
 */","* Construct an RPC server.
     * @param protocolImpl the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param numReaders input numberReaders.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param secretManager input secretManager.
     * 
     * @deprecated use Server#Server(Class, Object, 
     *      Configuration, String, int, int, int, int, boolean, SecretManager)
     * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processRpcOutOfBandRequest,"org.apache.hadoop.ipc.Server$Connection:processRpcOutOfBandRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)",2984,3012,"/**
 * Handles special RPC calls based on header.
 * @param header request header containing call ID
 * @param buffer data buffer for processing
 * @throws RpcServerException if invalid or unsupported call
 * @throws IOException on I/O errors
 * @throws InterruptedException if thread is interrupted
 */","* Establish RPC connection setup by negotiating SASL if required, then
     * reading and authorizing the connection header
     * @param header - RPC header
     * @param buffer - stream to request payload
     * @throws RpcServerException - setup failed due to SASL
     *         negotiation failure, premature or invalid connection context,
     *         or other state errors. This exception needs to be sent to the 
     *         client.
     * @throws IOException - failed to send a response back to the client
     * @throws InterruptedException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,connect,org.apache.hadoop.log.LogLevel$CLI:connect(java.net.URL),260,284,"/**
 * Establishes an authenticated URL connection.
 * @param url the target URL to connect to
 * @return the established URLConnection
 * @throws Exception if authentication fails or other errors occur
 */","* Connect to the URL. Supports HTTP/HTTPS and supports SPNEGO
     * authentication. It falls back to simple authentication if it fails to
     * initiate SPNEGO.
     *
     * @param url the URL address of the daemon servlet
     * @return a connected connection
     * @throws Exception if it can not establish a connection.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",378,428,"/**
 * Constructs a KMSClientProvider with the given URI and configuration.
 * @param uri KMS server URI
 * @param conf Hadoop configuration
 * @throws IOException if an I/O error occurs
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,runCmd,org.apache.hadoop.ha.HAAdmin:runCmd(java.lang.String[]),391,441,"/**
* Parses and processes cluster management commands.
* @param argv command-line arguments
* @return exit code or -1 on error
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,fenceOldActive,org.apache.hadoop.ha.ZKFailoverController:fenceOldActive(byte[]),532,541,"/**
* Masks data and handles exceptions.
* @param data input byte array to be masked
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2518,2521,"/**
 * Copies file from source to destination.
 * @param src source file path
 * @param dst destination file path
 */","* The src file is on the local disk.  Add it to filesystem at
   * the given dst name and the source is kept intact afterwards
   * @param src path
   * @param dst path
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2542,2545,"/**
 * Masks file from source to destination.
 * @param src source file path
 * @param dst destination file path
 * @throws IOException if I/O error occurs
 */","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name, removing the source afterwards.
   * @param src local path
   * @param dst path
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",349,353,"/**
 * Moves a file from source to destination.
 * @param delSrc true if source should be deleted after move
 * @param src path to the source file
 * @param dst path to the destination file
 * @throws IOException if an I/O error occurs
 */","* The src file is on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2601,2603,"/**
 * Copies file from source to destination.
 * @param src source file path
 * @param dst destination file path
 * @throws IOException if an I/O error occurs
 */","* Copy it a file from the remote filesystem to the local one.
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveToLocalFile,"org.apache.hadoop.fs.FileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2612,2614,"/**
 * Masks a file by moving it to a destination path.
 * @param src source file path
 * @param dst destination file path
 * @throws IOException if an I/O error occurs
 */","* Copy a file to the local filesystem, then delete it from the
   * remote filesystem (if successfully copied).
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",384,388,"/**
 * Moves a file or directory from source to destination.
 * @param delSrc true if source should be deleted after move
 * @param src source path
 * @param dst destination path
 * @throws IOException if an I/O error occurs
 */","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.
   * delSrc indicates if the src will be removed or not.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)",162,167,"/**
* Allocates and configures a file path.
* @param pathStr the file path as a string
* @param size the desired size of the file
* @param conf configuration settings for allocation
* @param checkWrite flag to check write permissions
* @return Path object representing the allocated file path
* @throws IOException if an I/O error occurs
*/","Get a path from the local FS. Pass size as 
   *  SIZE_UNKNOWN if not known apriori. We
   *  round-robin over the set of disks (via the configured dirs) and return
   *  the first complete path which has enough space 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @param checkWrite ensure that the path is writable
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createTmpFileForWrite,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",500,512,"/**
* Creates a masked file with given size in specified directory.
* @param pathStr directory path string
* @param size desired file size
* @param conf configuration settings
* @return created File object
* @throws IOException if I/O error occurs
*/","Creates a file on the local FS. Pass size as 
     * {@link LocalDirAllocator.SIZE_UNKNOWN} if not known apriori. We
     *  round-robin over the set of disks (via the configured dirs) and return
     *  a file on the first path which has enough space. The file is guaranteed
     *  to go away when the JVM exits.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathToRead,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",177,181,"/**
* Retrieves an allocator context and processes a path.
* @param pathStr string representation of the path
* @param conf configuration settings
* @return processed Path object
* @throws IOException if an I/O error occurs
*/","Get a path from the local FS for reading. We search through all the
   *  configured dirs for the file's existence and return the complete
   *  path to the file when we find one 
   *  @param pathStr the requested file (this will be searched)
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAllLocalPathsToRead,"org.apache.hadoop.fs.LocalDirAllocator:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",190,198,"/**
 * Retrieves paths based on given configuration.
 * @param pathStr string representation of the path
 * @param conf configuration settings
 * @return iterable collection of paths or throws IOException
 */","* Get all of the paths that currently exist in the working directories.
   * @param pathStr the path underneath the roots
   * @param conf the configuration to look up the roots in
   * @return all of the paths that exist under any of the roots
   * @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet)",213,216,"/**
* Constructs an NflyFSystem instance.
* @param uris array of URIs for system initialization
* @param conf configuration settings
* @param minReplication minimum replication factor
* @param nflyFlags flags for NFly operations
* @throws IOException if an I/O error occurs during construction
*/","* Creates a new Nfly instance.
   *
   * @param uris the list of uris in the mount point
   * @param conf configuration object
   * @param minReplication minimum copies to commit a write op
   * @param nflyFlags modes such readMostRecent
   * @throws IOException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,createFileSystem,"org.apache.hadoop.fs.viewfs.NflyFSystem:createFileSystem(java.net.URI[],org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.viewfs.FsGetter)",944,971,"/**
* Creates a FileSystem with specified URI and configuration.
* @param uris array of URIs for the file system
* @param conf Configuration object
* @param settings string containing key-value pairs for settings
* @param fsGetter FsGetter instance
* @return NflyFSystem object initialized with given parameters
* @throws IOException if an I/O error occurs
*/","* Initializes an nfly mountpoint in viewfs.
   *
   * @param uris destinations to replicate writes to
   * @param conf file system configuration
   * @param settings comma-separated list of k=v pairs.
   * @return an Nfly filesystem
   * @throws IOException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,expandArguments,org.apache.hadoop.fs.shell.Command:expandArguments(java.util.LinkedList),243,254,"/**
 * Expands arguments to PathData objects.
 * @param args list of argument strings
 * @return LinkedList of PathData objects
 * @throws IOException if expansion fails
 */","*  Expands a list of arguments into {@link PathData} objects.  The default
   *  behavior is to call {@link #expandArgument(String)} on each element
   *  which by default globs the argument.  The loop catches IOExceptions,
   *  increments the error count, and displays the exception.
   * @param args strings to expand into {@link PathData} objects
   * @return list of all {@link PathData} objects the arguments
   * @throws IOException if anything goes wrong...",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,open,"org.apache.hadoop.io.MapFile$Reader:open(org.apache.hadoop.fs.Path,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",532,556,"/**
* Initializes data and index readers for a directory.
* @param dir directory containing data and index files
* @param comparator custom comparator or null to use default
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,adjustPriorityQueue,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:adjustPriorityQueue(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor),3598,3609,"/**
* Processes a segment descriptor and performs operations based on its content.
* @param ms SegmentDescriptor object containing input stream and metadata
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1609,1620,"/**
 * Initializes a BlockCompressWriter with given configuration and options.
 * @param conf Hadoop configuration object
 * @param options additional options for writer
 * @throws IOException if initialization fails
 */",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$RecordCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1539,1542,"/**
* Initializes a new RecordCompressWriter instance.
* @param conf Configuration object for writer setup
* @param options Additional options for writer configuration
* @throws IOException if an I/O error occurs during initialization
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,getTokenFile,"org.apache.hadoop.security.token.DtFileOperations:getTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)",175,220,"/**
* Masks a token by fetching and potentially aliasing it.
* @param tokenFile file containing token information
* @param fileFormat format of the token file
* @param alias optional alias for the token
* @param service service associated with the token
* @param url URL for token retrieval
* @param renewer entity that can renew the token
* @param conf configuration settings
* @throws Exception if an error occurs during processing
*/","Fetch a token from a service and save to file in the local filesystem.
   *  @param tokenFile a local File object to hold the output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias overwrite service field of fetched token with this text.
   *  @param service use a DtFetcher implementation matching this service text.
   *  @param url pass this URL to fetcher after stripping any http/s prefix.
   *  @param renewer pass this renewer to the fetcher.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,aliasTokenFile,"org.apache.hadoop.security.token.DtFileOperations:aliasTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",230,243,"/**
 * Masks tokens in a file based on service and alias.
 * @param tokenFile file containing tokens
 * @param fileFormat format of the file
 * @param alias text for aliasing
 * @param service text representing the service
 * @param conf configuration settings
 */","Alias a token from a file and save back to file in the local filesystem.
   *  @param tokenFile a local File object to hold the input and output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias overwrite service field of fetched token with this text.
   *  @param service only apply alias to tokens matching this service text.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,appendTokenFiles,"org.apache.hadoop.security.token.DtFileOperations:appendTokenFiles(java.util.ArrayList,java.lang.String,org.apache.hadoop.conf.Configuration)",251,264,"/**
 * Merges credentials from token files into a single Credentials object.
 * @param tokenFiles list of token files to process
 * @param fileFormat format of the output file
 * @param conf configuration settings
 * @throws IOException if an I/O error occurs
 */","Append tokens from list of files in local filesystem, saving to last file.
   *  @param tokenFiles list of local File objects.  Last file holds the output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,removeTokenFromFile,"org.apache.hadoop.security.token.DtFileOperations:removeTokenFromFile(boolean,java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",275,291,"/**
* Masks or cancels tokens based on alias and configuration.
* @param cancel flag to determine if cancellation should occur
* @param tokenFile file containing token data
* @param fileFormat format of the token file
* @param alias text alias for token identification
* @param conf configuration settings
*/","Remove a token from a file in the local filesystem, matching alias.
   *  @param cancel cancel token as well as remove from file.
   *  @param tokenFile a local File object.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias remove only tokens matching alias; null matches all.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.
   *  @throws InterruptedException if the thread is interrupted.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,renewTokenFile,"org.apache.hadoop.security.token.DtFileOperations:renewTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",301,313,"/**
* Masks tokens in a given file.
* @param tokenFile file containing tokens
* @param fileFormat format of the token file
* @param alias text alias for token matching
* @param conf configuration settings
* @throws IOException if an I/O error occurs
* @throws InterruptedException if the operation is interrupted
*/","Renew a token from a file in the local filesystem, matching alias.
   *  @param tokenFile a local File object.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias renew only tokens matching alias; null matches all.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.
   *  @throws InterruptedException if the thread is interrupted.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,importTokenFile,"org.apache.hadoop.security.token.DtFileOperations:importTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,java.lang.String,org.apache.hadoop.conf.Configuration)",323,339,"/**
* Adds a token to credentials and writes to a file.
* @param tokenFile file containing token information
* @param fileFormat format of the output file
* @param alias optional alias for the token
* @param base64 base64 encoded token string
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/","Import a token from a base64 encoding into the local filesystem.
   * @param tokenFile A local File object.
   * @param fileFormat A string equal to FORMAT_PB or FORMAT_JAVA, for output.
   * @param alias overwrite Service field of fetched token with this text.
   * @param base64 urlString Encoding of the token to import.
   * @param conf Configuration object passed along.
   * @throws IOException Error to import the token into the file.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,putMetrics,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),822,861,"/**
* Writes metrics record to output stream.
* @param record MetricsRecord object containing data to write
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,getJarsInDirectory,org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String),1764,1766,"/**
 * Retrieves paths matching the given pattern.
 * @param path directory path to search in
 * @return list of matching Path objects
 */","* Returns all jars that are in the directory. It is useful in expanding a
   * wildcard path to return all jars from the directory to use in a classpath.
   * It operates only on local paths.
   *
   * @param path the path to the directory. The path may include the wildcard.
   * @return the list of jars as URLs, or an empty list if there are no jars, or
   * the directory does not exist locally",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,expandWildcard,"org.apache.hadoop.util.GenericOptionsParser:expandWildcard(java.util.List,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)",495,512,"/**
* Masks paths by processing directories and collecting JAR files.
* @param finalPaths list to store processed paths
* @param path directory to process
* @param fs file system instance
* @throws IOException if an I/O error occurs
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String),771,775,"/**
 * Generates a masked key version.
 * @param name key identifier
 * @return KeyVersion object
 * @throws NoSuchAlgorithmException if algorithm is not available
 * @throws IOException if I/O error occurs
 */",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String,byte[])",777,786,"/**
* Generates a KeyVersion from the given name and material.
* @param name identifier for the key
* @param material cryptographic material for the key
* @return KeyVersion object
* @throws IOException if an I/O error occurs
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,deleteKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:deleteKey(java.lang.String),497,507,"/**
* Executes method m1 with a name parameter.
* @param name the name to be processed
* @throws IOException if an I/O error occurs
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String,byte[])",509,520,"/**
* Creates a new key version.
* @param name key name
* @param material key material
* @return KeyVersion object
* @throws IOException if an I/O error occurs
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String),522,541,"/**
* Retrieves key version by name.
* @param name key identifier
* @return KeyVersion object
* @throws NoSuchAlgorithmException if algorithm is not found
* @throws IOException for I/O errors
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)",398,402,"/**
* Deprecated constructor for creating a server.
* @param instance server instance
* @param conf configuration settings
* @param bindAddress address to bind the server
* @param port port number for the server
* @throws IOException if an I/O error occurs
*/","* Construct an RPC server.
     * @param instance the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * 
     * @deprecated Use #Server(Class, Object, Configuration, String, int)
     * @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processOneRpc,org.apache.hadoop.ipc.Server$Connection:processOneRpc(java.nio.ByteBuffer),2785,2823,"/**
 * Processes an RPC request from a ByteBuffer.
 * @param bb input buffer containing the RPC data
 * @throws IOException if I/O error occurs
 * @throws InterruptedException if thread is interrupted
 */","* Process one RPC Request from buffer read from socket stream 
     *  - decode rpc in a rpc-Call
     *  - handle out-of-band RPC requests such as the initial connectionContext
     *  - A successfully decoded RpcCall will be deposited in RPC-Q and
     *    its response will be sent later when the request is processed.
     * 
     * Prior to this call the connectionHeader (""hrpc..."") has been handled and
     * if SASL then SASL has been established and the buf we are passed
     * has been unwrapped from SASL.
     * 
     * @param bb - contains the RPC request header and the rpc request
     * @throws IOException - internal error that should not be returned to
     *         client, typically failure to respond to client
     * @throws InterruptedException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,process,org.apache.hadoop.log.LogLevel$CLI:process(java.lang.String),292,311,"/**
 * Connects to a URL, reads content line by line, and prints lines containing a marker.
 * @param urlString the URL to connect to
 * @throws Exception if connection or reading fails
 */","* Configures the client to send HTTP/HTTPS request to the URL.
     * Supports SPENGO for authentication.
     * @param urlString URL and query string to the daemon's web UI
     * @throws Exception if unable to connect",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createProviders,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProviders(org.apache.hadoop.conf.Configuration,java.net.URL,int,java.lang.String)",311,326,"/**
* Creates an array of KMSClientProviders for given hosts.
* @param conf configuration settings
* @param origUrl original URL
* @param port port number
* @param hostsPart semicolon-separated list of hostnames
* @return array of KMSClientProvider objects
* @throws IOException if instantiation fails
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,run,org.apache.hadoop.ha.HAAdmin:run(java.lang.String[]),347,361,"/**
* Executes a function with arguments.
* @param argv array of string arguments
* @return result of the function or -1 on error
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,completeLocalOutput,"org.apache.hadoop.fs.FileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2686,2689,"/**
 * Masks file content and writes to output.
 * @param fsOutputFile destination path for masked file
 * @param tmpLocalFile temporary local file containing original data
 */","* Called when we're all done writing to the target.
   * A local FS will do nothing, because we've written to exactly the
   * right place.
   * A remote FS will copy the contents of tmpLocalFile to the correct target at
   * fsOutputFile.
   * @param fsOutputFile path of output file
   * @param tmpLocalFile path to local tmp file
   * @throws IOException IO failure",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1019,1043,"/**
 * Copies files from source to destination, optionally copying CRC.
 * @param src source path
 * @param dst destination path
 * @param copyCrc flag to indicate if CRC file should be copied
 * @throws IOException if an I/O error occurs
 */","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.
   * If src and dst are directories, the copyCrc parameter
   * determines whether to copy CRC files.
   * @param src src path.
   * @param dst dst path.
   * @param copyCrc copy csc flag.
   * @throws IOException if an I/O error occurs.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",145,148,"/**
 * Calls overloaded method with default recursive flag.
 * @param pathStr file path as string
 * @param size file size in bytes
 * @param conf configuration settings
 * @return Path object representing the file
 * @throws IOException if an I/O error occurs
 */","Get a path from the local FS. Pass size as 
   *  SIZE_UNKNOWN if not known apriori. We
   *  round-robin over the set of disks (via the configured dirs) and return
   *  the first complete path which has enough space 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createTmpFileForWrite,"org.apache.hadoop.fs.LocalDirAllocator:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",211,215,"/**
* Allocates and retrieves a file.
* @param pathStr file path string
* @param size file size in bytes
* @param conf configuration settings
* @return File object or throws IOException
*/","Creates a temporary file in the local FS. Pass size as -1 if not known 
   *  apriori. We round-robin over the set of disks (via the configured dirs) 
   *  and select the first complete path which has enough space. A file is
   *  created on this directory. The file is guaranteed to go away when the
   *  JVM exits.
   *  @param pathStr prefix for the temporary file
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @return a unique temporary file
   *  @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processRawArguments,org.apache.hadoop.fs.shell.Command:processRawArguments(java.util.LinkedList),229,232,"/**
 * Processes a list of arguments.
 * Masks and forwards them to another method.
 * @param args LinkedList containing input arguments
 * @throws IOException if processing fails
 */","* Allows commands that don't use paths to handle the raw arguments.
   * Default behavior is to expand the arguments via
   * {@link #expandArguments(LinkedList)} and pass the resulting list to
   * {@link #processArguments(LinkedList)} 
   * @param args the list of argument strings
   * @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",490,499,"/**
* Initializes a SequenceFile reader.
* @param dir directory containing the sequence file
* @param conf configuration settings
* @param opts optional reader options
* @throws IOException if an I/O error occurs
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:next(),3565,3591,"/**
 * Masks data by applying a function.
 * @return true if masking is successful, false otherwise
 * @throws IOException if an I/O error occurs
 */",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",274,294,"/**
* Creates a writer with specified options and compression.
* @param conf configuration settings
* @param opts optional writer configurations
* @return Writer instance based on compression type
* @throws IOException if an I/O error occurs
*/","* Create a new Writer with the given options.
   * @param conf the configuration to use
   * @param opts the options to create the file with
   * @return a new Writer
   * @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Get:execute(),241,245,"/**
* Masks file operations with specified parameters.
* @throws Exception if operation fails
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Edit:execute(),271,277,"/**
* Masks files in tokenFiles list.
* @throws Exception if operation fails
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Append:execute(),289,292,"/**
 * Masks files using specified token and format.
 * @throws Exception if an error occurs during file operations
 */",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Remove:execute(),320,326,"/**
* Masks files in tokenFiles list.
* @throws Exception if operation fails
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Renew:execute(),350,355,"/**
* Masks tokens in each file.
* @throws Exception if operation fails
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Import:execute(),381,385,"/**
* Masks file content using specified parameters.
* @throws Exception if operation fails
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,createJarWithClassPath,"org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Map)",1668,1753,"/**
 * Masks input class path by expanding wildcards and creating a JAR.
 * @param inputClassPath the original class path string
 * @param pwd working directory as Path
 * @param targetDir target directory for files as Path
 * @param callerEnv environment variables map
 * @return array of masked class path entries
 * @throws IOException if I/O operations fail
 */","* Create a jar file at the given path, containing a manifest with a classpath
   * that references all specified entries.
   *
   * Some platforms may have an upper limit on command line length.  For example,
   * the maximum command line length on Windows is 8191 characters, but the
   * length of the classpath may exceed this.  To work around this limitation,
   * use this method to create a small intermediate jar with a manifest that
   * contains the full classpath.  It returns the absolute path to the new jar,
   * which the caller may set as the classpath for a new process.
   *
   * Environment variable evaluation is not supported within a jar manifest, so
   * this method expands environment variables before inserting classpath entries
   * to the manifest.  The method parses environment variables according to
   * platform-specific syntax (%VAR% on Windows, or $VAR otherwise).  On Windows,
   * environment variables are case-insensitive.  For example, %VAR% and %var%
   * evaluate to the same value.
   *
   * Specifying the classpath in a jar manifest does not support wildcards, so
   * this method expands wildcards internally.  Any classpath entry that ends
   * with * is translated to all files at that path with extension .jar or .JAR.
   *
   * @param inputClassPath String input classpath to bundle into the jar manifest
   * @param pwd Path to working directory to save jar
   * @param targetDir path to where the jar execution will have its working dir
   * @param callerEnv Map {@literal <}String, String{@literal >} caller's
   * environment variables to use for expansion
   * @return String[] with absolute path to new jar in position 0 and
   *   unexpanded wild card entry path in position 1
   * @throws IOException if there is an I/O error while writing the jar file",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,constructUrlsFromClasspath,org.apache.hadoop.util.ApplicationClassLoader:constructUrlsFromClasspath(java.lang.String),107,126,"/**
* Parses classpath and constructs URLs for each element.
* @param classpath the classpath string to parse
* @return array of URL objects representing the classpath elements
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,validateFiles,"org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String,boolean)",425,488,"/**
 * Masks file paths, handling wildcards if specified.
 * @param files comma-separated list of file paths
 * @param expandWildcard flag to determine wildcard expansion
 * @return masked file paths as a single string
 * @throws IOException if an I/O error occurs
 */","* takes input as a comma separated list of files
   * and verifies if they exist. It defaults for file:///
   * if the files specified do not have a scheme.
   * it returns the paths uri converted defaulting to file:///.
   * So an input of  /home/user/file1,/home/user/file2 would return
   * file:///home/user/file1,file:///home/user/file2.
   *
   * @param files the input files argument
   * @param expandWildcard whether a wildcard entry is allowed and expanded. If
   * true, any directory followed by a wildcard is a valid entry and is replaced
   * with the list of jars in that directory. It is used to support the wildcard
   * notation in a classpath.
   * @return a comma-separated list of validated and qualified paths, or null
   * if the input files argument is null",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,readAndProcess,org.apache.hadoop.ipc.Server$Connection:readAndProcess(),2478,2565,"/**
* Handles RPC communication, reading headers and data.
* @return Count of bytes processed or -1 on error
* @throws IOException if I/O error occurs
* @throws InterruptedException if thread is interrupted
*/","* This method reads in a non-blocking fashion from the channel: 
     * this method is called repeatedly when data is present in the channel; 
     * when it has enough data to process one rpc it processes that rpc.
     * 
     * On the first pass, it processes the connectionHeader, 
     * connectionContext (an outOfBand RPC) and at most one RPC request that 
     * follows that. On future passes it will process at most one RPC request.
     *  
     * Quirky things: dataLengthBuffer (4 bytes) is used to read ""hrpc"" OR 
     * rpc request length.
     *    
     * @return -1 in case of error, else num bytes read so far
     * @throws IOException - internal error that should not be returned to
     *         client, typically failure to respond to client
     * @throws InterruptedException - if the thread is interrupted.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,unwrapPacketAndProcessRpcs,org.apache.hadoop.ipc.Server$Connection:unwrapPacketAndProcessRpcs(byte[]),2733,2767,"/**
* Processes input buffer using SASL server unwrap and handles data chunks.
* @param inBuf input buffer to be processed
*/","* Process a wrapped RPC Request - unwrap the SASL packet and process
     * each embedded RPC request 
     * @param inBuf - SASL wrapped request of one or more RPCs
     * @throws IOException - SASL packet cannot be unwrapped
     * @throws InterruptedException",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doGetLevel,org.apache.hadoop.log.LogLevel$CLI:doGetLevel(),236,238,"/**
* Sets log level for a specified class.
* @throws Exception if an error occurs during execution
*/","* Send HTTP/HTTPS request to get log level.
     *
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doSetLevel,org.apache.hadoop.log.LogLevel$CLI:doSetLevel(),246,249,"/**
* Sets log level for a class.
* @throws Exception if an error occurs during execution
*/","* Send HTTP/HTTPS request to set log level.
     *
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createProvider,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)",279,309,"/**
* Creates a KeyProvider for the given URI and configuration.
* @param providerUri URI of the key provider
* @param conf configuration settings
* @return KeyProvider instance or null if scheme does not match
* @throws IOException on invalid authority or port parsing
*/","* This provider expects URIs in the following form :
     * {@literal kms://<PROTO>@<AUTHORITY>/<PATH>}
     *
     * where :
     * - PROTO = http or https
     * - AUTHORITY = {@literal <HOSTS>[:<PORT>]}
     * - HOSTS = {@literal <HOSTNAME>[;<HOSTS>]}
     * - HOSTNAME = string
     * - PORT = integer
     *
     * This will always create a {@link LoadBalancingKMSClientProvider}
     * if the uri is correct.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,completeLocalOutput,"org.apache.hadoop.fs.FilterFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",408,412,"/**
* Moves a file from temporary local storage to final filesystem output.
* @param fsOutputFile destination path in the filesystem
* @param tmpLocalFile source path of the temporary local file
* @throws IOException if an I/O error occurs during the move operation
*/","* Called when we're all done writing to the target.  A local FS will
   * do nothing, because we've written to exactly the right place.  A remote
   * FS will copy the contents of tmpLocalFile to the correct target at
   * fsOutputFile.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,createTmpFileForWrite,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",830,838,"/**
* Allocates a file with given size and configuration.
* @param pathStr file path string
* @param size desired file size in bytes
* @param conf configuration settings
* @return allocated File object
* @throws IOException if allocation fails
*/","* Demand create the directory allocator, then create a temporary file.
     * This does not mark the file for deletion when a process exits.
     * {@link LocalDirAllocator#createTmpFileForWrite(String, long, Configuration)}.
     *
     * @param pathStr prefix for the temporary file.
     * @param size    the size of the file that is going to be written.
     * @param conf    the Configuration object.
     * @return a unique temporary file.
     * @throws IOException IO problems",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,org.apache.hadoop.conf.Configuration)",129,132,"/**
 * Calls another overloaded method with unknown size.
 * @param pathStr file path string
 * @param conf configuration settings
 * @return Path object representing the file
 * @throws IOException if an I/O error occurs
 */","Get a path from the local FS. This method should be used if the size of 
   *  the file is not known apriori. We go round-robin over the set of disks
   *  (via the configured dirs) and return the first complete path where
   *  we could create the parent directory of the passed path. 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,run,org.apache.hadoop.fs.shell.Command:run(java.lang.String[]),184,201,"/**
* Processes command-line arguments.
* @param argv variable number of command-line argument strings
* @return exit code indicating success or failure
*/","* Invokes the command handler.  The default behavior is to process options,
   * expand arguments, and then process each argument.
   * <pre>
   * run
   * |{@literal ->} {@link #processOptions(LinkedList)}
   * \{@literal ->} {@link #processRawArguments(LinkedList)}
   *      |{@literal ->} {@link #expandArguments(LinkedList)}
   *      |   \{@literal ->} {@link #expandArgument(String)}*
   *      \{@literal ->} {@link #processArguments(LinkedList)}
   *          |{@literal ->} {@link #processArgument(PathData)}*
   *          |   |{@literal ->} {@link #processPathArgument(PathData)}
   *          |   \{@literal ->} {@link #processPaths(PathData, PathData...)}
   *          |        \{@literal ->} {@link #processPath(PathData)}*
   *          \{@literal ->} {@link #processNonexistentPath(PathData)}
   * </pre>
   * Most commands will chose to implement just
   * {@link #processOptions(LinkedList)} and {@link #processPath(PathData)}
   * 
   * @param argv the list of command line arguments
   * @return the exit code for the command
   * @throws IllegalArgumentException if called with invalid arguments",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",101,104,"/**
 * Constructs a new Reader instance.
 * @param fs FileSystem object
 * @param file path to the file
 * @param conf Configuration settings
 * @throws IOException if an I/O error occurs
 */","* Construct an array reader for the named file.
     * @param fs FileSystem.
     * @param file file.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",124,127,"/**
* Initializes a new Reader.
* @param fs FileSystem instance
* @param dirName directory name for reading
* @param comparator custom comparator for sorting
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/","* Construct a set reader for the named set using the named comparator.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param comparator input comparator.
     * @param conf input Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",510,514,"/**
* Deprecated constructor for Reader.
* @param fs FileSystem instance
* @param dirName directory name as string
* @param conf Configuration object
* @throws IOException if an I/O error occurs
*/","* Construct a map reader for the named map.
     * @deprecated
     *
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",526,530,"/**
* Constructs a new Reader instance.
* @param fs FileSystem object
* @param dirName directory name for reading
* @param comparator WritableComparator for key comparison
* @param conf Configuration settings
* @throws IOException if an I/O error occurs
*/","* Construct a map reader for the named map using the named comparator.
     * @deprecated
     *
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param comparator WritableComparator.
     * @param conf Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",213,217,"/**
 * Constructs a new Reader for a directory with configuration.
 * @param dir path to the directory containing sequence files
 * @param conf configuration settings
 * @param options additional options for the reader
 * @throws IOException if an I/O error occurs
 */",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cloneFileAttributes,"org.apache.hadoop.io.SequenceFile$Sorter:cloneFileAttributes(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)",3406,3422,"/**
 * Masks input file content and writes to output file.
 * @param inputFile path to the input file
 * @param outputFile path to the output file
 * @param prog progress tracking object
 * @return Writer for the masked output
 * @throws IOException if I/O error occurs
 */","* Clones the attributes (like compression of the input file and creates a 
     * corresponding Writer
     * @param inputFile the path of the input file whose attributes should be 
     * cloned
     * @param outputFile the path of the output file 
     * @param prog the Progressable to report status during the file write
     * @return Writer
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,fix,"org.apache.hadoop.io.MapFile:fix(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.conf.Configuration)",934,1014,"/**
* Generates an index for a SequenceFile.
* @param fs FileSystem instance
* @param dir Directory containing the SequenceFile
* @param keyClass Key class of the SequenceFile
* @param valueClass Value class of the SequenceFile
* @param dryrun If true, performs a dry run without writing changes
* @param conf Configuration object
* @return Number of entries processed or -1 if index already exists
*/","* This method attempts to fix a corrupt MapFile by re-creating its index.
   * @param fs filesystem
   * @param dir directory containing the MapFile data and index
   * @param keyClass key class (has to be a subclass of Writable)
   * @param valueClass value class (has to be a subclass of Writable)
   * @param dryrun do not perform any changes, just report what needs to be done
   * @param conf configuration.
   * @return number of valid entries in this MapFile, or -1 if no fixing was needed
   * @throws Exception Exception.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,flush,"org.apache.hadoop.io.SequenceFile$Sorter$SortPass:flush(int,int,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,boolean)",3217,3251,"/**
 * Writes data to output file with compression.
 * @param count number of entries to write
 * @param bytesProcessed total bytes processed so far
 * @param compressionType type of compression to apply
 * @param codec compression codec to use
 * @param done flag indicating if final segment
 * @throws IOException if an I/O error occurs
 */",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])",312,357,"/**
* Initializes a Writer for sequence files.
* @param conf configuration settings
* @param dirName directory path for the sequence file
* @param opts optional writer options
* @throws IOException if initialization fails
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)",308,315,"/**
* Deprecated method for creating a writer.
* @param fs FileSystem instance
* @param conf Configuration settings
* @param name Path to the file
* @param keyClass Class type for keys
* @param valClass Class type for values
* @return Writer object
* @throws IOException if an I/O error occurs
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",330,339,"/**
* Deprecated method for creating a Writer.
* @param fs FileSystem instance
* @param conf Configuration object
* @param name Path to the file
* @param keyClass Class type for keys
* @param valClass Class type for values
* @param compressionType Compression type for output
* @return Writer instance or throws IOException
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",355,366,"/**
* Creates a Writer instance with specified parameters.
* @param fs FileSystem object
* @param conf Configuration object
* @param name Path for the file
* @param keyClass Class type for keys
* @param valClass Class type for values
* @param compressionType Compression type for output
* @param progress Progressable object for tracking progress
* @return Writer instance
* @throws IOException if an I/O error occurs
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param progress The Progressable object to track progress.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",382,392,"/**
* Deprecated method for creating a writer with specified parameters.
* @param fs FileSystem instance
* @param conf Configuration settings
* @param name Path to the file
* @param keyClass Class type for keys
* @param valClass Class type for values
* @param compressionType Compression type
* @param codec Compression codec
* @return Writer object
* @throws IOException if an I/O error occurs
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",410,423,"/**
* Deprecated method for creating a writer.
* @param fs file system object
* @param conf configuration settings
* @param name path to the file
* @param keyClass class of keys
* @param valClass class of values
* @param compressionType type of compression
* @param codec compression codec
* @param progress progress tracker
* @param metadata associated metadata
* @return Writer object for writing data
* @throws IOException if an I/O error occurs
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",444,461,"/**
* Creates a new writer for writing to a file system.
* @param fs FileSystem instance
* @param conf Configuration settings
* @param name Path of the file to write
* @param keyClass Class type for keys
* @param valClass Class type for values
* @param bufferSize Size of buffer
* @param replication Replication factor
* @param blockSize Block size
* @param compressionType Compression type
* @param codec Compression codec
* @param progress Progressable object
* @param metadata Metadata for the file
* @return Writer instance for writing to the file system
* @throws IOException if an I/O error occurs
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param bufferSize buffer size for the underlaying outputstream.
   * @param replication replication factor for the file.
   * @param blockSize block size for the file.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",539,551,"/**
* Creates a Writer instance with specified parameters.
* @param fs FileSystem object
* @param conf Configuration settings
* @param name Path to the file
* @param keyClass Class type for keys
* @param valClass Class type for values
* @param compressionType Type of compression
* @param codec Compression codec
* @param progress Progressable callback
* @return Writer object
* @throws IOException if an I/O error occurs
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)",567,577,"/**
* Creates a deprecated Writer instance.
* @param conf configuration settings
* @param out file data output stream
* @param keyClass class of the key
* @param valClass class of the value
* @param compressionType type of compression
* @param codec compression codec
* @param metadata additional metadata
* @return Writer object
* @throws IOException if an I/O error occurs
*/","* Construct the preferred type of 'raw' SequenceFile Writer.
   * @param conf The configuration.
   * @param out The stream on top which the writer is to be constructed.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",592,600,"/**
* Deprecated method to create a Writer instance.
* @param conf Configuration object
* @param out FSDataOutputStream for output
* @param keyClass Class of the key
* @param valClass Class of the value
* @param compressionType Compression type
* @param codec Compression codec
* @return Writer instance
* @throws IOException if an I/O error occurs
*/","* Construct the preferred type of 'raw' SequenceFile Writer.
   * @param conf The configuration.
   * @param out The stream on top which the writer is to be constructed.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,createJarWithClassPath,"org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,java.util.Map)",1632,1635,"/**
* Calls another m1 method with adjusted parameters.
* @param inputClassPath path to the class file
* @param pwd current working directory
* @param callerEnv environment variables for the caller
* @return array of strings as result from the other m1 method
* @throws IOException if an I/O error occurs
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,<init>,"org.apache.hadoop.util.ApplicationClassLoader:<init>(java.lang.String,java.lang.ClassLoader,java.util.List)",102,105,"/**
* Constructs an ApplicationClassLoader with specified classpath and system classes.
* @param classpath the classpath string to be parsed
* @param parent the parent ClassLoader
* @param systemClasses list of system class names
* @throws MalformedURLException if malformed URL is encountered in classpath
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,validateFiles,org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String),405,407,"/**
* Calls overloaded method with default false flag.
* @param files comma-separated file paths
* @return processed result as string
*/","* Takes input as a comma separated list of files
   * and verifies if they exist. It defaults for file:///
   * if the files specified do not have a scheme.
   * it returns the paths uri converted defaulting to file:///.
   * So an input of  /home/user/file1,/home/user/file2 would return
   * file:///home/user/file1,file:///home/user/file2.
   *
   * This method does not recognize wildcards.
   *
   * @param files the input files argument
   * @return a comma-separated list of validated and qualified paths, or null
   * if the input files argument is null",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRead,org.apache.hadoop.ipc.Server$Listener:doRead(java.nio.channels.SelectionKey),1641,1671,"/**
* Processes a connection using a selection key.
* Handles exceptions and updates connection status.
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,sendLogLevelRequest,org.apache.hadoop.log.LogLevel$CLI:sendLogLevelRequest(),126,139,"/**
* Executes operation based on input.
* @throws HadoopIllegalArgumentException if invalid operation
* @throws Exception for other errors
*/","* Send HTTP/HTTPS request to the daemon.
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,create,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",807,817,"/**
* Creates a DataBlock for file upload.
* @param index unique identifier for the data block
* @param limit size limit for the data block
* @param statistics upload statistics tracker
* @return DataBlock object representing the file on disk
*/","* Create a temp file and a {@link DiskBlock} instance to manage it.
     *
     * @param index      block index.
     * @param limit      limit of the block.
     * @param statistics statistics to update.
     * @return the new block.
     * @throws IOException IO problems",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getTempFilePath,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getTempFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",649,658,"/**
 * Creates a temporary file in the local directory.
 * @param conf configuration settings
 * @param localDirAllocator allocator for local directories
 * @return Path to the created temporary file
 * @throws IOException if an I/O error occurs
 */","* Create temporary file based on the file path retrieved from local dir allocator
   * instance. The file is created with .bin suffix. The created file has been granted
   * posix file permissions available in TEMP_FILE_ATTRS.
   *
   * @param conf the configuration.
   * @param localDirAllocator the local dir allocator instance.
   * @return path of the file created.
   * @throws IOException if IO error occurs while local dir allocator tries to retrieve path
   * from local FS or file creation fails or permission set fails.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,run,org.apache.hadoop.fs.FsShell:run(java.lang.String[]),300,351,"/**
* Executes shell command based on input arguments.
* @param argv array of command-line arguments
* @return exit code of the executed command
*/",* run,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",112,114,"/**
 * Constructs a new Reader instance.
 * @param fs FileSystem object
 * @param dirName directory name
 * @param conf Configuration object
 * @throws IOException if an I/O error occurs
 */","* Construct a set reader for the named set.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param conf input Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",219,223,"/**
* Constructs a Reader instance.
* @param fs FileSystem object
* @param dirName directory name as string
* @param conf Configuration object
* @throws IOException if an I/O error occurs
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,boolean)",225,229,"/**
* Deprecated constructor for Reader.
* @param fs FileSystem instance
* @param dirName directory name
* @param comparator WritableComparator for sorting
* @param conf Configuration settings
* @param open flag to indicate if the reader should be opened
* @throws IOException if an I/O error occurs
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",231,235,"/**
* Constructs a new Reader instance.
* @param fs FileSystem object
* @param dirName directory name to read from
* @param comparator custom comparator for sorting
* @param conf configuration settings
* @throws IOException if an I/O error occurs
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:merge(),3623,3723,"/**
 * Merges segments into a single raw key-value iterator.
 * @throws IOException if an I/O error occurs
 * @return RawKeyValueIterator for merged data
 */","This is the single level merge that is called multiple times 
       * depending on the factor size and the number of segments
       * @return RawKeyValueIterator
       * @throws IOException",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,run,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:run(boolean),3100,3179,"/**
 * Processes input files to generate segments.
 * @param deleteInput flag to delete processed files
 * @return number of segments generated
 * @throws IOException if an I/O error occurs
 */",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.io.SequenceFile$CompressionType)",82,89,"/**
* Initializes a Writer for sequence files.
* @param conf Hadoop configuration
* @param fs FileSystem instance
* @param dirName directory name for output
* @param comparator custom comparator for keys
* @param compress compression type for files
* @throws IOException if an I/O error occurs
*/","* Create a set naming the element comparator and compression type.
     *
     * @param conf input Configuration.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param comparator input comparator.
     * @param compress input compress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])",158,164,"/**
* Initializes a Writer with configuration, directory, and options.
* @param conf Hadoop configuration
* @param dir output directory path
* @param options SequenceFile writer options
* @throws IOException if initialization fails
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)",50,55,"/**
* Initializes a Writer for writing data to a file.
* @param conf configuration settings
* @param fs FileSystem instance
* @param file path of the output file
* @param valClass class type of the writable values
* @throws IOException if an I/O error occurs
*/","* Create the named file for values of the named class.
     *
     * @param conf configuration.
     * @param fs file system.
     * @param file file.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",68,77,"/**
 * Constructs a Writer for writing data to a file.
 * @param conf Hadoop configuration
 * @param fs FileSystem instance
 * @param file Output file path
 * @param valClass Class of the value to be written
 * @param compress Compression type
 * @param progress Progressable object for reporting progress
 * @throws IOException if an I/O error occurs
 */","* Create the named file for values of the named class.
     *
     * @param conf configuration.
     * @param fs file system.
     * @param file file.
     * @param valClass valClass.
     * @param compress compress.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)",112,117,"/**
* Deprecated constructor for creating a Writer.
* @param conf Hadoop configuration object
* @param fs FileSystem instance
* @param dirName directory name for output
* @param keyClass class type of the keys
* @param valClass class type of the values
* @throws IOException if an I/O error occurs
*/","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",132,139,"/**
 * Deprecated constructor for Writer.
 * @param conf Hadoop configuration
 * @param fs FileSystem instance
 * @param dirName output directory name
 * @param keyClass key class type
 * @param valClass value class type
 * @param compress compression type
 * @param progress progressable object
 * @throws IOException if an I/O error occurs
 */","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",155,162,"/**
* Constructs a Writer instance.
* @param conf Hadoop configuration
* @param fs FileSystem to use
* @param dirName output directory name
* @param keyClass key class type
* @param valClass value class type
* @param compress compression type
* @param codec compression codec
* @param progress progressable object
* @throws IOException if an I/O error occurs
*/","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @param codec codec.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",175,181,"/**
* Deprecated constructor for Writer.
* @param conf Hadoop configuration
* @param fs FileSystem instance
* @param dirName directory name for output
* @param keyClass key class type
* @param valClass value class type
* @param compress compression type
* @throws IOException if an I/O error occurs
*/","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)",192,198,"/**
* Constructs a Writer instance.
* @param conf Hadoop configuration
* @param fs FileSystem object
* @param dirName output directory name
* @param comparator custom comparator for sorting
* @param valClass class of the values to be written
* @throws IOException if an I/O error occurs
*/","Create the named map using the named key comparator. 
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",210,216,"/**
* Deprecated constructor for Writer.
* @param conf Configuration object
* @param fs FileSystem object
* @param dirName directory name for output
* @param comparator WritableComparator for key comparison
* @param valClass class of values
* @param compress compression type for sequence file
* @throws IOException if an I/O error occurs
*/","Create the named map using the named key comparator.
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress compress.
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",231,239,"/**
* Deprecated constructor for Writer.
* @param conf configuration settings
* @param fs file system to write to
* @param dirName directory name
* @param comparator key comparator
* @param valClass value class type
* @param compress compression type
* @param progress progressable object
* @throws IOException if an I/O error occurs
*/","* Create the named map using the named key comparator.
     * @deprecated Use Writer(Configuration, Path, Option...)} instead.
     *
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress CompressionType.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",255,263,"/**
* Deprecated constructor for Writer.
* @param conf Hadoop configuration
* @param fs FileSystem instance
* @param dirName output directory name
* @param comparator WritableComparator for key comparison
* @param valClass value class type
* @param compress compression type
* @param codec compression codec
* @param progress Progressable object
* @throws IOException if an I/O error occurs
*/","* Create the named map using the named key comparator.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress CompressionType.
     * @param codec codec.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,open,"org.apache.hadoop.io.MapFile$Merger:open(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",1059,1090,"/**
* Initializes readers for input map files and a writer for the output map file.
* @param inMapFiles array of input map file paths
* @param outMapFile path for the output map file
* @throws IOException if an I/O error occurs
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",513,522,"/**
* Creates a Writer for a file in HDFS.
* @param fc FileContext object
* @param conf Configuration settings
* @param name Path to the file
* @param keyClass Class of keys
* @param valClass Class of values
* @param compressionType Compression type for the file
* @param codec Compression codec to use
* @param metadata Metadata for the file
* @param createFlag Flags for file creation
* @param opts Additional options for file creation
* @return Writer object for writing to the file
* @throws IOException if an I/O error occurs
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fc The context for the specified file.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @param createFlag gives the semantics of create: overwrite, append etc.
   * @param opts file creation options; see {@link CreateOpts}.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Classpath.java,main,org.apache.hadoop.util.Classpath:main(java.lang.String[]),64,113,"/**
* Handles command-line arguments for masking operations.
* @param args command-line arguments array
*/","* Main entry point.
   *
   * @param args command-line arguments",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,createClassLoader,"org.apache.hadoop.util.RunJar:createClassLoader(java.io.File,java.io.File)",344,383,"/**
* Creates a ClassLoader based on the provided file and work directory.
* @param file main classpath file
* @param workDir working directory for classpath
* @return configured ClassLoader
* @throws MalformedURLException if URL formation fails
*/","* Creates a classloader based on the environment that was specified by the
   * user. If HADOOP_USE_CLIENT_CLASSLOADER is specified, it creates an
   * application classloader that provides the isolation of the user class space
   * from the hadoop classes and their dependencies. It forms a class space for
   * the user jar as well as the HADOOP_CLASSPATH. Otherwise, it creates a
   * classloader that simply adds the user jar to the classpath.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,processGeneralOptions,org.apache.hadoop.util.GenericOptionsParser:processGeneralOptions(org.apache.commons.cli.CommandLine),291,364,"/**
* Parses and processes command-line options for configuration.
* @param line CommandLine object containing user input
*/","* Modify configuration according user-specified generic options.
   *
   * @param line User-specified generic options",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRunLoop,org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop(),1486,1527,"/**
 * Handles incoming connections and reads data.
 * Continuously processes pending connections and selectors.
 */",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,run,org.apache.hadoop.log.LogLevel$CLI:run(java.lang.String[]),109,119,"/**
 * Executes methods with error handling.
 * @param args array of arguments for method m2
 * @return 0 on success, -1 if HadoopIllegalArgumentException occurs
 */",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getCacheFilePath,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getCacheFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",496,500,"/**
 * Generates a masked file path.
 * @param conf configuration settings
 * @param localDirAllocator directory allocator
 * @return Path object representing the masked file path
 * @throws IOException if an I/O error occurs
 */","* Return temporary file created based on the file path retrieved from local dir allocator.
   *
   * @param conf The configuration object.
   * @param localDirAllocator Local dir allocator instance.
   * @return Path of the temporary file created.
   * @throws IOException if IO error occurs while local dir allocator tries to retrieve path
   * from local FS or file creation fails or permission set fails.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,isCacheSpaceAvailable,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:isCacheSpaceAvailable(long,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",621,633,"/**
 * Checks if sufficient cache space is available.
 * @param fileSize size of the file to be cached
 * @param conf configuration settings
 * @param localDirAllocator allocator for local directories
 * @return true if enough space, false otherwise
 */","* Determine if the cache space is available on the local FS.
   *
   * @param fileSize The size of the file.
   * @param conf The configuration.
   * @param localDirAllocator Local dir allocator instance.
   * @return True if the given file size is less than the available free space on local FS,
   * False otherwise.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(java.util.List,org.apache.hadoop.fs.Path)",3313,3319,"/**
 * Merges segment data into a single iterator.
 * @param segments list of segment descriptors to merge
 * @param tmpDir temporary directory for intermediate files
 * @return RawKeyValueIterator over merged data
 * @throws IOException if an I/O error occurs during merging
 */","* Merges the list of segments of type <code>SegmentDescriptor</code>
     * @param segments the list of SegmentDescriptors
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIterator
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,int,org.apache.hadoop.fs.Path)",3349,3364,"/**
* Merges input files into a single iterator.
* @param inNames array of input file paths
* @param deleteInputs flag to delete input files after processing
* @param factor merge factor
* @param tmpDir temporary directory for intermediate files
* @return RawKeyValueIterator over merged data
* @throws IOException if an I/O error occurs
*/","* Merges the contents of files passed in Path[]
     * @param inNames the array of path names
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @param factor the factor that will be used as the maximum merge fan-in
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3375,3394,"/**
* Merges input files into a single output file.
* @param inNames array of input file paths
* @param tempDir temporary directory for merging
* @param deleteInputs flag to delete input files after merge
* @return iterator over merged key-value pairs
*/","* Merges the contents of files passed in Path[]
     * @param inNames the array of path names
     * @param tempDir the directory for creating temp files during merge
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",3479,3489,"/**
* Initializes and returns a RawKeyValueIterator for merging segments.
* @param inName input file path
* @param indexIn index file path
* @param tmpDir temporary directory for merge operations
* @return RawKeyValueIterator for merged data
* @throws IOException if an I/O error occurs
*/","Used by mergePass to merge the output of the sort
     * @param inName the name of the input file containing sorted segments
     * @param indexIn the offsets of the sorted segments
     * @param tmpDir the relative directory to store intermediate results in
     * @return RawKeyValueIterator
     * @throws IOException",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sortPass,org.apache.hadoop.io.SequenceFile$Sorter:sortPass(boolean),3064,3076,"/**
* Executes a sort pass and returns the result.
* @param deleteInput flag to delete input after processing
* @return result of the sort operation
* @throws IOException if an I/O error occurs during sorting
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",65,70,"/**
* Initializes a Writer with specified configuration and compression.
* @param conf Hadoop configuration
* @param fs FileSystem instance
* @param dirName output directory name
* @param keyClass class of the key
* @param compress compression type for sequence files
* @throws IOException if an I/O error occurs
*/","* Create a set naming the element class and compression type.
     *
     * @param conf input Configuration.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param keyClass input keyClass.
     * @param compress input compress.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",90,97,"/**
* Constructs a Writer instance with specified configuration and parameters.
* @param conf Hadoop configuration
* @param fs FileSystem to use
* @param dirName directory name for output
* @param keyClass class of the key
* @param valClass class of the value
* @param compress compression type
* @param codec compression codec
* @param progress progressable callback
* @throws IOException if an I/O error occurs
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",99,106,"/**
* Constructs a Writer instance with specified parameters.
* @deprecated Use the newer constructor instead.
* @param conf Hadoop configuration
* @param fs FileSystem to write to
* @param dirName directory name for output
* @param keyClass class of keys
* @param valClass class of values
* @param compress compression type
* @param progress progress tracker
* @throws IOException if an I/O error occurs
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",108,115,"/**
* Deprecated constructor for Writer.
* @param conf Hadoop configuration
* @param fs FileSystem instance
* @param dirName directory name
* @param keyClass key class type
* @param valClass value class type
* @param compress compression type
* @throws IOException if an I/O error occurs
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",117,125,"/**
* Constructs a Writer instance.
* @deprecated Use the alternative constructor instead.
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",127,134,"/**
* Constructs a Writer instance with specified configuration and parameters.
* @param conf Hadoop configuration
* @param fs FileSystem to use
* @param dirName directory name for output
* @param comparator custom WritableComparator
* @param valClass value class type
* @param compress compression type
* @param progress Progressable object for tracking progress
* @throws IOException if an I/O error occurs
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",136,142,"/**
* Deprecated constructor for Writer.
* @param conf Hadoop configuration
* @param fs FileSystem instance
* @param dirName directory name for output
* @param comparator WritableComparator for key sorting
* @param valClass class of value objects
* @param compress compression type for output
* @throws IOException if an I/O error occurs
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)",144,149,"/**
* Deprecated constructor for Writer.
* @param conf Configuration object
* @param fs FileSystem object
* @param dirName directory name
* @param comparator WritableComparator instance
* @param valClass value class type
* @throws IOException if an I/O error occurs
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)",151,156,"/**
* Deprecated constructor for initializing a Writer.
* @param conf Configuration object
* @param fs FileSystem object
* @param dirName directory name for output
* @param keyClass class of the key
* @param valClass class of the value
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,main,org.apache.hadoop.io.MapFile:main(java.lang.String[]),1160,1191,"/**
* Copies a MapFile from one location to another.
* @param args input and output file paths
* @throws Exception if an I/O error occurs
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)",50,53,"/**
* Initializes a Writer to FileSystem with specified directory and key class.
* @param fs FileSystem instance
* @param dirName directory name for output
* @param keyClass key class type
* @throws IOException if initialization fails
*/","* Create the named set for keys of the named class.
     * @deprecated pass a Configuration too
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param keyClass input keyClass.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,merge,"org.apache.hadoop.io.MapFile$Merger:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)",1039,1053,"/**
* Processes input map files and writes to output file.
* @param inMapFiles array of input map files
* @param deleteInputs flag to delete input files after processing
* @param outMapFile output map file path
* @throws IOException if an I/O error occurs
*/","* Merge multiple MapFiles to one Mapfile.
     *
     * @param inMapFiles input inMapFiles.
     * @param deleteInputs deleteInputs.
     * @param outMapFile input outMapFile.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,boolean,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)",480,496,"/**
* Creates a writer for writing to a file in the specified filesystem.
* @param fs FileSystem instance
* @param conf Configuration settings
* @param name Path of the file to create
* @param keyClass Class type for keys
* @param valClass Class type for values
* @param bufferSize Size of buffer for data transfer
* @param replication Replication factor for the file
* @param blockSize Block size for the file
* @param createParent Whether to create parent directories if they don't exist
* @param compressionType Type of compression to use
* @param codec Compression codec to use
* @param metadata Metadata for the file
* @return Writer object for writing to the file
* @throws IOException if an I/O error occurs
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param bufferSize buffer size for the underlaying outputstream.
   * @param replication replication factor for the file.
   * @param blockSize block size for the file.
   * @param createParent create parent directory if non-existent
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,run,org.apache.hadoop.util.RunJar:run(java.lang.String[]),248,334,"/**
 * Executes a JAR file with specified arguments.
 * @param args command-line arguments including JAR file and optional main class
 */",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,parseGeneralOptions,"org.apache.hadoop.util.GenericOptionsParser:parseGeneralOptions(org.apache.commons.cli.Options,java.lang.String[])",572,588,"/**
* Parses command-line arguments using provided options.
* @param opts command-line options configuration
* @param args array of command-line arguments
* @return true if parsing successful, false otherwise
*/","* Parse the user-specified options, get the generic options, and modify
   * configuration accordingly.
   *
   * @param opts Options to use for parsing args.
   * @param args User-specified arguments
   * @return true if the parse was successful",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Listener$Reader:run(),1472,1484,"/**
* Logs start, executes main function, and ensures read selector is closed.
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,put,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:put(int,java.nio.ByteBuffer,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",370,413,"/**
 * Processes a block by reading or writing data.
 * @param blockNumber identifier for the block
 * @param buffer data buffer
 * @param conf configuration settings
 * @param localDirAllocator directory allocator for local storage
 * @throws IOException if an I/O error occurs
 */","* Puts the given block in this cache.
   *
   * @param blockNumber the block number, used as a key for blocks map.
   * @param buffer buffer contents of the given block to be added to this cache.
   * @param conf the configuration.
   * @param localDirAllocator the local dir allocator instance.
   * @throws IOException if either local dir allocator fails to allocate file or if IO error
   * occurs while writing the buffer content to the file.
   * @throws IllegalArgumentException if buffer is null, or if buffer.limit() is zero or negative.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)",3331,3337,"/**
* Initializes a raw key-value iterator.
* @param inNames input file paths
* @param deleteInputs flag to delete input files after processing
* @param tmpDir temporary directory path
* @return RawKeyValueIterator instance
*/","* Merges the contents of files passed in Path[] using a max factor value
     * that is already set
     * @param inNames the array of path names
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,mergePass,org.apache.hadoop.io.SequenceFile$Sorter:mergePass(org.apache.hadoop.fs.Path),3458,3470,"/**
* Performs a merge pass on sorted data files.
* @param tmpDir temporary directory for processing
* @return 0 indicating successful completion
*/",sort calls this to generate the final merged output,,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,main,org.apache.hadoop.util.RunJar:main(java.lang.String[]),244,246,"/**
* Runs a jar file with provided arguments.
* @param args command-line arguments to pass to the jar
*/","Run a Hadoop job jar.  If the main class is not in the jar's manifest,
   * then it must be provided on the command line.
   *
   * @param args args.
   * @throws Throwable error.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])",178,182,"/**
* Parses command-line arguments using specified configuration and options.
* @param conf Configuration object for parsing
* @param options Set of options to be parsed from arguments
* @param args Command-line arguments array
* @throws IOException if an I/O error occurs during parsing
*/","* Create a <code>GenericOptionsParser</code> to parse given options as well 
   * as generic Hadoop options. 
   * 
   * The resulting <code>CommandLine</code> object can be obtained by 
   * {@link #getCommandLine()}.
   * 
   * @param conf the configuration to modify  
   * @param options options built by the caller 
   * @param args User-specified arguments
   * @throws IOException raised on errors performing I/O.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sortAndIterate,"org.apache.hadoop.io.SequenceFile$Sorter:sortAndIterate(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3032,3052,"/**
* Merges input files into a single output file and returns an iterator.
* @param inFiles array of input file paths
* @param tempDir temporary directory for intermediate files
* @param deleteInput flag to delete input files after processing
* @return RawKeyValueIterator for the merged data
* @throws IOException if an I/O error occurs during processing
*/","* Perform a file sort from a set of input files and return an iterator.
     * @param inFiles the files to be sorted
     * @param tempDir the directory where temp files are created during sort
     * @param deleteInput should the input files be deleted as they are read?
     * @return iterator the RawKeyValueIterator
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",3445,3455,"/**
 * Processes input files and writes output to a specified file.
 * @param inFiles array of input file paths
 * @param outFile path of the output file
 * @throws IOException if output file already exists or other I/O errors occur
 */","Merge the provided files.
     * @param inFiles the array of input path names
     * @param outFile the final output file
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,"org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3009,3022,"/**
 * Merges input files into an output file.
 * @param inFiles array of input file paths
 * @param outFile path to the output file
 * @param deleteInput flag to delete input files after processing
 * @throws IOException if output file already exists or other I/O errors occur
 */","* Perform a file sort from a set of input files into an output file.
     * @param inFiles the files to be sorted
     * @param outFile the sorted output file
     * @param deleteInput should the input files be deleted as they are read?
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,<init>,"org.apache.hadoop.service.launcher.ServiceLauncher$MinimalGenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])",1081,1084,"/**
* Initializes parser with configuration, options, and arguments.
* @param conf Configuration object
* @param options Command-line options
* @param args Command-line arguments
* @throws IOException if I/O error occurs during parsing
*/",,,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.commons.cli.Options,java.lang.String[])",135,138,"/**
* Constructs a new GenericOptionsParser with default configuration.
* @param opts command-line options
* @param args command-line arguments
* @throws IOException if an I/O error occurs
*/","* Create an options parser with the given options to parse the args.
   * @param opts the options
   * @param args the command line arguments
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,org.apache.hadoop.util.GenericOptionsParser:<init>(java.lang.String[]),145,148,"/**
 * Initializes parser with default configuration and options.
 * @param args command-line arguments to parse
 * @throws IOException if an I/O error occurs during parsing
 */","* Create an options parser to parse the args.
   * @param args the command line arguments
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,java.lang.String[])",161,164,"/**
* Initializes parser with configuration and command-line arguments.
* @param conf Configuration object
* @param args Command-line arguments array
* @throws IOException if an I/O error occurs
*/","* Create a <code>GenericOptionsParser</code> to parse only the generic
   * Hadoop arguments.
   * 
   * The array of string arguments other than the generic arguments can be 
   * obtained by {@link #getRemainingArgs()}.
   * 
   * @param conf the <code>Configuration</code> to modify.
   * @param args command-line arguments.
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,"org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",3060,3062,"/**
 * Copies content from one file to another.
 * @param inFile source file path
 * @param outFile destination file path
 * @throws IOException if an I/O error occurs
 */","* The backwards compatible interface to sort.
     * @param inFile the input file to sort.
     * @param outFile the sorted output file.
     * @throws IOException raised on errors performing I/O.",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,createGenericOptionsParser,"org.apache.hadoop.service.launcher.ServiceLauncher:createGenericOptionsParser(org.apache.hadoop.conf.Configuration,java.lang.String[])",979,982,"/**
* Parses command line arguments using configuration.
* @param conf Configuration object
* @param argArray Command line arguments array
* @return GenericOptionsParser instance
* @throws IOException if parsing fails
*/","* Override point: create a generic options parser or subclass thereof.
   * @param conf Hadoop configuration
   * @param argArray array of arguments
   * @return a generic options parser to parse the arguments
   * @throws IOException on any failure",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,main,org.apache.hadoop.util.ConfTest:main(java.lang.String[]),227,300,"/**
* Validates configuration files or directory.
* @param args command line arguments
* @throws IOException on input/output errors
*/",,,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,run,"org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])",62,83,"/**
* Executes a tool with configuration and arguments.
* @param conf configuration object, may be null
* @param tool the tool to execute
* @param args command-line arguments
* @return exit code from tool execution
*/","* Runs the given <code>Tool</code> by {@link Tool#run(String[])}, after 
   * parsing with the given generic arguments. Uses the given 
   * <code>Configuration</code>, or builds one if null.
   * 
   * Sets the <code>Tool</code>'s configuration with the possibly modified 
   * version of the <code>conf</code>.  
   * 
   * @param conf <code>Configuration</code> for the <code>Tool</code>.
   * @param tool <code>Tool</code> to run.
   * @param args command-line arguments to the tool.
   * @return exit code of the {@link Tool#run(String[])} method.
   * @throws Exception Exception.",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,parseCommandArgs,"org.apache.hadoop.service.launcher.ServiceLauncher:parseCommandArgs(org.apache.hadoop.conf.Configuration,java.util.List)",917,970,"/**
 * Parses command-line arguments and updates configuration.
 * @param conf Configuration object to update
 * @param args List of command-line arguments
 * @return List of remaining arguments after parsing
 */","* Parse the command arguments, extracting the service class as the last
   * element of the list (after extracting all the rest).
   *
   * The field {@link #commandOptions} field must already have been set.
   * @param conf configuration to use
   * @param args command line argument list
   * @return the remaining arguments
   * @throws ServiceLaunchException if processing of arguments failed",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,exec,"org.apache.hadoop.security.KDiag:exec(org.apache.hadoop.conf.Configuration,java.lang.String[])",1052,1056,"/**
* Executes a diagnostic tool with configuration and arguments.
* @param conf Configuration object for the tool
* @param argv Variable arguments for the tool execution
* @return Exit status code from the tool execution
* @throws Exception if an error occurs during execution
*/","* Inner entry point, with no logging or system exits.
   *
   * @param conf configuration
   * @param argv argument list
   * @return an exception
   * @throws Exception Exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,main,org.apache.hadoop.security.token.DtUtilShell:main(java.lang.String[]),393,395,"/**
 * Executes shell command with provided arguments.
 * @param args command-line arguments
 */",,,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,main,org.apache.hadoop.security.alias.CredentialShell:main(java.lang.String[]),534,537,"/**
 * Executes a credential shell command.
 * @param args command-line arguments
 * @throws Exception if execution fails
 */","* Main program.
   *
   * @param args
   *          Command line arguments
   * @throws Exception exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,main,org.apache.hadoop.crypto.key.KeyShell:main(java.lang.String[]),552,555,"/**
* Executes a key shell command with provided arguments.
* @param args command-line arguments for the key shell
*/","* main() entry point for the KeyShell.  While strictly speaking the
   * return is void, it will System.exit() with a return code: 0 is for
   * success and 1 for failure.
   *
   * @param args Command line arguments.
   * @throws Exception raised on errors performing I/O.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,run,"org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.util.Tool,java.lang.String[])",95,98,"/**
* Recursively calls m2 with modified arguments.
* @param tool Tool instance
* @param args Array of string arguments
* @return Result of recursive call
*/","* Runs the <code>Tool</code> with its <code>Configuration</code>.
   * 
   * Equivalent to <code>run(tool.getConf(), tool, args)</code>.
   * 
   * @param tool <code>Tool</code> to run.
   * @param args command-line arguments to the tool.
   * @return exit code of the {@link Tool#run(String[])} method.
   * @throws Exception exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,extractCommandOptions,"org.apache.hadoop.service.launcher.ServiceLauncher:extractCommandOptions(org.apache.hadoop.conf.Configuration,java.util.List)",896,905,"/**
* Masks arguments based on configuration.
* @param conf Configuration object
* @param args List of input arguments
* @return Masked list of arguments or empty list if size <= 1
*/","* Extract the command options and apply them to the configuration,
   * building an array of processed arguments to hand down to the service.
   *
   * @param conf configuration to update.
   * @param args main arguments. {@code args[0]}is assumed to be
   * the service classname and is skipped.
   * @return the remaining arguments
   * @throws ExitUtil.ExitException if JVM exiting is disabled.",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,main,org.apache.hadoop.security.KDiag:main(java.lang.String[]),1062,1072,"/**
* Masks input arguments and handles exceptions.
* @param argv command line arguments
*/","* Main entry point.
   * @param argv args list",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,main,org.apache.hadoop.fs.FsShell:main(java.lang.String[]),383,395,"/**
* Executes a file system command with arguments.
* @param argv array of command-line arguments
*/","* main() has some simple utility methods
   * @param argv the command and its arguments
   * @throws Exception upon error",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,main,org.apache.hadoop.log.LogLevel:main(java.lang.String[]),73,76,"/**
* Executes command-line interface with provided arguments.
* @param args command-line arguments
*/","* A command line implementation
   * @param args input args.
   * @throws Exception exception.",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,main,org.apache.hadoop.util.FindClass:main(java.lang.String[]),378,386,"/**
* Executes FindClass with provided arguments.
* @param args command-line arguments for execution
*/","* Main entry point. 
   * Runs the class via the {@link ToolRunner}, then
   * exits with an appropriate exit code. 
   * @param args argument list",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchServiceAndExit,org.apache.hadoop.service.launcher.ServiceLauncher:launchServiceAndExit(java.util.List),281,315,"/**
* Processes command-line arguments, logs details, and handles exceptions.
* @param args list of command-line arguments
*/","* Launch the service and exit.
   *
   * <ol>
   * <li>Parse the command line.</li> 
   * <li>Build the service configuration from it.</li>
   * <li>Start the service.</li>
   * <li>If it is a {@link LaunchableService}: execute it</li>
   * <li>Otherwise: wait for it to finish.</li>
   * <li>Exit passing the status code to the {@link #exit(int, String)}
   * method.</li>
   * </ol>
   * @param args arguments to the service. {@code arg[0]} is 
   * assumed to be the service classname.",,,True,42
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceMain,org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.util.List),1064,1073,"/**
* Processes a list of arguments and launches a service.
* @param argsList list of string arguments
*/",,,,True,43
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,main,org.apache.hadoop.service.launcher.ServiceLauncher:main(java.lang.String[]),1043,1045,"/**
* Masks input arguments and processes them.
* @param args command line arguments to be masked
*/","* This is the JVM entry point for the service launcher.
   *
   * Converts the arguments to a list, then invokes {@link #serviceMain(List)}
   * @param args command line arguments.",,,True,44
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceMain,org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.lang.String[]),1052,1054,"/**
 * Invokes m2 with processed arguments.
 * @param args variable number of string arguments
 */","* Varargs version of the entry point for testing and other in-JVM use.
   * Hands off to {@link #serviceMain(List)}
   * @param args command line arguments.",,,True,44
