file_path,Name,full_name,Start Line,End Line,Comment,Pre_Comment,child Name,domain,inner_method,node_level
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileRange.java,createFileRange,"org.apache.hadoop.fs.FileRange:createFileRange(long,int)",73,75,"/**
 * Creates a FileRange object with the given offset and length.
 */","* Factory method to create a FileRange object.
   * @param offset starting offset of the range.
   * @param length length of the range.
   * @return a new instance of FileRangeImpl.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileRange.java,createFileRange,"org.apache.hadoop.fs.FileRange:createFileRange(long,int,java.lang.Object)",84,86,"/**
 * Creates a FileRange object with given offset, length, and reference.
 */","* Factory method to create a FileRange object.
   * @param offset starting offset of the range.
   * @param length length of the range.
   * @param reference nullable reference to store in the range.
   * @return a new instance of FileRangeImpl.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateRangeRequest,org.apache.hadoop.fs.VectoredReadUtils:validateRangeRequest(org.apache.hadoop.fs.FileRange),65,75,"/**
 * Validates a FileRange object; throws exceptions for invalid state.
 * @param range The FileRange object to validate.
 * @return The same FileRange object if valid.
 */","* Validate a single range.
   * @param range range to validate.
   * @return the range.
   * @param <T> range type
   * @throws IllegalArgumentException the range length is negative or other invalid condition
   * is met other than the those which raise EOFException or NullPointerException.
   * @throws EOFException the range offset is negative
   * @throws NullPointerException if the range is null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNull,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNull(java.lang.Object,java.lang.String)",45,47,"/**
* Checks if the object is null, throws exception if so.
* @param obj The object to check.
* @param argName Name of the argument being checked.
*/","* Validates that the given reference argument is not null.
   * @param obj the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPositiveInteger,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPositiveInteger(long,java.lang.String)",54,56,"/**
 * Validates that the given value is a positive integer.
 * @param value Value to validate.
 * @param argName Name of the argument being validated.
 */","* Validates that the given integer argument is not zero or negative.
   * @param value the argument value to validate
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNegative,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNegative(long,java.lang.String)",63,65,"/**
 * Asserts value is non-negative, throws exception if negative.
 * @param value value to check
 * @param argName name of the argument being checked
 */
","* Validates that the given integer argument is not negative.
   * @param value the argument value to validate
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkRequired,"org.apache.hadoop.fs.impl.prefetch.Validate:checkRequired(boolean,java.lang.String)",72,74,"/**
 * Logs a required argument message if isPresent is false.
 * @param isPresent Flag indicating if argument is present.
 * @param argName Name of the argument.
 */
","* Validates that the expression (that checks a required field is present) is true.
   * @param isPresent indicates whether the given argument is present.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValid,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String)",81,83,"/**
 * Logs an invalid status message using m1.
 * @param isValid Flag indicating validity.
 * @param argName Name of the argument being checked.
 */","* Validates that the expression (that checks a field is valid) is true.
   * @param isValid indicates whether the given argument is valid.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValid,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String,java.lang.String)",91,96,"/**
 * Logs an invalid argument message with provided details.
 * @param isValid Flag indicating validity, argName, validValues
 */","* Validates that the expression (that checks a field is valid) is true.
   * @param isValid indicates whether the given argument is valid.
   * @param argName the name of the argument being validated.
   * @param validValues the list of values that are allowed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValuesEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValuesEqual(long,java.lang.String,long,java.lang.String)",201,213,"/**
 * Logs a message if value1 and value2 are not equal.
 * @param value1 First value.
 * @param value1Name Name of the first value.
 * @param value2 Second value.
 * @param value2Name Name of the second value.
 */
","* Validates that the given two values are equal.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkIntegerMultiple,"org.apache.hadoop.fs.impl.prefetch.Validate:checkIntegerMultiple(long,java.lang.String,long,java.lang.String)",222,234,"/**
 * Checks if value1 is a multiple of value2 and logs a message if not.
 * @param value1 The first long value.
 * @param value1Name Name of the first value.
 * @param value2 The second long value.
 * @param value2Name Name of the second value.
 */","* Validates that the first value is an integer multiple of the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkGreater,"org.apache.hadoop.fs.impl.prefetch.Validate:checkGreater(long,java.lang.String,long,java.lang.String)",243,255,"/**
 * Checks if value1 is greater than value2 and logs a message if not.
 * @param value1 First value.
 * @param value1Name Name of the first value.
 * @param value2 Second value.
 * @param value2Name Name of the second value.
 */
","* Validates that the first value is greater than the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkGreaterOrEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkGreaterOrEqual(long,java.lang.String,long,java.lang.String)",264,276,"/**
 * Checks if value1 >= value2 and logs a message if not.
 * @param value1 First long value.
 * @param value1Name Name of value1.
 * @param value2 Second long value.
 * @param value2Name Name of value2.
 */
","* Validates that the first value is greater than or equal to the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkLessOrEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkLessOrEqual(long,java.lang.String,long,java.lang.String)",285,297,"/**
* Checks if value1 <= value2 and logs a message if not.
* @param value1 The first value.
* @param value1Name Name of the first value.
* @param value2 The second value.
* @param value2Name Name of the second value.
*/
","* Validates that the first value is less than or equal to the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkWithinRange,"org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(long,java.lang.String,long,long)",306,318,"/**
 * Validates value is within the specified inclusive range.
 * @param value Value to validate.
 * @param valueName Name of the value.
 */
","* Validates that the given value is within the given range of values.
   * @param value the value to check.
   * @param valueName the name of the argument.
   * @param minValueInclusive inclusive lower limit for the value.
   * @param maxValueInclusive inclusive upper limit for the value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkWithinRange,"org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(double,java.lang.String,double,double)",327,339,"/**
 * Validates value is within the specified inclusive range.
 * @param value Value to validate.
 * @param valueName Name of the value.
 */
","* Validates that the given value is within the given range of values.
   * @param value the value to check.
   * @param valueName the name of the argument.
   * @param minValueInclusive inclusive lower limit for the value.
   * @param maxValueInclusive inclusive upper limit for the value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotEmpty(int,java.lang.String)",393,398,"/**
 * Checks if arraySize is positive; throws exception if not.
 * @param arraySize Size of the array.
 * @param argName Name of the array.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BulkDeleteUtils.java,validateBulkDeletePaths,"org.apache.hadoop.fs.BulkDeleteUtils:validateBulkDeletePaths(java.util.Collection,int,org.apache.hadoop.fs.Path)",39,48,"/**
 * Validates paths, checks size, and ensures they are absolute & under basePath.
 */","* Preconditions for bulk delete paths.
   * @param paths paths to delete.
   * @param pageSize maximum number of paths to delete in a single operation.
   * @param basePath base path for the delete operation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(java.io.File),177,182,"/**
* Initializes BlockUploadData with the given file.
* @param file The file to be uploaded.
*/
","* File constructor; input stream and byteArray will be null.
     *
     * @param file file to upload",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,requireIOStatisticsSnapshot,org.apache.hadoop.io.wrappedio.WrappedStatistics:requireIOStatisticsSnapshot(java.io.Serializable),352,356,"/**
 * Casts the input to IOStatisticsSnapshot.
 * @param snapshot Serializable object to cast.
 * @return Casted IOStatisticsSnapshot object.
 */","* Require the parameter to be an instance of {@link IOStatisticsSnapshot}.
   * @param snapshot object to validate
   * @return cast value
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,<init>,"org.apache.hadoop.net.SocketInputWrapper:<init>(java.net.Socket,java.io.InputStream)",43,52,"/**
 * Constructs a SocketInputWrapper with a Socket and InputStream.
 * @param s The Socket.
 * @param is The InputStream.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfigurationHelper.java,mapEnumNamesToValues,"org.apache.hadoop.util.ConfigurationHelper:mapEnumNamesToValues(java.lang.String,java.lang.Class)",109,124,"/**
 * Creates a map of lowercase enum names to enum constants.
 * @param prefix Prefix to prepend to lowercase enum names.
 * @param enumClass The enum class to process.
 * @return Map of lowercase enum names to enum constants.
 */
","* Given an enum class, build a map of lower case names to values.
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param enumClass class of enum
   * @param <E> enum type
   * @return a mutable map of lower case names to enum values
   * @throws IllegalArgumentException if there are two entries which differ only by case.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,sortRanges,org.apache.hadoop.fs.VectoredReadUtils:sortRanges(java.util.List),358,361,"/**
* Transforms a list of FileRange objects using m1 and m2.
*/","* Sort the input ranges by offset; no validation is done.
   * <p>
   * This method is used externally and must be retained with
   * the signature unchanged.
   * @param input input ranges.
   * @return a new list of the ranges, sorted by offset.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,perms,org.apache.hadoop.fs.Options$CreateOpts:perms(org.apache.hadoop.fs.permission.FsPermission),65,67,"/**
 * Creates a Perms object from a FsPermission.
 * @param perm The FsPermission to wrap.
 * @return A new Perms object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,seekInternal,org.apache.hadoop.fs.sftp.SFTPInputStream:seekInternal(),79,96,"/**
* Advances the stream position based on nextPos.
* Handles stream wrapping and potential exceptions.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,checkNotClosed,org.apache.hadoop.fs.sftp.SFTPInputStream:checkNotClosed(),135,141,"/**
 * Throws an IOException if the stream is closed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,isParentOf,"org.apache.hadoop.fs.ftp.FTPFileSystem:isParentOf(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",648,657,"/**
 * Checks if child path is within the parent path.
 * @param parent Parent Path object.
 * @param child Child Path object.
 * @return True if child is within parent, false otherwise.
 */
","* Probe for a path being a parent of another
   * @param parent parent path
   * @param child possible child path
   * @return true if the parent's path matches the start of the child's",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,isSameFS,"org.apache.hadoop.fs.FileContext:isSameFS(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2306,2312,"/**
 * Compares paths based on URIs, excluding nested URI comparisons.
 */","* Are qualSrc and qualDst of the same file system?
   * @param qualPath1 - fully qualified path
   * @param qualPath2 - fully qualified path
   * @return is same fs true,not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,deleteOnExit,org.apache.hadoop.fs.FileSystem:deleteOnExit(org.apache.hadoop.fs.Path),1804,1812,"/**
 * Marks a file for deletion after exit.
 * @param f the Path of the file to be marked
 * @throws IOException if an I/O error occurs
 */
","* Mark a path to be deleted when its FileSystem is closed.
   * When the JVM shuts down cleanly, all cached FileSystem objects will be
   * closed automatically. These the marked paths will be deleted as a result.
   *
   * If a FileSystem instance is not cached, i.e. has been created with
   * {@link #createFileSystem(URI, Configuration)}, then the paths will
   * be deleted in when {@link #close()} is called on that instance.
   *
   * The path must exist in the filesystem at the time of the method call;
   * it does not have to exist at the time of JVM shutdown.
   *
   * Notes
   * <ol>
   *   <li>Clean shutdown of the JVM cannot be guaranteed.</li>
   *   <li>The time to shut down a FileSystem will depends on the number of
   *   files to delete. For filesystems where the cost of checking
   *   for the existence of a file/directory and the actual delete operation
   *   (for example: object stores) is high, the time to shutdown the JVM can be
   *   significantly extended by over-use of this feature.</li>
   *   <li>Connectivity problems with a remote filesystem may delay shutdown
   *   further, and may cause the files to not be deleted.</li>
   * </ol>
   * @param f the path to delete.
   * @return  true if deleteOnExit is successful, otherwise false.
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,processDeleteOnExit,org.apache.hadoop.fs.FileSystem:processDeleteOnExit(),1833,1848,"/**
 * Deletes paths from the 'deleteOnExit' list, ignoring failures.
 */","* Delete all paths that were marked as delete-on-exit. This recursively
   * deletes all files and directories in the specified paths.
   *
   * The time to process this operation is {@code O(paths)}, with the actual
   * time dependent on the time for existence and deletion operations to
   * complete, successfully or not.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,keystoreExists,org.apache.hadoop.security.alias.KeyStoreProvider:keystoreExists(),58,61,"/**
* Delegates file system operation.
* @return True if successful, false otherwise.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,compareTo,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:compareTo(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode),140,151,"/**
 * Compares status modification times with another node.
 * @param other The other MRNflyNode to compare with.
 * @return Comparison result based on modification times.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getModificationTime,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getModificationTime(),504,507,"/**
 * Delegates the call to the m1() method of the realStatus object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getModificationTime,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getModificationTime(),80,83,"/**
* Delegates the call to the wrapped FileSystem's m1() method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,decodeFileName,org.apache.hadoop.fs.HarFileSystem:decodeFileName(java.lang.String),261,268,"/**
 * Masks a filename based on metadata version.
 * @param fname Filename to mask.
 * @return Masked filename or original if version is not 2/3.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,compareTo,org.apache.hadoop.fs.shell.PathData:compareTo(org.apache.hadoop.fs.shell.PathData),593,596,"/**
* Delegates m1 call to the wrapped Path object.
* @param o PathData object containing the path.
* @return Result of the delegated m1 call.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,isChecksumFile,org.apache.hadoop.fs.ChecksumFs:isChecksumFile(org.apache.hadoop.fs.Path),98,101,"/**
* Checks if a file name ends with "".crc"" after a dot.
* @param file The Path object representing the file.
* @return True if the file name ends with "".crc"", false otherwise.
*/
","* Return true iff file is a checksum file name.
   *
   * @param file the file path.
   * @return if is checksum file true,not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,isChecksumFile,org.apache.hadoop.fs.ChecksumFileSystem:isChecksumFile(org.apache.hadoop.fs.Path),130,133,"/**
* Checks if a file name ends with "".crc"" after splitting by ""."".
*/","* Return true if file is a checksum file name.
   *
   * @param file the file path.
   * @return if file is a checksum file true, not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,fixBlockLocations,"org.apache.hadoop.fs.HarFileSystem:fixBlockLocations(org.apache.hadoop.fs.BlockLocation[],long,long,long)",423,457,"/**
* Adjusts BlockLocation start/end based on given range.
* @param locations BlockLocation array to adjust
* @param start starting offset of the range
* @param len length of the range
* @param fileOffsetInHar file offset in the HAR archive
* @return Adjusted BlockLocation array
*/
","* Fix offset and length of block locations.
   * Note that this method modifies the original array.
   * @param locations block locations of har part file
   * @param start the start of the desired range in the contained file
   * @param len the length of the desired range
   * @param fileOffsetInHar the offset of the desired file in the har part file
   * @return block locations with fixed offset and length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,compareTo,org.apache.hadoop.fs.FileStatus:compareTo(org.apache.hadoop.fs.FileStatus),411,413,"/**
* Delegates to the outer m1().m2() with the file status's ID.
*/","* Compare this FileStatus to another FileStatus based on lexicographical
   * order of path.
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,stat2Paths,org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[]),114,122,"/**
 * Converts FileStatus array to Path array by calling m1() on each.
 * @param stats Array of FileStatus objects.
 * @return Array of Path objects.
 */","* convert an array of FileStatus to an array of Path
   *
   * @param stats
   *          an array of FileStatus objects
   * @return an array of paths corresponding to the input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,addPartFileStatuses,org.apache.hadoop.fs.HarFileSystem$HarMetaData:addPartFileStatuses(org.apache.hadoop.fs.Path),1148,1152,"/**
 * Iterates through files under the given path and processes them.
 * @param path The directory path to iterate through.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,merge,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])",1228,1242,"/**
* Combines two arrays of FileStatus, filtering based on paths.
* @param toStatuses FileStatuses to include initially.
* @param fromStatuses FileStatuses to filter and add.
* @return Combined FileStatus array.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getPath,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPath(),529,532,"/**
 * Delegates the call to m1() to the realStatus object.
 * @return Path object returned by realStatus.m1()
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,merge,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])",1622,1636,"/**
* Combines two arrays of FileStatus, filtering based on paths.
* @param toStatuses FileStatus array to add to the result.
* @param fromStatuses FileStatus array to filter and add.
* @return Combined FileStatus array.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolveIntermediate,org.apache.hadoop.fs.FileContext:resolveIntermediate(org.apache.hadoop.fs.Path),2353,2361,"/**
 * Resolves a path using a custom FileSystem resolver.
 * @param f The path to resolve.
 * @return The resolved path.
 */
","* Resolves all symbolic links in the specified path leading up 
   * to, but not including the final path component.
   * @param f path to resolve
   * @return the new path object.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getReplication,org.apache.hadoop.fs.FileSystem:getReplication(org.apache.hadoop.fs.Path),1603,1606,"/**
* Delegates to m1(src) and returns the result of m1's m2() call.
*/","* Get the replication factor.
   *
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @param src file name
   * @return file replication
   * @throws FileNotFoundException if the path does not resolve.
   * @throws IOException an IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getReplication,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getReplication(),499,502,"/**
* Delegates the call to the m1() method of the realStatus object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getReplication,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getReplication(),75,78,"/**
* Delegates the call to the m1() method of the myFs object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBlockSize,org.apache.hadoop.fs.FileSystem:getBlockSize(org.apache.hadoop.fs.Path),2742,2745,"/**
* Delegates to m1(Path) and returns the result of m2() on it.
*/","* Get the block size for a particular file.
   * @param f the filename
   * @return the number of bytes in a block
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @throws FileNotFoundException if the path is not present
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getBlockSize,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getBlockSize(),494,497,"/**
* Delegates the call to the m1() method of the realStatus object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getBlockSize,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getBlockSize(),70,73,"/**
* Delegates the call to the underlying FileSystem's m1() method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getAccessTime,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getAccessTime(),509,512,"/**
* Delegates the call to the m1() method of the realStatus object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getAccessTime,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getAccessTime(),85,88,"/**
 * Delegates the call to the wrapped FileSystem's m1() method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getPermission,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPermission(),514,517,"/**
 * Delegates to the underlying realStatus to get the FsPermission.
 * @return The FsPermission from the realStatus.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getPermission,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getPermission(),90,93,"/**
* Delegates the call to the underlying FsPermission method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,stashOriginalFilePermissions,org.apache.hadoop.security.alias.KeyStoreProvider:stashOriginalFilePermissions(),73,79,"/**
* Retrieves file permissions from the file status.
* @throws IOException if an I/O error occurs.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,isPermissionLoaded,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:isPermissionLoaded(),927,929,"/**
* Negates the result of calling m2 on the result of m1.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getOwner,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getOwner(),519,522,"/**
* Delegates method call to the realStatus object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getOwner,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getOwner(),95,98,"/**
* Delegates the call to the underlying FileSystem object's m1() method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getGroup,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getGroup(),524,527,"/**
* Delegates the call to the m1() method of the realStatus object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getGroup,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getGroup(),100,103,"/**
* Delegates the call to the underlying FileSystem object's m1() method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,msync,org.apache.hadoop.fs.HarFileSystem:msync(),661,664,"/**
 * Delegates the m1 operation to the underlying file system.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,msync,org.apache.hadoop.fs.FilterFileSystem:msync(),465,468,"/**
* Delegates the m1 operation to the underlying file system.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.HarFileSystem:getDefaultReplication(),1294,1298,"/**
* Delegates m1() call to the fs object and returns the result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),2785,2787,"/**
* Delegates to a default implementation of m1().
* Returns a short value.
*/","* Get the default replication for a path.
   * The given path will be used to locate the actual FileSystem to query.
   * The full path does not have to exist.
   * @param path of the file
   * @return default replication for the path's filesystem",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication(),431,434,"/**
* Delegates m1() call to the fs object and returns the result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,cleanUp,org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReference:cleanUp(),4148,4159,"/**
* Updates statistics data in a synchronized block.
* Updates rootData and allData with provided data.
*/","* Performs clean-up action when the associated thread is garbage
       * collected.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,<init>,"org.apache.hadoop.fs.FsUrlConnection:<init>(org.apache.hadoop.conf.Configuration,java.net.URL)",48,53,"/**
 * Constructs a FsUrlConnection with the given Hadoop configuration and URL.
 * @param conf Hadoop configuration
 * @param url URL to connect to
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,validatePositionedReadArgs,"org.apache.hadoop.fs.FSInputStream:validatePositionedReadArgs(long,byte[],int,int)",102,116,"/**
* Copies data to buffer, validating position, length, and buffer size.
*/","* Validation code, available for use in subclasses.
   * @param position position: if negative an EOF exception is raised
   * @param buffer destination buffer
   * @param offset offset within the buffer
   * @param length length of bytes to read
   * @throws EOFException if the position is negative
   * @throws IndexOutOfBoundsException if there isn't space for the amount of
   * data requested.
   * @throws IllegalArgumentException other arguments are invalid.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkUploadId,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkUploadId(byte[]),80,85,"/**
* Validates the uploadId: must be non-null and non-empty.
*/","* Utility method to validate uploadIDs.
   * @param uploadId Upload ID
   * @throws IllegalArgumentException invalid ID",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPartHandles,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPartHandles(java.util.Map),92,100,"/**
 * Validates part handles, throwing an error if empty or invalid.
 * @param partHandles Map of part handles to validate.
 */","* Utility method to validate partHandles.
   * @param partHandles handles
   * @throws IllegalArgumentException if the parts are invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/PathCapabilitiesSupport.java,validatePathCapabilityArgs,"org.apache.hadoop.fs.impl.PathCapabilitiesSupport:validatePathCapabilityArgs(org.apache.hadoop.fs.Path,java.lang.String)",42,49,"/**
 * Returns the capability string, formatted in English locale.
 * @param path Path object (unused)
 * @param capability Capability string to format
 */","* Validate the arguments to
   * {@link PathCapabilities#hasPathCapability(Path, String)}.
   * @param path path to query the capability of.
   * @param capability non-null, non-empty string to query the path for support.
   * @return the string to use in a switch statement.
   * @throws IllegalArgumentException if a an argument is invalid.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,<init>,"org.apache.hadoop.service.launcher.InterruptEscalator:<init>(org.apache.hadoop.service.launcher.ServiceLauncher,int)",74,78,"/**
 * Constructs an InterruptEscalator with an owner and shutdown time.
 * @param owner The ServiceLauncher owning this escalator.
 * @param shutdownTimeMillis Time in milliseconds for shutdown.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,<init>,"org.apache.hadoop.service.launcher.IrqHandler:<init>(java.lang.String,org.apache.hadoop.service.launcher.IrqHandler$Interrupted)",78,83,"/**
 * Constructs an IrqHandler with the given name and handler.
 * @param name handler name
 * @param handler interrupt handler function
 */
","* Create an IRQ handler bound to the specific interrupt.
   * @param name signal name
   * @param handler handler",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,partition,"org.apache.hadoop.util.Lists:partition(java.util.List,int)",269,284,"/**
* Partitions a list into sublists of specified page size.
* @param originalList List to partition.
* @param pageSize Size of each sublist.
* @return List of sublists.
*/","* Returns consecutive sub-lists of a list, each of the same size
   * (the final list may be smaller).
   * @param originalList original big list.
   * @param pageSize desired size of each sublist ( last one
   *                 may be smaller)
   * @param <T> Generics Type.
   * @return a list of sub lists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,<init>,"org.apache.hadoop.util.JsonSerialization:<init>(java.lang.Class,boolean,boolean)",105,113,"/**
 * Constructs a JsonSerialization with config.
 * @param classType Class to serialize/deserialize.
 * @param failOnUnknownProperties Handles unknown properties.
 * @param pretty Enables pretty printing of JSON.
 */
","* Create an instance bound to a specific type.
   * @param classType class to marshall
   * @param failOnUnknownProperties fail if an unknown property is encountered.
   * @param pretty generate pretty (indented) output?",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,fetch,"org.apache.hadoop.fs.FileSystemStorageStatistics:fetch(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData,java.lang.String)",86,116,"/**
 * Returns a statistic value based on the provided key.
 * @param data StatisticsData object
 * @param key Key for the statistic to retrieve
 * @return Long value of the statistic or null if not found
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,<init>,"org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)",47,54,"/**
 * Constructs a StorageStatisticsFromIOStatistics object.
 * @param name Statistics name.
 * @param scheme Storage scheme.
 * @param ioStatistics IO statistics to use.
 */
","* Instantiate.
   * @param name storage statistics name.
   * @param scheme FS scheme; may be null.
   * @param ioStatistics IOStatistics source.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/EmptyStorageStatistics.java,<init>,org.apache.hadoop.fs.EmptyStorageStatistics:<init>(java.lang.String),28,30,"/**
 * Constructs an EmptyStorageStatistics object with the given name.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,<init>,"org.apache.hadoop.fs.UnionStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.StorageStatistics[])",79,92,"/**
 * Creates a UnionStorageStatistics object.
 * @param name Name of the statistics.
 * @param stats Array of StorageStatistics.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getScheme,org.apache.hadoop.fs.FileSystemStorageStatistics:getScheme(),127,130,"/**
* Delegates m1() call to the stats object.
* @return String value returned by stats.m1()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatistics,org.apache.hadoop.fs.FileSystem:getStatistics(),4560,4567,"/**
* Creates a map of Statistics, keyed by their identifier.
* Uses statisticsTable.m3() to iterate and populate the map.
*/
","* Get the Map of Statistics object indexed by URI Scheme.
   * @return a Map having a key as URI scheme and value as Statistics object
   * @deprecated use {@link #getGlobalStorageStatistics()}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFs:listXAttrs(org.apache.hadoop.fs.Path),846,851,"/**
 * Delegates path resolution and retrieves strings.
 * @param path Path to resolve; returns strings from resolved FS.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listXAttrs,org.apache.hadoop.fs.FilterFs:listXAttrs(org.apache.hadoop.fs.Path),387,390,"/**
* Delegates to the underlying FileSystem's m1 method.
* @param path Path to operate on
* @return List of Strings returned by the FileSystem
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",1275,1284,"/**
* Creates a data output stream to write to a file.
* @param f Path to the file, permission, flags, buffer size, etc.
* @return FSDataOutputStream
*/","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * @param f the file name to open
   * @param permission file permission
   * @param flags {@link CreateFlag}s to use for this stream.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,create,"org.apache.hadoop.fs.FilterFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",201,212,"/**
 * Delegates to the underlying file system's create output stream.
 * @param f Path to create stream on, permissions, flags, etc.
 * @return FSDataOutputStream
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1456,1463,"/**
* Creates a data output stream with specified parameters.
* @param f Path of the file, permission, overwrite, etc.
* @return FSDataOutputStream object.
*/","* Opens an FSDataOutputStream at the indicated Path with write-progress
   * reporting. Same as create(), except fails if parent directory doesn't
   * already exist.
   * @param f the file name to open
   * @param permission file permission
   * @param overwrite if a file with this name already exists, then if true,
   * the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FilterFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",222,229,"/**
 * Delegates file output stream creation to the underlying filesystem.
 * @param f Path to create stream on, permissions, flags, etc.
 * @return FSDataOutputStream
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String),24,26,"/**
 * Constructs a PathAccessDeniedException with the given path.
 * @param path The path that access was denied for.
 */
",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String),26,28,"/**
 * Constructs a PathPermissionException with the given path.
 * @param path The path for which permission is denied.
 */
",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String)",34,36,"/**
 * Constructs a PathPermissionException with the given path and error.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String),26,28,"/**
 * Constructs a PathNotFoundException with the specified path.
 * @param path The path that was not found.
 */
",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String)",34,36,"/**
 * Constructs a PathNotFoundException with a path and error message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathExistsException.java,<init>,org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String),26,28,"/**
 * Constructs a PathExistsException with the given path.
 * @param path The path of the existing file.
 */
",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathExistsException.java,<init>,"org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String,java.lang.String)",30,32,"/**
* Constructs a PathExistsException with a path and error message.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,<init>,org.apache.hadoop.fs.PathIOException:<init>(java.lang.String),43,45,"/**
 * Constructs a PathIOException with the given path.
 * @param path The path associated with the I/O error.
 */
","* Constructor a generic I/O error exception
   *  @param path for the exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ClosedIOException.java,<init>,"org.apache.hadoop.fs.ClosedIOException:<init>(java.lang.String,java.lang.String)",36,38,"/**
* Constructs a ClosedIOException with a path and message.
* @param path The path associated with the closed resource.
* @param message The error message.
*/
","* Appends the custom error-message to the default error message.
   * @param path path that encountered the closed resource.
   * @param message custom error message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getThisBuilder,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getThisBuilder(),102,102,"/**
* Returns a masked functional interface instance.
*/",* Return the concrete implementation of the builder instance.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.permission.FsPermission),43,47,"/**
 * Converts a FsPermission to a FsPermissionProto.
 * @param p The FsPermission to convert.
 * @return The equivalent FsPermissionProto.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkReturnValue,"org.apache.hadoop.fs.FileUtil:checkReturnValue(boolean,java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1510,1518,"/**
 * Throws IOException if permission setting fails for a path.
 * @param rv boolean indicating success; throws if false.
 * @param p the file path
 * @param permission the file permission
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,write,org.apache.hadoop.fs.permission.FsPermission:write(java.io.DataOutput),179,183,"/**
* Writes the result of m1() to the output stream.
* @param out Output stream to write to.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toExtendedShort,org.apache.hadoop.fs.permission.FsPermission:toExtendedShort(),240,243,"/**
* Delegates to m1() and returns its short value.
*/","* Encodes the object to a short.  Unlike {@link #toShort()}, this method may
   * return values outside the fixed range 00000 - 01777 if extended features
   * are encoded into this permission, such as the ACL bit.
   *
   * @return short extended short representation of this permission",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toOctal,org.apache.hadoop.fs.permission.FsPermission:toOctal(),251,255,"/**
 * Extracts a short value from the input based on bitwise operations.
 */","* Returns the FsPermission in an octal format.
   *
   * @return short Unlike {@link #toShort()} which provides a binary
   * representation, this method returns the standard octal style permission.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,hashCode,org.apache.hadoop.fs.permission.FsPermission:hashCode(),269,270,"/**
* Delegates to m1() and returns its result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringInterner.java,internStringsInArray,org.apache.hadoop.util.StringInterner:internStringsInArray(java.lang.String[]),81,86,"/**
 * Transforms each string in the array using m1.
 * @param strings Array of strings to transform.
 * @return Modified string array.
 */
","* Interns all the strings in the given array in place,
   * returning the same array.
   *
   * @param strings strings.
   * @return internStringsInArray.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleStartProperty,org.apache.hadoop.conf.Configuration$Parser:handleStartProperty(),3267,3294,"/**
 * Parses configuration attributes from the reader.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,isDir,org.apache.hadoop.fs.FileStatus:isDir(),240,243,"/**
* Delegates to m1() and returns its boolean result.
*/","* Old interface, instead use the explicit {@link FileStatus#isFile()},
   * {@link FileStatus#isDirectory()}, and {@link FileStatus#isSymlink()}
   * @return true if this is a directory.
   * @deprecated Use {@link FileStatus#isFile()},
   * {@link FileStatus#isDirectory()}, and {@link FileStatus#isSymlink()}
   * instead.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,isDirectory,org.apache.hadoop.fs.ChecksumFs:isDirectory(org.apache.hadoop.fs.Path),446,453,"/**
 * Processes a Path using chained methods, returns true on success.
 * @param f Path to process; throws IOException, UnresolvedLinkException
 */
","True iff the named path is a directory.
   * Note: Avoid using this method. Instead reuse the FileStatus 
   * returned by getFileStatus() or listStatus() methods.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processPath,org.apache.hadoop.fs.shell.CopyCommands$Merge:processPath(org.apache.hadoop.fs.shell.PathData),133,143,"/**
 * Processes PathData: masks if valid, otherwise queues for later.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,isPathRecursable,org.apache.hadoop.fs.shell.Command:isPathRecursable(org.apache.hadoop.fs.shell.PathData),418,420,"/**
 * Checks a PathData item's status using m1().
 * @param item The PathData item to check.
 * @throws IOException if an I/O error occurs.
 */
","* Determines whether a {@link PathData} item is recursable. Default
   * implementation is to recurse directories but can be overridden to recurse
   * through symbolic links.
   *
   * @param item
   *          a {@link PathData} object
   * @return true if the item is recursable, false otherwise
   * @throws IOException
   *           if anything goes wrong in the user-implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,getAclEntries,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:getAclEntries(org.apache.hadoop.fs.shell.PathData),283,289,"/**
* Returns ACL entries based on m1() result and item state.
*/","* Returns the ACL entries to use in the API call for the given path.  For a
     * recursive operation, returns all specified ACL entries if the item is a
     * directory or just the access ACL entries if the item is a file.  For a
     * non-recursive operation, returns all specified ACL entries.
     *
     * @param item PathData path to check
     * @return List<AclEntry> ACL entries to use in the API call",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPathArgument,org.apache.hadoop.fs.shell.FsUsage$Du:processPathArgument(org.apache.hadoop.fs.shell.PathData),209,217,"/**
* Processes PathData. Calls m3 if summary is false, else calls super.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,isDirectory,org.apache.hadoop.fs.FileSystem:isDirectory(org.apache.hadoop.fs.Path),1877,1884,"/**
 * Delegates to m1(f).m2(), returning false on FileNotFoundException.
 */","True iff the named path is a directory.
   * Note: Avoid using this method. Instead reuse the FileStatus
   * returned by getFileStatus() or listStatus() methods.
   *
   * @param f path to check
   * @throws IOException IO failure
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @return if f is directory true, not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isDirectory(),484,487,"/**
* Delegates method call to the wrapped object's m1 method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isDirectory,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isDirectory(),60,63,"/**
* Delegates m1() call to the underlying FileSystem object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ChmodParser.java,applyNewPermission,org.apache.hadoop.fs.permission.ChmodParser:applyNewPermission(org.apache.hadoop.fs.FileStatus),48,54,"/**
 * Calculates a short mask based on file permissions.
 * @param file The FileStatus object.
 * @return A short representing the calculated mask.
 */
","* Apply permission against specified file and determine what the
   * new mode would be
   * @param file File against which to apply mode
   * @return File's new mode if applied.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,isFile,org.apache.hadoop.fs.FileStatus:isFile(),220,222,"/**
* Returns true if both m1() and m2() return false.
*/","* Is this a file?
   * @return true if this is a file",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,getSymlink,org.apache.hadoop.fs.FileStatus:getSymlink(),393,398,"/**
* Returns the target path of a symbolic link.
* @throws IOException if the path is not a symbolic link.
*/","* @return The contents of the symbolic link.
   *
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isSymlink(),489,492,"/**
* Delegates m1() call to the realStatus object.
* Returns the result of the delegated call.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isSymlink,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isSymlink(),65,68,"/**
* Delegates the call to the underlying FileSystem object's m1().
* @return The result of the delegated m1() call.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getFileLength,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getFileLength(),270,275,"/**
 * Returns the file length, caching the value.
 * @return The file length as a long.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getFileLength,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getFileLength(),330,335,"/**
 * Returns the length of the file.
 * @return File length, or -1 if not initialized.
 */","* Calculate length of file if not already cached.
     * @return file length.
     * @throws IOException any IOE.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,totalPartsLen,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:totalPartsLen(java.util.List),168,174,"/**
 * Calculates the total length of files from a list of paths.
 * @param partHandles List of file paths.
 * @return Total file length.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLength,org.apache.hadoop.fs.FileSystem:getLength(org.apache.hadoop.fs.Path),1912,1915,"/**
 * Calls m1(f) and returns the result of m2() on the result.
 */","* The number of bytes in a file.
   * @param f the path.
   * @return the number of bytes; 0 for a directory
   * @deprecated Use {@link #getFileStatus(Path)} instead.
   * @throws FileNotFoundException if the path does not resolve
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getLen,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getLen(),474,477,"/**
* Delegates the call to the m1() method of the realStatus object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getLen,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getLen(),50,53,"/**
 * Delegates the call to the underlying FileSystem's m1() method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String,byte)",82,91,"/**
 * Constructs a FsServerDefaults with default encryption flag.
 * @param blockSize Block size for file system operations.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:getStoragePolicy(org.apache.hadoop.fs.Path),418,422,"/**
* Delegates BlockStoragePolicySpi retrieval to the underlying FileSystem.
* @param src Path to the file.
* @return BlockStoragePolicySpi object.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:getStoragePolicy(org.apache.hadoop.fs.Path),914,919,"/**
 * Retrieves a BlockStoragePolicySpi based on a Path.
 * @param src Path to resolve; returns a BlockStoragePolicySpi.
 * @throws IOException if an I/O error occurs.
 */
","* Retrieve the storage policy for a given file or directory.
   *
   * @param src file or directory path.
   * @return storage policy for give file.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getStoragePolicy,org.apache.hadoop.fs.FilterFs:getStoragePolicy(org.apache.hadoop.fs.Path),432,436,"/**
* Delegates BlockStoragePolicySpi retrieval to the underlying file system.
* @param src Path to the source.
* @return BlockStoragePolicySpi object.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",816,822,"/**
* Sets an extended attribute on a file.
* @param path File path, name, value, and flag for setting attribute.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,setXAttr,"org.apache.hadoop.fs.AbstractFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",1358,1362,"/**
* Sets an Xattr with CREATE/REPLACE flags.
* @param path Path to the file.
* @param name Xattr name.
* @param value Xattr value.
*/
","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setXAttr,"org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",365,369,"/**
 * Delegates m1 call to the underlying file system.
 * @param path Path to the file.
 * @param name Attribute name.
 * @param value Attribute value.
 * @param flag Attribute set flags.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,blockSize,org.apache.hadoop.fs.Options$CreateOpts:blockSize(long),46,48,"/**
 * Creates a BlockSize object with the given block size.
 * @param bs The block size value.
 * @return A BlockSize object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,bufferSize,org.apache.hadoop.fs.Options$CreateOpts:bufferSize(int),49,51,"/**
 * Creates a new BufferSize object with the given size.
 * @param bs The initial buffer size.
 * @return A new BufferSize object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,repFac,org.apache.hadoop.fs.Options$CreateOpts:repFac(short),52,54,"/**
 * Creates a ReplicationFactor object from a short value.
 * @param rf The replication factor value.
 * @return A ReplicationFactor object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,bytesPerChecksum,org.apache.hadoop.fs.Options$CreateOpts:bytesPerChecksum(short),55,57,"/**
 * Creates a BytesPerChecksum object from a CRC value.
 * @param crc The CRC value to use.
 * @return A BytesPerChecksum object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,checksumParam,org.apache.hadoop.fs.Options$CreateOpts:checksumParam(org.apache.hadoop.fs.Options$ChecksumOpt),58,61,"/**
 * Creates a ChecksumParam object from a ChecksumOpt.
 * @param csumOpt The ChecksumOpt object to use.
 * @return A new ChecksumParam object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,progress,org.apache.hadoop.fs.Options$CreateOpts:progress(org.apache.hadoop.util.Progressable),62,64,"/**
 * Creates a new Progress object wrapping the given Progressable.
 * @param prog The Progressable to wrap.
 * @return A new Progress object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,createParent,org.apache.hadoop.fs.Options$CreateOpts:createParent(),68,70,"/**
 * Creates and returns a CreateParent object with 'isMask' set to true.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,donotCreateParent,org.apache.hadoop.fs.Options$CreateOpts:donotCreateParent(),71,73,"/**
 * Creates and returns a CreateParent object with 'isMask' as false.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,"org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",32,36,"/**
 * Constructs a PathAccessDeniedException with path, error, and cause.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",38,42,"/**
 * Constructs a PathPermissionException with path, error, and cause.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",38,42,"/**
 * Constructs a PathNotFoundException with path, error, and cause.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,<init>,"org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.Throwable)",52,54,"/**
* Constructs a PathIOException with the given path and cause.
*/
","* Appends the text of a Throwable to the default error message
   * @param path for the exception
   * @param cause a throwable to extract the error message",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFs:removeAcl(org.apache.hadoop.fs.Path),794,800,"/**
 * Recursively calls m3 on the target file system.
 * @param path The path to resolve and process.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeAcl,org.apache.hadoop.fs.FilterFs:removeAcl(org.apache.hadoop.fs.Path),344,347,"/**
* Delegates method call to the underlying file system.
* @param path The path to operate on.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,seek,org.apache.hadoop.fs.AvroFSInput:seek(long),75,78,"/**
* Delegates the m1 method call to the stream.
* @param p The parameter passed to the stream's m1 method.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)",924,937,"/**
 * Initializes a HarFsInputStream for reading a portion of a file.
 * @param fs FileSystem, path to file, start/end offsets, buffer size
 * @throws IOException if an I/O error occurs
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,skip,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:skip(long),1006,1022,"/**
 * Reads up to 'n' bytes from the stream.
 * @param n number of bytes to read
 * @return actual bytes read, or 0 if n <= 0
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,seek,org.apache.hadoop.io.SequenceFile$Reader:seek(long),2818,2824,"/**
 * Calls in.m1(position) and decompresses values if blockCompressed.
 */","* Set the current byte position in the input file.
     *
     * <p>The position passed must be a position returned by {@link
     * SequenceFile.Writer#getLength()} when writing this file.  To seek to an arbitrary
     * position, use {@link SequenceFile.Reader#sync(long)}. </p>
     *
     * @param position input position.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,"org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[],int,int)",87,106,"/**
* Reads up to `len` bytes from input stream `b`, starting at `off`.
* Returns the number of bytes read, or -1 if end of stream.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,tell,org.apache.hadoop.fs.AvroFSInput:tell(),80,83,"/**
 * Delegates to stream.m1() and returns the result.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,available,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:available(),939,946,"/**
 * Calculates the remaining bytes to read, capped at Integer.MAX_VALUE.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readRecordLength,org.apache.hadoop.io.SequenceFile$Reader:readRecordLength(),2538,2558,"/**
 * Reads a length value from the input stream, handling sync checks.
 * Returns length or -1 if end is reached.
 */","* Read and return the next record length, potentially skipping over 
     * a sync block.
     * @return the length of the next record or -1 if there is no next record
     * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getPosition,org.apache.hadoop.io.SequenceFile$Reader:getPosition(),2873,2875,"/**
 * Reads a long value from the input stream.
 * @return A long value read from the stream.
 * @throws IOException if an I/O error occurs.
 */
","* @return Return the current byte position in the input file.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setOwner,"org.apache.hadoop.fs.DelegateToFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",214,219,"/**
 * Calls m1 and delegates to fsImpl.m2 with provided Path, username, groupname.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processPath,org.apache.hadoop.fs.FsShellPermissions$Chown:processPath(org.apache.hadoop.fs.shell.PathData),173,190,"/**
* Changes file ownership if owner/group need updating.
* @param item PathData object containing file system info.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setOwner,"org.apache.hadoop.fs.FilterFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",533,537,"/**
* Delegates the call to the underlying file system.
* @param p Path object
* @param username Username
* @param groupname Group name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,registerExpression,org.apache.hadoop.fs.shell.find.ExpressionFactory:registerExpression(java.lang.Class),59,69,"/**
 * Registers an expression class.
 * @param expressionClass Class to register, must have a register method.
 */","* Invokes ""static void registerExpression(FindExpressionFactory)"" on the
   * given class. This method abstracts the contract between the factory and the
   * expression class. Do not assume that directly invoking registerExpression
   * on the given class will have the same effect.
   *
   * @param expressionClass
   *          class to allow an opportunity to register",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,registerCommands,org.apache.hadoop.fs.shell.CommandFactory:registerCommands(java.lang.Class),64,72,"/**
 * Registers commands using the provided registrar class.
 * @param registrarClass Class responsible for command registration.
 */","* Invokes ""static void registerCommands(CommandFactory)"" on the given class.
   * This method abstracts the contract between the factory and the command
   * class.  Do not assume that directly invoking registerCommands on the
   * given class will have the same effect.
   * @param registrarClass class to allow an opportunity to register",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doResponse,"org.apache.hadoop.ipc.Server$RpcCall:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)",1308,1328,"/**
* Handles RPC call completion, logging errors or finalizing status.
* @param t The exception, if any.
* @param status The RPC status.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path),436,443,"/**
 * Retrieves a FileStatus object for the given path.
 * @param f the path to retrieve the FileStatus for
 * @return FileStatus object
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getUri,org.apache.hadoop.fs.FilterFs:getUri(),179,182,"/**
* Delegates m1() call to the underlying FileSystem object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setSymlink(org.apache.hadoop.fs.Path),544,547,"/**
* Delegates the Path processing to the realStatus object.
* @param p The Path to be processed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(),968,972,"/**
 * Calls m1 with a buffer and offset, returns first byte or -1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(byte[]),978,982,"/**
 * Calls m1 with the provided byte array from start to end.
 * @param b byte array to process
 * @return int value returned by the overloaded method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,seek,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:seek(long),1029,1034,"/**
* Advances the stream position and writes to the underlying stream.
* @param pos Offset from the starting position.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(long,byte[],int,int)",1058,1071,"/**
 * Reads data from the stream, adjusting length if necessary.
 * @param pos  File position.
 * @param b  Buffer.
 * @param offset Offset in buffer.
 * @param length Number of bytes to read.
 * @return Number of bytes read, or -1 on error.
 */",* implementing position readable.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,readFully,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:readFully(long,byte[],int,int)",1076,1087,"/**
 * Reads bytes from the stream, ensuring bounds and delegating to underlying stream.
 */",* position readable again.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,setReadahead,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setReadahead(java.lang.Long),1089,1092,"/**
* Delegates m1 call to the underlying stream.
* @param readahead The read-ahead value to pass.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,setDropBehind,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setDropBehind(java.lang.Boolean),1094,1097,"/**
* Delegates m1 call to the underlying stream.
* @param dropBehind Flag to indicate dropping behind.
* @throws IOException if an I/O error occurs.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,getCurrentTrashDir,org.apache.hadoop.fs.Trash:getCurrentTrashDir(org.apache.hadoop.fs.Path),198,200,"/**
* Delegates path processing to the trash policy.
* @param path The path to process.
* @return Path processed by the trash policy.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,completed,"org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:completed(java.lang.Integer,java.lang.Integer)",365,383,"/**
 * Processes data buffer based on result and range.
 * @param result Read result; -1 indicates EOF.
 * @param r Index of the buffer/range being processed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobExpander.java,expandLeftmost,org.apache.hadoop.fs.GlobExpander:expandLeftmost(org.apache.hadoop.fs.GlobExpander$StringWithOffset),86,146,"/**
 * Parses file pattern, extracts alternatives, and returns expanded list.
 */","* Expand the leftmost outer curly bracket pair containing a
   * slash character (""/"") in <code>filePattern</code>.
   * @param filePatternWithOffset
   * @return expanded file patterns
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatusBatch,"org.apache.hadoop.fs.FileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])",2060,2068,"/**
 * Creates DirectoryEntries from a file listing.
 * @param f Path to directory.
 * @param token Authentication token.
 * @return DirectoryEntries object.
 */
","* Given an opaque iteration token, return the next batch of entries in a
   * directory. This is a private API not meant for use by end users.
   * <p>
   * This method should be overridden by FileSystem subclasses that want to
   * use the generic {@link FileSystem#listStatusIterator(Path)} implementation.
   * @param f Path to list
   * @param token opaque iteration token returned by previous call, or null
   *              if this is the first call.
   * @return directory entries.
   * @throws FileNotFoundException when the path does not exist.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/XAttrCodec.java,encodeValue,"org.apache.hadoop.fs.XAttrCodec:encodeValue(byte[],org.apache.hadoop.fs.XAttrCodec)",109,119,"/**
 * Masks byte array using specified encoding (HEX, BASE64, or UTF-8).
 * @param value byte array to mask
 * @param encoding encoding type
 * @return masked string representation
 */","* Encode byte[] value to string representation with encoding. 
   * Values encoded as text strings are enclosed in double quotes (\""), 
   * while strings encoded as hexadecimal and base64 are prefixed with 
   * 0x and 0s, respectively.
   * @param value byte[] value
   * @param encoding encoding.
   * @return String string representation of value
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2076,2085,"/**
 * Filters file statuses based on a provided filter and adds to results.
 * @param results List to store filtered FileStatus objects.
 * @param f Path to list
 * @param filter Filter to apply to file statuses
 */
","* Filter files/directories in the given path using the user-supplied path
   * filter. Results are added to the given array <code>results</code>.
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,<init>,"org.apache.hadoop.metrics2.MetricsTag:<init>(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",43,46,"/**
 * Creates a MetricsTag with provided info and value.
 * @param info MetricsInfo object.
 * @param value Tag value as a string.
 */
","* Construct the tag with name, description and value
   * @param info  of the tag
   * @param value of the tag",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounter.java,<init>,org.apache.hadoop.metrics2.lib.MutableCounter:<init>(org.apache.hadoop.metrics2.MetricsInfo),35,37,"/**
 * Constructs a MutableCounter with the given MetricsInfo.
 * @param info MetricsInfo object containing counter details.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGauge.java,<init>,org.apache.hadoop.metrics2.lib.MutableGauge:<init>(org.apache.hadoop.metrics2.MetricsInfo),35,37,"/**
 * Constructs a MutableGauge with the provided MetricsInfo.
 * @param info MetricsInfo object containing gauge details.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,<init>,org.apache.hadoop.metrics2.lib.MutableRates:<init>(org.apache.hadoop.metrics2.lib.MetricsRegistry),48,50,"/**
* Constructs a MutableRates with the given metrics registry.
* @param registry The metrics registry to use.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsInfoImpl.java,<init>,"org.apache.hadoop.metrics2.lib.MetricsInfoImpl:<init>(java.lang.String,java.lang.String)",34,37,"/**
 * Constructs a MetricsInfo object with the given name and description.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/AbstractMetric.java,<init>,org.apache.hadoop.metrics2.AbstractMetric:<init>(org.apache.hadoop.metrics2.MetricsInfo),41,43,"/**
 * Constructs a new AbstractMetric with the given metrics info.
 * @param info MetricsInfo object containing metric details.
 */
","* Construct the metric
   * @param info  about the metric",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String,java.lang.String)",389,403,"/**
 * Renews a delegation token.
 * @param url URL for renewal, token to renew, renewer, doAsUser
 * @return Renewed delegation token.
 */","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @param doAsUser the user to do as, which will be the token owner.
   * @return a delegation token.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",433,446,"/**
 * Delegates a request using Kerberos delegation token.
 * @param url Request URL.
 * @param token Token object.
 * @param doAsUser User to impersonate.
 * @return Result code.
 */
","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",472,484,"/**
 * Authenticates a URL using Kerberos delegation token.
 * @param url URL to authenticate, @param token Token object
 */","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,<init>,"org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)",222,260,"/**
 * Creates a ValueQueue with specified parameters for managing values.
 * @param numValues Number of values to manage.
 * @param lowWatermark Low watermark threshold.
 */","* Constructor takes the following tunable configuration parameters
   * @param numValues The number of values cached in the Queue for a
   *    particular key.
   * @param lowWatermark The ratio of (number of current entries/numValues)
   *    below which the <code>fillQueueForKey()</code> funciton will be
   *    invoked to fill the Queue.
   * @param expiry Expiry time after which the Key and associated Queue are
   *    evicted from the cache.
   * @param numFillerThreads Number of threads to use for the filler thread
   * @param policy The SyncGenerationPolicy to use when client
   *    calls ""getAtMost""
   * @param refiller implementation of the QueueRefiller",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Preconditions.java,checkNotNull,org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object),68,70,"/**
* Delegates to another method with a default error message.
* @param obj The object to process.
* @return The processed object.
*/
","* <p>Preconditions that the specified argument is not {@code null},
   * throwing a NPE exception otherwise.
   *
   * <p>The message of the exception is
   * &quot;The validated object is null&quot;.</p>
   *
   * @param <T> the object type
   * @param obj  the object to check
   * @return the validated object
   * @throws NullPointerException if the object is {@code null}
   * @see #checkNotNull(Object, Object)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFs:getAclStatus(org.apache.hadoop.fs.Path),809,814,"/**
 * Delegates AclStatus retrieval to the target file system.
 * @param path The path to check ACL status for.
 * @return AclStatus object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getAclStatus,org.apache.hadoop.fs.FilterFs:getAclStatus(org.apache.hadoop.fs.Path),354,357,"/**
 * Delegates AclStatus retrieval to the underlying FileSystem.
 * @param path The path for which to retrieve the AclStatus.
 * @return AclStatus object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobalStorageStatistics.java,put,"org.apache.hadoop.fs.GlobalStorageStatistics:put(java.lang.String,org.apache.hadoop.fs.GlobalStorageStatistics$StorageStatisticsProvider)",73,93,"/**
* Retrieves or creates StorageStatistics for a given name.
* @param name Statistics name.
* @param provider Provider to create stats if not found.
* @return StorageStatistics object.
*/","* Create or return the StorageStatistics object with the given name.
   *
   * @param name        The storage statistics object name.
   * @param provider    An object which can create a new StorageStatistics
   *                      object if needed.
   * @return            The StorageStatistics object with the given name.
   * @throws RuntimeException  If the StorageStatisticsProvider provides a null
   *                           object or a new StorageStatistics object with the
   *                           wrong name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobalStorageStatistics.java,next,org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator:next(),127,139,"/**
 * Retrieves the next StorageStatistics.
 * @return StorageStatistics object or throws NoSuchElementException
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,clearStatistics,org.apache.hadoop.fs.FileSystem:clearStatistics(),4610,4612,"/**
* Calls the m1 method on the GlobalStorageStatistics instance.
*/",* Reset all statistics for all file systems.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell$UnknownCommandException:<init>(),410,410,"/**
* Default constructor for UnknownCommandException.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,close,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:close(),258,266,"/**
 * Closes the block upload data, logs deletion, and nulls fields.
 */","* Close: closes any upload stream and byteArray provided in the
     * constructor.
     *
     * @throws IOException inherited exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,copyFileUnbuffered,"org.apache.hadoop.io.nativeio.NativeIO:copyFileUnbuffered(java.io.File,java.io.File)",1138,1161,"/**
* Copies a file from src to dst, using native methods if available.
*/","* Unbuffered file copy from src to dst without tainting OS buffer cache
   *
   * In POSIX platform:
   * It uses FileChannel#transferTo() which internally attempts
   * unbuffered IO on OS with native sendfile64() support and falls back to
   * buffered IO otherwise.
   *
   * It minimizes the number of FileChannel#transferTo call by passing the the
   * src file size directly instead of a smaller size as the 3rd parameter.
   * This saves the number of sendfile64() system call when native sendfile64()
   * is supported. In the two fall back cases where sendfile is not supported,
   * FileChannle#transferTo already has its own batching of size 8 MB and 8 KB,
   * respectively.
   *
   * In Windows Platform:
   * It uses its own native wrapper of CopyFileEx with COPY_FILE_NO_BUFFERING
   * flag, which is supported on Windows Server 2008 and above.
   *
   * Ideally, we should use FileChannel#transferTo() across both POSIX and Windows
   * platform. Unfortunately, the wrapper(Java_sun_nio_ch_FileChannelImpl_transferTo0)
   * used by FileChannel#transferTo for unbuffered IO is not implemented on Windows.
   * Based on OpenJDK 6/7/8 source code, Java_sun_nio_ch_FileChannelImpl_transferTo0
   * on Windows simply returns IOS_UNSUPPORTED.
   *
   * Note: This simple native wrapper does minimal parameter checking before copy and
   * consistency check (e.g., size) after copy.
   * It is recommended to use wrapper function like
   * the Storage#nativeCopyFileUnbuffered() function in hadoop-hdfs with pre/post copy
   * checks.
   *
   * @param src                  The source path
   * @param dst                  The destination path
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,closeStream,org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable),276,280,"/**
* Processes a Closeable stream, handling null input.
*/","* Closes the stream ignoring {@link Throwable}.
   * Must only be called in cleaning up from exception handlers.
   *
   * @param stream the Stream to close",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,closeStreams,org.apache.hadoop.io.IOUtils:closeStreams(java.io.Closeable[]),288,292,"/**
* Processes an array of Closeable streams.
* @param streams Array of Closeable objects to process.
*/","* Closes the streams ignoring {@link Throwable}.
   * Must only be called in cleaning up from exception handlers.
   *
   * @param streams the Streams to close",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,stop,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:stop(),207,218,"/**
* Stops the sink thread and closes the sink resource.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,close,org.apache.hadoop.crypto.OpensslCtrCryptoCodec:close(),111,117,"/**
 * Closes the random number generator if it's a Closeable.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,close,org.apache.hadoop.crypto.random.OsSecureRandom:close(),120,126,"/**
 * Closes the stream if it exists, logging any errors.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,diskIoCheckWithoutNativeIo,org.apache.hadoop.util.DiskChecker:diskIoCheckWithoutNativeIo(java.io.File),283,302,"/**
 * Writes a single byte to a file and attempts deletion.
 * @param file The file to write to and potentially delete.
 * @throws IOException if file write or deletion fails.
 */","* Try to perform some disk IO by writing to the given file
   * without using Native IO.
   *
   * @param file
   * @throws IOException if there was a non-retriable error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hflush,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hflush(),501,504,"/**
* Calls the m1() method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hsync,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hsync(),510,514,"/**
* Executes a sequence of operations, potentially throwing IOException.
*/","* HSync calls sync on fhe file descriptor after a local flush() call.
     * @throws IOException failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,close,org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:close(),95,105,"/**
 * Logs I/O statistics with a failure suffix if applicable.
 * @param name The name to use for the statistic.
 * @param value The value of the statistic.
 */","* Set the finished time and then update the statistics.
   * If the operation failed then the key + .failures counter will be
   * incremented by one.
   * The operation min/mean/max values will be updated with the duration;
   * on a failure these will all be the .failures metrics.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,skip,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:skip(long),274,283,"/**
 * Reads a specified number of bytes from the input stream.
 * @param n number of bytes to read
 * @return number of bytes actually read
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(byte[],int,int)",479,488,"/**
* Writes data to the stream, updates statistics, and handles errors.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(int),490,499,"/**
 * Writes bytes to the output stream and updates IO statistics.
 * @param b The byte to write.
 * @throws IOException if an IO error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hasCapability,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hasCapability(java.lang.String),516,527,"/**
 * Checks if capability is supported; delegates otherwise.
 * @param capability Capability string to check.
 * @return True if capability is supported, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List,org.apache.hadoop.ipc.RemoteException)",52,58,"/**
 * Constructs a PartialListing with either data or an exception.
 * @param listedPath Path of the listing.
 * @param partialListing Partial listing data.
 * @param exception Remote exception, if any.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,setCount,org.apache.hadoop.io.DataOutputBuffer$Buffer:setCount(int),78,83,"/**
* Updates the count and returns the previous count.
* @param newCount The new count value.
* @return The previous count value.
*/
","* Set the count for the current buf.
     * @param newCount the new count to set
     * @return the original count",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,"org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object,java.lang.Throwable,org.apache.hadoop.io.retry.CallReturn$State)",60,65,"/**
 * Constructs a CallReturn object with a result, Throwable, and State.
 * Result and Throwable must be mutually exclusive.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getConnectorAddress,org.apache.hadoop.http.HttpServer2:getConnectorAddress(int),1330,1342,"/**
 * Gets the InetSocketAddress for a connector at the given index.
 * @param index Index of the connector. Returns null if invalid.
 */","* Get the address that corresponds to a particular connector.
   *
   * @param index index.
   * @return the corresponding address for the connector, or null if there's no
   *         such connector or the connector is not bounded or was closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec:calculateIV(byte[],long,byte[],int)",63,80,"/**
 * Initializes IV by XORing with initIV and counter.
 * @param initIV Initialization vector.
 * @param counter Counter value to incorporate.
 * @param iv Output IV array.
 * @param blockSize Block size.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceCtrCryptoCodec:calculateIV(byte[],long,byte[],int)",55,72,"/**
* Initializes IV by XORing with initIV and counter.
* @param initIV Initial IV block.
* @param counter Counter value to incorporate.
* @param iv Output IV block.
* @param blockSize Block size in bytes.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,<init>,"org.apache.hadoop.util.GcTimeMonitor:<init>(long,long,int,org.apache.hadoop.util.GcTimeMonitor$GcTimeAlertHandler)",125,151,"/**
 * Creates a GcTimeMonitor with specified parameters for GC time monitoring.
 * @param observationWindowMs Observation window in milliseconds.
 * @param sleepIntervalMs Interval between samples in milliseconds.
 */
","* Create an instance of GCTimeMonitor. Once it's started, it will stay alive
   * and monitor GC time percentage until shutdown() is called. If you don't
   * put a limit on the number of GCTimeMonitor instances that you create, and
   * alertHandler != null, you should necessarily call shutdown() once the given
   * instance is not needed. Otherwise, you may create a memory leak, because
   * each running GCTimeMonitor will keep its alertHandler object in memory,
   * which in turn may reference and keep in memory many more other objects.
   *
   * @param observationWindowMs the interval over which the percentage
   *   of GC time should be calculated. A practical value would be somewhere
   *   between 30 sec and several minutes.
   * @param sleepIntervalMs how frequently this thread should wake up to check
   *   GC timings. This is also a frequency with which alertHandler will be
   *   invoked if GC time percentage exceeds the specified limit. A practical
   *   value would likely be 500..1000 ms.
   * @param maxGcTimePercentage A GC time percentage limit (0..100) within
   *   observationWindowMs. Once this is exceeded, alertHandler will be
   *   invoked every sleepIntervalMs milliseconds until GC time percentage
   *   falls below this limit.
   * @param alertHandler a single method in this interface is invoked when GC
   *   time percentage exceeds the specified limit.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ServletUtil.java,getRawPath,"org.apache.hadoop.util.ServletUtil:getRawPath(javax.servlet.http.HttpServletRequest,java.lang.String)",107,110,"/**
 * Retrieves a value from the request based on the servlet name.
 * @param request HTTP request object
 * @param servletName Servlet name to use for retrieval.
 */","* Parse the path component from the given request and return w/o decoding.
   * @param request Http request to parse
   * @param servletName the name of servlet that precedes the path
   * @return path component, null if the default charset is not supported",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getMovableTypes,org.apache.hadoop.fs.StorageType:getMovableTypes(),78,80,"/**
 * Returns a list of StorageType objects from m1().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getTypesSupportingQuota,org.apache.hadoop.fs.StorageType:getTypesSupportingQuota(),82,84,"/**
 * Returns a list of StorageType objects. Delegates to m1().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,parseStorageType,org.apache.hadoop.fs.StorageType:parseStorageType(java.lang.String),90,92,"/**
 * Converts a string to a StorageType using StringUtils.m1().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,initMode,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initMode(),631,638,"/**
 * Determines the init mode from system property or env variable.
 * Returns InitMode enum based on available configuration.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",838,844,"/**
 * Delegates file metadata retrieval to the target file system.
 * @param path Path to the file.
 * @param names List of metadata names to retrieve.
 * @return Map of metadata names to byte array values.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttrs,"org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",381,385,"/**
 * Delegates to underlying FileSystem's m1 method.
 * @param path Path to fetch files from.
 * @param names List of file names to fetch.
 * @return Map of file names to byte arrays.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,unbuffer,org.apache.hadoop.fs.FSDataInputStream:unbuffer(),237,240,"/**
* Delegates method execution to StreamCapabilitiesPolicy.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,equals,org.apache.hadoop.fs.FileStatus:equals(java.lang.Object),435,445,"/**
 * Checks if this FileStatus is equal to another.
 * @param o the object to compare with
 * @return true if equal, false otherwise
 */
","Compare if this object is equal to another object
   * @param   o the object to be compared.
   * @return  true if two file status has the same path name; false if not.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,equals,org.apache.hadoop.fs.shell.PathData:equals(java.lang.Object),598,603,"/**
 * Checks if object is a PathData and its path matches.
 * @param o The object to check.
 * @return True if object is a PathData and paths match.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,hashCode,org.apache.hadoop.fs.FileStatus:hashCode(),453,456,"/**
* Delegates to the result of m1() and returns its m2() value.
*/","* Returns a hash code value for the object, which is defined as
   * the hash code of the path name.
   *
   * @return  a hash code value for the path name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,hashCode,org.apache.hadoop.fs.shell.PathData:hashCode(),605,608,"/**
* Delegates to the parent class's m1() method.
* @return The integer value returned by the parent's m1().
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setPath,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setPath(org.apache.hadoop.fs.Path),534,537,"/**
 * Delegates the Path processing to the realStatus object.
 * @param p The Path to be processed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,calculateFolderSize,org.apache.hadoop.fs.DUHelper:calculateFolderSize(java.lang.String),38,43,"/**
 * Calculates the size of a folder.
 * @param folder path to the folder
 * @return size of the folder in bytes
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,check,org.apache.hadoop.fs.DUHelper:check(java.lang.String),45,53,"/**
 * Calculates folder size & usage.
 * @param folder Path to the folder.
 * @return String with size, file count, and usage.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",853,858,"/**
 * Delegates file operation to target file system.
 * @param path Path to resolve.
 * @param name File name to operate on.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeXAttr,"org.apache.hadoop.fs.FilterFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",392,395,"/**
 * Delegates the operation to the underlying file system.
 * @param path The path to operate on.
 * @param name The name associated with the path.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,setSamplesAndSum,"org.apache.hadoop.fs.statistics.MeanStatistic:setSamplesAndSum(long,long)",157,161,"/**
* Updates internal state using sampleCount and newSum.
*/","* Set the sum and samples.
   * Synchronized.
   * @param sampleCount new sample count.
   * @param newSum new sum",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,add,org.apache.hadoop.fs.statistics.MeanStatistic:add(org.apache.hadoop.fs.statistics.MeanStatistic),212,230,"/**
* Merges with another MeanStatistic, updating samples and sum.
* @param other The MeanStatistic to merge with.
* @return This MeanStatistic object.
*/","* Add another MeanStatistic.
   * @param other other value
   * @return mean statistic.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,equals,org.apache.hadoop.fs.statistics.MeanStatistic:equals(java.lang.Object),254,269,"/**
 * Checks if two MeanStatistic objects are equal.
 * Compares m1(), m2(), m3(), and m4() values.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,toString,org.apache.hadoop.fs.statistics.MeanStatistic:toString(),285,289,"/**
* Formats a string with sample count, sum, and mean.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,mapToString,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.lang.String)",133,149,"/**
 * Appends map entries to StringBuilder, separated by a separator.
 * @param sb StringBuilder to append to
 * @param type Type string
 * @param map Map to iterate through
 * @param separator Separator string
 */","* Given a map, add its entryset to the string.
   * The entries are only sorted if the source entryset
   * iterator is sorted, such as from a TreeMap.
   * @param sb string buffer to append to
   * @param type type (for output)
   * @param map map to evaluate
   * @param separator separator
   * @param <E> type of values of the map",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,entryToString,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:entryToString(java.util.Map$Entry),136,139,"/**
* Delegates to a helper method with key and value from entry.
*/","* Convert an entry to the string format used in logging.
   *
   * @param entry entry to evaluate
   * @param <E> entry type
   * @return formatted string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationTrackerFactory.java,trackDuration,"org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String,long)",48,50,"/**
* Creates a DurationTracker with the given key and count.
* @param key Identifier for the duration.
* @param count The count associated with the duration.
*/
","* Initiate a duration tracking operation by creating/returning
   * an object whose {@code close()} call will
   * update the statistics.
   *
   * The statistics counter with the key name will be incremented
   * by the given count.
   *
   * The expected use is within a try-with-resources clause.
   *
   * The default implementation returns a stub duration tracker.
   * @param key statistic key prefix
   * @param count  #of times to increment the matching counter in this
   * operation.
   * @return an object to close after an operation completes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/EmptyPrefetchingStatistics.java,prefetchOperationStarted,org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:prefetchOperationStarted(),45,48,"/**
* Returns the DurationTracker instance.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,getLong,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLong(java.lang.String),97,104,"/**
 * Retrieves a Long value associated with the given key.
 * Tries multiple sources if the key is not found initially.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,isTracked,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:isTracked(java.lang.String),106,110,"/**
* Checks if the key exists in either m1 or m3.
* @param key The key to search for.
* @return True if key exists, false otherwise.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,toLongStatistic,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:toLongStatistic(java.util.Map$Entry),85,87,"/**
 * Creates a LongStatistic from a map entry's values.
 * @param e Map.Entry containing long values for statistic.
 * @return A new LongStatistic object.
 */","* Convert a counter/gauge entry to a long statistics.
   * @param e entry
   * @return statistic",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,next,org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:next(),70,78,"/**
 * Retrieves a LongStatistic based on the next key in KEYS.
 * @return A LongStatistic object containing key and calculated value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EvaluatingStatisticsMap.java,<init>,org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:<init>(),51,53,"/**
 * Constructs an EvaluatingStatisticsMap with a passthrough function.
 */",* Construct with the copy function being simple passthrough.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addCounterFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addCounterFunction(java.lang.String,java.util.function.Function)",91,93,"/**
* Delegates counter evaluation to the `counters` object.
* @param key Key for the counter.
* @param eval Function to evaluate the counter.
*/
","* add a mapping of a key to a counter function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addGaugeFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addGaugeFunction(java.lang.String,java.util.function.Function)",100,102,"/**
* Updates a gauge value using the provided key and evaluation function.
*/","* add a mapping of a key to a gauge function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMinimumFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMinimumFunction(java.lang.String,java.util.function.Function)",109,111,"/**
* Delegates the key and evaluation function to the minimums object.
*/","* add a mapping of a key to a minimum function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMaximumFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMaximumFunction(java.lang.String,java.util.function.Function)",118,120,"/**
* Delegates key and evaluation function to the maximums m1 method.
*/","* add a mapping of a key to a maximum function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMeanStatisticFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMeanStatisticFunction(java.lang.String,java.util.function.Function)",127,130,"/**
* Delegates mean statistics evaluation to the meanStatistics object.
* @param key Key for the statistic.
* @param eval Function to evaluate the statistic.
*/
","* add a mapping of a key to a meanStatistic function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,wrap,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:wrap(org.apache.hadoop.fs.statistics.IOStatistics),116,118,"/**
 * Wraps provided IOStatistics in a SourceWrappedStatistics.
 * @param statistics The IOStatistics to wrap.
 * @return A SourceWrappedStatistics instance.
 */
","* Take an IOStatistics instance and wrap it in a source.
   * @param statistics statistics.
   * @return a source which will return the values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,getAggregator,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getAggregator(),49,52,"/**
 * Returns an empty IO statistics aggregator.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,emptyStatisticsStore,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatisticsStore(),107,109,"/**
 * Returns an empty IO statistics store.
 */","* Get the shared instance of the immutable empty statistics
   * store.
   * @return an empty statistics object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,getIOStatistics,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getIOStatistics(),54,57,"/**
 * Returns an empty IO statistics instance.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,emptyStatistics,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatistics(),98,100,"/**
 * Returns an empty IO statistics instance.
 */","* Get the shared instance of the immutable empty statistics
   * object.
   * @return an empty statistics object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setCounter,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setCounter(java.lang.String,long)",172,176,"/**
* Sets a counter value for the given key.
* @param key The key for the counter.
* @param value The value to set for the counter.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMaximum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMaximum(java.lang.String,long)",199,202,"/**
* Updates a value associated with a key in the map.
* @param key The key to update.
* @param value The new value to associate with the key.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMinimum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMinimum(java.lang.String,long)",209,212,"/**
* Updates a value in the map using a key.
* @param key The key to update.
* @param value The new value to set.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setGauge,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setGauge(java.lang.String,long)",235,238,"/**
* Updates a gauge value for a given key.
* @param key The key for the gauge.
* @param value The new value to set.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementCounter,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter(java.lang.String,long)",178,197,"/**
* Increments counter for a key. Returns the final counter value.
* @param key counter key
* @param value increment value
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementMaximum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMaximum(java.lang.String,long)",204,207,"/**
* Calculates a value based on a key and value, using helper methods.
* @param key The key used for lookup.
* @param value The value to be processed.
* @return The calculated value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementMinimum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMinimum(java.lang.String,long)",214,217,"/**
* Calculates a value based on a key and value, using helper methods.
* @param key The key to use in the calculation.
* @param value The value to use in the calculation.
* @return The calculated value.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementGauge,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementGauge(java.lang.String,long)",240,243,"/**
* Updates gauge value for a key.
* @param key The key for the gauge.
* @param value The new value to set.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMinimumSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMinimumSample(java.lang.String,long)",219,225,"/**
* Updates the minimum value associated with a key.
* @param key Key to update.
* @param value New minimum value.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMaximumSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMaximumSample(java.lang.String,long)",227,233,"/**
* Updates the maximum value associated with a key.
* @param key The key for the maximum value.
* @param value The new maximum value.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMeanStatisticSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMeanStatisticSample(java.lang.String,long)",253,259,"/**
* Updates the mean statistic for a given key.
* @param key Key for the statistic.
* @param value Value to update the statistic with.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getCounterReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getCounterReference(java.lang.String),372,375,"/**
* Retrieves an AtomicLong from the counterMap for the given key.
*/","* Get a reference to the atomic instance providing the
   * value for a specific counter. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMaximumReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMaximumReference(java.lang.String),385,388,"/**
* Retrieves an AtomicLong value from the maximumMap for the given key.
*/","* Get a reference to the atomic instance providing the
   * value for a specific maximum. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMinimumReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMinimumReference(java.lang.String),398,401,"/**
* Retrieves an AtomicLong value associated with the given key.
* @param key the key to look up
* @return AtomicLong value or null if not found
*/","* Get a reference to the atomic instance providing the
   * value for a specific minimum. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getGaugeReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getGaugeReference(java.lang.String),411,414,"/**
* Retrieves an AtomicLong value from the gaugeMap for the given key.
*/","* Get a reference to the atomic instance providing the
   * value for a specific gauge. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMeanStatistic,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMeanStatistic(java.lang.String),422,425,"/**
* Retrieves the MeanStatistic for the given key.
* @param key The key to look up in the map.
* @return The MeanStatistic object.
*/
","* Get a mean statistic.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/PairedDurationTrackerFactory.java,asDuration,org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:asDuration(),87,90,"/**
* Delegates to the firstDuration's m1() method.
* Returns the Duration returned by the delegate.
*/
",* @return the global duration,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,counters,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:counters(),55,58,"/**
* Delegates to the result of m1().m2() and returns a map.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,gauges,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:gauges(),79,82,"/**
* Delegates to the underlying map.
* Returns a map containing string keys and long values.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,minimums,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:minimums(),84,87,"/**
* Delegates to the result of m1().m2() and returns a Map.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,maximums,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:maximums(),89,92,"/**
 * Delegates to the result of m1().m2() and returns a map.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,meanStatistics,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:meanStatistics(),94,97,"/**
* Delegates to the underlying map.
* Returns the map from the underlying object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,setWrapped,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:setWrapped(org.apache.hadoop.fs.statistics.IOStatistics),73,77,"/**
* Sets the wrapped IOStatistics, overwrites if not already set.
*/","* Set the wrapped statistics.
   * Will fail if the field is already set.
   * @param wrapped new value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,activeInstance,org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:activeInstance(),63,66,"/**
 * Returns the DynamicIOStatistics instance.
 * Checks if the instance is already built.
 */","* Get the statistics instance.
   * @return the instance to build/return
   * @throws IllegalStateException if the builder has already been built.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,checkMutable,org.apache.hadoop.fs.impl.FlagSet:checkMutable(),125,128,"/**
* Checks if FlagSet is mutable; throws exception if immutable.
*/","* Check for mutability before any mutating operation.
   * @throws IllegalStateException if the set is still mutable",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toByteArray,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:toByteArray(),235,250,"/**
 * Reads data into a byte array from file or stream.
 * @return byte array containing the data, or null if error.
 */","* Convert to a byte array.
     * If the data is stored in a file, it will be read and returned.
     * If the data was passed in via an input stream (which happens if the
     * data is stored in a bytebuffer) then it will be converted to a byte
     * array -which will then be cached for any subsequent use.
     *
     * @return byte[] after converting the uploadBlock.
     * @throws IOException throw if an exception is caught while reading
     *                     File/InputStream or closing InputStream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,bind,org.apache.hadoop.service.launcher.IrqHandler:bind(),89,100,"/**
* Binds a signal handler.
* @param name signal name; throws IllegalArgumentException on failure.
*/","* Bind to the interrupt handler.
   * @throws IllegalArgumentException if the exception could not be set",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CloseableReferenceCount.java,unreference,org.apache.hadoop.util.CloseableReferenceCount:unreference(),65,70,"/**
* Checks if the status is closed based on the returned value.
*/","* Decrement the reference count.
   *
   * @return          True if the object is closed and has no outstanding
   *                  references.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,run,org.apache.hadoop.ha.HealthMonitor$MonitorDaemon:run(),285,296,"/**
 * Repeatedly calls m2() and m3() until shouldRun is false.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setZooKeeperRef,org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:setZooKeeperRef(org.apache.zookeeper.ZooKeeper),1226,1231,"/**
* Sets the ZooKeeper instance.
* @param zk The ZooKeeper instance to set.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,snapshotMap,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map,java.util.function.Function)",214,221,"/**
 * Copies entries from a map to a ConcurrentHashMap using a function.
 * @param source source map
 * @param copyFn function to copy values
 * @return ConcurrentHashMap with copied entries
 */
","* Take a snapshot of a supplied map, using the copy function
   * to replicate the source values.
   * @param source source map
   * @param copyFn function to copy the value
   * @param <E> type of values.
   * @return a concurrent hash map referencing the same values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE)",445,450,"/**
 * Executes a Callable and extracts a value using DurationTracker.
 * @param factory DurationTrackerFactory
 * @param statistic Statistic name
 * @param input Callable raising IOE, returns B
 * @return Extracted value of type B
 */","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @param <B> return type.
   * @return the result of the operation.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,pairedTrackerFactory,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:pairedTrackerFactory(org.apache.hadoop.fs.statistics.DurationTrackerFactory,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",687,691,"/**
 * Creates a PairedDurationTrackerFactory from two factories.
 * @param first The first DurationTrackerFactory.
 * @param second The second DurationTrackerFactory.
 * @return A PairedDurationTrackerFactory.
 */
","* Create a DurationTrackerFactory which aggregates the tracking
   * of two other factories.
   * @param first first tracker factory
   * @param second second tracker factory
   * @return a factory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableCounterLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",60,66,"/**
* Records metrics based on 'all' flag and m1() result.
* @param builder MetricsRecordBuilder to populate
* @param all If true, always records; otherwise, depends on m1()
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheHit,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheHit(),82,84,"/**
 * Retrieves a cached value using the cacheHit object's m1() method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheCleared,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheCleared(),86,88,"/**
* Returns the value of the cacheCleared m1 property.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheUpdated,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheUpdated(),90,92,"/**
* Returns the value of the cacheUpdated m1 property.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getClientBackoffDisconnected,org.apache.hadoop.ipc.metrics.RpcMetrics:getClientBackoffDisconnected(),358,360,"/**
 * Retrieves a backoff value from the rpc client.
 * @return Long representing the backoff duration.
 */
","* Returns the number of disconnected backoffs.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getRpcSlowCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcSlowCalls(),420,422,"/**
* Returns the value of rpcSlowCalls.m1().
*/","* Returns the number of slow calls.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getRpcRequeueCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcRequeueCalls(),428,431,"/**
* Delegates to rpcRequeueCalls.m1() to retrieve a long value.
*/","* Returns the number of requeue calls.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,counters,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:counters(),48,51,"/**
* Delegates to the underlying map.
* Returns a map containing key-value pairs.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,gauges,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:gauges(),53,56,"/**
 * Delegates to the underlying map.
 * @return A map of strings to longs.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,minimums,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:minimums(),58,61,"/**
 * Delegates to the result of m1().m2(), returning a Map.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,maximums,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:maximums(),63,66,"/**
* Delegates to the m2() method of the underlying m1() object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,meanStatistics,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:meanStatistics(),68,71,"/**
* Delegates to the underlying object's m2() method.
* Returns a map of strings to MeanStatistic objects.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,aggregate,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:aggregate(org.apache.hadoop.fs.statistics.IOStatistics),73,76,"/**
* Delegates m2 call to the underlying m1 object.
* @param statistics IOStatistics object, can be null.
* @return Result of the delegated m2 call.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementCounter,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementCounter(java.lang.String,long)",78,81,"/**
* Delegates key-value pair storage to the underlying m1().
* @param key The key for storage.
* @param value The value associated with the key.
* @return The result of the underlying m1().m2() call.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setCounter,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setCounter(java.lang.String,long)",83,86,"/**
* Delegates method call to the underlying m1 instance.
* @param key The key to be used in the delegated method.
* @param value The value to be used in the delegated method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setGauge,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setGauge(java.lang.String,long)",88,91,"/**
* Delegates method call to the underlying m1().m2 method.
* @param key The key to be passed.
* @param value The value to be passed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementGauge,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementGauge(java.lang.String,long)",93,96,"/**
* Delegates key-value pair storage to the underlying m1().
* @param key The key for storage.
* @param value The value associated with the key.
* @return The result of the delegated operation.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMaximum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMaximum(java.lang.String,long)",98,101,"/**
* Delegates method call to the underlying m1().m2 method.
* @param key The key to be passed.
* @param value The value to be passed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementMaximum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMaximum(java.lang.String,long)",103,106,"/**
* Delegates the key-value pair processing to m1().
* @param key The key to be processed.
* @param value The value associated with the key.
* @return The result of m1().m2(key, value).
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMinimum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMinimum(java.lang.String,long)",108,112,"/**
 * Delegates the key-value pair to the underlying m1().m2 method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementMinimum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMinimum(java.lang.String,long)",114,118,"/**
* Delegates key-value pair processing to the underlying m1().
* @param key The key for the value.
* @param value The value associated with the key.
* @return The result of the delegated operation.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMinimumSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMinimumSample(java.lang.String,long)",120,124,"/**
* Delegates method call to the underlying m1().m2 method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMaximumSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMaximumSample(java.lang.String,long)",126,129,"/**
* Delegates method call to the underlying m1().m2.
* @param key key to be passed
* @param value value to be passed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",131,135,"/**
* Delegates the key-value pair to the parent's m2 method.
* @param key The key for the statistic.
* @param value The value of the statistic.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMeanStatisticSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMeanStatisticSample(java.lang.String,long)",137,141,"/**
 * Delegates the key-value pair to the underlying m1().m2() method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,reset,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:reset(),143,146,"/**
 * Calls the m2 method of the object returned by m1().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getCounterReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getCounterReference(java.lang.String),148,151,"/**
* Delegates to m1().m2(key) to retrieve an AtomicLong.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMaximumReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMaximumReference(java.lang.String),153,156,"/**
* Delegates the key-based AtomicLong retrieval to m1().m2().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMinimumReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMinimumReference(java.lang.String),158,161,"/**
* Delegates to m1().m2(key) to retrieve an AtomicLong.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getGaugeReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getGaugeReference(java.lang.String),163,166,"/**
* Delegates to m1().m2(key) to retrieve an AtomicLong.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMeanStatistic,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMeanStatistic(java.lang.String),168,171,"/**
* Delegates mean statistic retrieval to the underlying m1().
* @param key Key used to identify the statistic.
* @return MeanStatistic object.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,long)",173,178,"/**
* Delegates m2 call to the m1().m2 method.
* @param prefix String prefix for the operation.
* @param durationMillis Duration in milliseconds.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,java.time.Duration)",180,184,"/**
 * Delegates m2 call to the underlying m1 instance.
 * @param prefix String prefix for the operation.
 * @param duration Duration of the operation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,withDurationTracking,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withDurationTracking(java.lang.String[]),77,93,"/**
 * Configures IO statistics stores with given prefixes.
 * @param prefixes Array of prefixes for configuring stats.
 * @return Returns the builder instance for chaining.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,withSampleTracking,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withSampleTracking(java.lang.String[]),95,105,"/**
 * Applies prefixes to internal statistics trackers.
 * @param prefixes String array of prefixes to apply.
 * @return Returns the builder instance for chaining.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,reset,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:reset(),101,105,"/**
* Clears IOStatisticsContext and logs the action.
*/",* Reset the thread +.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,<init>,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>(),113,115,"/**
 * Initializes the IOStatisticsSnapshot by creating necessary maps.
 */",* Construct.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setCounter,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setCounter(java.lang.String,long)",226,229,"/**
 * Delegates key-value operation to m1().m2().
 * @param key The key for the operation.
 * @param value The value associated with the key.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setGauge,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setGauge(java.lang.String,long)",231,235,"/**
* Calls m1().m2() with the provided key and value.
* @param key The key to pass to m2.
* @param value The value to pass to m2.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMaximum,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMaximum(java.lang.String,long)",237,241,"/**
* Delegates key-value pair processing to m1().m2().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMinimum,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMinimum(java.lang.String,long)",243,246,"/**
* Delegates key-value pair storage to m1().m2().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",248,251,"/**
* Delegates key-value pair processing to m1().m2().
* @param key The key associated with the value.
* @param value The statistic value to be processed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,enabled,org.apache.hadoop.fs.statistics.IOStatisticsContext:enabled(),95,97,"/**
* Delegates to IOStatisticsContextIntegration.m1() and returns its result.
*/","* Static probe to check if the thread-level IO statistics enabled.
   *
   * @return if the thread-level IO statistics enabled.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,retrieveIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:retrieveIOStatistics(java.lang.Object),78,88,"/**
 * Retrieves IOStatistics from a source object.
 * @param source Object that may contain IOStatistics.
 * @return IOStatistics object or null if not found.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:<init>(),123,133,"/**
* Initializes a new BuiltInGzipDecompressor instance.
* Resets the internal state and CRC.
*/",* Creates a new (pure Java) gzip decompressor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,available,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:available(),189,192,"/**
* Adds the result of datas.m1() to the result of super.m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,available,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:available(),225,228,"/**
* Sums the result of m1() from datas and the superclass.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,seekToNewSource,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seekToNewSource(long),221,227,"/**
 * Processes data at targetPos, updates sums, and checks for new data.
 * @param targetPos Position to process; returns true if successful.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,readChunk,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])",229,267,"/**
* Reads data and verifies checksums.
* @param pos Current position.
* @param buf Buffer to read into.
* @param offset Offset in buffer.
* @param len Number of bytes to read.
* @param checksum Checksum array.
* @return Number of bytes read.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,verifySums,"org.apache.hadoop.fs.FSInputChecker:verifySums(byte[],int,int)",336,359,"/**
* Verifies checksums of data chunks.
* @param b data array, off offset, read number of bytes to read
* @throws ChecksumException if checksum verification fails
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,throwChecksumException,"org.apache.hadoop.util.DataChecksum:throwChecksumException(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.lang.String,long,int,int)",514,521,"/**
 * Throws a ChecksumException when checksums don't match.
 * @param type Checksum type, filename, errPos, expected, computed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getCounter,org.apache.hadoop.crypto.CryptoInputStream:getCounter(long),288,290,"/**
 * Calculates a masked position by dividing by codec's m2 value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getPadding,org.apache.hadoop.crypto.CryptoInputStream:getPadding(long),292,294,"/**
 * Calculates a masked value from a given position.
 * @param position The input position (long).
 * @return The masked byte value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,updateEncryptor,org.apache.hadoop.crypto.CryptoOutputStream:updateEncryptor(),220,228,"/**
 * Encrypts data using a codec and encryptor, updating internal state.
 */",Update the {@link #encryptor}: calculate counter and {@link #padding}.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,checkBufferSize,"org.apache.hadoop.crypto.CryptoStreamUtils:checkBufferSize(org.apache.hadoop.crypto.CryptoCodec,int)",90,95,"/**
 * Adjusts buffer size to be a multiple of codec's alignment.
 * @param codec CryptoCodec used for alignment calculation.
 * @param bufferSize Initial buffer size.
 */
","* Check and floor buffer size.
   *
   * @param codec crypto codec.
   * @param bufferSize the size of the buffer to be used.
   * @return calc buffer size.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,link,"org.apache.hadoop.io.nativeio.NativeIO:link(java.io.File,java.io.File)",1080,1087,"/**
* Copies a file. Uses native method if available, else uses HardLink.
* @param src Source file.
* @param dst Destination file.
*/
","* Creates a hardlink ""dst"" that points to ""src"".
   *
   * This is deprecated since JDK7 NIO can create hardlinks via the
   * {@link java.nio.file.Files} API.
   *
   * @param src source file
   * @param dst hardlink location
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,getInstance,org.apache.hadoop.fs.DelegationTokenRenewer:getInstance(),200,205,"/**
 * Returns the singleton DelegationTokenRenewer instance.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,makeRequestIfNeeded,org.apache.hadoop.fs.BatchedRemoteIterator:makeRequestIfNeeded(),84,96,"/**
 * Handles conditional logic based on index and entries.
 * Updates entries or calls m3() based on conditions.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java,<init>,"org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)",38,40,"/**
* Constructor for MD5MD5CRC32GzipFileChecksum.
* @param bytesPerCRC CRC bytes per segment
* @param crcPerBlock CRC bytes per block
* @param md5 MD5 hash object
*/
","* Create a MD5FileChecksum.
   *
   * @param bytesPerCRC bytesPerCRC.
   * @param crcPerBlock crcPerBlock.
   * @param md5 md5.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:<init>(),43,45,"/**
 * Default constructor. Initializes with default values.
 */
","Same as this(0, 0, null)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java,<init>,"org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)",38,40,"/**
 * Constructs a CastagnoliFileChecksum with given parameters.
 * @param bytesPerCRC Bytes per CRC value.
 * @param crcPerBlock CRC value per block.
 * @param md5 MD5 hash object.
 */
","* Create a MD5FileChecksum.
   *
   * @param bytesPerCRC bytesPerCRC.
   * @param crcPerBlock crcPerBlock.
   * @param md5 md5.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,accept,org.apache.hadoop.fs.GlobFilter:accept(org.apache.hadoop.fs.Path),79,82,"/**
* Checks if the path matches the pattern and passes the user filter.
* @param path The path to check.
* @return True if both conditions are met, false otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,set,org.apache.hadoop.fs.GlobPattern:set(java.lang.String),74,157,"/**
* Converts a glob pattern to a regex pattern, escaping special chars.
* @param glob The glob pattern string to convert.
*/","* Set and compile a glob pattern
   * @param glob  the glob pattern string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFsStatus,org.apache.hadoop.fs.FilterFs:getFsStatus(org.apache.hadoop.fs.Path),146,150,"/**
 * Delegates the m1 operation to the underlying FileSystem object.
 * @param f the path to operate on
 * @return the status of the operation
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listStatusIterator,org.apache.hadoop.fs.FilterFileSystem:listStatusIterator(org.apache.hadoop.fs.Path),291,295,"/**
 * Delegates to the file system's m1 method for RemoteIterator.
 * @param f The path to fetch the iterator for.
 * @return A RemoteIterator of FileStatus objects.
 */
",Return a remote iterator for listing in a directory,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,isRegularFile,org.apache.hadoop.fs.FileUtil:isRegularFile(java.io.File),632,634,"/**
 * Calls m1 with default flag.
 * @param file The file to process.
 * @return True if successful, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeShellPath,"org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File,boolean)",696,703,"/**
 * Returns file path, using canonical or absolute path.
 * @param file The file object.
 * @param makeCanonicalPath Whether to use canonical path.
 */","* Convert a os-native filename to a path that works for the shell.
   * @param file The filename to convert
   * @param makeCanonicalPath
   *          Whether to make canonical path for the file passed
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,permissionsFromMode,org.apache.hadoop.fs.FileUtil:permissionsFromMode(int),783,793,"/**
* Calculates PosixFilePermission set based on the given mode.
*/","* The permission operation of this method only involves users, user groups, and others.
   * If SUID is set, only executable permissions are reserved.
   * @param mode Permissions are represented by numerical values
   * @return The original permissions for files are stored in collections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unpackEntries,"org.apache.hadoop.fs.FileUtil:unpackEntries(org.apache.commons.compress.archivers.tar.TarArchiveInputStream,org.apache.commons.compress.archivers.tar.TarArchiveEntry,java.io.File)",1130,1186,"/**
* Extracts a TarArchiveEntry to a specified output directory.
* @param tis Input TarArchiveInputStream
* @param entry TarArchiveEntry to extract
* @param outputDir Directory to extract to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,join,"org.apache.hadoop.util.StringUtils:join(char,java.lang.String[])",1084,1086,"/**
 * Concatenates strings with a given separator.
 * @param separator Separator character.
 * @param strings Array of strings to concatenate.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execute,org.apache.hadoop.util.Shell$ShellCommandExecutor:execute(),1275,1283,"/**
 * Validates command entries and executes m2().
 * Throws IOException if a null entry is found.
 */","* Execute the shell command.
     * @throws IOException if the command fails, or if the command is
     * not well constructed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkWindowsCommandLineLength,org.apache.hadoop.util.Shell:checkWindowsCommandLineLength(java.lang.String[]),127,140,"/**
 * Validates command line length against a maximum limit.
 * @param commands Array of commands to validate.
 * @throws IOException if the combined length exceeds the limit.
 */","* Checks if a given command (String[]) fits in the Windows maximum command
   * line length Note that the input is expected to already include space
   * delimiters, no extra count will be added for delimiters.
   *
   * @param commands command parts, including any space delimiters
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/PowerShellFencer.java,buildPSScript,"org.apache.hadoop.ha.PowerShellFencer:buildPSScript(java.lang.String,java.lang.String)",115,158,"/**
 * Creates a PowerShell script to terminate a process on a host.
 * @param processName Process name to terminate.
 * @param host Hostname where the process runs.
 * @return PowerShell script string or null on failure.
 */","* Build a PowerShell script to kill a java.exe process in a remote machine.
   *
   * @param processName Name of the process to kill. This is an attribute in
   *                    CommandLine.
   * @param host Host where the process is.
   * @return Path of the PowerShell script.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toString,org.apache.hadoop.fs.permission.FsPermission:toString(),272,283,"/**
 * Concatenates symbols, modifies if stickyBit is true.
 * Returns the resulting string.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,join,"org.apache.hadoop.util.StringUtils:join(char,java.lang.Iterable)",1058,1060,"/**
 * Concatenates strings from iterable, using specified separator.
 * @param separator Separator string to use between strings.
 * @param strings Iterable of strings to concatenate.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,close,org.apache.hadoop.fs.sftp.SFTPFileSystem$2:close(),710,722,"/**
* Calls super.m2(), returns if closed, releases connection.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,close,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:close(),644,653,"/**
 * Executes m2 on multiple objects and closes the resource.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,close,org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:close(),391,400,"/**
 * Executes m2 on multiple objects and closes the resource.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,<init>,"org.apache.hadoop.fs.DU:<init>(java.io.File,long,long,long)",36,41,"/**
 * Constructs a DiskUsage object with given parameters.
 * @param path File path
 * @param interval Interval in seconds
 * @param jitter Jitter in seconds
 * @param initialUsed Initial used space in bytes
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,refresh,org.apache.hadoop.fs.DU:refresh(),50,58,"/**
 * Gets disk usage info; logs error if failure.
 * Uses duShell.m3(); logs error with path from m1().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,connect,"org.apache.hadoop.fs.sftp.SFTPConnectionPool:connect(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)",123,183,"/**
 * Establishes an SFTP channel with given credentials.
 * @param host SFTP host
 * @param port SFTP port
 * @param user Username
 * @param password Password
 * @param keyFile Key file path
 * @return ChannelSftp object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,disconnect,org.apache.hadoop.fs.sftp.SFTPConnectionPool:disconnect(com.jcraft.jsch.ChannelSftp),185,211,"/**
* Handles SFTP channel connection based on live connection count.
* Closes connection if limit exceeded, otherwise calls m2.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,<init>,"org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics,long)",86,90,"/**
 * Creates a FSDataOutputStream wrapping an OutputStream.
 * @param out The OutputStream to wrap.
 * @param stats Statistics object.
 * @param startPosition Starting position.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,getChecksumSize,org.apache.hadoop.fs.FSOutputSummer:getChecksumSize(),197,199,"/**
 * Delegates to the sum object's m1() method and returns the result.
 */",@return the size for a checksum.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,getChecksumSize,org.apache.hadoop.util.DataChecksum:getChecksumSize(int),345,347,"/**
* Calculates a value based on dataSize and m1(), m2() results.
*/","* the required checksum size given the data length.
   * @param dataSize data size.
   * @return the required checksum size given the data length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,convertToByteStream,"org.apache.hadoop.fs.FSOutputSummer:convertToByteStream(java.util.zip.Checksum,int)",237,239,"/**
* Creates a byte array based on checksum and size.
* @param sum Checksum object to derive data from.
* @param checksumSize Size of the resulting byte array.
*/","* Converts a checksum integer value to a byte stream
   *
   * @param sum check sum.
   * @param checksumSize check sum size.
   * @return byte stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesRead,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesRead(long),4206,4208,"/**
 * Updates the bytes read by calling m1().
 * @param newBytes The number of bytes to add.
 */","* Increment the bytes read in the statistics.
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesWritten,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesWritten(long),4214,4216,"/**
 * Updates the bytes written counter in m1.
 * @param newBytes The number of bytes to add.
 */
","* Increment the bytes written in the statistics.
     * @param newBytes the additional bytes written",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementReadOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementReadOps(int),4222,4224,"/**
* Increments the readOps count in m1 by the given count.
* @param count The amount to increment readOps by.
*/
","* Increment the number of read operations.
     * @param count number of read operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementLargeReadOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementLargeReadOps(int),4230,4232,"/**
 * Increments the largeReadOps counter in m1 by the given count.
 */","* Increment the number of large read operations.
     * @param count number of large read operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementWriteOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementWriteOps(int),4238,4240,"/**
* Adds the given count to the writeOps field of m1().
*/","* Increment the number of write operations.
     * @param count number of write operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesReadErasureCoded,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadErasureCoded(long),4246,4248,"/**
* Adds newBytes to bytesReadErasureCoded in m1().
* @param newBytes The number of bytes to add.
*/","* Increment the bytes read on erasure-coded files in the statistics.
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesReadByDistance,"org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadByDistance(int,long)",4258,4275,"/**
 * Updates bytes read based on distance.
 * @param distance Distance value to determine byte read category.
 * @param newBytes Number of bytes to add to the appropriate counter.
 */","* Increment the bytes read by the network distance in the statistics
     * In the common network topology setup, distance value should be an even
     * number such as 0, 2, 4, 6. To make it more general, we group distance
     * by {1, 2}, {3, 4} and {5 and beyond} for accounting.
     * @param distance the network distance
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,increaseRemoteReadTime,org.apache.hadoop.fs.FileSystem$Statistics:increaseRemoteReadTime(long),4281,4283,"/**
* Updates remote read time by adding the given duration.
* @param durationMS Duration in milliseconds to add.
*/
","* Increment the time taken to read bytes from remote in the statistics.
     * @param durationMS time taken in ms to read bytes from remote",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,visitAll,org.apache.hadoop.fs.FileSystem$Statistics:visitAll(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsAggregator),4295,4302,"/**
 * Applies a statistics aggregator to root and all data.
 * @param visitor Aggregator to process data; returns result.
 */
","* Apply the given aggregator to all StatisticsData objects associated with
     * this Statistics object.
     *
     * For each StatisticsData object, we will call accept on the visitor.
     * Finally, at the end, we will call aggregate to get the final total.
     *
     * @param         visitor to use.
     * @return        The total.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.FilterFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),603,606,"/**
* Delegates the Path m1 call to the underlying filesystem.
* @param path The path to operate on.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary$Builder:<init>(),47,48,"/**
 * Constructs a new Builder instance.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeConsumed,org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(long[]),108,112,"/**
* Calls superclass's m1 method and returns the builder.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeQuota,"org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(org.apache.hadoop.fs.StorageType,long)",114,118,"/**
 * Calls superclass's m1 method and returns the builder.
 * @param type Storage type.
 * @param quota Storage quota.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeConsumed,"org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(org.apache.hadoop.fs.StorageType,long)",120,124,"/**
* Calls superclass m1 and returns the builder instance.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeQuota,org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(long[]),126,130,"/**
* Calls superclass's m1 method and returns the builder.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,build,org.apache.hadoop.fs.QuotaUsage$Builder:build(),93,95,"/**
 * Creates a QuotaUsage object based on the current object.
 * @return A new QuotaUsage object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary:<init>(org.apache.hadoop.fs.ContentSummary$Builder),194,204,"/**
 * Constructs a ContentSummary object from a Builder.
 * @param builder The builder containing initialization parameters.
 */
","* Constructor for ContentSummary.Builder.
   *
   * @param builder builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getAlgorithmName,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getAlgorithmName(),60,64,"/**
* Concatenates strings and calls m1().m2() to generate a value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getChecksumOpt,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getChecksumOpt(),94,97,"/**
 * Creates a ChecksumOpt with CRC value and bytesPerCRC.
 * @return ChecksumOpt object containing calculated checksum.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$ChecksumOpt:<init>(),255,257,"/**
* Default constructor. Initializes with default checksum type and count.
*/",* Create a uninitialized one,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,createDisabled,org.apache.hadoop.fs.Options$ChecksumOpt:createDisabled(),287,289,"/**
 * Creates a ChecksumOpt with a NULL checksum type and -1 value.
 */","* Create a ChecksumOpts that disables checksum.
     *
     * @return ChecksumOpt.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,getChecksumOpt,org.apache.hadoop.fs.CompositeCrcFileChecksum:getChecksumOpt(),69,72,"/**
 * Creates a ChecksumOpt object with specified CRC type and bytes.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,write,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:write(java.io.DataOutput),106,111,"/**
* Writes CRC and MD5 data to the output stream.
* @param out Output stream to write the data to.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,hasPattern,org.apache.hadoop.fs.GlobFilter:hasPattern(),75,77,"/**
* Delegates to pattern's m1() method and returns the result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validate,org.apache.hadoop.fs.CreateFlag:validate(java.util.EnumSet),149,162,"/**
 * Validates CreateFlag options, throws exception if invalid.
 * Checks for null, m1(), and conflicting append/overwrite flags.
 */","* Validate the CreateFlag and throw exception if it is invalid
   * @param flag set of CreateFlag
   * @throws HadoopIllegalArgumentException if the CreateFlag is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/XAttrSetFlag.java,validate,"org.apache.hadoop.fs.XAttrSetFlag:validate(java.lang.String,boolean,java.util.EnumSet)",53,70,"/**
* Validates XAttr flags based on existence.
* @param xAttrName Attribute name.
* @param xAttrExists Attribute existence status.
* @param flag Flag set to validate.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,checkScheme,"org.apache.hadoop.fs.AbstractFileSystem:checkScheme(java.net.URI,java.lang.String)",291,300,"/**
 * Validates URI scheme against a supported scheme.
 * @param uri The URI to validate.
 * @param supportedScheme The expected scheme.
 */
","* Check that the Uri's scheme matches.
   *
   * @param uri name URI of the FS.
   * @param supportedScheme supported scheme.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InvalidPathException.java,<init>,org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String),38,40,"/**
* Constructs an InvalidPathException with a given path string.
* @param path The invalid path name.
*/
","* Constructs exception with the specified detail message.
   * 
   * @param path invalid path.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InvalidPathException.java,<init>,"org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String,java.lang.String)",48,51,"/**
 * Constructs an InvalidPathException with a path and reason.
 * @param path The invalid path.
 * @param reason Optional reason for the invalid path.
 */
","* Constructs exception with the specified detail message.
   * 
   * @param path invalid path.
   * @param reason Reason <code>path</code> is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,findFirstValidInput,org.apache.hadoop.io.erasurecode.coder.util.HHUtil:findFirstValidInput(java.lang.Object[]),210,219,"/**
 * Returns the first non-null element from the input array.
 * @param inputs Array of elements; returns the first non-null.
 * @throws HadoopIllegalArgumentException if all elements are null.
 */","* Find the valid input from all the inputs.
   *
   * @param <T> Generics Type T.
   * @param inputs input buffers to look for valid input
   * @return the first valid input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,findFirstValidInput,org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:findFirstValidInput(java.lang.Object[]),163,172,"/**
 * Returns the first non-null element from the input array.
 * @param inputs Array of elements; returns first non-null.
 * @return The first non-null element or throws exception.
 */","* Find the valid input from all the inputs.
   * @param inputs input buffers to look for valid input
   * @return the first valid input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,checkBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:checkBuffers(byte[][]),91,103,"/**
 * Validates input buffers, ensuring they are non-null and of correct length.
 */","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,checkOutputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkOutputBuffers(java.nio.ByteBuffer[]),129,145,"/**
 * Validates a set of ByteBuffer instances against expected properties.
 * @param buffers Array of ByteBuffers to validate.
 */
","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,checkOutputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkOutputBuffers(byte[][]),121,133,"/**
 * Validates input buffers, ensuring they are non-null and of correct length.
 */","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,checkBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:checkBuffers(java.nio.ByteBuffer[]),91,107,"/**
 * Validates a set of ByteBuffer instances.
 * @param buffers Array of ByteBuffers to validate.
 */","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkPrimitive,org.apache.hadoop.io.ArrayPrimitiveWritable:checkPrimitive(java.lang.Class),71,79,"/**
 * Validates component type: must be a non-null primitive type.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkDeclaredComponentType,org.apache.hadoop.io.ArrayPrimitiveWritable:checkDeclaredComponentType(java.lang.Class),81,88,"/**
 * Validates component type matches declared type, throws exception if mismatch.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkArray,org.apache.hadoop.io.ArrayPrimitiveWritable:checkArray(java.lang.Object),90,98,"/**
 * Validates input: must be non-null and its m1() method must return true.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseGetLevelArgs,"org.apache.hadoop.log.LogLevel$CLI:parseGetLevelArgs(java.lang.String[],int)",172,188,"/**
 * Parses ""-getlevel"" command, sets operation, and extracts host/class.
 * @param args Command-line arguments. @param index Starting index.
 * @return Next index after parsing.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseSetLevelArgs,"org.apache.hadoop.log.LogLevel$CLI:parseSetLevelArgs(java.lang.String[],int)",190,207,"/**
 * Parses -setlevel command, sets operation, and extracts parameters.
 * @param args Command-line arguments. @param index Starting index.
 * @return Next index after parsing.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,stopProxy,org.apache.hadoop.ipc.RPC:stopProxy(java.lang.Object),792,821,"/**
* Closes a proxy object, handling Closeable and invocation handlers.
* Throws exception if proxy is null or not closeable.
*/","* Stop the proxy. Proxy must either implement {@link Closeable} or must have
   * associated {@link RpcInvocationHandler}.
   * 
   * @param proxy
   *          the RPC proxy object to be stopped
   * @throws HadoopIllegalArgumentException
   *           if the proxy does not implement {@link Closeable} interface or
   *           does not have closeable {@link InvocationHandler}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,<init>,org.apache.hadoop.util.ZKUtil$BadAclFormatException:<init>(java.lang.String),206,208,"/**
 * Constructs a BadAclFormatException with the given error message.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,<init>,org.apache.hadoop.util.ZKUtil$BadAuthFormatException:<init>(java.lang.String),216,218,"/**
 * Constructs a BadAuthFormatException with the given error message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,processChecksumOpt,"org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt,int)",302,328,"/**
 * Selects checksum options, prioritizing user settings.
 * @param defaultOpt Default checksum option.
 * @param userOpt User-provided checksum option.
 * @param userBytesPerChecksum User-specified bytes per checksum.
 * @return Selected ChecksumOpt based on provided parameters.
 */","* A helper method for processing user input and default value to 
     * create a combined checksum option. This is a bit complicated because
     * bytesPerChecksum is kept for backward compatibility.
     *
     * @param defaultOpt Default checksum option
     * @param userOpt User-specified checksum option. Ignored if null.
     * @param userBytesPerChecksum User-specified bytesPerChecksum
     *                Ignored if {@literal <} 0.
     * @return ChecksumOpt.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setPermission,"org.apache.hadoop.fs.DelegateToFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",221,226,"/**
 * Sets file access permissions.
 * @param f The path of the file.
 * @param permission The desired file permissions.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setPermission,"org.apache.hadoop.fs.FilterFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",545,549,"/**
* Sets the permission of a file system path.
* @param p the path
* @param permission the permission to set
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",877,882,"/**
 * Delegates file system operation to target FS.
 * @param path The path to operate on.
 * @param snapshotName Snapshot name for the operation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,deleteSnapshot,"org.apache.hadoop.fs.FilterFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",409,413,"/**
* Delegates snapshot creation to the underlying file system.
* @param path Path to the snapshot.
* @param snapshotName Name of the snapshot.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getDefaultPortIfDefined,org.apache.hadoop.fs.DelegateToFileSystem:getDefaultPortIfDefined(org.apache.hadoop.fs.FileSystem),69,72,"/**
 * Gets the default port from FileSystem or uses a fallback.
 * @param theFsImpl FileSystem implementation.
 * @return Default port number.
 */
","* Returns the default port if the file system defines one.
   * {@link FileSystem#getDefaultPort()} returns 0 to indicate the default port
   * is undefined.  However, the logic that consumes this value expects to
   * receive -1 to indicate the port is undefined, which agrees with the
   * contract of {@link URI#getPort()}.
   *
   * @param theFsImpl file system to check for default port
   * @return default port, or -1 if default port is undefined",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,canonicalizeUri,org.apache.hadoop.fs.FileSystem:canonicalizeUri(java.net.URI),402,417,"/**
 * Modifies URI if m1() is -1 and m2() is positive.
 * Reconstructs URI with new components if necessary.
 */","* Canonicalize the given URI.
   *
   * This is implementation-dependent, and may for example consist of
   * canonicalizing the hostname using DNS and adding the default
   * port if not specified.
   *
   * The default implementation simply fills in the default port if
   * not specified and if {@link #getDefaultPort()} returns a
   * default port.
   *
   * @param uri url.
   * @return URI
   * @see NetUtils#getCanonicalUri(URI, int)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.DelegateToFileSystem:getInitialWorkingDirectory(),74,77,"/**
* Delegates m1() call to the underlying file system implementation.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.FilterFileSystem:getInitialWorkingDirectory(),324,327,"/**
* Delegates m1() call to the underlying file system.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.FilterFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),484,488,"/**
 * Retrieves the status of a file or directory.
 * @param f Path to the file/directory.
 * @return FileStatus object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.LocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),162,165,"/**
 * Delegates FileStatus retrieval to the underlying FileSystem.
 * @param f The path for which to retrieve the FileStatus.
 * @return FileStatus object representing the path.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),136,146,"/**
 * Retrieves a FileStatus for the given path, updating metadata.
 * @param f the path to get the FileStatus for
 * @return FileStatus object
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getLinkTarget,org.apache.hadoop.fs.DelegateToFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),257,260,"/**
* Delegates Path m1 operation to the underlying file system.
* @param f The Path object to operate on.
* @return The result of the m1 operation.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getLinkTarget,org.apache.hadoop.fs.FilterFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),494,496,"/**
* Delegates m1 operation to the file system.
* @param f The Path object to operate on.
* @return Path object returned by file system's m1.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,getLinkTarget,org.apache.hadoop.fs.LocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),167,170,"/**
* Delegates m1 operation to the underlying filesystem.
* @param f The path to operate on.
* @return The result of the m1 operation.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,truncate,"org.apache.hadoop.fs.DelegateToFileSystem:truncate(org.apache.hadoop.fs.Path,long)",200,204,"/**
* Resizes file to new length using the file system implementation.
* @param f Path to the file. @param newLength Desired file length.
* @throws IOException if an I/O error occurs.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,truncate,"org.apache.hadoop.fs.FilterFileSystem:truncate(org.apache.hadoop.fs.Path,long)",260,263,"/**
* Delegates file length modification to the file system.
* @param f Path to the file. @param newLength Desired length.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setReplication,"org.apache.hadoop.fs.DelegateToFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",228,233,"/**
 * Delegates file replication to the underlying file system.
 * @param f Path to the file. @param replication Replication factor.
 * @return True on success, false otherwise.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setReplication,"org.apache.hadoop.fs.FilterFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",240,243,"/**
* Delegates m1 call to the underlying file system.
* @param src Path object
* @param replication replication factor
* @return Result of the delegated call
*/
","* Set replication for an existing file.
   * 
   * @param src file name
   * @param replication new replication
   * @throws IOException raised on errors performing I/O.
   * @return true if successful;
   *         false if file does not exist or is a directory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setTimes,"org.apache.hadoop.fs.DelegateToFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",235,239,"/**
* Calls m1 and delegates to the underlying file system implementation.
* @param f The file path.
* @param mtime Modification time.
* @param atime Access time.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,updateTime,org.apache.hadoop.fs.shell.TouchCommands$Touch:updateTime(org.apache.hadoop.fs.shell.PathData),177,195,"/**
* Sets file modification and access times based on timestamp or current time.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setTimes,"org.apache.hadoop.fs.FilterFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",539,543,"/**
* Delegates m1 call to the underlying filesystem.
* @param p Path object
* @param mtime Modification time
* @param atime Access time
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setVerifyChecksum,org.apache.hadoop.fs.DelegateToFileSystem:setVerifyChecksum(boolean),241,244,"/**
* Calls the m1 method on the underlying file system implementation.
* @param verifyChecksum Flag to verify checksum during the operation.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setVerifyChecksum,org.apache.hadoop.fs.FilterFileSystem:setVerifyChecksum(boolean),512,515,"/**
* Delegates m1 call to the underlying fs object.
* @param verifyChecksum Flag to verify checksum during operation.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,supportsSymlinks,org.apache.hadoop.fs.DelegateToFileSystem:supportsSymlinks(),246,249,"/**
* Delegates m1() call to the underlying fsImpl.
* @return The result of fsImpl.m1()
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,supportsSymlinks,org.apache.hadoop.fs.FilterFileSystem:supportsSymlinks(),490,492,"/**
* Delegates to the underlying fs.m1() method and returns its result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,createSymlink,"org.apache.hadoop.fs.DelegateToFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",251,255,"/**
* Delegates file system operation to the underlying implementation.
* @param target The target path.
* @param link The link path.
* @param createParent Whether to create parent directories.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createSymlink,"org.apache.hadoop.fs.FilterFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",476,482,"/**
 * Delegates file creation to the file system.
 * @param target target path
 * @param link link path
 * @param createParent whether to create parent directories
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,createSymlink,"org.apache.hadoop.fs.LocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",156,160,"/**
* Delegates file system operation to the underlying file system.
* @param target Target path.
* @param link Link path.
* @param createParent Whether to create parent directories.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFs:truncate(org.apache.hadoop.fs.Path,long)",559,566,"/**
 * Updates file length via target file system.
 * @param f Path to file. @param newLength New file length.
 * @return True if successful, false otherwise.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,create,"org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",69,75,"/**
* Throws an UnsupportedOperationException.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,create,"org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",69,75,"/**
 * Throws an UnsupportedOperationException.
 * Represents an operation not yet supported.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,append,"org.apache.hadoop.fs.http.HttpsFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",77,81,"/**
* Throws an UnsupportedOperationException. Not implemented.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,append,"org.apache.hadoop.fs.http.HttpFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",77,81,"/**
* Throws an UnsupportedOperationException.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,rename,"org.apache.hadoop.fs.http.HttpsFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,86,"/**
* Throws an UnsupportedOperationException.
* Indicates the operation is not yet implemented.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,rename,"org.apache.hadoop.fs.http.HttpFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,86,"/**
* Throws an UnsupportedOperationException.
* Indicates the function is not yet implemented.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,delete,"org.apache.hadoop.fs.http.HttpsFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",88,91,"/**
* Throws an UnsupportedOperationException.
* @param path The path to operate on.
* @param b A boolean value, currently unused.
* @throws IOException If an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,delete,"org.apache.hadoop.fs.http.HttpFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",88,91,"/**
* Throws an UnsupportedOperationException.
* Represents an unimplemented functionality.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,listStatus,org.apache.hadoop.fs.http.HttpsFileSystem:listStatus(org.apache.hadoop.fs.Path),93,96,"/**
* Throws an UnsupportedOperationException; no status retrieval.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,listStatus,org.apache.hadoop.fs.http.HttpFileSystem:listStatus(org.apache.hadoop.fs.Path),93,96,"/**
* Throws an UnsupportedOperationException; does not fetch statuses.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,mkdirs,"org.apache.hadoop.fs.http.HttpsFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",107,111,"/**
 * Always returns false. Sets file access permissions.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,mkdirs,"org.apache.hadoop.fs.http.HttpFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",107,111,"/**
 * Sets file access permissions. Returns false if operation fails.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.http.HttpsFileSystem:getWorkingDirectory(),102,105,"/**
 * Returns the working directory as a Path object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.http.HttpFileSystem:getWorkingDirectory(),102,105,"/**
 * Returns the working directory as a Path object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.http.HttpsFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),98,100,"/**
 * Processes a given file path.
 * @param path The file path to process.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.http.HttpFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),98,100,"/**
 * Processes a given file path.
 * @param path The file path to be processed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getUri,org.apache.hadoop.fs.http.HttpsFileSystem:getUri(),56,59,"/**
 * Returns the URI.
 * @return The URI value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getUri,org.apache.hadoop.fs.http.HttpFileSystem:getUri(),56,59,"/**
 * Returns the URI.
 * @return The URI value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,skip,org.apache.hadoop.fs.BufferedFSInputStream:skip(long),70,78,"/**
* Applies a mask to a number, calls m2, and returns the number.
* @param n The number to be masked.
* @return The original number.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,minSeekForVectorReads,org.apache.hadoop.fs.BufferedFSInputStream:minSeekForVectorReads(),169,172,"/**
* Delegates m1() call to the underlying PositionedReadable object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,minSeekForVectorReads,org.apache.hadoop.fs.FSDataInputStream:minSeekForVectorReads(),294,297,"/**
 * Delegates m1() call to the underlying PositionedReadable object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,maxReadSizeForVectorReads,org.apache.hadoop.fs.BufferedFSInputStream:maxReadSizeForVectorReads(),174,177,"/**
* Delegates m1() call to the underlying PositionedReadable object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,maxReadSizeForVectorReads,org.apache.hadoop.fs.FSDataInputStream:maxReadSizeForVectorReads(),299,302,"/**
* Delegates m1() call to the underlying PositionedReadable object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBPartHandle.java,from,org.apache.hadoop.fs.BBPartHandle:from(java.nio.ByteBuffer),40,42,"/**
* Creates a PartHandle from a ByteBuffer.
* @param byteBuffer The ByteBuffer to wrap.
* @return A PartHandle instance.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBPartHandle.java,equals,org.apache.hadoop.fs.BBPartHandle:equals(java.lang.Object),54,62,"/**
 * Compares this PartHandle with another.
 * @param other The object to compare to.
 * @return True if equal, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",891,897,"/**
* Delegates path resolution and policy application to the FS.
* @param path The path to resolve.
* @param policyName The policy name to apply.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setStoragePolicy,"org.apache.hadoop.fs.FilterFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",420,424,"/**
* Delegates the m1 call to the underlying file system.
* @param path The path to operate on.
* @param policyName The policy name to apply.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,disconnect,org.apache.hadoop.fs.ftp.FTPFileSystem:disconnect(org.apache.commons.net.ftp.FTPClient),248,260,"/**
 * Disconnects from the FTP client, logging errors if logout fails.
 * @param client The FTPClient object to disconnect.
 * @throws IOException if an I/O error occurs.
 */","* Logout and disconnect the given FTPClient. *
   * 
   * @param client
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,close,org.apache.hadoop.fs.ftp.FTPFileSystem$1:close(),104,107,"/**
* Delegates the m1 method call to the wrapped object.
*/",* Close the underlying output stream.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,close,org.apache.hadoop.fs.ftp.FTPInputStream:close(),103,121,"/**
 * Executes m1, checks client connection, and completes transfer.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFsAction,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFsAction(int,org.apache.commons.net.ftp.FTPFile)",440,453,"/**
 * Determines FsAction based on access group and FTPFile.
 * @param accessGroup Access group identifier.
 * @param ftpFile The FTPFile to check permissions for.
 * @return FsAction representing the allowed actions.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hasNext,org.apache.hadoop.fs.FileSystem$DirListingIterator:hasNext(),2320,2324,"/**
* Checks if a condition based on entries is true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary:<init>(),149,150,"/**
 * Default constructor for ContentSummary. Deprecated.
 */",Constructor deprecated by ContentSummary.Builder,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,"org.apache.hadoop.fs.ContentSummary:<init>(long,long,long,long,long,long)",177,187,"/**
 * Creates a ContentSummary with provided size and count details.
 */
","* Constructor, deprecated by ContentSummary.Builder.
   *
   * @param length length.
   * @param fileCount file count.
   * @param directoryCount directory count.
   * @param quota quota.
   * @param spaceConsumed space consumed.
   * @param spaceQuota space quota.
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,equals,org.apache.hadoop.fs.ContentSummary:equals(java.lang.Object),257,275,"/**
* Checks if this object equals to another object.
* @param to The object to compare with.
* @return True if objects are equal, false otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,hashCode,org.apache.hadoop.fs.ContentSummary:hashCode(),277,284,"/**
 * Calculates a value by XORing results of other methods.
 * Returns the XOR of the result and the superclass's m9().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",824,829,"/**
 * Delegates file read to the target file system.
 * @param path Path to the file.
 * @param name File name.
 * @return Byte array of file content.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttr,"org.apache.hadoop.fs.FilterFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",371,374,"/**
* Delegates file read operation to the underlying FileSystem.
* @param path Path to the file.
* @param name Name of the file.
* @return Byte array representing the file content.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,getDelay,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:getDelay(java.util.concurrent.TimeUnit),82,86,"/**
 * Calculates remaining renewal time in specified units.
 * @param unit Time unit (e.g., SECONDS, DAYS)
 * @return Remaining renewal time in the given unit.
 */",Get the delay until this event should happen.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,updateRenewalTime,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:updateRenewalTime(long),116,118,"/**
* Calculates renewal time based on delay, adjusting for precision.
*/","* Set a new time for the renewal.
     * It can only be called when the action is not in the queue or any
     * collection because the hashCode may change
     * @param delay the renewal time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,touch,org.apache.hadoop.ipc.Client$Connection:touch(),465,467,"/**
* Updates the last activity timestamp.
* Records the current time as the last activity.
*/
",Update lastActivity with the current time.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ThreadUtil.java,sleepAtLeastIgnoreInterrupts,org.apache.hadoop.util.ThreadUtil:sleepAtLeastIgnoreInterrupts(long),39,50,"/**
 * Sleeps for the specified duration, handling interruptions.
 * @param millis Duration to sleep in milliseconds.
 */","* Cause the current thread to sleep as close as possible to the provided
   * number of milliseconds. This method will log and ignore any
   * {@link InterruptedException} encountered.
   * 
   * @param millis the number of milliseconds for the current thread to sleep",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,now,org.apache.hadoop.util.Timer:now(),39,41,"/**
* Delegates to Time.m1() and returns the result.
*/","* Current system time.  Do not use this to calculate a duration or interval
   * to sleep, because it will be broken by settimeofday.  Instead, use
   * monotonicNow.
   * @return current time in msec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AsyncDiskService.java,awaitTermination,org.apache.hadoop.util.AsyncDiskService:awaitTermination(long),131,147,"/**
 * Awaits termination of all thread pool executors.
 * @param milliseconds Timeout in milliseconds. Returns true on success.
 */","* Wait for the termination of the thread pools.
   * 
   * @param milliseconds  The number of milliseconds to wait
   * @return   true if all thread pools are terminated without time limit
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,ceiling,"org.apache.hadoop.fs.TrashPolicyDefault$Emptier:ceiling(long,long)",322,324,"/**
* Calculates a masked time value.
* @param time The base time value.
* @param interval The interval to add.
* @return The masked time value.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,readChunk,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])",266,307,"/**
* Reads data and verifies checksums.
* @param pos current position
* @param buf buffer to read into
* @param offset offset in buffer
* @param len number of bytes to read
* @param checksum checksum array
* @return number of bytes read
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,seekToNewSource,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:seekToNewSource(long),258,264,"/**
 * Processes data at targetPos, updates sums, and checks for new data.
 * @param targetPos the position to process
 * @return true if data was updated or new data was found
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,checkBytes,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:checkBytes(java.nio.ByteBuffer,long,java.nio.ByteBuffer,long,int,org.apache.hadoop.fs.Path)",376,426,"/**
* Verifies checksums for data chunks and throws exception on mismatch.
* @param sumsBytes Byte buffer containing checksum sums.
* @param file Path to the file being checked.
*/","* Check the data against the checksums.
     * @param sumsBytes the checksum data
     * @param sumsOffset where from the checksum file this buffer started
     * @param data the file data
     * @param dataOffset where the file data started (must be a multiple of
     *                  bytesPerSum)
     * @param bytesPerSum how many bytes per a checksum
     * @param file the path of the filename
     * @return the data buffer
     * @throws CompletionException if the checksums don't match",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getSumBufferSize,"org.apache.hadoop.fs.ChecksumFs:getSumBufferSize(int,int,org.apache.hadoop.fs.Path)",124,131,"/**
 * Calculates buffer size, considering file size and defaults.
 * @param bytesPerSum Bytes per sum; used for proportional sizing.
 * @param bufferSize Initial buffer size.
 * @param file The file to analyze.
 * @return Calculated buffer size.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,open,org.apache.hadoop.fs.AbstractFileSystem:open(org.apache.hadoop.fs.Path),721,724,"/**
 * Opens an FSDataInputStream for the given path.
 * @param f Path to open; uses default replication factor.
 * @return FSDataInputStream for the path.
 */","* The specification of this method matches that of
   * {@link FileContext#open(Path)} except that Path f must be for this
   * file system.
   *
   * @param f the path.
   * @throws AccessControlException access control exception.
   * @throws FileNotFoundException file not found exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.
   * @return input stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getServerDefaults,org.apache.hadoop.fs.FilterFs:getServerDefaults(org.apache.hadoop.fs.Path),163,166,"/**
* Delegates to the underlying FileSystem's m1 method.
* @param f Path to delegate to.
* @return FsServerDefaults object.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getChecksumFileLength,"org.apache.hadoop.fs.ChecksumFs:getChecksumFileLength(org.apache.hadoop.fs.Path,long)",111,113,"/**
* Calculates a value using fileSize and a constant.
* @param file The file object.
* @param fileSize The size of the file.
* @return A long value calculated from fileSize and m1().
*/
","* Return the length of the checksum file given the size of the
   * actual file.
   *
   * @param file the file path.
   * @param fileSize file size.
   * @return check sum file length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,listLocatedStatus,org.apache.hadoop.fs.ChecksumFs:listLocatedStatus(org.apache.hadoop.fs.Path),582,614,"/**
 * Wraps an iterator to filter LocatedFileStatus objects.
 * @param f the path to iterate over
 * @return A filtered RemoteIterator of LocatedFileStatus.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,useStatIfAvailable,org.apache.hadoop.fs.RawLocalFileSystem:useStatIfAvailable(),96,99,"/**
* Sets useDeprecatedFileStatus based on the result of Stat.m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createOutputStream,"org.apache.hadoop.fs.RawLocalFileSystem:createOutputStream(org.apache.hadoop.fs.Path,boolean)",570,573,"/**
* Creates an OutputStream for the given Path, optionally appending.
* @param f Path to open
* @param append Whether to append to the file
* @return OutputStream object
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatus,org.apache.hadoop.fs.FileSystem:getStatus(org.apache.hadoop.fs.Path),3041,3043,"/**
 * Returns a fixed FsStatus object.
 * Always returns a status with max values.
 */","* Returns a status object describing the use and capacity of the
   * filesystem. If the filesystem has multiple partitions, the
   * use and capacity of the partition pointed to by the specified
   * path is reflected.
   * @param p Path for which status should be obtained. null means
   * the default partition.
   * @return a FsStatus object
   * @throws IOException
   *           see specific implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFsStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFsStatus(),1130,1133,"/**
* Returns a default FsStatus with all fields initialized to 0.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFsStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFsStatus(),445,449,"/**
* Returns a default FsStatus object with all values set to 0.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,registerCommands,org.apache.hadoop.fs.FsShellPermissions:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),50,54,"/**
 * Registers chmod, chown, and chgrp commands with the factory.
 */","* Register the permission related commands with the factory
   * @param factory the command factory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,registerCommands,org.apache.hadoop.fs.shell.Test:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),37,39,"/**
 * Calls factory.m1 with Test.class and ""-test"" as arguments.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,registerCommands,org.apache.hadoop.fs.shell.SnapshotCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),41,45,"/**
 * Registers snapshot commands with the given command factory.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,registerCommands,org.apache.hadoop.fs.shell.find.Find:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),49,51,"/**
 * Configures the CommandFactory with a Find command.
 * @param factory The CommandFactory to configure.
 */
","* Register the names for the count command
   * 
   * @param factory the command factory that will instantiate this class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,registerCommands,org.apache.hadoop.fs.shell.Head:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),40,42,"/**
 * Configures the command factory with a head command.
 * @param factory The command factory to configure.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,registerCommands,org.apache.hadoop.fs.shell.Ls:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),45,48,"/**
 * Configures the command factory with Ls and Lsr commands.
 * @param factory The command factory to configure.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,registerCommands,org.apache.hadoop.fs.shell.Tail:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),42,44,"/**
 * Registers a tail command with the given command factory.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,registerCommands,org.apache.hadoop.fs.shell.FsUsage:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),46,50,"/**
 * Configures the command factory with default flags.
 * @param factory The CommandFactory to configure.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,registerCommands,org.apache.hadoop.fs.shell.XAttrCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),42,45,"/**
 * Registers commands with the factory: Getfattr and Setfattr.
 * @param factory CommandFactory instance to register commands.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,registerCommands,org.apache.hadoop.fs.shell.Delete:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),50,55,"/**
 * Registers commands with the given factory.
 * @param factory CommandFactory to register commands with.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,registerCommands,org.apache.hadoop.fs.shell.Count:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),46,48,"/**
 * Registers a count command with the given command factory.
 */","* Register the names for the count command
   * @param factory the command factory that will instantiate this class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,registerCommands,org.apache.hadoop.fs.shell.TouchCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),43,46,"/**
 * Registers Touchz and Touch commands with the given factory.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,registerCommands,org.apache.hadoop.fs.shell.Mkdir:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),39,41,"/**
 * Registers the Mkdir command with the given factory.
 * @param factory CommandFactory instance to register with.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Concat.java,registerCommands,org.apache.hadoop.fs.shell.Concat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),38,40,"/**
* Registers the Concat command with the given factory.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,registerCommands,org.apache.hadoop.fs.shell.CopyCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),44,52,"/**
 * Registers commands with the given factory, associating them with strings.
 * @param factory CommandFactory instance to register commands with.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,registerCommands,org.apache.hadoop.fs.shell.MoveCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),35,39,"/**
 * Registers command classes with the given factory.
 * @param factory CommandFactory instance to register commands.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,registerCommands,org.apache.hadoop.fs.shell.Stat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),53,55,"/**
 * Configures the CommandFactory with a Stat-related command.
 * @param factory The CommandFactory to configure.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,registerCommands,org.apache.hadoop.fs.shell.Display:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),62,66,"/**
 * Registers command classes with the given factory.
 * @param factory CommandFactory instance to register commands.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,registerCommands,org.apache.hadoop.fs.shell.AclCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),47,50,"/**
 * Registers getfacl and setfacl commands with the factory.
 * @param factory CommandFactory instance to register commands.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,registerCommands,org.apache.hadoop.fs.shell.Truncate:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),35,37,"/**
* Registers the Truncate command with the given factory.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,registerCommands,org.apache.hadoop.fs.shell.SetReplication:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),37,39,"/**
 * Configures the CommandFactory with a set replication command.
 * @param factory The CommandFactory to configure.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat:<init>(java.lang.String,int,int,java.lang.String[])",45,48,"/**
 * Constructs a CommandFormat with a name, min, max, and options.
 * @param name Command name
 * @param min Minimum number of arguments
 * @param max Maximum number of arguments
 */
","* @deprecated use replacement since name is an unused parameter
   * @param name of command, but never used
   * @param min see replacement
   * @param max see replacement
   * @param possibleOpt see replacement
   * @see #CommandFormat(int, int, String...)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,"org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(org.apache.hadoop.fs.Path,boolean)",361,403,"/**
 * Deletes trash checkpoints older than deletionInterval.
 * @param trashRoot Root directory of the trash.
 * @param deleteImmediately Flag to force immediate deletion.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystemPathHandle.java,verify,org.apache.hadoop.fs.LocalFileSystemPathHandle:verify(org.apache.hadoop.fs.FileStatus),54,61,"/**
 * Validates FileStatus, throws exception if null or content changed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",802,807,"/**
 * Delegates path access to the target file system, applying ACLs.
 * @param path The path to access.
 * @param aclSpec ACL specifications to apply.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setAcl,"org.apache.hadoop.fs.FilterFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",349,352,"/**
* Delegates ACL modification to the underlying file system.
* @param path Path to modify ACLs on.
* @param aclSpec List of ACL entries to apply.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,startUpload,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:startUpload(org.apache.hadoop.fs.Path),98,110,"/**
 * Uploads a file, creates a handle, and returns a future.
 * @param filePath Path to the file to upload.
 * @return CompletableFuture containing the UploadHandle.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,putPart,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:putPart(org.apache.hadoop.fs.UploadHandle,int,org.apache.hadoop.fs.Path,java.io.InputStream,long)",112,122,"/**
* Uploads a part of a file.
* @param uploadId Upload handle.
* @param partNumber Part number.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,complete,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:complete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)",176,185,"/**
 * Processes an upload, masks the file, and returns a PathHandle.
 * @param uploadId Upload handle. @param filePath File path.
 * @param handleMap Map of part handles.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,eval,org.apache.hadoop.fs.impl.FutureIOSupport:eval(org.apache.hadoop.util.functional.CallableRaisingIOE),179,182,"/**
 * Executes a CallableRaisingIOE in a future, handling IOE.
 * @param callable Callable that might raise an IOE
 * @return CompletableFuture representing the result.
 */","* Evaluate a CallableRaisingIOE in the current thread,
   * converting IOEs to RTEs and propagating.
   * See {@link FutureIO#eval(CallableRaisingIOE)}.
   *
   * @param callable callable to invoke
   * @param <T> Return type.
   * @return the evaluated result.
   * @throws UnsupportedOperationException fail fast if unsupported
   * @throws IllegalArgumentException invalid argument",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,concat,"org.apache.hadoop.fs.FilterFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",188,191,"/**
* Delegates file system operation to the underlying file system.
* @param f file path
* @param psrcs source paths
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,rejectUnknownMandatoryKeys,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:rejectUnknownMandatoryKeys(java.util.Collection,java.lang.String)",358,362,"/**
* Calls overloaded method with mandatory keys and extra error text.
*/","* Reject a configuration if one or more mandatory keys are
   * not in the set of mandatory keys.
   * The first invalid key raises the exception; the order of the
   * scan and hence the specific key raising the exception is undefined.
   * @param knownKeys a possibly empty collection of known keys
   * @param extraErrorText extra error text to include.
   * @throws IllegalArgumentException if any key is unknown.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileRangeImpl.java,toString,org.apache.hadoop.fs.impl.FileRangeImpl:toString(),54,58,"/**
* Formats a string with range, length, and reference details.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,pathCapabilities,org.apache.hadoop.fs.impl.FlagSet:pathCapabilities(),209,213,"/**
* Transforms names to values based on capability.
* Returns a list of transformed values.
*/","* Generate the list of capabilities.
   * @return a possibly empty list.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,buildHttpReferrer,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:buildHttpReferrer(),190,223,"/**
* Constructs a referrer header URI. Returns """" on failure.
*/","* Build the referrer string.
   * This includes dynamically evaluating all of the evaluated
   * attributes.
   * If there is an error creating the string it will be logged once
   * per entry, and """" returned.
   * @return a referrer string or """"",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,<init>,"org.apache.hadoop.util.WeakReferenceMap:<init>(java.util.function.Function,java.util.function.Consumer)",100,106,"/**
 * Constructs a WeakReferenceMap with a factory and optional listener.
 * @param factory Creates values for keys.
 * @param referenceLost Listener for lost references.
 */
","* instantiate.
   * @param factory supplier of new instances
   * @param referenceLost optional callback on lost references.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/StoreImplementationUtils.java,hasCapability,"org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.OutputStream,java.lang.String)",80,82,"/**
* Delegates to m1 with the provided output stream and capability.
*/","* Probe for an output stream having a capability; returns true
   * if the stream implements {@link StreamCapabilities} and its
   * {@code hasCapabilities()} method returns true for the capability.
   * @param out output stream
   * @param capability capability to probe for
   * @return true if the stream declares that it supports the capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/StoreImplementationUtils.java,hasCapability,"org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.InputStream,java.lang.String)",92,94,"/**
* Delegates to m1 with the provided input stream and capability.
*/","* Probe for an input stream having a capability; returns true
   * if the stream implements {@link StreamCapabilities} and its
   * {@code hasCapabilities()} method returns true for the capability.
   * @param in input stream
   * @param capability capability to probe for
   * @return true if the stream declares that it supports the capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/ExecutorServiceFuturePool.java,shutdown,"org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:shutdown(org.slf4j.Logger,long,java.util.concurrent.TimeUnit)",81,83,"/**
* Delegates execution to HadoopExecutors, using provided logger and timeout.
*/","* Utility to shutdown the {@link ExecutorService} used by this class. Will wait up to a
   * certain timeout for the ExecutorService to gracefully shutdown.
   *
   * @param logger Logger
   * @param timeout the maximum time to wait
   * @param unit the time unit of the timeout argument",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,<init>,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:<init>(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),128,131,"/**
 * Constructs an End object.
 * @param op The Operation object associated with the end.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getSummary(java.lang.StringBuilder),133,137,"/**
 * Appends ""E"" to StringBuilder, then calls superclass's m2.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getDebugInfo,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getDebugInfo(),139,142,"/**
* Calls super.m1(), then calls m2 on the result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,add,org.apache.hadoop.fs.impl.prefetch.BlockOperations:add(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),160,166,"/**
 * Executes an operation and logs it if in debug mode.
 * @param op The operation to execute. Returns the same operation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,canRelease,org.apache.hadoop.fs.impl.prefetch.BufferPool:canRelease(org.apache.hadoop.fs.impl.prefetch.BufferData),318,322,"/**
* Checks if data transition from DONE to READY is valid.
* @param data BufferData object to validate.
* @return True if transition is valid, false otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:get(),411,420,"/**
 * Prefetches a block, logs errors if any occur.
 * @param data Block data to prefetch.
 * @param taskQueuedStartTime Start time of the task.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,distance,"org.apache.hadoop.fs.impl.prefetch.BufferPool:distance(org.apache.hadoop.fs.impl.prefetch.BufferData,int)",226,228,"/**
* Calculates a masked value based on data and block number.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,find,org.apache.hadoop.fs.impl.prefetch.BufferPool:find(int),305,316,"/**
 * Finds a buffer data with the given block number and not done.
 * @param blockNumber The block number to search for.
 * @return BufferData object or null if not found.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,close,org.apache.hadoop.fs.impl.prefetch.BufferPool:close(),257,275,"/**
 * Processes buffer data, releases resources, and updates statistics.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,acquire,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:acquire(),70,73,"/**
* Calls m1 with the 'true' flag as an argument.
*/",* Acquires a resource blocking if necessary until one becomes available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,tryAcquire,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:tryAcquire(),78,81,"/**
 * Calls m1 with 'false' as an argument and returns the result.
 */",* Acquires a resource blocking if one is immediately available. Otherwise returns null.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,close,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:close(),113,124,"/**
 * Releases resources held by createdItems and items.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:numAvailable(),148,150,"/**
* Calculates a masked value based on size, m1(), and items.m2().
*/","* Number of items available to be acquired. Mostly for testing purposes.
   * @return the number available.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,duration,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:duration(),144,146,"/**
 * Calculates a value based on m1() and op.m1(), divided by 1e9.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,analyze,org.apache.hadoop.fs.impl.prefetch.BlockOperations:analyze(java.lang.StringBuilder),297,367,"/**
* Analyzes operations, groups by block, and logs discrepancies.
* @param sb StringBuilder to append analysis results to.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,<init>,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:<init>(org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics,int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",223,234,"/**
 * Constructs a SingleFilePerBlockCache with given parameters.
 * @param prefetchingStatistics Prefetching statistics object.
 * @param maxBlocksCount Max number of blocks to cache.
 * @param trackerFactory Duration tracker factory.
 */
","* Constructs an instance of a {@code SingleFilePerBlockCache}.
   *
   * @param prefetchingStatistics statistics for this stream.
   * @param maxBlocksCount max blocks count to be kept in cache at any time.
   * @param trackerFactory tracker with statistics to update",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,<init>,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",71,82,"/**
 * Constructs a SemaphoredDelegatingExecutor.
 * @param executorDelegatee Executor to delegate to.
 * @param permitCount Semaphore permits.
 */
","* Instantiate.
   * @param executorDelegatee Executor to delegate to
   * @param permitCount number of permits into the queue permitted
   * @param fair should the semaphore be ""fair""
   * @param trackerFactory duration tracker factory.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,addToLinkedListHead,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListHead(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),314,321,"/**
 * Processes an entry, acquiring and releasing locks.
 * @param entry The entry to be processed.
 */","* Helper method to add the given entry to the head of the linked list.
   *
   * @param entry Block entry to add.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,validateEntry,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:validateEntry(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry,java.nio.ByteBuffer)",551,566,"/**
* Validates entry size and checksum against the provided buffer.
* @param entry The entry to validate.
* @param buffer The buffer to check against.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setDone,org.apache.hadoop.fs.impl.prefetch.BufferData:setDone(),223,231,"/**
 * Verifies checksum, sets state to DONE, and clears action.
 */",* Indicates that this block is no longer of use and can be reclaimed.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,close,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:close(),502,508,"/**
 * Performs an action if not closed, logs a message, and then acts.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,toString,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:toString(),540,549,"/**
 * Constructs a string containing statistics and block information.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,toString,org.apache.hadoop.fs.impl.prefetch.BufferData:toString(),289,299,"/**
* Creates a formatted string with block details.
* @return Formatted string containing block information.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,throwIfInvalidBuffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:throwIfInvalidBuffer(),298,300,"/**
* Checks if 'buffer' is null; throws exception if so.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,bufferSize,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:bufferSize(int),133,136,"/**
* Sets buffer size and returns a FutureDataInputStreamBuilder.
* @param bufSize The desired buffer size.
*/
","* Set the size of the buffer to be used.
   *
   * @param bufSize buffer size.
   * @return FutureDataInputStreamBuilder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,builder,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:builder(),146,148,"/**
* Returns a FutureDataInputStreamBuilder instance.
*/","* Get the builder.
   * This must be used after the constructor has been invoked to create
   * the actual builder: it allows for subclasses to do things after
   * construction.
   *
   * @return FutureDataInputStreamBuilder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,getForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:getForCurrentThread(),45,47,"/**
* Delegates to m1() and then m2, returning the result.
*/","* Get the value for the current thread, creating if needed.
   * @return an instance.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,removeForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:removeForCurrentThread(),53,55,"/**
* Delegates to m1() and then m2. Returns the result.
*/","* Remove the reference for the current thread.
   * @return any reference value which existed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,setForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:setForCurrentThread(java.lang.Object),70,90,"/**
 * Updates a value associated with an ID.
 * @param newVal The new value to set.
 * @return The new value.
 */
","* Set the new value for the current thread.
   * @param newVal new reference to set for the active thread.
   * @return the previously set value, possibly null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,<init>,"org.apache.hadoop.fs.impl.CombinedFileRange:<init>(long,long,org.apache.hadoop.fs.FileRange)",44,47,"/**
 * Creates a CombinedFileRange with given offset, end, and original range.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,merge,"org.apache.hadoop.fs.impl.CombinedFileRange:merge(long,long,org.apache.hadoop.fs.FileRange,int,int)",79,89,"/**
* Checks if a range overlaps and updates size.
* @param otherOffset, otherEnd: Range boundaries.
* @param other: FileRange object.
* @param minSeek, maxSize: Seek and size constraints.
*/
","* Merge this input range into the current one, if it is compatible.
   * It is assumed that otherOffset is greater or equal the current offset,
   * which typically happens by sorting the input ranges on offset.
   * @param otherOffset the offset to consider merging
   * @param otherEnd the end to consider merging
   * @param other the underlying FileRange to add if we merge
   * @param minSeek the minimum distance that we'll seek without merging the
   *                ranges together
   * @param maxSize the maximum size that we'll merge into a single range
   * @return true if we have merged the range into this one",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createBulkDelete,org.apache.hadoop.fs.FileSystem:createBulkDelete(org.apache.hadoop.fs.Path),5002,5006,"/**
* Creates a DefaultBulkDeleteOperation for the given path.
* @param path Path to delete.
* @return New DefaultBulkDeleteOperation instance.
*/
","* Create a bulk delete operation.
   * The default implementation returns an instance of {@link DefaultBulkDeleteOperation}.
   * @param path base path for the operation.
   * @return an instance of the bulk delete.
   * @throws IllegalArgumentException any argument is invalid.
   * @throws IOException if there is an IO problem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getBufferSize,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBufferSize(),64,67,"/**
* Calls the m1 method of the superclass and returns its result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getReplication,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getReplication(),69,72,"/**
* Calls the m1() method of the superclass and returns its result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getFlags,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFlags(),74,77,"/**
* Delegates the call to the superclass's m1() method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getChecksumOpt,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getChecksumOpt(),79,82,"/**
 * Delegates the call to the superclass's m1() method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getBlockSize,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBlockSize(),84,87,"/**
* Calls the parent class's m1() method and returns its result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processOptions,org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processOptions(java.util.LinkedList),153,177,"/**
* Parses arguments, extracts name/value, and validates input.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeXAttr,"org.apache.hadoop.fs.FilterFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",656,659,"/**
* Delegates the m1 operation to the file system.
* @param path The path to operate on.
* @param name The name to use in the operation.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processOptions,org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processOptions(java.util.LinkedList),53,59,"/**
 * Processes arguments, throws exception if ""-t"" is present.
 * Calls the superclass's m2 method to continue processing.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/And.java,registerExpression,org.apache.hadoop.fs.shell.find.And:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),31,35,"/**
* Configures the ExpressionFactory with And operator definitions.
*/",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,registerExpression,org.apache.hadoop.fs.shell.find.Print:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),30,34,"/**
 * Configures ExpressionFactory with print and print0 actions.
 * @param factory The ExpressionFactory to configure.
 */",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,registerExpression,org.apache.hadoop.fs.shell.find.Name:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),33,37,"/**
* Configures ExpressionFactory with name and iname filters.
* @param factory ExpressionFactory instance to configure.
*/",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,<init>,org.apache.hadoop.fs.shell.find.Print:<init>(),45,47,"/**
 * Constructs a Print object with a newline character as the initial value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name:<init>(boolean),57,62,"/**
 * Constructs a Name object.
 * @param caseSensitive Whether comparison is case-sensitive.
 */
","* Construct a Name {@link Expression} with a specified case sensitivity.
   *
   * @param caseSensitive if true the comparisons are case sensitive.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,apply,"org.apache.hadoop.fs.shell.find.Name:apply(org.apache.hadoop.fs.shell.PathData,int)",82,93,"/**
 * Checks if a file/directory name matches the glob pattern.
 * @param item PathData object
 * @param depth int depth
 * @return Result.PASS if matches, otherwise Result.FAIL
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,addCodec,org.apache.hadoop.io.compress.CompressionCodecFactory:addCodec(org.apache.hadoop.io.compress.CompressionCodec),64,75,"/**
* Registers a codec with the system, associating it by name.
* @param codec The codec object to register.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodec,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodec(org.apache.hadoop.fs.Path),199,217,"/**
* Finds a CompressionCodec for the given file based on its name.
* @param file The Path object representing the file.
* @return CompressionCodec or null if no codec is found.
*/","* Find the relevant compression codec for the given file based on its
   * filename suffix.
   * @param file the filename to check
   * @return the codec object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsConfig:<init>(org.apache.commons.configuration2.Configuration,java.lang.String)",94,96,"/**
 * Initializes MetricsConfig with a Configuration and prefix.
 * @param c Configuration object.
 * @param prefix Metric prefix string.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,isLocalhost,org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:isLocalhost(java.lang.String),491,501,"/**
 * Checks if a host is a local host based on defined LOCALHOSTS.
 * @param host The host string to check.
 * @return True if the host is a local host, false otherwise.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,resolvePropertyName,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:resolvePropertyName(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String)",216,221,"/**
 * Formats a message using the provided template and mode string.
 */","* Resolves a property name to its client/server version if applicable.
   * <p>
   * NOTE: This method is public for testing purposes.
   *
   * @param mode client/server mode.
   * @param template property name template.
   * @return the resolved property name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,hasCapability,org.apache.hadoop.crypto.CryptoInputStream:hasCapability(java.lang.String),860,880,"/**
 * Checks if the stream supports the given capability.
 * @param capability Capability to check for.
 * @return True if the capability is supported, false otherwise.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherSuite.java,getConfigSuffix,org.apache.hadoop.crypto.CipherSuite:getConfigSuffix(),98,106,"/**
* Generates a suffix string from the name using '/' as a delimiter.
*/","* Returns suffix of cipher suite configuration.
   * @return String configuration suffix",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,combine,org.apache.hadoop.fs.shell.find.Result:combine(org.apache.hadoop.fs.shell.find.Result),59,62,"/**
 * Combines results from two Result objects using AND operations.
 */","* Returns the combination of this and another result.
   * @param other other.
   * @return result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,negate,org.apache.hadoop.fs.shell.find.Result:negate(),68,70,"/**
* Creates a Result object with inverted m1() and m2() values.
*/","* Negate this result.
   * @return Result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,toString,org.apache.hadoop.fs.shell.find.Result:toString(),72,75,"/**
* Concatenates results of m1() and m2() into a string.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name$Iname:<init>(),97,99,"/**
 * Constructs an Iname object, initializing it with a new Name.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,<init>,org.apache.hadoop.fs.shell.find.Print$Print0:<init>(),72,74,"/**
 * Constructs a Print0 object, initializing with a null character.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,createOptions,org.apache.hadoop.fs.shell.find.Find:createOptions(),244,252,"/**
 * Creates and configures FindOptions object with various streams.
 * @return FindOptions object with configured input/output streams.
 */
",Create a new set of find options.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isExpression,org.apache.hadoop.fs.shell.find.Find:isExpression(java.lang.String),445,448,"/**
* Delegates expression evaluation to ExpressionFactory.
* @param expressionName Name of the expression to evaluate.
*/",Asks the factory whether an expression is recognized.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,setOptions,org.apache.hadoop.fs.shell.find.BaseExpression:setOptions(org.apache.hadoop.fs.shell.find.FindOptions),67,73,"/**
 * Processes FindOptions on child expressions.
 * @param options FindOptions object for processing.
 * @throws IOException if an I/O error occurs.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,prepare,org.apache.hadoop.fs.shell.find.BaseExpression:prepare(),75,80,"/**
* Recursively calls m1() on each child expression.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,finish,org.apache.hadoop.fs.shell.find.BaseExpression:finish(),82,87,"/**
* Recursively calls m1 on each child expression.
* Throws IOException if any child throws it.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,isAction,org.apache.hadoop.fs.shell.find.BaseExpression:isAction(),147,155,"/**
 * Checks if any child expression satisfies the condition.
 * Iterates through children and returns true if any do.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,toString,org.apache.hadoop.fs.shell.find.BaseExpression:toString(),119,145,"/**
* Generates a string representation of the method with arguments/children.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,addChildren,"org.apache.hadoop.fs.shell.find.BaseExpression:addChildren(java.util.Deque,int)",219,223,"/**
 * Processes a specified number of expressions from the deque.
 * @param exprs Deque of expressions to process.
 * @param count Number of expressions to process.
 */
","* Add a specific number of children to this expression. The children are
   * popped off the head of the expressions.
   *
   * @param exprs
   *          deque of expressions from which to take the children
   * @param count
   *          number of children to be added",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,addArguments,"org.apache.hadoop.fs.shell.find.BaseExpression:addArguments(java.util.Deque,int)",250,254,"/**
 * Processes a deque of strings a specified number of times.
 * @param args Deque of strings to process.
 * @param count Number of iterations.
 */
","* Add a specific number of arguments to this expression. The children are
   * popped off the head of the expressions.
   *
   * @param args
   *          deque of arguments from which to take the argument
   * @param count
   *          number of children to be added",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:validate(),244,274,"/**
* Checks alias and provider, prompts for confirmation if interactive.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:validate(),354,381,"/**
* Checks if deletion is allowed, prompts for confirmation if interactive.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,confirmForceManual,org.apache.hadoop.ha.HAAdmin:confirmForceManual(),466,479,"/**
* Prompts user about the FORCEMANUAL flag and returns their response.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,relativize,"org.apache.hadoop.fs.shell.PathData:relativize(java.net.URI,java.net.URI,boolean)",410,435,"/**
 * Calculates a relative path from cwdUri to srcUri.
 * @param cwdUri Current working directory URI.
 * @param srcUri Source URI.
 * @param isDir Whether srcUri is a directory.
 * @return Relative path string.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,stringToUri,org.apache.hadoop.fs.shell.PathData:stringToUri(java.lang.String),550,591,"/**
 * Parses a string and constructs a URI object.
 * @param pathString String to parse into a URI.
 * @throws IOException if an I/O error occurs.
 */","Construct a URI from a String with unescaped special characters
   *  that have non-standard semantics. e.g. /, ?, #. A custom parsing
   *  is needed to prevent misbehavior.
   *  @param pathString The input path in string form
   *  @return URI",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setHumanReadable,org.apache.hadoop.fs.shell.FsUsage$Df:setHumanReadable(boolean),69,71,"/**
 * Sets the human-readable flag.
 * @param humanReadable flag indicating readability
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setHumanReadable,org.apache.hadoop.fs.shell.FsUsage$Du:setHumanReadable(boolean),69,71,"/**
 * Sets the human-readable flag.
 * @param humanReadable Whether output should be human-readable.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Df:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder),65,67,"/**
* Sets the usages table for further processing.
* @param usagesTable The table to be set.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Du:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder),65,67,"/**
 * Sets the usages table for further processing.
 * @param usagesTable The table to be set.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,getUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Df:getUsagesTable(),61,63,"/**
 * Returns the usages table builder.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,getUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Du:getUsagesTable(),61,63,"/**
 * Returns the usages table builder.
 * @return TableBuilder instance.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,isSorted,org.apache.hadoop.fs.shell.Ls:isSorted(),248,254,"/**
* Returns true if any of m1, m2, m3, or m4 are true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,initialiseOrderComparator,org.apache.hadoop.fs.shell.Ls:initialiseOrderComparator(),374,402,"/**
 * Sets orderComparator based on m1(), m2(), and m4() conditions.
 */","* Initialise the comparator to be used for sorting files. If multiple options
   * are selected then the order is chosen in the following precedence: -
   * Modification time (or access time if requested) - File size - File name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getAdditionalTokenIssuers,org.apache.hadoop.fs.FileSystem:getAdditionalTokenIssuers(),718,723,"/**
* Returns an array of DelegationTokenIssuers.
* Delegates to m1() to retrieve the array.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:<init>(int,int)",218,220,"/**
 * Constructs a NotEnoughArgumentsException with expected and actual counts.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:<init>(int,int)",202,204,"/**
* Constructs a TooManyArgumentsException with expected and actual counts.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,getMessage,org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:getMessage(),222,225,"/**
* Calls superclass's m1() and prepends an error message.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,getMessage,org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:getMessage(),206,209,"/**
* Calls super.m1() and prepends a message.
* Returns a string with the prepended message.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processOptions,org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processOptions(java.util.LinkedList),70,100,"/**
 * Parses arguments, sets name/encoding, and validates input.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttrs,org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path),640,643,"/**
* Delegates file system operation to the underlying file system.
* @param path Path to the file.
* @return Map of string to byte array.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttr,"org.apache.hadoop.fs.FilterFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",635,638,"/**
 * Delegates file read operation to the underlying file system.
 * @param path Path to the file.
 * @param name Name of the file.
 * @return Byte array representing the file content.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,popPreserveOption,org.apache.hadoop.fs.shell.CopyCommands$Cp:popPreserveOption(java.util.List),191,210,"/**
* Processes command-line arguments.
* Iterates through args, handles ""--"" and ""-p"" flags.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processArguments(java.util.LinkedList),159,171,"/**
 * Renames a snapshot under a path, if no errors occurred.
 * @param items List of PathData objects; expected size is 1.
 * @throws IOException if file system operations fail.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.FilterFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",579,583,"/**
* Delegates the m1 call to the underlying file system.
* @param path Path to operate on.
* @param snapshotOldName Old snapshot name.
* @param snapshotNewName New snapshot name.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,isDeprecated,org.apache.hadoop.fs.shell.Command:isDeprecated(),557,559,"/**
* Checks if m1() returns a non-null value.
*/","* Is the command deprecated?
   * @return boolean",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getName,org.apache.hadoop.fs.shell.Command:getName(),519,523,"/**
* Returns a processed name, using m3 or m2 based on conditions.
*/","* The name of the command.  Will first try to use the assigned name
   * else fallback to the command's preferred name
   * @return name of the command",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getAclStatus,org.apache.hadoop.fs.FilterFileSystem:getAclStatus(org.apache.hadoop.fs.Path),618,621,"/**
 * Delegates AclStatus retrieval to the underlying filesystem.
 * @param path The path to check ACL status for.
 * @return AclStatus object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,setPreserve,org.apache.hadoop.fs.shell.CommandWithDestination:setPreserve(boolean),125,133,"/**
 * Preserves file attributes or resets status based on 'preserve'.
 */","* If true, the last modified time, last access time,
   * owner, group and permission information of the source
   * file will be preserved as far as target {@link FileSystem}
   * implementation allows.
   *
   * @param preserve preserve.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setAcl,"org.apache.hadoop.fs.FilterFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",613,616,"/**
* Delegates ACL specification to the file system.
* @param path Path to apply ACLs to.
* @param aclSpec List of ACL entries to apply.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,<init>,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:<init>(java.lang.String,java.lang.String)",41,45,"/**
 * Constructs an MBeanInfoBuilder with a name and description.
 * @param name MBean name.
 * @param desc MBean description.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:<init>(org.apache.hadoop.metrics2.MetricsCollector,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,boolean)",58,68,"/**
 * Constructs a MetricsRecordBuilder with given parent, info, filters,
 * and acceptable flag.
 */
","* @param parent {@link MetricsCollector} using this record builder
   * @param info metrics information
   * @param rf
   * @param mf
   * @param acceptable",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,<init>,"org.apache.hadoop.util.ChunkedArrayList:<init>(int,int)",103,107,"/**
 * Constructs a ChunkedArrayList with initial capacity and max size.
 * @param initialChunkCapacity Initial capacity of the chunks.
 * @param maxChunkSize Maximum size of each chunk.
 */
","* @param initialChunkCapacity the capacity of the first chunk to be
   * allocated
   * @param maxChunkSize the maximum size of any chunk allocated",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ScopedAclEntries.java,calculatePivotOnDefaultEntries,org.apache.hadoop.fs.permission.ScopedAclEntries:calculatePivotOnDefaultEntries(java.util.List),87,94,"/**
 * Finds the index of the first AclEntry with DEFAULT scope.
 * @param aclBuilder List of AclEntry objects
 * @return Index of the entry or PIVOT_NOT_FOUND if not found.
 */","* Returns the pivot point in the list between the access entries and the
   * default entries.  This is the index of the first element in the list that
   * is a default entry.
   *
   * @param aclBuilder ArrayList<AclEntry> containing entries to build
   * @return int pivot point, or -1 if list contains no default entries",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeAcl,org.apache.hadoop.fs.FilterFileSystem:removeAcl(org.apache.hadoop.fs.Path),608,611,"/**
* Delegates the m1 operation to the underlying file system.
* @param path The path to operate on.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.FilterFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",591,595,"/**
* Delegates ACL specification to the file system.
* @param path The path to apply the ACL to.
* @param aclSpec ACL entries to apply.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.FilterFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",597,601,"/**
* Delegates ACL specification to the file system.
* @param path The path to apply ACLs to.
* @param aclSpec ACL entries to apply.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processArguments(java.util.LinkedList),77,88,"/**
 * Creates a snapshot path based on provided PathData.
 * @param items LinkedList of PathData objects.
 * @throws IOException if an I/O error occurs.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createSnapshot,org.apache.hadoop.fs.FileSystem:createSnapshot(org.apache.hadoop.fs.Path),3089,3091,"/**
* Calls m1 with a null options parameter.
* @param path The path to process.
* @throws IOException if an I/O error occurs.
*/","* Create a snapshot with a default name.
   * @param path The directory where snapshots will be taken.
   * @return the snapshot path.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createSnapshot,"org.apache.hadoop.fs.FilterFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",573,577,"/**
 * Delegates snapshot creation to the underlying file system.
 * @param path Path to snapshot.
 * @param snapshotName Snapshot name.
 * @return Path to the created snapshot.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,addOptionWithValue,org.apache.hadoop.fs.shell.CommandFormat:addOptionWithValue(java.lang.String),73,78,"/**
* Adds an option, throwing exception if duplicated.
* @param option The option to add.
*/
","* add option with value
   *
   * @param option option name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processArguments(java.util.LinkedList),117,128,"/**
 * Deletes a snapshot under a path, if no errors occurred.
 * @param items LinkedList of PathData objects.
 * @throws IOException if an I/O error occurs.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.FilterFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",585,589,"/**
 * Delegates snapshot creation to the underlying file system.
 * @param path Path to snapshot.
 * @param snapshotName Snapshot name.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,<init>,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:<init>(java.lang.Object[]),274,278,"/**
 * Creates a table with given headers.
 * @param headers array of header objects
 */
","* Create a table with headers
     * @param headers list of headers",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,isEmpty,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:isEmpty(),348,350,"/**
 * Checks if m1() returns 0.
 * @return True if m1() is 0, false otherwise.
 */
","* Does table have any rows 
     * @return boolean",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path),831,836,"/**
 * Delegates to the target file system to retrieve data.
 * @param path Path to the file.
 * @return Map of string to byte array representing file data.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttrs,org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path),376,379,"/**
 * Delegates file system operation to the underlying file system.
 * @param path Path to the file.
 * @return Map containing file system data.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$HandleOpt$Location:<init>(boolean),496,498,"/**
 * Constructs a Location object.
 * @param allowChanged Whether to allow location changes.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$HandleOpt$Data:<init>(boolean),471,473,"/**
 * Constructs a Data object.
 * @param allowChanged Flag to allow changes.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,toString,org.apache.hadoop.fs.CompositeCrcFileChecksum:toString(),84,87,"/**
* Concatenates m1() result with CRC value as hex string.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell:<init>(long),910,912,"/**
 * Constructs a Shell with a specified interval.
 * @param interval the time interval in milliseconds
 */
","* Create an instance with a minimum interval between executions; stderr is
   * not merged with stdout.
   * @param interval interval in milliseconds between command executions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,equals,org.apache.hadoop.fs.RawPathHandle:equals(java.lang.Object),72,79,"/**
 * Compares this PathHandle with another, delegating to m1().
 * @param other The object to compare to.
 * @return True if equal, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,hashCode,org.apache.hadoop.fs.RawPathHandle:hashCode(),81,84,"/**
* Delegates to the result of m1() and returns its m2() value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,toString,org.apache.hadoop.fs.RawPathHandle:toString(),86,89,"/**
* Delegates to the result of m1().m2(), returning a String.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,run,org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:run(),209,236,"/**
 * Refreshes disk information periodically, with jitter.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setOwner,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setOwner(org.apache.hadoop.io.Text),93,99,"/**
* Sets the 'owner' of the current object to the provided Text.
* If owner is null, initializes owner to a new Text object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setRealUser,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRealUser(org.apache.hadoop.io.Text),122,128,"/**
* Sets the realUser field. If null, initializes to a new Text.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,"org.apache.hadoop.security.token.Token:<init>(byte[],byte[],org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",86,91,"/**
 * Constructs a Token object with provided identifier, password, kind, and service.
 * Uses defaults if any input is null.
 */
","* Construct a token from the components.
   * @param identifier the token identifier
   * @param password the token's password
   * @param kind the kind of token
   * @param service the service for this token",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,org.apache.hadoop.security.token.Token:<init>(),96,101,"/**
 * Initializes a new Token object with empty byte arrays and Text objects.
 */",* Default constructor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",65,72,"/**
 * Constructs a Globber with provided context, pattern, and filter.
 * @param fc FileContext, the file system context.
 * @param pathPattern Path pattern to match.
 * @param filter Path filter to apply.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)",100,110,"/**
 * Constructs a Globber with the given file context, path pattern,
 * filter, and symlink resolution flag.
 */
","* File Context constructor for use by {@link GlobBuilder}.
   * @param fc file context
   * @param pathPattern path pattern
   * @param filter optional filter
   * @param resolveSymlinks should symlinks be resolved.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getMessage,org.apache.hadoop.fs.PathIOException:getMessage(),86,104,"/**
 * Constructs a detailed message string based on various parameters.
 * Returns the combined message as a String.
 */","Format:
   * cmd: {operation} `path' {to `target'}: error string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,hasNext,org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:hasNext(),49,52,"/**
 * Checks if m1() returns a non-null value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,next,org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:next(),64,71,"/**
* Delegates to the iterator's m2() method.
* @return LongStatistic returned by the iterator.
* @throws NoSuchElementException if iterator is null.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAndIncrDirNumLastAccessed,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$Context:getAndIncrDirNumLastAccessed(),282,284,"/**
 * Calls m1 with default value 1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",860,866,"/**
 * Delegates m3 operation to the target file system.
 * @param path Input path, @param snapshotName snapshot name
 * @return Path representing the result of the operation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createSnapshot,"org.apache.hadoop.fs.FilterFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",397,401,"/**
* Delegates snapshot creation to the underlying FileSystem.
* @param path Path to snapshot.
* @param snapshotName Snapshot name.
* @return Path of the created snapshot.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAbstractFileSystem,"org.apache.hadoop.fs.FileContext:getAbstractFileSystem(org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration)",339,363,"/**
 * Gets an AbstractFileSystem for the given URI using user privileges.
 * @param user UserGroupInformation object
 * @param uri URI of the filesystem
 * @param conf Hadoop configuration
 * @return AbstractFileSystem object
 * @throws IOException if filesystem access fails
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsUser,"org.apache.hadoop.security.SecurityUtil:doAsUser(org.apache.hadoop.security.UserGroupInformation,java.security.PrivilegedExceptionAction)",552,559,"/**
 * Executes action with given UserGroupInformation.
 * @param ugi UserGroupInformation context
 * @param action Action to execute
 * @return Result of the action
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleSaslConnectionFailure,"org.apache.hadoop.ipc.Client$Connection:handleSaslConnectionFailure(int,int,java.io.IOException,java.util.Random,org.apache.hadoop.security.UserGroupInformation)",705,757,"/**
 * Retries connection setup, backoffs on failure, throws on max retries.
 * @param currRetries Current retry count.
 * @param maxRetries Max allowed retries.
 */","* If multiple clients with the same principal try to connect to the same
     * server at the same time, the server assumes a replay attack is in
     * progress. This is a feature of kerberos. In order to work around this,
     * what is done is that the client backs off randomly and tries to initiate
     * the connection again. The other problem is to do with ticket expiry. To
     * handle that, a relogin is attempted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,isViewFileSystem,org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:isViewFileSystem(org.apache.hadoop.fs.FileSystem),50,52,"/**
 * Checks if the file system uses the VIEWFS scheme.
 * @param fileSystem The file system to check.
 * @return True if VIEWFS scheme is used, false otherwise.
 */","* Check if the FileSystem is a ViewFileSystem.
   *
   * @param fileSystem file system.
   * @return true if the fileSystem is ViewFileSystem",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,open,"org.apache.hadoop.fs.FilterFileSystem:open(org.apache.hadoop.fs.PathHandle,int)",171,175,"/**
* Delegates to the underlying FSDataInputStream.
* @param fd PathHandle object
* @param bufferSize buffer size
* @return FSDataInputStream object
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,primitiveMkdir,"org.apache.hadoop.fs.FilterFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",561,566,"/**
* Delegates set permissions to the underlying FileSystem object.
* @param f Path to the file.
* @param abdolutePermission Permissions to set.
* @return True if successful, false otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setQuota,"org.apache.hadoop.fs.FileSystem:setQuota(org.apache.hadoop.fs.Path,long,long)",1965,1968,"/**
* Initializes resources using m1(), with namespace and storage quotas.
*/","* Set quota for the given {@link Path}.
   *
   * @param src the target path to set quota for
   * @param namespaceQuota the namespace quota (i.e., # of files/directories)
   *                       to set
   * @param storagespaceQuota the storage space quota to set
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setQuotaByStorageType,"org.apache.hadoop.fs.FileSystem:setQuotaByStorageType(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.StorageType,long)",1978,1981,"/**
* Initializes storage with given quota and type.
* @param src Source path.
* @param type Storage type.
* @param quota Storage quota in bytes.
*/","* Set per storage type quota for the given {@link Path}.
   *
   * @param src the target path to set storage type quota for
   * @param type the storage type to set
   * @param quota the quota to set for the given storage type
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createMultipartUploader,org.apache.hadoop.fs.FileSystem:createMultipartUploader(org.apache.hadoop.fs.Path),4987,4992,"/**
* Creates a MultipartUploaderBuilder with the given base path.
* @param basePath The base path for the uploader.
* @throws IOException If an I/O error occurs.
*/
","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listCorruptFileBlocks,org.apache.hadoop.fs.FilterFileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path),277,281,"/**
 * Delegates to the file system's m1 method for RemoteIterator.
 * @param path The path to start the iteration from.
 * @return A RemoteIterator for the specified path.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listLocatedStatus,org.apache.hadoop.fs.FileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),2261,2264,"/**
 * Lists files under a path, using the default filter.
 * @param f the path to list
 * @return RemoteIterator of LocatedFileStatus objects
 */","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   * Return the file's status and block locations If the path is a file.
   *
   * If a returned status is a file, it contains the file's block locations.
   *
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories
   *         in the given path
   *
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws IOException If an I/O error occurred",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.ChecksumFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),980,984,"/**
 * Returns a remote iterator for files matching the default filter.
 * @param f the path to iterate over
 * @return RemoteIterator of LocatedFileStatus objects
 */","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   *
   * @param f
   *          given path
   * @return the statuses of the files/directories in the given patch
   * @throws IOException if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listLocatedStatus,"org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",214,219,"/**
 * Delegates to the file system's iterator for located file statuses.
 * @param f The path to start the iteration from.
 * @param filter Path filter to apply.
 * @return RemoteIterator of LocatedFileStatus.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,java.lang.String)",97,102,"/**
* Creates an AccessControlException for read-only filesystem operations.
* @param operation The operation being attempted.
* @param p The path involved in the operation.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,java.lang.String)",175,180,"/**
 * Creates an AccessControlException for readonly ViewFileSystem.
 * @param operation The operation being attempted.
 * @param p The path involved in the operation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.String),41,43,"/**
 * Constructs an AuthorizationException with the given error message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,resolveLink,org.apache.hadoop.fs.FilterFileSystem:resolveLink(org.apache.hadoop.fs.Path),498,500,"/**
 * Delegates Path retrieval to the file system.
 * @param f The Path to retrieve.
 * @return The Path object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileChecksum,org.apache.hadoop.fs.FileSystem:getFileChecksum(org.apache.hadoop.fs.Path),2980,2982,"/**
 * Calculates checksum of a file.
 * @param f Path to the file.
 * @return FileChecksum object.
 */","* Get the checksum of a file, if the FS supports checksums.
   *
   * @param f The file path
   * @return The file checksum.  The default return value is null,
   *  which indicates that no checksum algorithm is implemented
   *  in the corresponding FileSystem.
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",507,510,"/**
 * Delegates checksum calculation to the file system.
 * @param f Path to the file. @param length File length.
 * @return FileChecksum object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setXAttr,"org.apache.hadoop.fs.FileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",3244,3248,"/**
* Sets an attribute with CREATE/REPLACE flags.
* @param path Path to the file.
* @param name Attribute name.
* @param value Attribute value.
*/","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default outcome).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setXAttr,"org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",629,633,"/**
 * Delegates m1 call to the underlying file system.
 * @param path Path to the file.
 * @param name Attribute name.
 * @param value Attribute value.
 * @param flag Attribute set flags.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttrs,"org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",645,649,"/**
* Delegates to the file system's m1 method.
* @param path Path to operate on.
* @param names List of names to process.
* @return Map returned by the file system's m1.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listXAttrs,org.apache.hadoop.fs.FilterFileSystem:listXAttrs(org.apache.hadoop.fs.Path),651,654,"/**
 * Delegates the call to the underlying file system's m1 method.
 * @param path The path to operate on.
 * @return List of strings returned by the file system's m1.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),661,664,"/**
 * Delegates the m1 operation to the underlying file system.
 * @param src The Path object representing the source.
 * @throws IOException if an I/O error occurs.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.FilterFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",666,670,"/**
* Delegates m1 call to the underlying file system.
* @param src Path to the source.
* @param policyName Policy name to apply.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),672,675,"/**
* Delegates method execution to the underlying filesystem.
* @param src The Path object representing the source.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),677,681,"/**
* Delegates BlockStoragePolicySpi retrieval to the underlying filesystem.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAllStoragePolicies(),1925,1939,"/**
 * Retrieves all BlockStoragePolicySpi instances across file systems.
 * Returns a collection of BlockStoragePolicySpi objects.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.FilterFileSystem:getAllStoragePolicies(),683,687,"/**
* Delegates to the underlying file system to get policies.
* @return Collection of BlockStoragePolicySpi implementations.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",4806,4816,"/**
 * Opens a file stream using provided parameters.
 * @param path Path to the file.
 * @param parameters Parameters for opening the file.
 * @return CompletableFuture wrapping FSDataInputStream.
 */","* Execute the actual open file operation.
   *
   * This is invoked from {@code FSDataInputStreamBuilder.build()}
   * and from {@link DelegateToFileSystem} and is where
   * the action of opening the file should begin.
   *
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)} in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.ChecksumFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",1090,1101,"/**
 * Opens a file stream asynchronously using provided parameters.
 * @param path Path to the file.
 * @param parameters Parameters for opening the file.
 * @return CompletableFuture holding the FSDataInputStream.
 */","* Open the file as a blocking call to {@link #open(Path, int)}.
   *
   * {@inheritDoc}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.AbstractFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",1603,1612,"/**
 * Opens a file stream for reading.
 * @param path Path to the file.
 * @param parameters OpenFileParameters object.
 * @return CompletableFuture with FSDataInputStream.
 */","* Open a file with the given set of options.
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)}in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)",4834,4852,"/**
* Opens a file stream asynchronously.
* @param pathHandle Path to the file.
* @param parameters Parameters for opening the file.
* @return CompletableFuture wrapping FSDataInputStream.
*/","* Execute the actual open file operation.
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)} in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param pathHandle path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key
   * @throws UnsupportedOperationException PathHandles are not supported.
   * This may be deferred until the future is evaluated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,isValidName,org.apache.hadoop.fs.FilterFs:isValidName(java.lang.String),322,325,"/**
 * Delegates the m1 operation to the underlying FileSystem object.
 * @param src The input string to be processed.
 * @return The result of the delegated operation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",770,776,"/**
 * Delegates path processing to the target file system.
 * @param path The path to process.
 * @param aclSpec ACL specifications to apply.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,modifyAclEntries,"org.apache.hadoop.fs.FilterFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",327,331,"/**
* Delegates ACL specification to the underlying file system.
* @param path Path to be modified.
* @param aclSpec List of ACL entries to apply.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",778,784,"/**
 * Delegates path modification to the target file system.
 * @param path The path to modify.
 * @param aclSpec ACL specifications for the modification.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeAclEntries,"org.apache.hadoop.fs.FilterFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",333,337,"/**
 * Delegates ACL setting to the underlying file system.
 * @param path Path to be updated.
 * @param aclSpec ACL entries to apply.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),786,792,"/**
* Recursively calls m3 on the target file system.
* @param path The path to resolve and process.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeDefaultAcl,org.apache.hadoop.fs.FilterFs:removeDefaultAcl(org.apache.hadoop.fs.Path),339,342,"/**
* Delegates the Path m1 operation to the underlying FileSystem.
* @param path The path to operate on.
* @throws IOException if an I/O error occurs.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",868,875,"/**
 * Delegates snapshot creation to the target file system.
 * @param path The path to the snapshot.
 * @param snapshotOldName Old snapshot name.
 * @param snapshotNewName New snapshot name.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameSnapshot,"org.apache.hadoop.fs.FilterFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",403,407,"/**
* Delegates snapshot creation to the underlying FileSystem.
* @param path Path to snapshot.
* @param snapshotOldName Old snapshot name.
* @param snapshotNewName New snapshot name.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),401,404,"/**
* Delegates the path processing to the underlying FileSystem.
* @param path The path to be processed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),884,889,"/**
* Resolves and executes m3 on the target file system.
* @param path The path to resolve and operate on.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.FilterFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),415,418,"/**
* Delegates the Path m1 operation to the underlying FileSystem.
* @param path The Path to operate on.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),899,905,"/**
* Resolves and executes m3 on the target file system.
* @param src Path to resolve and process.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,unsetStoragePolicy,org.apache.hadoop.fs.FilterFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),426,430,"/**
* Delegates the m1 operation to the underlying file system.
* @param src The Path object representing the source.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ChRootedFs:getAllStoragePolicies(),424,428,"/**
* Delegates to the underlying FileSystem's m1() method.
* @return Collection of BlockStoragePolicySpi instances.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAllStoragePolicies,org.apache.hadoop.fs.FileContext:getAllStoragePolicies(),2916,2919,"/**
* Delegates to defaultFS to retrieve BlockStoragePolicySpis.
* @return Collection of BlockStoragePolicySpi instances.
*/
","* Retrieve all the storage policies supported by this file system.
   *
   * @return all storage policies supported by this filesystem.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getAllStoragePolicies,org.apache.hadoop.fs.FilterFs:getAllStoragePolicies(),438,442,"/**
* Delegates to the file system's method to get policies.
* @return Collection of BlockStoragePolicySpi implementations
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,supportsSymlinks,org.apache.hadoop.fs.viewfs.ChRootedFs:supportsSymlinks(),436,439,"/**
* Delegates the m1 call to the underlying FileSystem object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,supportsSymlinks,org.apache.hadoop.fs.FilterFs:supportsSymlinks(),296,299,"/**
* Delegates m1() call to the wrapped FileSystem object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createSymlink,"org.apache.hadoop.fs.FilterFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",301,305,"/**
* Delegates file system operation to the underlying file system.
* @param target Target path.
* @param link Link path.
* @param createParent Whether to create parent directories.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFs:getLinkTarget(org.apache.hadoop.fs.Path),669,674,"/**
 * Resolves a path within the file system.
 * @param f The path to resolve.
 * @return Resolved Path object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getLinkTarget,org.apache.hadoop.fs.FilterFs:getLinkTarget(org.apache.hadoop.fs.Path),307,310,"/**
* Delegates Path m1 operation to the underlying FileSystem.
* @param f The Path to operate on.
* @return The result of the delegated operation.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getDelegationTokens,org.apache.hadoop.fs.viewfs.ChRootedFs:getDelegationTokens(java.lang.String),459,462,"/**
 * Delegates to FileSystem's m1 method.
 * @param renewer renewer string
 * @return List of Tokens
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getDelegationTokens,org.apache.hadoop.fs.FilterFs:getDelegationTokens(java.lang.String),317,320,"/**
* Delegates to FileSystem's m1 method.
* @param renewer renewer string, passed to FileSystem.
* @return List of Tokens returned by FileSystem.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path),1073,1078,"/**
 * Checks if path is a file; throws FileNotFoundException if not.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,open,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)",1301,1306,"/**
 * Opens an input stream for the file at the given path.
 * @param f Path to the file.
 * @param bufferSize Buffer size for the stream.
 * @throws FileNotFoundException if path is a directory.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,initializeMountedFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:initializeMountedFileSystems(java.util.List),943,959,"/**
 * Creates a map of FileSystem objects from mount points.
 * @param mountPoints List of mount points to process.
 * @return Map of source path to FileSystem.
 */","* Initialize the target filesystem for all mount points.
   * @param mountPoints The mount points
   * @return Mapping of mount point and the initialized target filesystems
   * @throws RuntimeException when the target file system cannot be initialized",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getDelegationTokens,org.apache.hadoop.fs.viewfs.ViewFs:getDelegationTokens(java.lang.String),732,761,"/**
 * Retrieves tokens for each mount point and fallback FS.
 * @param renewer renewer string, used for token retrieval
 * @return List of tokens.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1422,1425,"/**
* Throws NotInMountpointException for getXAttr requests.
* @param path The path being accessed.
* @param name Attribute name.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path),1427,1430,"/**
 * Throws NotInMountpointException for unsupported path.
 * @param path The path being accessed.
 * @throws IOException if the path is not supported.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",1432,1436,"/**
* Throws NotInMountpointException for the given path.
* @param path The path being accessed.
* @param names List of names (unused).
* @throws IOException if the path is not within the mountpoint.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path),1438,1441,"/**
* Throws NotInMountpointException for listXAttrs operation.
* @param path The path being accessed.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path),1785,1788,"/**
* Throws NotInMountpointException if path is not within a mountpoint.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultBlockSize(org.apache.hadoop.fs.Path),1790,1793,"/**
 * Throws NotInMountpointException for paths not in a mountpoint.
 * @param f the Path object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultReplication(org.apache.hadoop.fs.Path),1795,1798,"/**
 * Throws NotInMountpointException for given path.
 * @param f The path to check.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1848,1851,"/**
* Throws NotInMountpointException for requested attribute.
* @param path The path being accessed.
* @param name Attribute name.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path),1853,1856,"/**
* Throws NotInMountpointException for unsupported path.
* @param path The path being accessed.
* @throws IOException if the path is not supported.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",1858,1862,"/**
 * Throws NotInMountpointException for unsupported path.
 * @param path The path being accessed.
 * @param names List of names (unused).
 * @throws IOException If the path is not within the mountpoint.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path),1864,1867,"/**
 * Throws NotInMountpointException for unsupported path.
 * @param path the path being accessed
 * @throws IOException if the path is not within a mountpoint
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getQuotaUsage(org.apache.hadoop.fs.Path),1896,1899,"/**
 * Throws NotInMountpointException if path is not in a mountpoint.
 * @param f the path to check
 * @throws IOException if the path is not in a mountpoint
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStoragePolicy(org.apache.hadoop.fs.Path),1920,1923,"/**
* Throws NotInMountpointException when getStoragePolicy is called.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java,serializeToString,org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:serializeToString(),111,116,"/**
* Concatenates strings to form a combined result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize(),961,964,"/**
 * Throws NotInMountpointException for getDefaultBlockSize.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication(),966,969,"/**
* Throws NotInMountpointException, indicating operation not supported.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults(),971,974,"/**
* Throws NotInMountpointException, indicating operation not allowed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,delete,"org.apache.hadoop.fs.viewfs.ViewFs:delete(org.apache.hadoop.fs.Path,boolean)",366,378,"/**
 * Deletes a file or directory recursively.
 * @param f Path to delete.
 * @param recursive Whether to delete recursively.
 * @return True if successful, false otherwise.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatusIterator,org.apache.hadoop.fs.viewfs.ViewFs:listStatusIterator(org.apache.hadoop.fs.Path),451,469,"/**
* Returns a RemoteIterator of FileStatus for the given path.
* @param f the path to iterate over
* @return RemoteIterator of FileStatus objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ViewFs:listLocatedStatus(org.apache.hadoop.fs.Path),471,490,"/**
 * Retrieves a remote iterator of located file statuses for a path.
 * @param f The path to retrieve file statuses for.
 * @return A RemoteIterator of LocatedFileStatus objects.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,buildResolveResultForRegexMountPoint,"org.apache.hadoop.fs.viewfs.InodeTree:buildResolveResultForRegexMountPoint(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)",1053,1081,"/**
 * Resolves a file system target and returns a ResolveResult.
 * @param resultKind Result kind of the operation.
 */","* Build resolve result.
   * Here's an example
   * Mountpoint: fs.viewfs.mounttable.mt
   *     .linkRegex.replaceresolveddstpath:_:-#.^/user/(??&lt;username&gt;\w+)
   * Value: /targetTestRoot/$username
   * Dir path to test:
   * viewfs://mt/user/hadoop_user1/hadoop_dir1
   * Expect path: /targetTestRoot/hadoop-user1/hadoop_dir1
   * resolvedPathStr: /user/hadoop_user1
   * targetOfResolvedPathStr: /targetTestRoot/hadoop-user1
   * remainingPath: /hadoop_dir1
   *
   * @param resultKind resultKind.
   * @param resolvedPathStr resolvedPathStr.
   * @param targetOfResolvedPathStr targetOfResolvedPathStr.
   * @param remainingPath remainingPath.
   * @return targetFileSystem or null on exceptions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,fsGetter,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:fsGetter(),216,219,"/**
* Returns a ChildFsGetter instance, initialized with m1().
*/","* This method is overridden because in ViewFileSystemOverloadScheme if
   * overloaded scheme matches with mounted target fs scheme, file system
   * should be created without going into {@literal fs.<scheme>.impl} based
   * resolution. Otherwise it will end up in an infinite loop as the target
   * will be resolved again to ViewFileSystemOverloadScheme as
   * {@literal fs.<scheme>.impl} points to ViewFileSystemOverloadScheme.
   * So, below method will initialize the
   * {@literal fs.viewfs.overload.scheme.target.<scheme>.impl}.
   * Other schemes can follow fs.newInstance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,getBlockLocations,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getBlockLocations(),112,115,"/**
* Delegates to the underlying FileSystem's m1() method.
* @return BlockLocation[] returned by FileSystem's m1()
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",167,169,"/**
 * Constructs a directory node with the given path and user group.
 * @param pathToNode Path to the directory node.
 * @param aUgi UserGroupInformation for access control.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.Object,java.lang.String[])",344,349,"/**
 * Constructs a NodeLink with provided path, UGI, target FS, and links.
 */",* Construct a mergeLink or nfly.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.util.function.Function,java.lang.String)",354,362,"/**
 * Constructs a NodeLink with a target directory link.
 * @param pathToNode Node path.
 * @param aUgi UserGroupInformation.
 * @param createFileSystemMethod Creates a file system.
 * @param aTargetDirLink Target directory link.
 */
",* Construct a simple link (i.e. not a mergeLink).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addLink,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addLink(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",222,228,"/**
 * Adds a link to the children, throwing if the path already exists.
 * @param pathComponent Path to add the link to.
 * @param link The link to add.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,buildLinkRegexEntry,"org.apache.hadoop.fs.viewfs.InodeTree:buildLinkRegexEntry(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)",818,845,"/**
 * Parses mount entry, extracts link key path and settings.
 * @param config Configuration object
 * @param ugi UserGroupInformation
 * @return LinkEntry object with parsed values
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,processThrowable,"org.apache.hadoop.fs.viewfs.NflyFSystem:processThrowable(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode,java.lang.String,java.lang.Throwable,java.util.List,org.apache.hadoop.fs.Path[])",917,933,"/**
 * Logs an IO exception with a formatted message and adds it to a list.
 * @param nflyNode Node context, op operation name, t Throwable,
 *                   ioExceptions list of IOExceptions, f Paths.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getWorkingDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem:getWorkingDirectory(),861,864,"/**
 * Delegates m1() call to the file system of the first node.
 * @return Path object returned by the delegated m1() call.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,replaceRegexCaptureGroupInPath,"org.apache.hadoop.fs.viewfs.RegexMountPoint:replaceRegexCaptureGroupInPath(java.lang.String,java.util.regex.Matcher,java.lang.String,java.util.Set)",246,261,"/**
 * Masks the destination path using regex group values.
 * @param parsedDestPath Destination path to mask.
 * @param srcMatcher Matcher object for regex matching.
 * @param regexGroupNameOrIndexStr Group name/index.
 * @param groupRepresentationStrSetInDest Set of variable names.
 */","* Use capture group named regexGroupNameOrIndexStr in mather to replace
   * parsedDestPath.
   * E.g. link: ^/user/(?<username>\\w+) => s3://$user.apache.com/_${user}
   * srcMatcher is from /user/hadoop.
   * Then the params will be like following.
   * parsedDestPath: s3://$user.apache.com/_${user},
   * regexGroupNameOrIndexStr: user
   * groupRepresentationStrSetInDest: {user:$user; user:${user}}
   * return value will be s3://hadoop.apache.com/_hadoop
   * @param parsedDestPath
   * @param srcMatcher
   * @param regexGroupNameOrIndexStr
   * @param groupRepresentationStrSetInDest
   * @return return parsedDestPath while ${var},$var replaced or
   * parsedDestPath nothing found.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootDir,org.apache.hadoop.fs.viewfs.InodeTree:getRootDir(),520,523,"/**
* Returns the root INodeDir. Asserts root.m1() is true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootLink,org.apache.hadoop.fs.viewfs.InodeTree:getRootLink(),525,528,"/**
 * Returns the root node as an INodeLink. Asserts root is not masked.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootFallbackLink,org.apache.hadoop.fs.viewfs.InodeTree:getRootFallbackLink(),543,546,"/**
 * Returns the root fallback link after precondition check.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,tryStart,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStart(),156,184,"/**
 * Starts a daemon thread to process asynchronous calls.
 * Uses running flag and daemon to manage execution.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,kill,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:kill(org.apache.hadoop.util.Daemon),192,198,"/**
 * Stops a daemon.
 * @param d The daemon to stop.
 * @throws PreconditionException if the daemon could not be stopped.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,offer,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:offer(java.lang.Object),101,104,"/**
* Adds an element to the queue.
* @param c The element to add.
* @throws PreconditionException if addition fails.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,java.nio.ByteBuffer,byte)",246,265,"/**
 * Decrypts data from inBuffer to outBuffer using the given decryptor.
 * @param decryptor Decryption logic.
 */","* Do the decryption using inBuffer as input and outBuffer as output.
   * Upon return, inBuffer is cleared; the decrypted data starts at 
   * outBuffer.position() and ends at outBuffer.limit();",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,checkState,org.apache.hadoop.crypto.OpensslCipher:checkState(),289,291,"/**
* Validates that the context is not zero.
*/",Check whether context is initialized.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,parentZNodeExists,org.apache.hadoop.ha.ActiveStandbyElector:parentZNodeExists(),341,350,"/**
 * Checks if a znode exists.
 * @return True if znode exists, false otherwise.
 * @throws IOException if an I/O error occurs.
 */","* @return true if the configured parent znode exists
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getConfigViewFsPrefix,org.apache.hadoop.fs.viewfs.ConfigUtil:getConfigViewFsPrefix(),43,46,"/**
* Calls m1 with the default mount table prefix.
* Returns a String value based on the prefix.
*/","* Get the config variable prefix for the default mount table
   * @return the config variable prefix for the default mount table",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path),1537,1542,"/**
 * Checks if path is a file; throws FileNotFoundException if not.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)",1732,1737,"/**
 * Throws FileNotFoundException if path points to a directory.
 * @param f Path object
 * @param bufferSize buffer size
 * @throws FileNotFoundException if path is a directory
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java,deserializeFromString,org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:deserializeFromString(java.lang.String),125,136,"/**
 * Parses serialized string to create an interceptor.
 * @param serializedString Serialized interceptor data.
 * @return RegexMountPointResolvedDstPathReplaceInterceptor object.
 */","* Create interceptor from config string. The string should be in
   * replaceresolvedpath:wordToReplace:replaceString
   * Note that we'll assume there's no ':' in the regex for the moment.
   *
   * @return Interceptor instance or null on bad config.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,getReturnValue,org.apache.hadoop.io.retry.CallReturn:getReturnValue(),71,77,"/**
 * Returns the stored return value after checking the state.
 * Throws the stored exception if in exception state.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,getReadableByteChannel,org.apache.hadoop.net.SocketInputWrapper:getReadableByteChannel(),81,86,"/**
* Returns the input channel associated with this socket.
*/","* @return an underlying ReadableByteChannel implementation.
   * @throws IllegalStateException if this socket does not have a channel",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getChecksumFileLength,"org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFileLength(org.apache.hadoop.fs.Path,long)",143,145,"/**
 * Masks file size using an internal value.
 * @param file The file object (unused).
 * @param fileSize The file size to be masked.
 * @return The masked file size.
 */
","* Return the length of the checksum file given the size of the
   * actual file.
   *
   * @param file the file path.
   * @param fileSize file size.
   * @return checksum length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getFilesystem,org.apache.hadoop.fs.DF:getFilesystem(),72,82,"/**
* Returns filesystem path, OS-dependent logic applied.
*/","* @return a string indicating which filesystem volume we're checking.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getMount,org.apache.hadoop.fs.DF:getMount(),110,127,"/**
 * Retrieves the mount string based on the OS.
 * @return The mount string.
 * @throws IOException if the directory doesn't exist.
 */","* @return the filesystem mount point for the indicated volume.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DFCachingGetSpaceUsed.java,refresh,org.apache.hadoop.fs.DFCachingGetSpaceUsed:refresh(),44,47,"/**
* Calls used.m2() with the result of df.m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getPercentUsed,org.apache.hadoop.fs.DF:getPercentUsed(),100,104,"/**
 * Calculates usage percentage based on cap and used values.
 */","@return the amount of the volume full, as a percent.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,startPositionWithoutWindowsDrive,org.apache.hadoop.fs.Path:startPositionWithoutWindowsDrive(java.lang.String),324,330,"/**
 * Masks path based on m1 result. Returns 0, 2, 3 based on path.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,toString,org.apache.hadoop.fs.Path:toString(),476,503,"/**
* Constructs a URI string from the URI object's components.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,<init>,"org.apache.hadoop.fs.FSInputChecker:<init>(org.apache.hadoop.fs.Path,int,boolean,java.util.zip.Checksum,int,int)",87,91,"/**
 * Constructs an FSInputChecker with file, retries, and checksum options.
 */","Constructor
   * 
   * @param file The name of the file to be read
   * @param numOfRetries Number of read retries when ChecksumError occurs
   * @param sum the type of Checksum engine
   * @param chunkSize maximun chunk size
   * @param checksumSize the number byte of each checksum
   * @param verifyChecksum verify check sum.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,seek,org.apache.hadoop.fs.FSInputChecker:seek(long),428,451,"/**
* Seeks to a given position within the chunk.
* @param pos The absolute position to seek to.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ByteBufferUtil.java,streamHasByteBufferRead,org.apache.hadoop.fs.ByteBufferUtil:streamHasByteBufferRead(java.io.InputStream),37,46,"/**
 * Checks if an InputStream is a ByteBufferReadable or FSDataInputStream.
 */",* Determine if a stream can do a byte buffer read via read(ByteBuffer buf),,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,init,org.apache.hadoop.fs.audit.CommonAuditContext:init(),193,197,"/**
* Calls m1 with PARAM_THREAD1 and a function to get thread ID.
*/",* Initialize.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,noteEntryPoint,org.apache.hadoop.fs.audit.CommonAuditContext:noteEntryPoint(java.lang.Object),278,288,"/**
* Extracts and logs a name from a tool's class name.
* Requires a tool and checks a global context.
*/","* Add the entry point as a context entry with the key
   * {@link AuditConstants#PARAM_COMMAND}
   * if it has not  already been recorded.
   * This is called via ToolRunner but may be used at any
   * other entry point.
   * @param tool object loaded/being launched.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,<init>,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:<init>(),404,405,"/**
 * Private constructor to prevent direct instantiation of Builder.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,int)",93,95,"/**
 * Delegates to m1 with the provided key and value.
 * @param key The key. @param value The value.
 */
","* Set optional int parameter for the Builder.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,float)",108,111,"/**
 * Calls m1 with the given key and value (converted to long).
 * @param key The key. @param value The value (as float).
 */","* This parameter is converted to a long and passed
   * to {@link #optLong(String, long)} -all
   * decimal precision is lost.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)
   * @deprecated use {@link #optDouble(String, double)}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,long)",121,123,"/**
* Delegates to m1, passing the key and value.
*/","* Set optional long parameter for the Builder.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @deprecated use  {@link #optLong(String, long)} where possible.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,double)",136,139,"/**
 * Calls m1 with the given key and value (converted to long).
 * @param key The key to use.
 * @param value The value to pass (as a long).
 * @return The result of calling m1.
 */
","* Pass an optional double parameter for the Builder.
   * This parameter is converted to a long and passed
   * to {@link #optLong(String, long)} -all
   * decimal precision is lost.
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)
   * @deprecated use {@link #optDouble(String, double)}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,int)",207,209,"/**
 * Delegates to m1 with the provided key and value.
 * @param key The key. @param value The value.
 */
","* Set mandatory int option.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,float)",221,224,"/**
 * Calls m1 with the given key and value (converted to long).
 * @param key The key to use.
 * @param value The value to use (casted to long).
 * @return The result of m1.
 */
","* This parameter is converted to a long and passed
   * to {@link #mustLong(String, long)} -all
   * decimal precision is lost.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @deprecated use {@link #mustDouble(String, double)} to set floating point.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,long)",234,237,"/**
 * Calls m1 with the given key and value.
 * @param key The key.
 * @param value The value.
 * @return The result of m1.
 */
","* Set mandatory long option.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,double)",248,251,"/**
* Calls m1 with the given key and value (casted to long).
* @param key The key to pass to m1.
* @param value The value to pass to m1 (as a long).
* @return The result of calling m1.
*/
","* Set mandatory long option, despite passing in a floating
   * point value.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,getRow,org.apache.hadoop.tools.TableListing$Column:getRow(int),100,116,"/**
 * Extracts and formats a string based on index, wrapping and justification.
 * @param idx Index to extract the string from; returns formatted String[].
 */","* Return the ith row of the column as a set of wrapped strings, each at
     * most wrapWidth in length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBUploadHandle.java,from,org.apache.hadoop.fs.BBUploadHandle:from(java.nio.ByteBuffer),40,42,"/**
 * Creates a BBUploadHandle from a ByteBuffer.
 * @param byteBuffer The ByteBuffer to wrap.
 * @return A BBUploadHandle instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBUploadHandle.java,equals,org.apache.hadoop.fs.BBUploadHandle:equals(java.lang.Object),54,61,"/**
 * Checks if this UploadHandle is equal to another.
 * @param other The object to compare to.
 * @return True if equal, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,startLocalOutput,"org.apache.hadoop.fs.FilterFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",396,400,"/**
* Delegates file operation to the file system.
* @param fsOutputFile Output file path.
* @param tmpLocalFile Temporary file path.
* @return Path object representing the result.
*/","* Returns a local File that the user can write output to.  The caller
   * provides both the eventual FS target name and the local working
   * file.  If the FS is local, we write directly into the target.  If
   * the FS is remote, we write into the tmp local area.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setWriteChecksum,org.apache.hadoop.fs.FilterFileSystem:setWriteChecksum(boolean),517,520,"/**
 * Calls the m1 method of the fs object, passing writeChecksum.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,"org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)",86,88,"/**
 * Constructs a FsPermission with specified user, group, other actions.
 * @param u User action
 * @param g Group action
 * @param o Other action
 * @param sb Boolean value for some flag
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,fromShort,org.apache.hadoop.fs.permission.FsPermission:fromShort(short),174,177,"/**
* Calls m1 with derived values from n.
* Extracts bits from n and passes them to m1.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,getEffectivePermission,"org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry,org.apache.hadoop.fs.permission.FsPermission)",247,277,"/**
* Calculates effective permission based on entry and permission.
* @param entry AclEntry object
* @param permArg FsPermission object
* @return FsAction representing the effective permission
*/","* Get the effective permission for the AclEntry. <br>
   * Recommended to use this API ONLY if client communicates with the old
   * NameNode, needs to pass the Permission for the path to get effective
   * permission, else use {@link AclStatus#getEffectivePermission(AclEntry)}.
   * @param entry AclEntry to get the effective action
   * @param permArg Permission for the path. However if the client is NOT
   *          communicating with old namenode, then this argument will not have
   *          any preference.
   * @return Returns the effective permission for the entry.
   * @throws IllegalArgumentException If the client communicating with old
   *           namenode and permission is not passed as an argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,<init>,"org.apache.hadoop.fs.permission.PermissionStatus$2:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",72,76,"/**
 * Constructs a PermissionStatus with user, group, and permission.
 */","* Constructor.
   *
   * @param user user.
   * @param group group.
   * @param permission permission.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,parseAclEntry,"org.apache.hadoop.fs.permission.AclEntry:parseAclEntry(java.lang.String,boolean)",263,323,"/**
 * Parses an ACL string and creates an AclEntry.
 * @param aclStr ACL string to parse.
 * @param includePermission Whether to include permission.
 * @return AclEntry object.
 */","* Parses a string representation of an ACL into a AclEntry object.<br>
   * The expected format of ACL entries in the string parameter is the same
   * format produced by the {@link #toStringStable()} method.
   * 
   * @param aclStr
   *          String representation of an ACL.<br>
   *          Example: ""user:foo:rw-""
   * @param includePermission
   *          for setAcl operations this will be true. i.e. Acl should include
   *          permissions.<br>
   *          But for removeAcl operation it will be false. i.e. Acl should not
   *          contain permissions.<br>
   *          Example: ""user:foo,group:bar,mask::""
   * @return Returns an {@link AclEntry} object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,<init>,"org.apache.hadoop.fs.permission.FsCreateModes:<init>(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",65,70,"/**
 * Constructs a FsCreateModes with masked and unmasked permissions.
 * @param masked The masked permission.
 * @param unmasked The unmasked permission.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,equals,org.apache.hadoop.fs.permission.FsCreateModes:equals(java.lang.Object),88,101,"/**
 * Checks if two FsCreateModes objects are equal.
 * @param o the object to compare to
 * @return true if equal, false otherwise
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,toStringStable,org.apache.hadoop.fs.permission.AclEntry:toStringStable(),119,136,"/**
 * Builds a string representation of the AclEntry.
 * Returns a formatted string based on scope, type, name, & permission.
 */","* Returns a string representation guaranteed to be stable across versions to
   * satisfy backward compatibility requirements, such as for shell command
   * output or serialization.  The format of this string representation matches
   * what is expected by the {@link #parseAclSpec(String, boolean)} and
   * {@link #parseAclEntry(String, boolean)} methods.
   *
   * @return stable, backward compatible string representation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntryType.java,toString,org.apache.hadoop.fs.permission.AclEntryType:toString(),59,65,"/**
* Delegates to m1() and returns its result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStrings,"org.apache.hadoop.util.StringUtils:getStrings(java.lang.String,java.lang.String)",418,424,"/**
 * Splits string by delimiter. Returns null if empty.
 * @param str String to split
 * @param delim Delimiter string
 * @return String array or null if empty
 */
","* Returns an arraylist of strings.
   * @param str the string values
   * @param delim delimiter to separate the values
   * @return the arraylist of the separated string values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStringCollection,org.apache.hadoop.util.StringUtils:getStringCollection(java.lang.String),431,434,"/**
 * Splits a string by comma and returns a collection of strings.
 * @param str The string to split.
 * @return A Collection of strings.
 */","* Returns a collection of strings.
   * @param str comma separated string values
   * @return an <code>ArrayList</code> of string values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionParser.java,<init>,"org.apache.hadoop.fs.permission.PermissionParser:<init>(java.lang.String,java.util.regex.Pattern,java.util.regex.Pattern)",51,62,"/**
 * Parses permission mode string using symbolic or octal patterns.
 * @param modeStr Input permission mode string.
 * @throws IllegalArgumentException if parsing fails.
 */
","* Begin parsing permission stored in modeStr
   * 
   * @param modeStr Permission mode, either octal or symbolic
   * @param symbolic Use-case specific symbolic pattern to match against
   * @throws IllegalArgumentException if unable to parse modeStr",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionParser.java,combineModes,"org.apache.hadoop.fs.permission.PermissionParser:combineModes(int,boolean)",175,183,"/**
 * Masks existing value with type and mode bits.
 * @param existing The value to be masked.
 * @param exeOk Flag indicating execution permission.
 * @return Masked integer value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",649,651,"/**
 * Constructs a ByteBufferBlockFactory.
 * @param keyToBufferDir Path to buffer directory.
 * @param conf Configuration object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",526,528,"/**
 * Constructs an ArrayBlockFactory with a key and configuration.
 * @param keyToBufferDir Key for buffer directory.
 * @param conf Configuration object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,requestBuffer,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:requestBuffer(int),659,663,"/**
 * Requests a buffer of specified size from the buffer pool.
 * @param limit The size of the buffer to request.
 * @return The requested ByteBuffer.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,releaseBuffer,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:releaseBuffer(java.nio.ByteBuffer),665,669,"/**
* Releases a buffer back to the buffer pool and decrements outstanding count.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$DiskBlock:<init>(java.io.File,int,long,org.apache.hadoop.fs.store.BlockUploadStatistics)",853,863,"/**
 * Constructs a DiskBlock with file, limit, index, and statistics.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:<init>(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",574,581,"/**
 * Initializes a ByteArrayBlock with given index, limit, and stats.
 * @param index Block index.
 * @param limit Maximum bytes to store.
 * @param statistics Upload statistics.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:hasCapacity(long),869,872,"/**
* Checks if adding 'bytes' exceeds the limit.
* @param bytes Number of bytes to add.
* @return True if within limit, false otherwise.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toString,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:toString(),945,955,"/**
 * Generates a String representation of the FileBlock object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,innerClose,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:innerClose(),905,933,"/**
* Handles cleanup based on the DestState, potentially deleting files.
*/","* The close operation will delete the destination file if it still
     * exists.
     *
     * @throws IOException IO problems",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,checkOpenState,org.apache.hadoop.fs.store.ByteBufferInputStream:checkOpenState(),89,92,"/**
* Checks if the stream is open; throws exception if closed.
*/","* Check the open state.
   * @throws IllegalStateException if the stream is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,enterState,"org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState,org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)",349,355,"/**
* Transitions to the next state, logging the change.
* @param current The current state.
* @param next The next state to transition to.
*/
","* Atomically enter a state, verifying current state.
     *
     * @param current current state. null means ""no check""
     * @param next    next state
     * @throws IllegalStateException if the current state is not as expected",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$DataBlock:write(byte[],int,int)",424,433,"/**
 * Validates input and returns 0. Performs a writing operation.
 * @param buffer Byte array to validate.
 * @param offset Start offset in the buffer.
 * @param length Number of bytes to read.
 */","* Write a series of bytes from the buffer, from the offset.
     * Returns the number of bytes written.
     * Only valid in the state {@code Writing}.
     * Base class verifies the state but does no writing.
     *
     * @param buffer buffer.
     * @param offset offset.
     * @param length length of write.
     * @return number of bytes written.
     * @throws IOException trouble",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,flush,org.apache.hadoop.fs.store.DataBlocks$DataBlock:flush(),442,444,"/**
* Calls m1 with the ""Writing"" argument.
*/","* Flush the output.
     * Only valid in the state {@code Writing}.
     * In the base class, this is a no-op
     *
     * @throws IOException any IO problem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,set,"org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:set(java.lang.String,java.lang.String)",246,248,"/**
* Calls m1(key) and then m2 with the result and value.
*/","* Set an attribute. If the value is non-null/empty,
   * it will be used as a query parameter.
   *
   * @param key key to set
   * @param value value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,extractQueryParameters,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:extractQueryParameters(java.lang.String),331,347,"/**
 * Extracts parameters from a URI and returns them as a map.
 * @param header URI string to parse.
 * @return Map of parameter names and values.
 */
","* Split up the string. Uses httpClient: make sure it is on the classpath.
   * Any query param with a name but no value, e.g ?something is
   * returned in the map with an empty string as the value.
   * @param header URI to parse
   * @return a map of parameters.
   * @throws URISyntaxException failure to build URI from header.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:hasCapacity(long),602,605,"/**
* Checks if adding 'bytes' exceeds the limit.
* @param bytes Number of bytes to add.
* @return True if within limit, false otherwise.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,remainingCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:remainingCapacity(),607,610,"/**
* Calculates a masked value by subtracting m1() from limit.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,dataSize,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:dataSize(),718,720,"/**
* Returns dataSize if available, otherwise calls m1() and returns its value.
*/","* Get the amount of data; if there is no buffer then the size is 0.
       *
       * @return the amount of data available to upload.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:hasCapacity(long),733,736,"/**
* Checks if the given byte count is less than or equal to m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hashCode,org.apache.hadoop.fs.FileSystem$Cache$Key:hashCode(),3891,3894,"/**
 * Calculates a combined value based on scheme, authority, ugi, and unique.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hashCode,org.apache.hadoop.security.UserGroupInformation$RealUser:hashCode(),492,495,"/**
* Delegates m1() call to the realUser.
* @return The result of realUser's m1() method.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,hashCode,org.apache.hadoop.ipc.Client$ConnectionId:hashCode(),1843,1859,"/**
* Calculates a hash code based on connection policy and settings.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,equals,org.apache.hadoop.fs.FileSystem$Cache$Key:equals(java.lang.Object),3900,3913,"/**
 * Checks if two Key objects are equal.
 * @param obj the object to compare to
 * @return true if equal, false otherwise
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,equals,org.apache.hadoop.security.UserGroupInformation$RealUser:equals(java.lang.Object),481,490,"/**
 * Checks if two objects are equal based on m1() and realUser.
 * @param o The object to compare with.
 * @return True if equal, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getInitialWorkingDirectory,org.apache.hadoop.fs.FilterFs:getInitialWorkingDirectory(),78,81,"/**
* Delegates m1() call to the underlying FileSystem object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolvePath,org.apache.hadoop.fs.FileContext:resolvePath(org.apache.hadoop.fs.Path),616,619,"/**
 * Delegates file path processing to m1.
 * @param f The input file path.
 * @return Processed path.
 */
","* Resolve the path following any symlinks or mount points
   * @param f to be resolved
   * @return fully qualified resolved path
   * 
   * @throws FileNotFoundException  If <code>f</code> does not exist
   * @throws AccessControlException if access denied
   * @throws IOException If an IO Error occurred
   * @throws UnresolvedLinkException If unresolved link occurred.
   *
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server
   *
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is not valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,msync,org.apache.hadoop.fs.FileContext:msync(),1267,1269,"/**
* Delegates m1 execution to the default file system.
* Throws IOException or UnsupportedOperationException.
*/","* Synchronize client metadata state.
   *
   * @throws IOException If an I/O error occurred.
   * @throws UnsupportedOperationException If file system for <code>f</code> is
   *                                       not supported.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,msync,org.apache.hadoop.fs.FilterFs:msync(),127,130,"/**
* Delegates m1() call to the underlying FileSystem object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,printStatistics,org.apache.hadoop.fs.FileContext:printStatistics(),2412,2414,"/**
* Calls the m1 method of the AbstractFileSystem class.
*/","* Prints the statistics to standard output. File System is identified by the
   * scheme and authority.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getStatistics,org.apache.hadoop.fs.AbstractFileSystem:getStatistics(java.net.URI),191,204,"/**
 * Retrieves statistics for a URI. Creates if not found.
 * @param uri The URI to retrieve statistics for.
 * @return Statistics object for the URI.
 */
","* Get the statistics for a particular file system.
   * 
   * @param uri
   *          used as key to lookup STATISTICS_TABLE. Only scheme and authority
   *          part of the uri are used.
   * @return a statistics object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listCorruptFileBlocks,org.apache.hadoop.fs.FilterFs:listCorruptFileBlocks(org.apache.hadoop.fs.Path),209,213,"/**
 * Delegates to the underlying FileSystem's m1 method.
 * @param path The path to iterate over.
 * @return A RemoteIterator of Path objects.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,createMultipartUploader,org.apache.hadoop.fs.AbstractFileSystem:createMultipartUploader(org.apache.hadoop.fs.Path),1634,1639,"/**
* Creates a MultipartUploaderBuilder with the given base path.
* @param basePath The base path for the uploader.
* @throws IOException if an I/O error occurs.
*/
","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,obtainContext,org.apache.hadoop.fs.LocalDirAllocator:obtainContext(java.lang.String),107,117,"/**
 * Retrieves an AllocatorPerContext for the given context name.
 * Creates and caches if not found.
 */","This method must be used to obtain the dir allocation context for a 
   * particular value of the context name. The context name must be an item
   * defined in the Configuration object for which we want to control the 
   * dir allocations (e.g., <code>mapred.local.dir</code>). The method will
   * create a context for that name if it doesn't already exist.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getStatistics,org.apache.hadoop.fs.FilterFs:getStatistics(),68,71,"/**
* Delegates the m1() call to the wrapped FileSystem object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,getPos,org.apache.hadoop.fs.FSDataOutputStream:getPos(),97,99,"/**
* Delegates m1() call to the PositionCache object.
*/","* Get the current position in the output stream.
   *
   * @return the current position in the output stream",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,syncFs,org.apache.hadoop.io.SequenceFile$Writer:syncFs(),1382,1387,"/**
 * Calls m1() on the 'out' object if it's not null.
 */","* flush all currently written data to the file system.
     * @deprecated Use {@link #hsync()} or {@link #hflush()} instead
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hflush,org.apache.hadoop.io.SequenceFile$Writer:hflush(),1396,1401,"/**
* Calls m1() on the associated 'out' object, if it exists.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hsync,org.apache.hadoop.io.SequenceFile$Writer:hsync(),1389,1394,"/**
 * Delegates m1() call to the nested 'out' object, if it exists.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,quota,org.apache.hadoop.fs.ContentSummary$Builder:quota(long),90,94,"/**
 * Calls superclass's m1 method and returns the builder.
 * @param quota The quota value to pass to the superclass.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,spaceConsumed,org.apache.hadoop.fs.ContentSummary$Builder:spaceConsumed(long),96,100,"/**
* Calls superclass's m1 method and returns the builder.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,spaceQuota,org.apache.hadoop.fs.ContentSummary$Builder:spaceQuota(long),102,106,"/**
* Calls superclass's m1 method and returns the builder.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,build,org.apache.hadoop.fs.ContentSummary$Builder:build(),132,136,"/**
 * Creates a content summary based on file/directory counts.
 * Returns a ContentSummary object representing the summary.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,<init>,"org.apache.hadoop.util.ApplicationClassLoader:<init>(java.net.URL[],java.lang.ClassLoader,java.util.List)",87,100,"/**
 * Constructs an ApplicationClassLoader with URLs, parent, and system classes.
 * @param urls URLs for class loading
 * @param parent Parent classloader
 * @param systemClasses System classes to load
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getTrimmedStringCollection,org.apache.hadoop.util.StringUtils:getTrimmedStringCollection(java.lang.String),490,495,"/**
 * Removes duplicates and empty strings from a string collection.
 * @param str The input string to process.
 * @return A collection of unique, non-empty strings.
 */
","* Splits a comma separated value <code>String</code>, trimming leading and
   * trailing whitespace on each value. Duplicate and empty values are removed.
   *
   * @param str a comma separated <code>String</code> with values, may be null
   * @return a <code>Collection</code> of <code>String</code> values, empty
   *         Collection if null String input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/LoggingStateChangeListener.java,<init>,org.apache.hadoop.service.LoggingStateChangeListener:<init>(),51,53,"/**
 * Constructs a LoggingStateChangeListener with the default logger.
 */",* Log events to the static log for this class,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,<init>,org.apache.hadoop.service.ServiceStateException:<init>(java.lang.String),48,50,"/**
 * Constructs a ServiceStateException with a message.
 * @param message The exception message.
 */
","* Instantiate
   * @param message error message",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,<init>,"org.apache.hadoop.service.ServiceStateException:<init>(int,java.lang.String,java.lang.Throwable)",78,83,"/**
 * Constructs ServiceStateException with exit code, message, and cause.
 * @param exitCode Exit code associated with the exception.
 * @param message Exception message.
 * @param cause  The underlying cause of the exception.
 */
","* Instantiate, using the specified exit code as the exit code
   * of the exception, irrespetive of any exit code supplied by any inner
   * cause.
   *
   * @param exitCode exit code to declare
   * @param message exception message
   * @param cause inner cause",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,convert,"org.apache.hadoop.service.ServiceStateException:convert(java.lang.String,java.lang.Throwable)",120,126,"/**
 * Wraps a fault as a RuntimeException or ServiceStateException.
 * @param text Error message. @param fault The exception to wrap.
 */","* Convert any exception into a {@link RuntimeException}.
   * If the caught exception is already of that type, it is typecast to a
   * {@link RuntimeException} and returned.
   *
   * All other exception types are wrapped in a new instance of
   * {@code ServiceStateException}.
   * @param text text to use if a new exception is created
   * @param fault exception or throwable
   * @return a {@link RuntimeException} to rethrow",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,convert,org.apache.hadoop.service.ServiceStateException:convert(java.lang.Throwable),101,107,"/**
 * Returns fault as RuntimeException or ServiceStateException.
 * If fault is RuntimeException, returns it; otherwise, wraps it.
 */","* Convert any exception into a {@link RuntimeException}.
   * All other exception types are wrapped in a new instance of
   * {@code ServiceStateException}.
   * @param fault exception or throwable
   * @return a {@link RuntimeException} to rethrow",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,<init>,org.apache.hadoop.service.ServiceStateModel:<init>(java.lang.String),60,62,"/**
 * Constructs a ServiceStateModel with the given name and NOTINITED state.
 * @param name The name of the service.
 */
","* Create the service state model in the {@link Service.STATE#NOTINITED}
   * state.
   *
   * @param name input name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,isInState,org.apache.hadoop.service.AbstractService:isInState(org.apache.hadoop.service.Service$STATE),451,454,"/**
* Checks if the service state matches the expected state.
* @param expected The expected service state.
* @return True if the state matches, false otherwise.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,isValidStateTransition,"org.apache.hadoop.service.ServiceStateModel:isValidStateTransition(org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)",149,153,"/**
* Checks if a state transition is allowed based on state maps.
* @param current Current state.
* @param proposed Proposed state.
* @return True if transition is allowed, false otherwise.
*/","* Is a state transition valid?
   * There are no checks for current==proposed
   * as that is considered a non-transition.
   *
   * using an array kills off all branch misprediction costs, at the expense
   * of cache line misses.
   *
   * @param current current state
   * @param proposed proposed new state
   * @return true if the transition to a new state is valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,toString,org.apache.hadoop.service.ServiceStateModel:toString(),159,163,"/**
* Concatenates a prefix based on name.m1() with state.m2().
*/","* return the state text as the toString() value
   * @return the current state's description",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/HadoopUncaughtExceptionHandler.java,<init>,org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:<init>(),71,73,"/**
 * Default constructor. Initializes with a null handler.
 */
","* Basic exception handler -logs simple exceptions, then continues.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,handle,org.apache.hadoop.service.launcher.IrqHandler:handle(sun.misc.Signal),125,131,"/**
* Processes an interrupt signal.
* @param s The interrupt signal received.
*/","* Handler for the JVM API for signal handling.
   * @param s signal raised",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,getService,org.apache.hadoop.service.launcher.InterruptEscalator:getService(),84,87,"/**
 * Delegates to the owner's m2() method, returning null if owner is null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,<init>,"org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:<init>(org.apache.hadoop.service.Service,int)",188,191,"/**
 * Constructs a ServiceForcedShutdown with service and shutdown time.
 * @param service The service to be shut down.
 * @param shutdownTimeMillis Shutdown delay in milliseconds.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,lookup,org.apache.hadoop.service.launcher.InterruptEscalator:lookup(java.lang.String),153,160,"/**
 * Finds an IrqHandler by signal name.
 * @param signalName The signal name to search for.
 * @return The matching IrqHandler or null.
 */
","* Look up the handler for a signal.
   * @param signalName signal name
   * @return a handler if found",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable)",49,51,"/**
* Constructs a ServiceLaunchException with exit code and cause.
*/
","* Create an exception with the specific exit code.
   * @param exitCode exit code
   * @param cause cause of the exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String)",58,60,"/**
 * Constructs a ServiceLaunchException with exit code and message.
 */
","* Create an exception with the specific exit code and text.
   * @param exitCode exit code
   * @param message message to use in exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String,java.lang.Object[])",74,79,"/**
 * Constructs a ServiceLaunchException with exit code, format, and args.
 * Optionally sets cause if last arg is a Throwable.
 */
","* Create a formatted exception.
   * <p>
   * This uses {@link String#format(String, Object...)}
   * to build the formatted exception in the ENGLISH locale.
   * <p>
   * If the last argument is a throwable, it becomes the cause of the exception.
   * It will also be used as a parameter for the format.
   * @param exitCode exit code
   * @param format format for message to use in exception
   * @param args list of arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String)",1083,1086,"/**
 * Constructs a KerberosDiagsFailure with category and message.
 * @param category Failure category.
 * @param message Failure message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable,java.lang.String,java.lang.Object[])",91,94,"/**
 * Constructs a ServiceLaunchException with exit code, cause, and format.
 */","* Create a formatted exception.
   * <p>
   * This uses {@link String#format(String, Object...)}
   * to build the formatted exception in the ENGLISH locale.
   * @param exitCode exit code
   * @param cause inner cause
   * @param format format for message to use in exception
   * @param args list of arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,run,org.apache.hadoop.service.launcher.ServiceShutdownHook:run(),82,85,"/**
* Calls the m1 method.
*/","* Shutdown handler.
   * Query the service hook reference -if it is still valid the 
   * {@link Service#stop()} operation is invoked.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,<init>,org.apache.hadoop.service.launcher.ServiceLauncher:<init>(java.lang.String),184,186,"/**
 * Constructs a ServiceLauncher with the same class name for both.
 * @param serviceClassName Class name for the service to launch.
 */
","* Create an instance of the launcher.
   * @param serviceClassName classname of the service",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,toString,org.apache.hadoop.service.launcher.ServiceLauncher:toString(),253,264,"/**
 * Creates a string describing the service launcher.
 * Appends serviceName, serviceClassName (if available), and service.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,noteException,org.apache.hadoop.service.launcher.ServiceLauncher:noteException(org.apache.hadoop.util.ExitUtil$ExitException),331,345,"/**
 * Handles ExitException: logs details and sets service exit code.
 */","* Record that an Exit Exception has been raised.
   * Save it to {@link #serviceException}, with its exit code in
   * {@link #serviceExitCode}
   * @param exitException exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,bindCommandOptions,org.apache.hadoop.service.launcher.ServiceLauncher:bindCommandOptions(),321,323,"/**
* Initializes commandOptions by calling m1().
*/","* Set the {@link #commandOptions} field to the result of
   * {@link #createOptions()}; protected for subclasses and test access.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,loadConfigurationClasses,org.apache.hadoop.service.launcher.ServiceLauncher:loadConfigurationClasses(),422,449,"/**
 * Loads and creates Configuration instances from a list; returns count.
 */","* @return This creates all the configurations defined by
   * {@link #getConfigurationsToCreate()} , ensuring that
   * the resources have been pushed in.
   * If one cannot be loaded it is logged and the operation continues
   * except in the case that the class does load but it isn't actually
   * a subclass of {@link Configuration}.
   * @throws ExitUtil.ExitException if a loaded class is of the wrong type",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,registerServiceListener,org.apache.hadoop.service.AbstractService:registerServiceListener(org.apache.hadoop.service.ServiceStateChangeListener),354,357,"/**
* Adds a ServiceStateChangeListener to the listener list.
* @param l The listener to add.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,registerGlobalListener,org.apache.hadoop.service.AbstractService:registerGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener),369,371,"/**
* Adds a ServiceStateChangeListener to the global listeners.
* @param l The ServiceStateChangeListener to add.
*/","* Register a global listener, which receives notifications
   * from the state change events of all services in the JVM
   * @param l listener",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,unregisterServiceListener,org.apache.hadoop.service.AbstractService:unregisterServiceListener(org.apache.hadoop.service.ServiceStateChangeListener),359,362,"/**
* Adds a ServiceStateChangeListener to the listener list.
* @param l The listener to add.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,unregisterGlobalListener,org.apache.hadoop.service.AbstractService:unregisterGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener),378,380,"/**
* Adds a ServiceStateChangeListener to the global listeners.
* @param l The listener to add.
* @return True if added successfully, false otherwise.
*/
","* unregister a global listener.
   * @param l listener to unregister
   * @return true if the listener was found (and then deleted)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,resetGlobalListeners,org.apache.hadoop.service.AbstractService:resetGlobalListeners(),385,388,"/**
 * Calls the m1 method on the globalListeners object.
 */",* Package-scoped method for testing -resets the global listener list,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,notifyListeners,org.apache.hadoop.service.AbstractService:notifyListeners(),409,416,"/**
 * Notifies listeners and global listeners about the event.
 * Handles exceptions during listener notification.
 */","* Notify local and global listeners of state changes.
   * Exceptions raised by listeners are NOT passed up.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,getServiceState,org.apache.hadoop.service.AbstractService:getServiceState(),118,121,"/**
* Returns the state model's m1 result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,serviceInit,org.apache.hadoop.service.AbstractService:serviceInit(org.apache.hadoop.conf.Configuration),312,317,"/**
 * Handles configuration override during initialization.
 * @param conf The new configuration object.
 */","* All initialization code needed by a service.
   *
   * This method will only ever be called once during the lifecycle of
   * a specific service instance.
   *
   * Implementations do not need to be synchronized as the logic
   * in {@link #init(Configuration)} prevents re-entrancy.
   *
   * The base implementation checks to see if the subclass has created
   * a new configuration instance, and if so, updates the base class value
   * @param conf configuration
   * @throws Exception on a failure -these will be caught,
   * possibly wrapped, and will trigger a service stop",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceStop,org.apache.hadoop.util.JvmPauseMonitor:serviceStop(),88,100,"/**
* Stops monitoring and calls super.m4().
* Stops the monitor thread and calls super.m4().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceStart,org.apache.hadoop.service.CompositeService:serviceStart(),115,126,"/**
 * Starts services retrieved from m1(), logs start, then calls super.m7().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,addIfService,org.apache.hadoop.service.CompositeService:addIfService(java.lang.Object),88,95,"/**
 * Checks if object is a Service and calls m1 if true.
 * @param object The object to check.
 * @return True if object is a Service, false otherwise.
 */","* If the passed object is an instance of {@link Service},
   * add it to the list of services managed by this {@link CompositeService}
   * @param object object.
   * @return true if a service is added, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,"org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.commons.logging.Log,org.apache.hadoop.service.Service)",79,88,"/**
 * Stops the service and logs any exceptions.
 * @param log Logger instance
 * @param service Service to stop
 * @return Exception if an error occurred, null otherwise
 */","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param log the log to warn at
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.
   * @deprecated to be removed with 3.4.0. Use {@link #stopQuietly(Logger, Service)} instead.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,"org.apache.hadoop.service.ServiceOperations:stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)",100,108,"/**
 * Stops the service and logs any exceptions.
 * @param log Logger instance for error logging.
 * @param service Service to be stopped.
 * @return Exception if an error occurred, otherwise null.
 */
","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param log the log to warn at
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.
   * @see ServiceOperations#stopQuietly(Service)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ProgressableOption:<init>(org.apache.hadoop.util.Progressable),969,971,"/**
 * Creates a ProgressableOption with the given Progressable value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ShortWritable.java,<init>,org.apache.hadoop.io.ShortWritable:<init>(short),37,39,"/**
 * Constructs a ShortWritable with the given short value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$LengthOption:<init>(long),1874,1876,"/**
* Initializes a LengthOption with the given value.
* @param value The length value for this option.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$StartOption:<init>(long),1867,1869,"/**
* Constructor for StartOption, initializes with a long value.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$BlockSizeOption:<init>(long),925,927,"/**
 * Constructor for BlockSizeOption, initializes with a given value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,read,org.apache.hadoop.io.DataInputByteBuffer$Buffer:read(),31,37,"/**
 * Calls m1 recursively and returns the least significant byte.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,reset,org.apache.hadoop.io.DataInputByteBuffer:reset(java.nio.ByteBuffer[]),83,85,"/**
* Delegates processing of ByteBuffer array to the 'buffers' object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,byteBufferPositionedReadable_readFullyAvailable,org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream),233,246,"/**
* Checks if input stream supports pread byte buffer.
* @param in Input stream to check.
* @return True if stream supports pread byte buffer.
*/","* Probe to see if the input stream is an instance of ByteBufferPositionedReadable.
   * If the stream is an FSDataInputStream, the wrapped stream is checked.
   * @param in input stream
   * @return true if the stream implements the interface (including a wrapped stream)
   * and that it declares the stream capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,isAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:isAvailable(),433,435,"/**
 * Delegates to m1().m2() and returns the result.
 */","* Is the wrapped IO class loaded?
   * @return true if the instance is loaded.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,toString,org.apache.hadoop.util.JsonSerialization:toString(java.lang.Object),352,359,"/**
 * Converts an instance to a string, handling JSON processing errors.
 */","* Convert an instance to a string form for output. This is a robust
   * operation which will convert any JSON-generating exceptions into
   * error text.
   * @param instance non-null instance
   * @return a JSON string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,toUncheckedFunction,org.apache.hadoop.util.functional.FunctionalIO:toUncheckedFunction(org.apache.hadoop.util.functional.FunctionRaisingIOE),84,86,"/**
* Adapts a FunctionRaisingIOE to a Function, suppressing exceptions.
*/","* Convert a {@link FunctionRaisingIOE} as a {@link Supplier}.
   * @param fun function to wrap
   * @param <T> type of input
   * @param <R> type of return value.
   * @return a new function which invokes the inner function and wraps
   * exceptions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,fromInstance,org.apache.hadoop.util.JsonSerialization:fromInstance(java.lang.Object),236,238,"/**
* Applies m1 and m2 to the input instance.
* @param instance The input instance to be transformed.
* @return The transformed instance.
*/","* clone by converting to JSON and back again.
   * This is much less efficient than any Java clone process.
   * @param instance instance to duplicate
   * @return a new instance
   * @throws IOException IO problems.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,fromBytes,org.apache.hadoop.util.JsonSerialization:fromBytes(byte[]),331,333,"/**
* Decodes byte array to String using UTF_8 and calls m1.
* @param bytes byte array to decode
* @return Result of m1 after decoding
*/
","* Deserialize from a byte array.
   * @param bytes byte array
   * @throws IOException IO problems
   * @throws EOFException not enough data
   * @return byte array.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,<init>,org.apache.hadoop.io.VIntWritable:<init>(int),38,38,"/**
* Constructs a VIntWritable with the given integer value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,equals,org.apache.hadoop.io.ElasticByteBufferPool$Key:equals(java.lang.Object),57,68,"/**
 * Checks if rhs is a Key and m1(rhs) equals 0.
 * @param rhs Object to check; returns false if null or invalid type.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$SyncIntervalOption:<init>(int),1009,1013,"/**
* Sets the sync interval. Uses default if value is negative.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ReplicationOption:<init>(int),932,934,"/**
 * Constructs a ReplicationOption with the given integer value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$BufferSizeOption:<init>(int),1881,1883,"/**
 * Constructs a BufferSizeOption with the given integer value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$BufferSizeOption:<init>(int),919,921,"/**
* Initializes a BufferSizeOption with the given value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8:<init>(org.apache.hadoop.io.UTF8),78,80,"/**
 * Copies the content of another UTF8 object.
 */","* Construct from a given string.
   * @param utf8 input utf8.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,writeString,"org.apache.hadoop.io.UTF8:writeString(java.io.DataOutput,java.lang.String)",351,365,"/**
* Writes a string to the output, truncating if too long.
* @param out Output stream to write to.
* @param s String to write. Returns length of written string.
*/","* @return Write a UTF-8 encoded string.
   *
   * @see DataOutput#writeUTF(String)
   * @param out input out.
   * @param s input s.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,skip,org.apache.hadoop.io.UTF8:skip(java.io.DataInput),144,147,"/**
* Reads data from input stream.
* @param in DataInput stream to read from.
*/","* Skips over one UTF8 in the input.
   * @param in datainput.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,skipCompressedByteArray,org.apache.hadoop.io.WritableUtils:skipCompressedByteArray(java.io.DataInput),54,59,"/**
 * Reads data from input stream.
 * @param in DataInput stream to read from.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compare,"org.apache.hadoop.io.SequenceFile$Sorter$SortPass$SeqFileComparator:compare(org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable)",3258,3263,"/**
 * Compares two IntWritable objects using a provided comparator.
 * @param I, J IntWritable objects to compare.
 * @return Comparison result from the comparator.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,org.apache.hadoop.io.SetFile:<init>(),35,35,"/**
 * Private constructor to prevent external instantiation of SetFile.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,org.apache.hadoop.io.ArrayFile:<init>(),35,35,"/**
* Default constructor, protected to prevent external instantiation.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,<init>,org.apache.hadoop.io.LongWritable:<init>(long),37,37,"/**
* Constructs a LongWritable with the given long value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,seek,org.apache.hadoop.io.ArrayFile$Reader:seek(long),112,115,"/**
* Calls key.m1(n) and then recursively calls m2(key).
*/","* Positions the reader before its <code>n</code>th value.
     *
     * @param n n key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,get,"org.apache.hadoop.io.ArrayFile$Reader:get(long,org.apache.hadoop.io.Writable)",147,151,"/**
* Calls m2 with the updated key and provided value.
* @param n key value, @param value writable value
* @return Writable object
*/
","* Return the <code>n</code>th value in the file.
     * @param n n key.
     * @param value value.
     * @throws IOException raised on errors performing I/O.
     * @return writable.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ValueClassOption:<init>(java.lang.Class),952,954,"/**
 * Constructs a ValueClassOption with the given class.
 * @param value The class to be wrapped.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,org.apache.hadoop.io.MapFile$Writer$KeyClassOption:<init>(java.lang.Class),270,272,"/**
 * Constructs a KeyClassOption with the given class.
 * @param value The class associated with this option.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$KeyClassOption:<init>(java.lang.Class),945,947,"/**
 * Constructs a KeyClassOption with the given class.
 * @param value The class associated with this option.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ByteWritable.java,<init>,org.apache.hadoop.io.ByteWritable:<init>(byte),34,34,"/**
* Constructs a ByteWritable with the given byte value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,<init>,org.apache.hadoop.io.OutputBuffer:<init>(),71,73,"/**
 * Constructs a new OutputBuffer using a default Buffer.
 */",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,getData,org.apache.hadoop.io.OutputBuffer:getData(),86,86,"/**
 * Delegates the call to buffer's m1() method.
 * Returns the byte array returned by buffer.m1().
 */
","* Returns the current contents of the buffer.
   *  Data is only valid to {@link #getLength()}.
   *
   * @return the current contents of the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,getLength,org.apache.hadoop.io.OutputBuffer:getLength(),93,93,"/**
 * Delegates the call to buffer's m1() method.
 */","* Returns the length of the valid data currently in the buffer.
   * @return the length of the valid data
   *          currently in the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,reset,org.apache.hadoop.io.OutputBuffer:reset(),96,99,"/**
 * Delegates m1() call to the internal buffer.
 * Returns the current OutputBuffer instance.
 */",@return Resets the buffer to empty.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compare,"org.apache.hadoop.io.WritableComparator:compare(java.lang.Object,java.lang.Object)",215,218,"/**
* Delegates to the more specific m1 method with casted arguments.
*/","* Compare two Object.
   *
   * @param a the first object to be compared.
   * @param b the second object to be compared.
   * @return compare result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,binarySearch,org.apache.hadoop.io.MapFile$Reader:binarySearch(org.apache.hadoop.io.WritableComparable),782,799,"/**
 * Finds the insertion point for a key in a sorted array.
 * @param key The key to search for.
 * @return Insertion point; negative if not found.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compareBytes,"org.apache.hadoop.io.WritableComparator:compareBytes(byte[],int,int,byte[],int,int)",230,233,"/**
 * Compares two byte arrays using FastByteComparisons.
 * @param b1, s1, l1 First array, start, length.
 * @param b2, s2, l2 Second array, start, length.
 */","* Lexicographic order of binary data.
   * @param b1 b1.
   * @param s1 s1.
   * @param l1 l1.
   * @param b2 b2.
   * @param s2 s2.
   * @param l2 l2.
   * @return compare bytes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,hashBytes,"org.apache.hadoop.io.WritableComparator:hashBytes(byte[],int)",255,257,"/**
 * Calls overloaded method with offset 0.
 * @param bytes byte array
 * @param length length of the array to process
 */","* Compute hash for binary data.
   * @param bytes bytes.
   * @param length length.
   * @return hash for binary data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readFloat,"org.apache.hadoop.io.WritableComparator:readFloat(byte[],int)",290,292,"/**
* Calculates a float value from byte array, starting at index.
*/","* Parse a float from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return float from a byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readLong,"org.apache.hadoop.io.WritableComparator:readLong(byte[],int)",300,303,"/**
 * Combines two 32-bit values from byte array into a 64-bit long.
 */","* Parse a long from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return long from a byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readVInt,"org.apache.hadoop.io.WritableComparator:readVInt(byte[],int)",347,349,"/**
* Calls m1 with the given byte array and start index.
* @param bytes byte array
* @param start start index
* @return int result of m1
*/
","* Reads a zero-compressed encoded integer from a byte array and returns it.
   * @param bytes byte array with the encoded integer
   * @param start start index
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,<init>,org.apache.hadoop.io.BytesWritable:<init>(byte[]),61,63,"/**
 * Constructs a BytesWritable with the given byte array.
 * @param bytes The byte array to wrap.
 */
","* Create a BytesWritable using the byte array as the initial value.
   * @param bytes This array becomes the backing storage for the object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,get,org.apache.hadoop.io.BytesWritable:get(),102,105,"/**
 * Calls m1() and returns the result. Deprecated.
 */","* Get the data from the BytesWritable.
   * @deprecated Use {@link #getBytes()} instead.
   * @return data from the BytesWritable.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,getSize,org.apache.hadoop.io.BytesWritable:getSize(),120,123,"/**
* Calls m1() and returns its result. Deprecated.
*/","* Get the current size of the buffer.
   * @deprecated Use {@link #getLength()} instead.
   * @return current size of the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,setCapacity,org.apache.hadoop.io.BytesWritable:setCapacity(int),155,160,"/**
* Resizes the data structures if the given capacity differs.
* @param capacity The new capacity to resize to.
*/
","* Change the capacity of the backing storage. The data is preserved.
   *
   * @param capacity The new capacity in bytes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IntWritable.java,<init>,org.apache.hadoop.io.IntWritable:<init>(int),37,37,"/**
* Constructs an IntWritable with the given integer value.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,getBuffer,"org.apache.hadoop.io.ElasticByteBufferPool:getBuffer(boolean,int)",89,101,"/**
 * Retrieves a ByteBuffer, creating one if not found.
 * @param direct if direct memory should be used
 * @param length the desired buffer length
 * @return ByteBuffer with specified length
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,putBuffer,org.apache.hadoop.io.ElasticByteBufferPool:putBuffer(java.nio.ByteBuffer),103,119,"/**
 * Processes a buffer, adds unique entries to a TreeMap.
 * Uses buffer methods for key generation and TreeMap operations.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,size,org.apache.hadoop.io.ElasticByteBufferPool:size(boolean),127,131,"/**
* Delegates to m1(direct) and returns the result of its m2() call.
*/","* Get the size of the buffer pool, for the specified buffer type.
   *
   * @param direct Whether the size is returned for direct buffers
   * @return The size",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>(),159,161,"/**
 * Default constructor for read operations.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,updateProgress,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:updateProgress(long),3611,3616,"/**
 * Updates progress based on processed bytes.
 * @param bytesProcessed Number of bytes processed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,readaheadStream,"org.apache.hadoop.io.ReadaheadPool:readaheadStream(java.lang.String,java.io.FileDescriptor,long,long,long,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest)",102,149,"/**
* Calculates and returns a ReadaheadRequest based on parameters.
* @param identifier Identifier, fd, curPos, readaheadLength, maxOffsetToRead, lastReadahead
* @return ReadaheadRequest object.
*/
","* Issue a request to readahead on the given file descriptor.
   * 
   * @param identifier a textual identifier that will be used in error
   * messages (e.g. the file name)
   * @param fd the file descriptor to read ahead
   * @param curPos the current offset at which reads are being issued
   * @param readaheadLength the configured length to read ahead
   * @param maxOffsetToRead the maximum offset that will be readahead
   *        (useful if, for example, only some segment of the file is
   *        requested by the user). Pass {@link Long#MAX_VALUE} to allow
   *        readahead to the end of the file.
   * @param lastReadahead the result returned by the previous invocation
   *        of this function on this file descriptor, or null if this is
   *        the first call
   * @return an object representing this outstanding request, or null
   *        if no readahead was performed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,append,org.apache.hadoop.io.SetFile$Writer:append(org.apache.hadoop.io.WritableComparable),97,99,"/**
* Calls m2 with the given key and a default NullWritable value.
* @param key The key to pass to the overloaded method.
*/","* Append a key to a set.  The key must be strictly greater than the
     * previous key added to the set.
     * @param key input key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,next,org.apache.hadoop.io.SetFile$Reader:next(org.apache.hadoop.io.WritableComparable),144,147,"/**
* Calls m2 with the given key and a NullWritable value.
* @param key The key to pass to m2.
* @throws IOException if an I/O error occurs.
*/
","* Read the next key in a set into <code>key</code>.
     *
     * @param key input key.
     * @return Returns true if such a key exists
     *    and false when at the end of the set.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,compare,"org.apache.hadoop.io.Text$Comparator:compare(byte[],int,int,byte[],int,int)",433,439,"/**
* Calls m2 after adjusting array offsets based on WritableUtils.m1.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,key,org.apache.hadoop.io.ArrayFile$Reader:key(),136,138,"/**
 * Returns a value obtained by calling key.m1().
 * @return Long value returned by key.m1()
 */","* Returns the key associated with the most recent call to {@link
     * #seek(long)}, {@link #next(Writable)}, or {@link
     * #get(long,Writable)}.
     *
     * @return key key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$InputStreamOption:<init>(org.apache.hadoop.fs.FSDataInputStream),1860,1862,"/**
 * Constructs an InputStreamOption from an FSDataInputStream.
 * @param value The FSDataInputStream to wrap.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,comparator,org.apache.hadoop.io.MapFile$Writer:comparator(org.apache.hadoop.io.WritableComparator),289,291,"/**
 * Creates a ComparatorOption from a WritableComparator.
 * @param value The WritableComparator to wrap.
 * @return A ComparatorOption instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DoubleWritable.java,<init>,org.apache.hadoop.io.DoubleWritable:<init>(double),41,43,"/**
 * Constructs a DoubleWritable with the given double value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VersionedWritable.java,readFields,org.apache.hadoop.io.VersionedWritable:readFields(java.io.DataInput),49,54,"/**
* Reads version from input, validates against internal version.
* @param in DataInput to read version from; throws IOException.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,comparator,org.apache.hadoop.io.MapFile$Reader:comparator(org.apache.hadoop.io.WritableComparator),476,478,"/**
 * Creates a ComparatorOption from a WritableComparator.
 * @param value The WritableComparator to wrap.
 * @return A ComparatorOption instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,access,"org.apache.hadoop.io.nativeio.NativeIO$Windows:access(java.lang.String,org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight)",815,818,"/**
 * Checks access to a path with specified access right.
 * @param path The path to check.
 * @param access Access right value.
 * @return True if access is allowed, false otherwise.
 */","* Checks whether the current process has desired access rights on
     * the given path.
     * 
     * Longer term this native function can be substituted with JDK7
     * function Files#isReadable, isWritable, isExecutable.
     *
     * @param path input path
     * @param desiredAccess ACCESS_READ, ACCESS_WRITE or ACCESS_EXECUTE
     * @return true if access is allowed
     * @throws IOException I/O exception on error",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isAvailable,org.apache.hadoop.io.nativeio.NativeIO$POSIX:isAvailable(),360,362,"/**
 * Checks if NativeCodeLoader.m1() returns true AND nativeLoaded is true.
 */",* @return Return true if the JNI-based native IO extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isAvailable,org.apache.hadoop.io.nativeio.NativeIO:isAvailable(),869,871,"/**
* Checks if NativeCodeLoader.m1() returns true AND nativeLoaded is true.
*/",* @return Return true if the JNI-based native IO extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java,<init>,org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:<init>(),38,48,"/**
 * Initializes the Unix groups mapping implementation.
 * Uses JNI if available, otherwise falls back to shell based.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java,<init>,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:<init>(),37,47,"/**
 * Initializes the group mapping implementation, using JNI if available.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/NativeCrc32.java,isAvailable,org.apache.hadoop.util.NativeCrc32:isAvailable(),35,41,"/**
 * Returns false if Sparc, otherwise calls NativeCodeLoader.m1().
 */",* Return true if the JNI-based native CRC extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,setPmdkSupportState,org.apache.hadoop.io.nativeio.NativeIO$POSIX:setPmdkSupportState(int),173,181,"/**
 * Finds SupportState by code.
 * @param stateCode The state code to search for.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getPmdkSupportStateMessage,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getPmdkSupportStateMessage(),183,189,"/**
 * Returns a string with PMDK support state and library path.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isPmdkAvailable,org.apache.hadoop.io.nativeio.NativeIO$POSIX:isPmdkAvailable(),191,194,"/**
 * Checks if PMDK support is enabled.
 * @return True if PMDK is supported, false otherwise.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,chmod,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:chmod(java.lang.String,int)",387,404,"/**
* Changes file permissions. Calls m5, handles NativeIO errors.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,posixFadviseIfPossible,"org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:posixFadviseIfPossible(java.lang.String,java.io.FileDescriptor,long,long,int)",293,298,"/**
* Calls native m1 function.
* @param identifier Identifier string.
* @param fd File descriptor.
* @param offset Offset.
* @param len Length.
* @param flags Flags.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,munmap,org.apache.hadoop.io.nativeio.NativeIO$POSIX:munmap(java.nio.MappedByteBuffer),491,501,"/**
 * Unmaps the provided buffer if supported; logs reason otherwise.
 */","* Unmaps the block from memory. See munmap(2).
     *
     * There isn't any portable way to unmap a memory region in Java.
     * So we use the sun.nio method here.
     * Note that unmapping a memory region could cause crashes if code
     * continues to reference the unmapped code.  However, if we don't
     * manually unmap the memory, we are dependent on the finalizer to
     * do it, and we have no idea when the finalizer will run.
     *
     * @param buffer    The buffer to unmap.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,freeDB,org.apache.hadoop.crypto.CryptoStreamUtils:freeDB(java.nio.ByteBuffer),47,57,"/**
 * Releases the buffer, attempting unmapping if supported.
 * Logs an error or reason if unmapping fails or is unavailable.
 */","* Forcibly free the direct buffer.
   *
   * @param buffer buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getName,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:getName(org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache,int)",629,648,"/**
 * Retrieves a name from the cache or native implementation.
 * @param domain IdCache enum (USER or GROUP)
 * @param id Identifier for the name to retrieve
 * @return The name associated with the given ID.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getOperatingSystemPageSize,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getOperatingSystemPageSize(),289,291,"/**
 * Delegates to NativeIO.m1() and returns the result.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,memSync,org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:memSync(org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion),252,258,"/**
 * Executes m5 or m4 based on region.m1(). m4 takes region.m2(), region.m3().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,<init>,"org.apache.hadoop.io.ArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[])",57,60,"/**
 * Constructs an ArrayWritable with a given value class and values.
 * @param valueClass Class of the Writable objects in the array.
 * @param values The array of Writable objects.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,"org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(byte[],int,int)",59,61,"/**
 * Initializes with a buffer, offset, and limit.
 * @param buf The buffer.
 * @param offset The offset.
 * @param limit The limit.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,write,"org.apache.hadoop.io.DataOutputBuffer:write(java.io.DataInput,int)",132,134,"/**
* Delegates buffer processing to the internal buffer.
* @param in Input stream to read from.
* @param length Number of bytes to read.
*/
","* Writes bytes from a DataInput directly into the buffer.
   * @param in data input.
   * @param length length.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/TwoDArrayWritable.java,<init>,"org.apache.hadoop.io.TwoDArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[][])",38,41,"/**
 * Creates a TwoDArrayWritable with the given value class and array.
 * @param valueClass Class of the array elements.
 * @param values The 2D array to be wrapped.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,<init>,"org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet,java.lang.Class)",70,72,"/**
 * Constructs an EnumSetWritable from an EnumSet.
 * @param value The EnumSet to wrap.
 * @param elementType The element type of the EnumSet.
 */
","* Construct a new EnumSetWritable. If the <tt>value</tt> argument is null or
   * its size is zero, the <tt>elementType</tt> argument must not be null. If
   * the argument <tt>value</tt>'s size is bigger than zero, the argument
   * <tt>elementType</tt> is not be used.
   * 
   * @param value enumSet value.
   * @param elementType elementType.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readStringArray,org.apache.hadoop.io.WritableUtils:readStringArray(java.io.DataInput),165,173,"/**
 * Reads an array of strings from the input stream.
 * @param in DataInput to read from; returns null if empty.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeStringArray,"org.apache.hadoop.io.WritableUtils:writeStringArray(java.io.DataOutput,java.lang.String[])",136,141,"/**
 * Writes array length and each string in the array to output.
 * @param out DataOutput to write to.
 * @param s String array to write.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,equals,org.apache.hadoop.io.SortedMapWritable:equals(java.lang.Object),200,216,"/**
 * Checks if this SortedMapWritable is equal to another map.
 * @param obj The object to compare to.
 * @return True if equal, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,write,"org.apache.hadoop.io.OutputBuffer$Buffer:write(java.io.InputStream,int)",56,65,"/**
* Extends the buffer by reading 'len' bytes from 'in'.
* @param in Input stream to read from.
* @param len Number of bytes to read.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,fillReservoir,org.apache.hadoop.crypto.random.OsSecureRandom:fillReservoir(int),61,73,"/**
 * Fills the reservoir with data from the stream if needed.
 * Resets 'pos' to 0 after filling.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,<init>,org.apache.hadoop.io.DataInputBuffer:<init>(),134,136,"/**
 * Constructs a DataInputBuffer with a new internal buffer.
 */",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,reset,"org.apache.hadoop.io.DataInputBuffer:reset(byte[],int)",149,151,"/**
 * Delegates processing of the input array to the buffer.
 * @param input Input byte array.
 * @param length Number of bytes to process from the input.
 */","* Resets the data that the buffer reads.
   *
   * @param input input.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,reset,"org.apache.hadoop.io.DataInputBuffer:reset(byte[],int,int)",160,162,"/**
* Delegates buffer processing to the internal buffer.
* @param input Input byte array.
* @param start Start index in the array.
* @param length Number of bytes to process.
*/","* Resets the data that the buffer reads.
   *
   * @param input input.
   * @param start start.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getData,org.apache.hadoop.io.DataInputBuffer:getData(),164,166,"/**
* Delegates to the buffer's m1() method and returns the result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getPosition,org.apache.hadoop.io.DataInputBuffer:getPosition(),173,173,"/**
* Delegates to the buffer's m1() method and returns the result.
*/","* Returns the current position in the input.
   *
   * @return position.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getLength,org.apache.hadoop.io.DataInputBuffer:getLength(),181,181,"/**
 * Delegates the call to buffer's m1() method.
 */","* Returns the index one greater than the last valid character in the input
   * stream buffer.
   *
   * @return length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECSchema.java,<init>,org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.util.Map),69,93,"/**
 * Constructs an ECSchema from provided options.
 * @param allOptions Map of schema options; must not be null/empty.
 */
","* Constructor with schema name and provided all options. Note the options may
   * contain additional information for the erasure codec to interpret further.
   * @param allOptions all schema options",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECSchema.java,<init>,"org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.lang.String,int,int)",101,103,"/**
 * Constructs an ECSchema with codec name, data units, and parity units.
 */","* Constructor with key parameters provided.
   * @param codecName codec name
   * @param numDataUnits number of data units used in the schema
   * @param numParityUnits number os parity units used in the schema",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,setCodecOptions,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:setCodecOptions(org.apache.hadoop.io.erasurecode.ErasureCodecOptions),65,68,"/**
* Sets codec options and schema from provided ErasureCodecOptions.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,getRequiredNumDataBlocks,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumDataBlocks(),54,56,"/**
 * Retrieves a value from the schema.
 * @return An integer value obtained from schema.m1()
 */
","* Get required data blocks count in a BlockGroup.
   * @return count of required data blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,getRequiredNumParityBlocks,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumParityBlocks(),62,64,"/**
* Returns a value from the schema's m1 method.
*/","* Get required parity blocks count in a BlockGroup.
   * @return count of required parity blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.ErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",40,47,"/**
 * Initializes an ErasureCodec with a configuration and options.
 * @param conf Configuration object.
 * @param options ErasureCodecOptions object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ErasureCoderOptions.java,<init>,"org.apache.hadoop.io.erasurecode.ErasureCoderOptions:<init>(int,int)",34,36,"/**
 * Creates ErasureCoderOptions with default settings.
 * @param numDataUnits Number of data units.
 * @param numParityUnits Number of parity units.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,getName,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getName(),49,51,"/**
* Retrieves a value from the schema.
* @return A string value obtained from the schema.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,createBlockGrouper,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:createBlockGrouper(),86,91,"/**
 * Creates and configures a BlockGrouper object.
 * Uses m1() to initialize the BlockGrouper.
 * @return A configured BlockGrouper instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecRegistry.java,<init>,org.apache.hadoop.io.erasurecode.CodecRegistry:<init>(),62,69,"/**
 * Initializes the CodecRegistry with coders from ServiceLoader.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecRegistry.java,getCoderByName,"org.apache.hadoop.io.erasurecode.CodecRegistry:getCoderByName(java.lang.String,java.lang.String)",161,172,"/**
 * Finds a RawErasureCoderFactory by codec and coder names.
 * @param codecName Codec name to search for.
 * @param coderName Coder name to search for.
 * @return RawErasureCoderFactory or null if not found.
 */
","* Get a specific coder factory defined by codec name and coder name.
   * @param codecName name of the codec
   * @param coderName name of the coder
   * @return the specific coder, null if not exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,makeBlockGroup,"org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:makeBlockGroup(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])",72,77,"/**
 * Creates an ECBlockGroup with provided data and parity blocks.
 */","* Calculating and organizing BlockGroup, to be called by ECManager
   * @param dataBlocks Data blocks to compute parity blocks against
   * @param parityBlocks To be computed parity blocks
   * @return ECBlockGroup.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECBlockGroup.java,getErasedCount,org.apache.hadoop.io.erasurecode.ECBlockGroup:getErasedCount(),61,73,"/**
 * Counts erased blocks in data and parity block lists.
 * Returns the total count of erased blocks.
 */","* Get erased blocks count
   * @return erased count of blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getNumErasedBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlock[]),143,152,"/**
 * Counts blocks where m1() returns true.
 * @param inputBlocks Array of ECBlock objects.
 * @return Number of blocks where m1() is true.
 */","* Find out how many blocks are erased.
   * @param inputBlocks all the input blocks
   * @return number of erased blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,hasCodec,org.apache.hadoop.io.erasurecode.CodecUtil:hasCodec(java.lang.String),163,165,"/**
* Checks if a codec exists in the registry.
* @param codecName Name of the codec to check.
* @return True if codec exists, false otherwise.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECChunk.java,toBuffers,org.apache.hadoop.io.erasurecode.ECChunk:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[]),83,97,"/**
 * Converts ECChunk array to ByteBuffer array by calling m1() on each.
 * @param chunks Array of ECChunk objects.
 * @return ByteBuffer array corresponding to the input chunks.
 */","* Convert an array of this chunks to an array of ByteBuffers
   * @param chunks chunks to convert into buffers
   * @return an array of ByteBuffers",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,release,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:release(),61,66,"/**
 * Delegates m1() call to rawEncoder if it's not null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,release,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:release(),78,86,"/**
 * Calls m1() on rsRawEncoder and xorRawEncoder if they exist.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),70,72,"/**
 * Retrieves an array of ECBlocks from the provided block group.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),59,84,"/**
 * Extracts ECBlocks from a block group based on a condition.
 * @param blockGroup The block group to extract blocks from.
 * @return Array of ECBlocks that satisfy the condition.
 */","* Which blocks were erased ? For XOR it's simple we only allow and return one
   * erased block, either data or parity.
   * @param blockGroup blockGroup.
   * @return output blocks to recover",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,getInputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),66,68,"/**
* Retrieves an array of ECBlocks from the provided block group.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumDataUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumDataUnits(),175,177,"/**
 * Delegates m1() call to the coderOptions object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumDataUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumDataUnits(),152,154,"/**
* Delegates m1() call to coderOptions. Returns the result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumParityUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumParityUnits(),179,181,"/**
* Delegates m1() call to coderOptions and returns the result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumParityUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumParityUnits(),156,158,"/**
 * Delegates m1() call to coderOptions. Returns the result.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getInputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),71,82,"/**
 * Creates an ECBlock array from a block group.
 * @param blockGroup ECBlockGroup to extract blocks from.
 * @return An array containing blocks from the group.
 */","* We have all the data blocks and parity blocks as input blocks for
   * recovering by default. It's codec specific
   * @param blockGroup blockGroup.
   * @return input blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,release,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:release(),60,65,"/**
 * Calls m1() on the rsRawDecoder if it is not null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,release,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:release(),88,96,"/**
 * Calls m1() on rsRawDecoder and xorRawEncoder if they exist.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),35,37,"/**
* Constructs an XORRawDecoder with provided options.
* @param coderOptions ErasureCoderOptions object.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Constructs a DummyRawDecoder with the given options.
 * @param coderOptions ErasureCoderOptions object
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,45,"/**
 * Constructs a NativeRawDecoder with provided ErasureCoderOptions.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,preferDirectBuffer,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:preferDirectBuffer(),101,104,"/**
 * Always returns true. A placeholder function.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,preferDirectBuffer,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:preferDirectBuffer(),98,101,"/**
 * Always returns true. A placeholder function.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,add,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int,int)",150,153,"/**
 * Returns the bitwise XOR of two integers.
 * @param x The first integer.
 * @param y The second integer.
 */
","* Compute the sum of two fields
   *
   * @param x input field
   * @param y input field
   * @return result of addition",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,multiply,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int,int)",162,165,"/**
 * Retrieves a value from a precomputed multiplication table.
 * @param x Index for the first dimension.
 * @param y Index for the second dimension.
 */","* Compute the multiplication of two fields
   *
   * @param x input field
   * @param y input field
   * @return result of multiplication",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,divide,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:divide(int,int)",174,177,"/**
 * Retrieves a value from the divTable based on x and y.
 * @param x Index for the row.
 * @param y Index for the column.
 */","* Compute the division of two fields
   *
   * @param x input field
   * @param y input field
   * @return x/y",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,power,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:power(int,int)",186,200,"/**
 * Calculates a value based on input x and n using lookup tables.
 * @param x Input value (>=0, < m1())
 * @param n Input value, used in calculation
 * @return Calculated integer value.
 */","* Compute power n of a field
   *
   * @param x input field
   * @param n power
   * @return x^n",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/DumpUtil.java,dumpChunk,org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunk(org.apache.hadoop.io.erasurecode.ECChunk),93,102,"/**
 * Dumps the chunk data as a hex string to the console.
 * @param chunk The chunk object to dump.
 */
","* Print data in hex format in a chunk.
   * @param chunk chunk.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,45,"/**
 * Constructs a NativeRawEncoder with the given options.
 * @param coderOptions ErasureCoderOptions object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),35,37,"/**
 * Constructs an XORRawEncoder with the given options.
 * @param coderOptions ErasureCoderOptions object for configuration.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Constructs a DummyRawEncoder with provided options.
 * @param coderOptions ErasureCoderOptions object
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumAllUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumAllUnits(),183,185,"/**
 * Delegates to coderOptions' m1() method and returns the result.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumAllUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumAllUnits(),160,162,"/**
 * Delegates m1() call to coderOptions. Returns the result.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,allowChangeInputs,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowChangeInputs(),202,204,"/**
* Delegates m1() call to coderOptions. Returns the result.
*/","* Allow change into input buffers or not while perform encoding/decoding.
   * @return true if it's allowed to change inputs, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,allowChangeInputs,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowChangeInputs(),179,181,"/**
* Delegates m1() call to coderOptions. Returns the result.
*/","* Allow change into input buffers or not while perform encoding/decoding.
   * @return true if it's allowed to change inputs, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,allowVerboseDump,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowVerboseDump(),210,212,"/**
 * Delegates m1() call to coderOptions. Returns the result.
 */","* Allow to dump verbose info during encoding/decoding.
   * @return true if it's allowed to do verbose dump, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,allowVerboseDump,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowVerboseDump(),187,189,"/**
* Delegates m1() call to coderOptions. Returns the result.
*/","* Allow to dump verbose info during encoding/decoding.
   * @return true if it's allowed to do verbose dump, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecodeImpl,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(byte[][],int[],int,int[],byte[][],int[])",98,109,"/**
 * Processes erasure recovery using Galois Field operations.
 * @param inputs,inputOffsets,dataLen,erasedIndexes,outputs,outputOffsets
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetBuffer,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(java.nio.ByteBuffer,int)",63,69,"/**
 * Masks a portion of the buffer with a calculated value.
 * @param buffer The buffer to mask.
 * @param len Length of the portion to mask.
 * @return The modified buffer.
 */
","* Ensure a buffer filled with ZERO bytes from current readable/writable
   * position.
   * @param buffer a buffer ready to read / write certain size bytes
   * @return the buffer itself, with ZERO bytes written, the position and limit
   *         are not changed after the call",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetBuffer,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(byte[],int,int)",77,82,"/**
* Masks a portion of the buffer using m1 and System.m2.
* @param buffer The buffer to mask.
* @param offset Start offset in the buffer.
* @param len Length of the portion to mask.
*/","* Ensure the buffer (either input or output) ready to read or write with ZERO
   * bytes fully in specified length of len.
   * @param buffer bytes array buffer
   * @return the buffer itself",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,convertToByteArrayState,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:convertToByteArrayState(),62,84,"/**
* Creates a ByteArrayEncodingState object with encoded data.
* @return ByteArrayEncodingState object containing encoded data.
*/",* Convert to a ByteArrayEncodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,convertToByteBufferState,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:convertToByteBufferState(),69,85,"/**
* Creates a ByteBufferEncodingState with new ByteBuffers.
* @return ByteBufferEncodingState object with prepared buffers.
*/",* Convert to a ByteBufferEncodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,convertToByteBufferState,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:convertToByteBufferState(),73,89,"/**
 * Creates a ByteBufferDecodingState with prepared inputs and outputs.
 * @return A new ByteBufferDecodingState object.
 */
",* Convert to a ByteBufferDecodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,convertToByteArrayState,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:convertToByteArrayState(),66,91,"/**
 * Creates a ByteArrayDecodingState object with decoded data.
 * @param decoder The decoder object
 * @param decodeLength The length to decode
 * @return A ByteArrayDecodingState object
 */
",* Convert to a ByteArrayDecodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,initTables,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:initTables(int,int,byte[],int,byte[])",47,58,"/**
 * Applies a coding matrix to data using GF256 tables.
 * @param k Coding matrix dimension.
 * @param rows Number of rows.
 * @param codingMatrix Coding matrix.
 * @param matrixOffset Offset into coding matrix.
 * @param gfTables GF256 lookup tables.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,genCauchyMatrix,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:genCauchyMatrix(byte[],int,int)",67,80,"/**
 * Fills a byte array with specific values based on m and k.
 */","* Ported from Intel ISA-L library.
   *
   * @param k k.
   * @param a a.
   * @param m m.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GF256.java,gfInvertMatrix,"org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfInvertMatrix(byte[],byte[],int)",203,262,"/**
* Applies a transformation to the input matrix.
* @param inMatrix Input matrix to be transformed.
* @param outMatrix Output matrix to store the result.
* @param n Size of the square matrix.
*/","* Invert a matrix assuming it's invertible.
   *
   * Ported from Intel ISA-L library.
   *
   * @param inMatrix inMatrix.
   * @param outMatrix outMatrix.
   * @param n n",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,encodeData,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],int,byte[][],int[],byte[][],int[])",97,143,"/**
 * Masks data using GF tables and input/output offsets.
 */","* Encode a group of inputs data and generate the outputs. It's also used for
   * decoding because, in this implementation, encoding and decoding are
   * unified.
   *
   * The algorithm is ported from Intel ISA-L library for compatible. It
   * leverages Java auto-vectorization support for performance.
   *
   * @param gfTables gfTables.
   * @param dataLen dataLen.
   * @param inputs inputs.
   * @param inputOffsets inputOffsets.
   * @param outputs outputs.
   * @param outputOffsets outputOffsets.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,encodeData,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],java.nio.ByteBuffer[],java.nio.ByteBuffer[])",152,200,"/**
* Processes input data using GF tables and writes to outputs.
* @param gfTables GF tables for processing.
* @param inputs Input ByteBuffers.
* @param outputs Output ByteBuffers.
*/","* See above. Try to use the byte[] version when possible.
   *
   * @param gfTables gfTables.
   * @param inputs inputs.
   * @param outputs outputs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,getInstance,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance(int,int)",102,115,"/**
 * Retrieves or creates a GaloisField instance based on size & polynomial.
 * @param fieldSize The size of the Galois field.
 * @param primitivePolynomial The primitive polynomial.
 * @return GaloisField instance.
 */","* Get the object performs Galois field arithmetics.
   *
   * @param fieldSize           size of the field
   * @param primitivePolynomial a primitive polynomial corresponds to the size
   * @return GaloisField.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,solveVandermondeSystem,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],int[])",210,212,"/**
* Calls overloaded method with array length as the size.
*/","* Given a Vandermonde matrix V[i][j]=x[j]^i and vector y, solve for z such
   * that Vz=y. The output z will be placed in y.
   *
   * @param x the vector which describe the Vandermonde matrix
   * @param y right-hand side of the Vandermonde system equation. will be
   *          replaced the output in this vector",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECBlock.java,<init>,org.apache.hadoop.io.erasurecode.ECBlock:<init>(),37,39,"/**
 * Default constructor for ECBlock, initializes with default flags.
 */",* A default constructor. isParity and isErased are false by default.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,compare,"org.apache.hadoop.io.LongWritable$DecreasingComparator:compare(byte[],int,int,byte[],int,int)",110,113,"/**
 * Delegates the call to the superclass's m1 method.
 * Passes arguments in reversed order.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,close,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:close(),444,453,"/**
* Calls input.m1(), then super.m1() if needsReset is false.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,updatePos,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:updatePos(boolean),560,564,"/**
* Calculates the compressed stream position based on a flag.
* @param shouldAddOn Flag to add an offset to the position.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,updateReportedByteCount,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:updateReportedByteCount(int),184,187,"/**
* Adds to reportedBytesReadFromCompressedStream and calls m1.
* @param count The number of bytes read.
*/","* This method is called by the client of this
   * class in case there are any corrections in
   * the stream position.  One common example is
   * when client of this code removes starting BZ
   * characters from the compressed stream.
   *
   * @param count count bytes are added to the reported bytes
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,readAByte,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:readAByte(java.io.InputStream),196,202,"/**
 * Reads a byte from the input stream.
 * Increments internal counter if a byte is read successfully.
 */","* This method reads a Byte from the compressed stream. Whenever we need to
  * read from the underlying compressed stream, this method should be called
  * instead of directly calling the read method of the underlying compressed
  * stream. This method does important record keeping to have the statistic
  * that how many bytes have been read off the compressed stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CRC.java,<init>,org.apache.hadoop.io.compress.bzip2.CRC:<init>(),86,88,"/**
 * Initializes the CRC calculation state.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,endBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:endBlock(),572,589,"/**
* Updates combined CRC based on computed and stored block CRC values.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,createHuffmanDecodingTables,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:createHuffmanDecodingTables(int,int)",793,819,"/**
 * Processes groups, calculates min/max lengths, and calls m1.
 * @param alphaSize Alphabet size.
 * @param nGroups Number of groups to process.
 */",* Called by recvDecodingTables() exclusively.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,initBlock,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:initBlock(),774,786,"/**
* Resets internal state, clears inUse flags, and calculates block size.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,bsPutUByte,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutUByte(int),940,942,"/**
 * Calls m1 with a fixed value (8) and the provided integer c.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,bsPutInt,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutInt(int),944,949,"/**
 * Sends the integer 'u' as four bytes via the m1 method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues4,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues4(),1199,1241,"/**
* Processes data flags and writes shadow bits to the output stream.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues5,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues5(int,int)",1243,1278,"/**
* Writes selector MTF data to the output stream.
* @param nGroups Number of groups.
* @param nSelectors Number of selectors.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues1,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues1(int,int)",1029,1145,"/**
 * Calculates and masks selector values based on group costs.
 * @param nGroups Number of groups.
 * @param alphaSize Size of the alphabet.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues3,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues3(int,int)",1174,1197,"/**
* Processes MTF values for specified groups, finding min/max lengths.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,mainQSort3,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainQSort3(org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int,int)",1634,1736,"/**
* Recursively partitions data using a modified quicksort approach.
* @param dataShadow Data object containing arrays for processing.
*/","* Method ""mainQSort3"", file ""blocksort.c"", BZip2 1.0.2",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:<init>(),66,68,"/**
 * Default constructor. Uses default flags and buffer size.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,setInput,"org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setInput(byte[],int,int)",70,88,"/**
* Copies byte array data and processes it.
* @param b byte array, off offset, len length.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,needsInput,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsInput(),112,130,"/**
* Checks buffer conditions; returns true if safe, otherwise false.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getBytesWritten,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesWritten(),182,185,"/**
 * Calls m1() and then returns the result of m2(stream).
 */","* Returns the total number of uncompressed bytes output so far.
   *
   * @return the total (non-negative) number of uncompressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getBytesRead,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesRead(),192,195,"/**
 * Calls m1() and returns the result of m2(stream).
 */","* Returns the total number of compressed bytes input so far.
   *
   * @return the total (non-negative) number of compressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getRemaining,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getRemaining(),204,208,"/**
* Calls m1(), then returns userBufLen + result of recursive m2(stream).
*/","* Returns the number of bytes remaining in the input buffers; normally
   * called when finished() is true to determine amount of post-gzip-stream
   * data.
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,reset,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:reset(),213,223,"/**
 * Initializes internal buffers and prepares for processing.
 */",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(),64,66,"/**
 * Default constructor. Uses default block size, work factor, and buffer size.
 */
","* Creates a new compressor with a default values for the
   * compression block size and work factor.  Compressed data will be
   * generated in bzip2 format.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,setInput,"org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setInput(byte[],int,int)",124,142,"/**
* Copies byte array to internal buffer and processes it.
* @param b byte array to copy
* @param off start offset in the array
* @param len number of bytes to copy
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,needsInput,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:needsInput(),158,181,"/**
 * Checks buffer conditions; returns true if compression is possible.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,getBytesWritten,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesWritten(),244,248,"/**
* Calls m1() and then calls m2(stream), returning the result.
*/","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,getBytesRead,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesRead(),255,259,"/**
 * Calls m1() and then returns the result of m2(stream).
 */","* Returns the total number of uncompressed bytes input so far.
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,reset,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reset(),261,274,"/**
 * Resets the internal state for processing a new data block.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:<init>(java.io.OutputStream),251,255,"/**
 * Creates a BZip2CompressionOutputStream writing to the given OutputStream.
 * @param out The OutputStream to write compressed data to.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,"org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",36,47,"/**
 * Initializes a CompressorStream with an OutputStream, compressor, and buffer size.
 * @param out Output stream for compressed data.
 * @param compressor Compressor implementation.
 * @param bufferSize Size of the internal buffer.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream),58,60,"/**
 * Initializes a new CompressorStream with the given output stream.
 */","* Allow derived classes to directly set the underlying stream.
   * 
   * @param out Underlying output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,writeStreamHeader,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:writeStreamHeader(),257,261,"/**
* Writes to the output stream if it exists.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,write,"org.apache.hadoop.io.compress.CompressorStream:write(byte[],int,int)",62,78,"/**
* Writes data to the compressor, handling bounds and compression.
* @param b the data buffer
* @param off the offset within the buffer
* @param len the number of bytes to write
* @throws IOException if write beyond stream end
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,finish,org.apache.hadoop.io.compress.CompressorStream:finish(),87,95,"/**
* Compresses data; calls m3 until compression succeeds.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,<init>,org.apache.hadoop.io.compress.snappy.SnappyDecompressor:<init>(),65,67,"/**
 * Default constructor, uses the default direct buffer size.
 */
",* Creates a new decompressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,setInput,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setInput(byte[],int,int)",83,101,"/**
* Copies byte array data and initializes internal buffers.
* @param b byte array to copy
* @param off start offset in the array
* @param len number of bytes to copy
*/
","* Sets input data for decompression.
   * This should be called if and only if {@link #needsInput()} returns
   * <code>true</code> indicating that more input data is required.
   * (Both native and non-native versions of various Decompressors require
   * that the data passed in via <code>b[]</code> remain unmodified until
   * the caller is explicitly notified--via {@link #needsInput()}--that the
   * buffer may be safely modified.  With this requirement, an extra
   * buffer-copy can be avoided.)
   *
   * @param b   Input data
   * @param off Start offset
   * @param len Length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,needsInput,org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsInput(),138,156,"/**
* Checks buffer conditions; returns true if safe, false otherwise.
*/","* Returns true if the input data buffer is empty and
   * {@link #setInput(byte[], int, int)} should be called to
   * provide more input.
   *
   * @return <code>true</code> if the input data buffer is empty and
   *         {@link #setInput(byte[], int, int)} should be called in
   *         order to provide more input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,finished,org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:finished(),310,313,"/**
* Returns true if endOfInput is true and super.m1() returns true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,decompress,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompress(byte[],int,int)",192,230,"/**
* Reads data into the provided byte array.
* @param b buffer to write to, off offset, len length
* @return number of bytes read
*/
","* Fills specified buffer with uncompressed data. Returns actual number
   * of bytes of uncompressed data. A return value of 0 indicates that
   * {@link #needsInput()} should be called in order to determine if more
   * input data is required.
   *
   * @param b   Buffer for the uncompressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,decompressDirect,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompressDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)",275,305,"/**
* Decompresses data from src to dst, updating buffers.
* @param src Input buffer containing compressed data.
* @param dst Output buffer for decompressed data.
* @return Number of decompressed bytes.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,reset,org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:reset(),315,319,"/**
* Calls super.m1() and sets endOfInput to true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,<init>,org.apache.hadoop.io.compress.snappy.SnappyCompressor:<init>(),67,69,"/**
 * Default constructor, uses the default direct buffer size.
 */
",* Creates a new compressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,compress,"org.apache.hadoop.io.compress.snappy.SnappyCompressor:compress(byte[],int,int)",177,225,"/**
 * Reads data into the provided byte array.
 * @param b buffer to write to, off offset, len length to read
 * @return number of bytes written
 */","* Fills specified buffer with compressed data. Returns actual number
   * of bytes of compressed data. A return value of 0 indicates that
   * needsInput() should be called in order to determine if more input
   * data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,reinit,org.apache.hadoop.io.compress.snappy.SnappyCompressor:reinit(org.apache.hadoop.conf.Configuration),248,251,"/**
 * Executes the m1 method.
 * @param conf Configuration object (unused).
 */","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration
   *
   * @param conf Configuration from which new setting are fetched",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int,int)",51,66,"/**
 * Constructs a DecompressorStream with the given input, decompressor, buffer sizes.
 * @param in Input stream. @param decompressor Decompression algorithm.
 * @param bufferSize Buffer size. @param skipBufferSize Skip buffer size.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream),85,87,"/**
 * Initializes a DecompressorStream with the given input stream.
 * @param in the input stream to decompress
 * @throws IOException if an I/O error occurs
 */
","* Allow derived classes to directly set the underlying stream.
   * 
   * @param in Underlying input stream.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SplitCompressionInputStream.java,<init>,"org.apache.hadoop.io.compress.SplitCompressionInputStream:<init>(java.io.InputStream,long,long)",39,44,"/**
 * Constructs a SplitCompressionInputStream.
 * @param in Input stream to compress.
 * @param start Start offset for compression.
 * @param end End offset for compression.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,getCompressedData,org.apache.hadoop.io.compress.DecompressorStream:getCompressedData(),175,180,"/**
* Reads data into the buffer.
* Calls m1(), then reads from input stream 'in'.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,available,org.apache.hadoop.io.compress.DecompressorStream:available(),215,219,"/**
* Returns 0 if end-of-file, otherwise 1. Calls m1() first.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,resetState,org.apache.hadoop.io.compress.BlockDecompressorStream:resetState(),137,142,"/**
 * Initializes variables and calls the parent class's m1 method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,compress,org.apache.hadoop.io.compress.BlockCompressorStream:compress(),147,155,"/**
* Compresses buffer data and writes to output.
* Uses compressor to compress, then writes len bytes.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,setInput,"org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setInput(byte[],int,int)",87,104,"/**
* Processes a byte array, validates input, and calls internal methods.
* @param b byte array to process
* @param off offset within the array
* @param len number of bytes to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,needsInput,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsInput(),134,151,"/**
 * Checks buffer conditions; returns true if ready, otherwise false.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,finished,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:finished(),302,305,"/**
* Returns true if endOfInput is true and super.m1() returns true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,getRemaining,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:getRemaining(),216,221,"/**
* Calculates the total bytes to consume.
* Returns the sum of userBufferBytesToConsume and remaining.
*/","* <p>Returns the number of bytes remaining in the input buffers;
   * normally called when finished() is true to determine amount of post-stream
   * data.</p>
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:reset(),226,238,"/**
 * Resets internal state for decompression processing.
 */",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,decompress,"org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:decompress(byte[],int,int)",166,207,"/**
* Reads data from a byte array, handling compression and validation.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,setInput,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setInput(byte[],int,int)",122,139,"/**
* Copies byte array data into internal buffers for processing.
* @param b The byte array to copy.
* @param off Start offset in the byte array.
* @param len Number of bytes to copy.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,needsInput,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:needsInput(),156,181,"/**
* Checks conditions to determine if decompression should proceed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,compress,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:compress(byte[],int,int)",195,243,"/**
 * Reads data into byte array, handling compression and bounds.
 * @param b byte array to fill, off offset, len length to read
 * @return number of bytes read
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesWritten(),250,254,"/**
* Writes data and returns the number of bytes written.
*/","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,getBytesRead,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesRead(),261,265,"/**
* Reads data and returns the number of bytes read.
*/","* <p>Returns the total number of uncompressed bytes input so far.</p>
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reset(),267,283,"/**
 * Resets the state of the processing context to its initial state.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,getCompressedData,org.apache.hadoop.io.compress.BlockDecompressorStream:getCompressedData(),114,135,"/**
 * Reads data into the buffer, resizing if necessary.
 * @return The number of bytes read.
 * @throws IOException if an I/O error occurs.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Decompressor:<init>(),77,79,"/**
 * Creates a new Lz4Decompressor with the default buffer size.
 */
",* Creates a new decompressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,setInput,"org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setInput(byte[],int,int)",95,113,"/**
* Copies byte array into internal buffer and processes it.
* @param b byte array to copy
* @param off starting offset in the array
* @param len number of bytes to copy
*/","* Sets input data for decompression.
   * This should be called if and only if {@link #needsInput()} returns
   * <code>true</code> indicating that more input data is required.
   * (Both native and non-native versions of various Decompressors require
   * that the data passed in via <code>b[]</code> remain unmodified until
   * the caller is explicitly notified--via {@link #needsInput()}--that the
   * buffer may be safely modified.  With this requirement, an extra
   * buffer-copy can be avoided.)
   *
   * @param b   Input data
   * @param off Start offset
   * @param len Length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,needsInput,org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsInput(),150,168,"/**
* Checks buffer conditions; returns true if safe, false otherwise.
*/","* Returns true if the input data buffer is empty and
   * {@link #setInput(byte[], int, int)} should be called to
   * provide more input.
   *
   * @return <code>true</code> if the input data buffer is empty and
   *         {@link #setInput(byte[], int, int)} should be called in
   *         order to provide more input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,decompress,"org.apache.hadoop.io.compress.lz4.Lz4Decompressor:decompress(byte[],int,int)",204,242,"/**
 * Reads data from the buffer into the provided byte array.
 * @param b buffer to write to, off offset, len length to read
 * @return number of bytes read
 */
","* Fills specified buffer with uncompressed data. Returns actual number
   * of bytes of uncompressed data. A return value of 0 indicates that
   * {@link #needsInput()} should be called in order to determine if more
   * input data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of uncompressed data.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(int),95,97,"/**
 * Creates a Lz4Compressor with a specified direct buffer size.
 * @param directBufferSize Size of the direct buffer to use.
 */
","* Creates a new compressor.
   *
   * @param directBufferSize size of the direct buffer to be used.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,compress,"org.apache.hadoop.io.compress.lz4.Lz4Compressor:compress(byte[],int,int)",212,260,"/**
 * Reads data into the provided byte array. Returns the number of bytes read.
 */","* Fills specified buffer with compressed data. Returns actual number
   * of bytes of compressed data. A return value of 0 indicates that
   * needsInput() should be called in order to determine if more input
   * data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,reinit,org.apache.hadoop.io.compress.lz4.Lz4Compressor:reinit(org.apache.hadoop.conf.Configuration),283,286,"/**
* Executes m1() within a synchronized block.
* @param conf Configuration object (unused)
*/
","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration
   *
   * @param conf Configuration from which new setting are fetched",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecByName,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecByName(java.lang.String),246,257,"/**
* Retrieves a CompressionCodec by name, trying multiple registries.
* @param codecName Name of the codec to retrieve.
* @return CompressionCodec object or null if not found.
*/
","* Find the relevant compression codec for the codec's canonical class name
   * or by codec alias.
   * <p>
   * Codec aliases are case insensitive.
   * <p>
   * The code alias is the short class name (without the package name).
   * If the short class name ends with 'Codec', then there are two aliases for
   * the codec, the complete short class name and the short class name without
   * the 'Codec' ending. For example for the 'GzipCodec' codec class name the
   * alias are 'gzip' and 'gzipcodec'.
   *
   * @param codecName the canonical class name of the codec
   * @return the codec object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,payback,"org.apache.hadoop.io.compress.CodecPool:payback(java.util.Map,java.lang.Object)",105,122,"/**
 * Adds/checks if a codec exists in the pool for a class.
 * @param pool Pool of codecs, keyed by class.
 * @param codec Codec to add/check. Returns true if present.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,updateLeaseCount,"org.apache.hadoop.io.compress.CodecPool:updateLeaseCount(org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache,java.lang.Object,int)",131,137,"/**
 * Updates usage count for a codec class.
 * @param usageCounts Cache of class usage counts.
 * @param codec The codec object.
 * @param delta Increment value for the count.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getLeasedCompressorsCount,org.apache.hadoop.io.compress.CodecPool:getLeasedCompressorsCount(org.apache.hadoop.io.compress.CompressionCodec),245,248,"/**
 * Returns a masked value based on the provided codec.
 * @param codec CompressionCodec object; null returns 0.
 */","* Return the number of leased {@link Compressor}s for this
   * {@link CompressionCodec}.
   *
   * @param codec codec.
   * @return the number of leased.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getLeasedDecompressorsCount,org.apache.hadoop.io.compress.CodecPool:getLeasedDecompressorsCount(org.apache.hadoop.io.compress.CompressionCodec),257,260,"/**
 * Calculates a value based on the codec, returning 0 if null.
 * @param codec CompressionCodec object to use for calculation
 * @return Calculated integer value.
 */","* Return the number of leased {@link Decompressor}s for this
   * {@link CompressionCodec}.
   *
   * @param codec codec.
   * @return the number of leased",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,checkNativeCodeLoaded,org.apache.hadoop.io.compress.ZStandardCodec:checkNativeCodeLoaded(),62,77,"/**
 * Checks if native zStandard libraries are available.
 * Throws RuntimeException if any library fails to load.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,isNativeCodeLoaded,org.apache.hadoop.io.compress.ZStandardCodec:isNativeCodeLoaded(),79,82,"/**
* Checks if both ZStandardCompressor and Decompressor m1() return true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,flush,org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:flush(),66,72,"/**
 * Delegates calls to m1, m2, and m3 of the wrapped stream.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,getCompressorType,org.apache.hadoop.io.compress.GzipCodec:getCompressorType(),69,74,"/**
* Returns compressor class based on configuration.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,getDecompressorType,org.apache.hadoop.io.compress.GzipCodec:getDecompressorType(),102,107,"/**
 * Returns the Decompressor class based on configuration.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibCompressorType,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressorType(org.apache.hadoop.conf.Configuration),97,101,"/**
* Returns compressor class based on configuration.
* @param conf Hadoop configuration object
* @return Compressor class (ZlibCompressor or BuiltInZlibDeflater)
*/
","* Return the appropriate type of the zlib compressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the zlib compressor.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDecompressorType,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressorType(org.apache.hadoop.conf.Configuration),121,125,"/**
* Selects Decompressor class based on Configuration.
* @param conf Configuration object to determine Decompressor.
* @return Class of the selected Decompressor.
*/
","* Return the appropriate type of the zlib decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the zlib decompressor.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,loadNativeZLib,org.apache.hadoop.io.compress.zlib.ZlibFactory:loadNativeZLib(),52,64,"/**
 * Checks and initializes the native-zlib library.
 * Logs success or failure based on initialization.
 */","* Load native library and set the flag whether to use native library. The
   * method is also used for reset the flag modified by setNativeZlibLoaded",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,init,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:init(org.apache.hadoop.conf.Configuration),156,165,"/**
 * Initializes the Deflater with compression level and strategy.
 * @param conf Configuration object for compression settings.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInZlibDeflater.java,reinit,org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:reinit(org.apache.hadoop.conf.Configuration),65,83,"/**
* Initializes the compressor with configuration, handling unsupported strategies.
* @param conf Configuration object for compressor setup.
*/","* reinit the compressor with the given configuration. It will reset the
   * compressor's compression level and compression strategy. Different from
   * <tt>ZlibCompressor</tt>, <tt>BuiltInZlibDeflater</tt> only support three
   * kind of compression strategy: FILTERED, HUFFMAN_ONLY and DEFAULT_STRATEGY.
   * It will use DEFAULT_STRATEGY as default if the configured compression
   * strategy is not supported.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader,int)",261,274,"/**
 * Initializes the ZlibCompressor with provided compression settings.
 */","* Creates a new compressor using the specified compression level.
   * Compressed data will be generated in ZLIB format.
   * 
   * @param level Compression level #CompressionLevel
   * @param strategy Compression strategy #CompressionStrategy
   * @param header Compression header #CompressionHeader
   * @param directBufferSize Size of the direct buffer to be used.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,setInput,"org.apache.hadoop.io.compress.zlib.ZlibCompressor:setInput(byte[],int,int)",300,318,"/**
* Copies byte array data into internal buffers for processing.
* @param b The byte array to copy.
* @param off Start offset in the byte array.
* @param len Number of bytes to copy.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,needsInput,org.apache.hadoop.io.compress.zlib.ZlibCompressor:needsInput(),340,367,"/**
* Checks conditions related to compressed/uncompressed buffers.
* Returns true if conditions allow processing, false otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesWritten(),432,436,"/**
* Calls m1() and returns the result of m2(stream).
*/","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,getBytesRead,org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesRead(),443,447,"/**
* Calls m1() and returns the result of m2(stream).
*/","* Returns the total number of uncompressed bytes input so far.
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibCompressor:reset(),449,461,"/**
 * Resets the state for a new compression cycle.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,compress,"org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:compress(byte[],int,int)",81,132,"/**
 * Compresses data using the internal compressor.
 * @param b input byte array
 * @param off start offset in the array
 * @param len number of bytes to compress
 * @return number of compressed bytes written
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,finished,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:finished(),360,363,"/**
* Returns true if endOfInput is true and super.m1() returns true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)",107,115,"/**
 * Initializes the ZlibDecompressor with header and buffer size.
 * @param header Compression header data.
 * @param directBufferSize Size of direct buffers.
 */
","* Creates a new decompressor.
   * @param header header.
   * @param directBufferSize directBufferSize.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,setInput,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setInput(byte[],int,int)",121,139,"/**
 * Copies byte array data and initializes internal buffers.
 * @param b byte array to copy
 * @param off starting offset in the array
 * @param len number of bytes to copy
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,needsInput,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:needsInput(),170,188,"/**
* Checks buffer conditions; returns true if all are met, false otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesWritten(),242,245,"/**
 * Calls m1() and returns the result of m2(stream).
 */","* Returns the total number of uncompressed bytes output so far.
   *
   * @return the total (non-negative) number of uncompressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getBytesRead,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesRead(),252,255,"/**
* Calls m1() and returns the result of m2(stream).
*/","* Returns the total number of compressed bytes input so far.
   *
   * @return the total (non-negative) number of compressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getRemaining,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getRemaining(),264,268,"/**
* Calculates a value using m1 and recursive call.
* Uses userBufLen and stream for the calculation.
*/","* Returns the number of bytes remaining in the input buffers; normally
   * called when finished() is true to determine amount of post-gzip-stream
   * data.
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:reset(),273,283,"/**
* Resets the state for processing a new input stream.
*/",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,finalize,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:finalize(),293,296,"/**
* Calls the m1 method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,executeTrailerState,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeTrailerState(),367,416,"/**
 * Processes gzip trailer data based on the current state.
 * Handles CRC and size verification, advancing the state.
 */","* Parse the gzip trailer (assuming we're in the appropriate state).
   * In order to deal with degenerate cases (e.g., user buffer is one byte
   * long), we copy trailer bytes (all 8 of 'em) to a local buffer.</p>
   *
   * See http://www.ietf.org/rfc/rfc1952.txt for the gzip spec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,processBasicHeader,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:processBasicHeader(),509,524,"/**
* Validates gzip file header, checking for magic ID, compression method, and flags.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readCompressedString,org.apache.hadoop.io.WritableUtils:readCompressedString(java.io.DataInput),87,91,"/**
 * Reads bytes from input and returns a String using UTF-8.
 * @param in DataInput to read from; null input returns null.
 * @return String representation of the read bytes, or null.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeVInt,"org.apache.hadoop.io.WritableUtils:writeVInt(java.io.DataOutput,int)",257,259,"/**
* Writes data to the output stream using m1.
* @param stream DataOutput stream to write to
* @param i Integer value used by m1
*/
","* Serializes an integer to a binary stream with zero-compressed encoding.
   * For -112 {@literal <=} i {@literal <=} 127, only one byte is used with the
   * actual value.
   * For other values of i, the first byte value indicates whether the
   * integer is positive or negative, and the number of bytes that follow.
   * If the first byte value v is between -113 and -116, the following integer
   * is positive, with number of bytes that follow are -(v+112).
   * If the first byte value v is between -121 and -124, the following integer
   * is negative, with number of bytes that follow are -(v+120). Bytes are
   * stored in the high-non-zero-byte-first order.
   *
   * @param stream Binary output stream
   * @param i Integer to be serialized
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,write,org.apache.hadoop.io.VLongWritable:write(java.io.DataOutput),54,57,"/**
* Writes the value to the output stream using WritableUtils.
* @param out DataOutput stream to write to.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVLong,org.apache.hadoop.io.WritableUtils:readVLong(java.io.DataInput),313,326,"/**
* Reads data from stream, processes it, and returns a long value.
*/","* Reads a zero-compressed encoded long from input stream and returns it.
   * @param stream Binary input stream
   * @throws IOException raised on errors performing I/O.
   * @return deserialized long from stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WeakReferencedElasticByteBufferPool.java,getBuffer,"org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getBuffer(boolean,int)",82,107,"/**
* Retrieves a ByteBuffer from a pool or allocates a new one.
* @param direct if true, allocates a direct buffer; otherwise, not.
* @param length the desired capacity of the buffer.
*/","* {@inheritDoc}
   *
   * @param direct whether we want a direct byte buffer or a heap one.
   * @param length length of requested buffer.
   * @return returns equal or next greater than capacity buffer from
   * pool if already available and not garbage collected else creates
   * a new buffer and return it.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WeakReferencedElasticByteBufferPool.java,putBuffer,org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:putBuffer(java.nio.ByteBuffer),113,130,"/**
 * Adds a buffer to the TreeMap, avoiding duplicates.
 * @param buffer ByteBuffer to be added.
 */","* Return buffer to the pool.
   * @param buffer buffer to be returned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,charAt,org.apache.hadoop.io.Text:charAt(int),163,169,"/**
 * Retrieves a value from the byte array at the given position.
 * @param position index of the value to retrieve
 * @return the value at the position, or -1 if out of bounds.
 */","* Returns the Unicode Scalar Value (32-bit integer value)
   * for the character at <code>position</code>. Note that this
   * method avoids using the converter or doing String instantiation.
   *
   * @param position input position.
   * @return the Unicode scalar value at position or -1
   *          if the position is invalid or points to a
   *          trailing byte",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(java.lang.String),228,237,"/**
 * Processes a string, extracting data using helper methods.
 * @param string The input string to process.
 */","* Set to contain the contents of a string.
   *
   * @param string input string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,encode,org.apache.hadoop.io.Text:encode(java.lang.String),514,517,"/**
* Converts a string to a ByteBuffer using default encoding.
* @param string The string to convert.
* @return ByteBuffer containing the encoded string.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,buildCacheKey,org.apache.hadoop.security.token.Token:buildCacheKey(),449,452,"/**
* Generates a masked string using kind, identifier, and password.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,"org.apache.hadoop.io.Text:set(byte[],int,int)",272,277,"/**
 * Processes a byte array segment.
 * @param utf8 byte array, start start index, len length
 */","* Set the Text to range of bytes.
   *
   * @param utf8 the data to copy from
   * @param start the first position of the new string
   * @param len the number of bytes of the new string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,append,"org.apache.hadoop.io.Text:append(byte[],int,int)",286,294,"/**
 * Copies data to the buffer, updates length, and resets textLength.
 */","* Append a range of bytes to the end of the given text.
   *
   * @param utf8 the data to copy from
   * @param start the first position to append from utf8
   * @param len the number of bytes to append",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readWithKnownLength,"org.apache.hadoop.io.Text:readWithKnownLength(java.io.DataInput,int)",383,388,"/**
* Reads 'len' bytes from input 'in' and sets internal length.
*/","* Read a Text object whose length is already known.
   * This allows creating Text from a stream which uses a different serialization
   * format.
   *
   * @param in input in.
   * @param len input len.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,org.apache.hadoop.io.Text:decode(byte[]),457,459,"/**
* Decodes a byte array as UTF-8 and returns the resulting string.
*/","* @return Converts the provided byte array to a String using the
   * UTF-8 encoding. If the input is malformed,
   * replace by a default value.
   *
   * @param utf8 input utf8.
   * @throws CharacterCodingException when a character
   *                                  encoding or decoding error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,"org.apache.hadoop.io.Text:decode(byte[],int,int)",461,464,"/**
 * Converts a byte array segment to a string using UTF-8.
 * @param utf8 byte array
 * @param start start index
 * @param length length of the segment
 * @throws CharacterCodingException if decoding fails
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,"org.apache.hadoop.io.Text:decode(byte[],int,int,boolean)",480,483,"/**
 * Decodes byte array to String.
 * @param utf8 byte array to decode
 * @param start start index in the byte array
 * @param length length of the byte array to decode
 * @param replace whether to replace invalid chars
 * @return decoded String
 */","* @return Converts the provided byte array to a String using the
   * UTF-8 encoding. If <code>replace</code> is true, then
   * malformed input is replaced with the
   * substitution character, which is U+FFFD. Otherwise the
   * method throws a MalformedInputException.
   *
   * @param utf8 input utf8.
   * @param start input start.
   * @param length input length.
   * @param replace input replace.
   * @throws CharacterCodingException when a character
   *                                  encoding or decoding error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,validateUTF8,org.apache.hadoop.io.Text:validateUTF8(byte[]),626,628,"/**
 * Calls m1 with the entire byte array.
 * @param utf8 The byte array to process.
 * @throws MalformedInputException if input is invalid.
 */
","* Check if a byte array contains valid UTF-8.
   *
   * @param utf8 byte array
   * @throws MalformedInputException if the byte array contains invalid UTF-8",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,addToMap,org.apache.hadoop.io.AbstractMapWritable:addToMap(java.lang.Class),91,101,"/**
 * Registers a class, assigns an ID, and calls m2 with the ID.
 * Throws exception if max class limit is exceeded.
 */","* Add a Class to the maps if it is not already present.
   * @param clazz clazz.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,<init>,org.apache.hadoop.io.AbstractMapWritable:<init>(),145,166,"/**
 * Initializes the AbstractMapWritable with default Writable types.
 */
",constructor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,readFields,org.apache.hadoop.io.AbstractMapWritable:readFields(java.io.DataInput),194,216,"/**
 * Reads class data from input, loads them, and registers.
 * @param in Input stream containing class data.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,write,org.apache.hadoop.io.AbstractMapWritable:write(java.io.DataOutput),180,192,"/**
 * Writes class data to the output stream.
 * @param out Output stream to write data to.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Metadata:<init>(),735,737,"/**
 * Default constructor, initializes with an empty sorted map.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,<init>,org.apache.hadoop.io.VLongWritable:<init>(long),38,38,"/**
* Constructs a VLongWritable with the given long value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,getSerializer,org.apache.hadoop.io.serializer.SerializationFactory:getSerializer(java.lang.Class),81,87,"/**
 * Delegates to m1 to obtain a Serializer, returns it or null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,getDeserializer,org.apache.hadoop.io.serializer.SerializationFactory:getDeserializer(java.lang.Class),89,95,"/**
 * Creates a Deserializer for type T.
 * @param c The class of the object to deserialize.
 * @return A Deserializer or null if m1 fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerializationComparator.java,compare,"org.apache.hadoop.io.serializer.JavaSerializationComparator:compare(java.lang.Object,java.lang.Object)",47,51,"/**
* Calls method m1 on o1 with o2 as argument.
* @param o1 The object on which m1 is called.
* @param o2 The argument passed to m1.
* @return The result of calling m1.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerialization.java,deserialize,org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:deserialize(java.lang.Object),54,63,"/**
 * Reads an object from the input stream.
 * @param object Unused parameter, required for signature.
 * @return Object read from the stream, or throws IOException.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,filesystem,org.apache.hadoop.io.SequenceFile$Writer:filesystem(org.apache.hadoop.fs.FileSystem),1002,1005,"/**
* Creates a FileSystemOption for SequenceFile writers.
* @param fs the FileSystem to use
* @return A FileSystemOption object
*/
","* @deprecated only used for backwards-compatibility in the createWriter methods
     * that take FileSystem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,<init>,org.apache.hadoop.io.DataInputByteBuffer:<init>(),74,76,"/**
* Constructs a DataInputByteBuffer using a new Buffer.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getData,org.apache.hadoop.io.DataInputByteBuffer:getData(),87,89,"/**
* Delegates to the underlying buffers' m1() method.
* @return An array of ByteBuffers.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getPosition,org.apache.hadoop.io.DataInputByteBuffer:getPosition(),91,93,"/**
* Delegates to the internal 'buffers' object's m1() method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getLength,org.apache.hadoop.io.DataInputByteBuffer:getLength(),95,97,"/**
* Delegates to the internal 'buffers' object's m1() method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,<init>,"org.apache.hadoop.util.bloom.Key:<init>(byte[],double)",98,100,"/**
 * Constructs a Key object with the given byte array and weight.
 * @param value The byte array representing the key.
 * @param weight The weight associated with the key.
 */
","* Constructor.
   * <p>
   * Builds a key with a specified weight.
   * @param value The value of <i>this</i> key.
   * @param weight The weight associated to <i>this</i> key.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cleanup,org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:cleanup(),3916,3921,"/**
* Executes m3, calls super.m1(), and parentContainer.m3.
*/","The default cleanup. Subclasses can override this with a custom 
       * cleanup",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,grow,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow(),3193,3200,"/**
* Resizes arrays: keyOffsets, pointers, keyLengths, rawValues.
* @param newLength the new size of the arrays
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:<init>(org.apache.hadoop.io.SequenceFile$CompressionType),977,979,"/**
 * Creates a CompressionOption with the given compression type.
 * @param value The compression type for the option.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compression,"org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",1056,1059,"/**
 * Creates a CompressionOption with the given type and codec.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericsUtil.java,toArray,org.apache.hadoop.util.GenericsUtil:toArray(java.util.List),86,88,"/**
* Converts a list to an array using helper methods m1 and m2.
* @param list List of elements to convert.
* @return Array containing elements from the list.
*/
","* Converts the given <code>List&lt;T&gt;</code> to a an array of 
   * <code>T[]</code>. 
   * @param list the list to convert
   * @param <T> Generics Type T.
   * @throws ArrayIndexOutOfBoundsException if the list is empty. 
   * Use {@link #toArray(Class, List)} if the list may be empty.
   * @return T Array.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,<init>,org.apache.hadoop.io.InputBuffer:<init>(),69,71,"/**
 * Constructs a new InputBuffer using a default Buffer.
 */",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,reset,"org.apache.hadoop.io.InputBuffer:reset(byte[],int)",83,85,"/**
* Delegates processing of the input array to the buffer.
* @param input The byte array to process.
* @param length The number of bytes to process.
*/
","* Resets the data that the buffer reads.
   * @param input input.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,reset,"org.apache.hadoop.io.InputBuffer:reset(byte[],int,int)",93,95,"/**
 * Delegates buffer processing to the internal buffer.
 * @param input Input byte array.
 * @param start Starting index in the array.
 * @param length Number of bytes to process.
 */
","* Resets the data that the buffer reads.
   * @param input input.
   * @param start start.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,getPosition,org.apache.hadoop.io.InputBuffer:getPosition(),101,101,"/**
* Delegates to the internal buffer's m1() method.
*/","* Returns the current position in the input.
   * @return the current position in the input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,getLength,org.apache.hadoop.io.InputBuffer:getLength(),107,107,"/**
 * Delegates the call to buffer's m1() method.
 */","* Returns the length of the input.
   * @return length of the input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,read,org.apache.hadoop.io.MD5Hash:read(java.io.DataInput),87,91,"/**
 * Calculates MD5 hash from input stream.
 * @param in Input stream to calculate hash from.
 * @return MD5Hash object containing the result.
 */
","* Constructs, reads and returns an instance.
   * @param in in.
   * @throws IOException raised on errors performing I/O.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(java.io.InputStream),138,147,"/**
 * Calculates MD5 hash of an input stream.
 * @param in Input stream to hash.
 * @return MD5Hash object containing the hash.
 */","* Construct a hash value for the content from the InputStream.
   * @param in input stream.
   * @return MD5Hash.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,"org.apache.hadoop.io.MD5Hash:digest(byte[],int,int)",156,162,"/**
 * Calculates MD5 hash of data portion.
 * @param data byte array to hash
 * @param start start index in the array
 * @param len length of data to hash
 * @return MD5Hash object containing the hash
 */
","* Construct a hash value for a byte array.
   * @param data data.
   * @param start start.
   * @param len len.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,"org.apache.hadoop.io.MD5Hash:digest(byte[][],int,int)",171,179,"/**
 * Calculates MD5 hash of data.
 * @param dataArr Byte array of data to hash.
 * @param start Start index in data array.
 * @param len Length of data to hash.
 * @return MD5Hash object containing the hash.
 */
","* Construct a hash value for an array of byte array.
   * @param dataArr dataArr.
   * @param start start.
   * @param len len.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,hashCode,org.apache.hadoop.io.MD5Hash:hashCode(),234,237,"/**
* Delegates to m1() and returns its result.
*/","Returns a hash code value for this object.
   * Only uses the first 4 bytes, since md5s are evenly distributed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,setDigest,org.apache.hadoop.io.MD5Hash:setDigest(java.lang.String),283,293,"/**
 * Parses a hex string into an MD5 digest byte array.
 * @param hex Hexadecimal string representing the MD5 digest.
 */","* Sets the digest value from a hex string.
   * @param hex hex.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,metadata,org.apache.hadoop.io.SequenceFile$Writer:metadata(org.apache.hadoop.io.SequenceFile$Metadata),1048,1050,"/**
 * Creates a MetadataOption from the given Metadata value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$StreamOption:<init>(org.apache.hadoop.fs.FSDataOutputStream),912,914,"/**
 * Constructs a StreamOption with the given output stream.
 * @param stream The FSDataOutputStream to wrap.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,org.apache.hadoop.io.ObjectWritable:<init>(java.lang.Object),48,50,"/**
 * Constructs an ObjectWritable with the given object.
 * @param instance The object to be wrapped in the writable.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,tryInstantiateProtobuf,"org.apache.hadoop.io.ObjectWritable:tryInstantiateProtobuf(java.lang.Class,java.io.DataInput)",351,389,"/**
 * Parses a message from a DataInput, handling InputStream or byte array.
 * @param protoClass The class of the message to parse.
 * @param dataIn Input stream containing the message data.
 * @return Parsed Message object.
 */","* Try to instantiate a protocol buffer of the given message class
   * from the given input stream.
   * 
   * @param protoClass the class of the generated protocol buffer
   * @param dataIn the input stream to read from
   * @return the instantiated Message instance
   * @throws IOException if an IO problem occurs",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,fsync,org.apache.hadoop.io.IOUtils:fsync(java.io.File),392,413,"/**
 * Synchronizes a file or directory.
 * @param fileToSync The file/directory to synchronize.
 * @throws IOException if an I/O error occurs.
 */","* Ensure that any writes to the given file is written to the storage device
   * that contains it. This method opens channel on given File and closes it
   * once the sync is done.<br>
   * Borrowed from Uwe Schindler in LUCENE-5588
   * @param fileToSync the file to fsync
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,equals,org.apache.hadoop.io.MapWritable:equals(java.lang.Object),78,94,"/**
 * Checks if two objects are equal based on internal state.
 * @param obj The object to compare with.
 * @return True if objects are equal, false otherwise.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,putAll,org.apache.hadoop.io.MapWritable:putAll(java.util.Map),123,128,"/**
 * Iterates through the map and calls m3 for each entry.
 * @param t Map of Writable objects to be processed.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$AppendIfExistsOption:<init>(boolean),939,941,"/**
* Calls superclass constructor if the value is true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$OnlyHeaderOption:<init>(),1889,1891,"/**
 * Default constructor. Calls super constructor with true.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$FileOption:<init>(org.apache.hadoop.fs.Path),1852,1854,"/**
 * Constructs a FileOption with the given Path.
 * @param value The Path representing the file option.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$FileOption:<init>(org.apache.hadoop.fs.Path),890,892,"/**
 * Constructs a FileOption with the given file path.
 * @param path The path to the file.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,<init>,org.apache.hadoop.io.BooleanWritable:<init>(boolean),41,43,"/**
 * Constructs a BooleanWritable with the given boolean value.
 */",* @param value value.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,toString,org.apache.hadoop.io.BooleanWritable:toString(),102,105,"/**
* Calls m1(), passes the result to Boolean.m2(), and returns.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,waitAsyncValue,"org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:waitAsyncValue(long,java.util.concurrent.TimeUnit)",205,217,"/**
 * Waits for a value, timing out after specified duration.
 * @param timeout Timeout duration.
 * @param unit Time unit for the timeout.
 * @return The value or throws TimeoutException if timed out.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:shouldRetry(java.lang.Exception,int,int,boolean)",225,230,"/**
 * Returns a RetryAction indicating a fail decision.
 * Returns a FAIL action with a predefined message.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicy.java,<init>,org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision),49,51,"/**
 * Constructs a RetryAction with a given decision and default values.
 * @param action the retry decision to execute
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicy.java,<init>,"org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long)",53,55,"/**
* Constructs a RetryAction with a delay time.
* @param action The retry decision.
* @param delayTime Delay in milliseconds.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithFixedSleep:<init>(int,long,java.util.concurrent.TimeUnit)",331,333,"/**
 * Initializes a RetryUpToMaximumCountWithFixedSleep.
 * @param maxRetries Max retries.
 * @param sleepTime Sleep time.
 * @param timeUnit Time unit for sleepTime.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry:<init>(int,long,java.util.concurrent.TimeUnit)",627,638,"/**
 * Constructs an ExponentialBackoffRetry with retry params.
 * @param maxRetries Max retry attempts.
 * @param sleepTime Initial sleep time.
 * @param timeUnit Time unit for sleepTime.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep:<init>(int,long,java.util.concurrent.TimeUnit)",367,369,"/**
 * Initializes the retry strategy with max retries, sleep time, and unit.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int)",669,672,"/**
 * Constructs a FailoverOnNetworkExceptionRetry with default settings.
 * @param fallbackPolicy The fallback policy to use.
 * @param maxFailovers Max number of failover attempts.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)",674,677,"/**
 * Constructs a FailoverOnNetworkExceptionRetry with default initial delay.
 * @param fallbackPolicy The fallback retry policy.
 * @param maxFailovers Max retry attempts.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,int,long,long)",217,222,"/**
 * Creates a FailoverOnNetworkExceptionRetry policy.
 * @param fallbackPolicy Fallback policy.
 * @return RetryPolicy instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryByRemoteException,"org.apache.hadoop.io.retry.RetryPolicies:retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",177,181,"/**
 * Creates a RemoteExceptionDependentRetry policy.
 * Uses default policy and exception-specific policies.
 */","* <p>
   * A retry policy for RemoteException
   * Set a default policy with some explicit handlers for specific exceptions.
   * </p>
   *
   * @param defaultPolicy defaultPolicy.
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$RemoteExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)",582,594,"/**
 * Determines retry action based on exception, using policy map.
 * @param e Exception occurred.
 * @param retries Number of retries.
 * @return RetryAction to execute.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,<init>,"org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:<init>(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler,org.apache.hadoop.io.retry.AsyncCallHandler)",237,243,"/**
 * Constructs an AsyncCall with provided parameters and handler.
 * @param asyncCallHandler Handler for asynchronous call processing.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,processWaitTimeAndRetryInfo,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:processWaitTimeAndRetryInfo(),268,277,"/**
 * Determines retry behavior based on wait time.
 * Returns WAIT_RETRY if wait time > 0, otherwise RETRY.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:<init>(long,org.apache.hadoop.io.retry.RetryPolicy$RetryAction,long,java.lang.Exception)",250,257,"/**
 * Creates a RetryInfo object with delay, action, count, and exception.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isEmpty,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:isEmpty(long),96,99,"/**
* Checks if elapsed time exceeds 'time' and queue is not empty.
*/",Is the queue empty for more than the given time in millisecond?,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,checkEmpty,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:checkEmpty(),106,110,"/**
* Sets empty start time if the queue is empty.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,clearNameMaps,org.apache.hadoop.security.ShellBasedIdMapping:clearNameMaps(),154,159,"/**
* Updates name maps and last update time.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,isExpired,org.apache.hadoop.security.ShellBasedIdMapping:isExpired(),161,163,"/**
 * Checks if the time elapsed since last update exceeds the timeout.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$MetricsUpdateRunner:run(),4207,4223,"/**
* Updates request metrics, calculating requests per second.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,now,org.apache.hadoop.util.SysInfoWindows:now(),61,64,"/**
* Returns a timestamp value. Used for testing purposes.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,monotonicNow,org.apache.hadoop.util.Timer:monotonicNow(),50,50,"/**
 * Delegates to Time.m1() and returns the result.
 */","* Current time from some arbitrary time base in the past, counting in
   * milliseconds, and not affected by settimeofday or similar system clock
   * changes.  This is appropriate to use when computing how much longer to
   * wait for an interval to expire.
   * @return a monotonic clock that counts in milliseconds.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryOtherThanRemoteAndSaslException,"org.apache.hadoop.io.retry.RetryPolicies:retryOtherThanRemoteAndSaslException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",194,199,"/**
 * Creates a RetryPolicy excluding Remote/Sasl exceptions.
 * @param defaultPolicy Default retry policy.
 * @param exceptionToPolicyMap Exception-specific retry policies.
 */
","* <p>
   * A retry policy where RemoteException and SaslException are not retried, other individual
   * exception types can have RetryPolicy overrides, and any other exception type without an
   * override is not retried.
   * </p>
   *
   * @param defaultPolicy defaultPolicy.
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/DefaultFailoverProxyProvider.java,getProxy,org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:getProxy(),45,48,"/**
 * Creates a ProxyInfo object using the provided proxy.
 * @return ProxyInfo object with the given proxy.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",332,338,"/**
 * Constructs a RetryInvocationHandler with provider, policy, and map.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,close,org.apache.hadoop.io.retry.RetryInvocationHandler:close(),457,460,"/**
* Delegates the m1 call to the underlying proxy descriptor.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/FailoverProxyProvider.java,getString,org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:getString(java.lang.String),50,52,"/**
 * Constructs a string indicating method call via proxy.
 * @param methodName The name of the method being called.
 * @return A formatted string with method and proxy info.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/FailoverProxyProvider.java,toString,org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:toString(),54,57,"/**
* Concatenates m1() result and proxyInfo into a String.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,getFailoverCount,org.apache.hadoop.io.retry.RetryInvocationHandler:getFailoverCount(),345,347,"/**
* Delegates m1() call to the proxyDescriptor and returns the result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeMethod,"org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])",432,443,"/**
 * Executes a method with arguments, handling exceptions.
 * @param method Method to execute.
 * @param args Arguments for the method.
 * @return Result of the method execution.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getCallId,org.apache.hadoop.ipc.Client:getCallId(),137,139,"/**
* Returns callId.m1() if not null, otherwise returns m2().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$Call:<init>(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable)",287,307,"/**
* Creates a new RPC call with given kind and request.
* @param rpcKind RPC kind.
* @param param Writable request parameter.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,getConnectionId,org.apache.hadoop.io.retry.RetryInvocationHandler:getConnectionId(),462,465,"/**
* Retrieves a ConnectionId using RPC.
* Uses proxyDescriptor to fetch the ConnectionId.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProxyCombiner.java,getConnectionId,org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:getConnectionId(),122,125,"/**
* Retrieves a ConnectionId using RPC call to the first proxy.
*/","* Since this is incapable of returning multiple connection IDs, simply
     * return the first one. In most cases, the connection ID should be the same
     * for all proxies.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isDone,org.apache.hadoop.io.retry.AsyncCallHandler$1:isDone(),226,228,"/**
* Checks if a value is present.
* @return True if a value exists, false otherwise.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,hashCode,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:hashCode(),451,454,"/**
* Delegates to the result of m1() and returns its m2() value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,equals,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:equals(java.lang.Object),456,464,"/**
 * Checks if two objects are equal based on m1() and m2().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,parseCommaSeparatedString,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parseCommaSeparatedString(java.lang.String),483,514,"/**
* Parses a string to create a MultipleLinearRandomRetry object.
* @param s String containing retry/sleep values, comma-separated.
* @return MultipleLinearRandomRetry object or null if invalid.
*/","* Parse the given string as a MultipleLinearRandomRetry object.
     * The format of the string is ""t_1, n_1, t_2, n_2, ..."",
     * where t_i and n_i are the i-th pair of sleep time and number of retries.
     * Note that the white spaces in the string are ignored.
     *
     * @param s input string.
     * @return the parsed object, or null if the parsing fails.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryByException,"org.apache.hadoop.io.retry.RetryPolicies:retryByException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",162,165,"/**
 * Creates an exception-dependent retry policy.
 * @param defaultPolicy Default retry policy.
 * @param exceptionToPolicyMap Mapping of exceptions to policies.
 */
","* <p>
   * Set a default policy with some explicit handlers for specific exceptions.
   * </p>
   *
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @param defaultPolicy defaultPolicy.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,calculateExponentialTime,"org.apache.hadoop.io.retry.RetryPolicies:calculateExponentialTime(long,int)",769,771,"/**
* Overloaded method, delegates to the primary m1 method.
* @param time Time value.
* @param retries Retry count.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getReason,org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:getReason(),353,356,"/**
* Returns a mask based on maxTime and timeUnit.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getReason,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:getReason(),293,295,"/**
* Calls m1 with maxRetries and returns the result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,hashCode,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:hashCode(),305,308,"/**
* Delegates to the result of m1() and returns its m2() value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,equals,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:equals(java.lang.Object),310,318,"/**
 * Checks if two objects are equal based on m1() and m2().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MultipleIOException.java,createIOException,org.apache.hadoop.io.MultipleIOException:createIOException(java.util.List),50,58,"/**
 * Handles a list of IOExceptions, returning one or a combined exception.
 */","* A convenient method to create an {@link IOException}.
   * @param exceptions IOException List.
   * @return IOException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCompressionAlgorithmByName,org.apache.hadoop.io.file.tfile.Compression:getCompressionAlgorithmByName(java.lang.String),359,370,"/**
 * Finds an algorithm by name.
 * @param compressName The name of the algorithm to find.
 * @return The Algorithm object or throws IllegalArgumentException.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getSupportedAlgorithms,org.apache.hadoop.io.file.tfile.Compression:getSupportedAlgorithms(),372,382,"/**
 * Retrieves algorithm names based on a condition.
 * @return String array of algorithm names.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getCompressionName(),524,526,"/**
* Returns the result of the compression algorithm's m1 method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getBlockCount,org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockCount(),687,689,"/**
* Returns a value derived from chained method calls on dataIndex.
*/","* Get the number of data blocks.
     * 
     * @return the number of data blocks.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,readAndVerify,org.apache.hadoop.io.file.tfile.BCFile$Magic:readAndVerify(java.io.DataInput),920,929,"/**
 * Validates BCFile by checking magic bytes.
 * @param in DataInput stream to read magic bytes from.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFileDumper.java,format,"org.apache.hadoop.io.file.tfile.TFileDumper$Align:format(long,int,org.apache.hadoop.io.file.tfile.TFileDumper$Align)",73,78,"/**
 * Formats a long as a string with specified width and alignment.
 * @param l long value to format
 * @param width width of the formatted string
 * @param align alignment type
 * @return formatted string
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,addEntry,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:addEntry(org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry),782,784,"/**
 * Calls m2 on the index with the entry's m1 value and the entry.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDefaultCompressionAlgorithm,org.apache.hadoop.io.file.tfile.BCFile$Writer:getDefaultCompressionAlgorithm(),341,343,"/**
* Delegates the m1() call to the dataIndex object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDefaultCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader:getDefaultCompressionName(),652,654,"/**
 * Retrieves a value by chaining method calls on dataIndex.
 */","* Get the name of the default compression algorithm.
     * 
     * @return the name of the default compression algorithm.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,hashCode,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:hashCode(),1973,1976,"/**
* Returns a masked value based on keyBuffer and m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/CompareUtils.java,compare,"org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:compare(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",45,49,"/**
 * Delegates to overloaded m4 with arguments from o1 and o2.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:<init>(java.io.DataInput),948,952,"/**
 * Constructs a BlockRegion from an input stream.
 * @param in Input stream containing block region data.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,readVInt,org.apache.hadoop.io.file.tfile.Utils:readVInt(java.io.DataInput),177,184,"/**
 * Reads a value from DataInput, casts to int, throws if out of range.
 */","* Decoding the variable-length integer. Synonymous to
   * <code>(int)Utils#readVLong(in)</code>.
   * 
   * @param in
   *          input stream
   * @return the decoded integer
   * @throws IOException raised on errors performing I/O.
   * 
   * @see Utils#readVLong(DataInput)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,makeComparator,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:makeComparator(java.lang.String),2070,2096,"/**
* Creates a BytesComparator based on the comparator string.
* Returns null if comparator is null/empty, throws IllegalArgumentException otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:write(java.io.DataOutput),960,964,"/**
 * Writes offset, compressedSize, and rawSize to the output stream.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,writeVInt,"org.apache.hadoop.io.file.tfile.Utils:writeVInt(java.io.DataOutput,int)",55,57,"/**
* Calls m1 with the provided output stream and integer.
* @param out DataOutput stream to write to.
* @param n Integer value to pass to m1.
*/
","* Encoding an integer into a variable-length encoding format. Synonymous to
   * <code>Utils#writeVLong(out, n)</code>.
   * 
   * @param out
   *          output stream
   * @param n
   *          The integer to be encoded
   * @throws IOException raised on errors performing I/O.
   * @see Utils#writeVLong(DataOutput, long)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,isSorted,org.apache.hadoop.io.file.tfile.TFile$Reader:isSorted(),864,866,"/**
* Delegates the call to tfileMeta.m1() and returns the result.
*/","* Is the TFile sorted?
     * 
     * @return true if TFile is sorted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCodec,org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:getCodec(),273,273,"/**
* Returns a compression codec for the specified mask.
* @return A compression codec.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,equals,org.apache.hadoop.io.file.tfile.Utils$Version:equals(java.lang.Object),395,400,"/**
* Checks if two Version objects are equal.
* @param other The object to compare to.
* @return True if equal, false otherwise.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getEntryCount,org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryCount(),873,875,"/**
* Retrieves a masked value from the transaction file metadata.
* @return A long representing the masked value.
*/
","* Get the number of key-value pair entries in TFile.
     * 
     * @return the number of key-value pairs in TFile",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Reader:close(),824,827,"/**
* Delegates the m1 method call to the readerBCF object.
*/","* Close the reader. The state of the Reader object is undefined after
     * close. Calling close() for multiple times has no effect.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getComparatorName,org.apache.hadoop.io.file.tfile.TFile$Reader:getComparatorName(),855,857,"/**
* Retrieves a value from the tfileMeta object.
*/","* Get the string representation of the comparator.
     * 
     * @return If the TFile is not sorted by keys, an empty string will be
     *         returned. Otherwise, the actual comparator string that is
     *         provided during the TFile creation time will be returned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/ByteArray.java,<init>,org.apache.hadoop.io.file.tfile.ByteArray:<init>(org.apache.hadoop.io.BytesWritable),40,42,"/**
 * Constructs a ByteArray from a BytesWritable.
 * @param other The BytesWritable to copy from.
 */
","* Constructing a ByteArray from a {@link BytesWritable}.
   * 
   * @param other other.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/ByteArray.java,<init>,org.apache.hadoop.io.file.tfile.ByteArray:<init>(byte[]),50,52,"/**
* Constructs a ByteArray with the specified byte array.
* @param buffer The byte array to wrap.
*/
","* Wrap a whole byte array as a RawComparable.
   * 
   * @param buffer
   *          the byte array buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockEntryCount,org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockEntryCount(int),2031,2033,"/**
* Retrieves a long value from tfileIndex based on curBid.
* @param curBid Bid used to index tfileIndex.
* @return Long value retrieved from tfileIndex.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,addEntry,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:addEntry(org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry),2262,2266,"/**
* Updates index and record number based on the key entry.
* @param keyEntry Entry containing data to process.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,register,"org.apache.hadoop.io.file.tfile.BCFile$Writer$DataBlockRegister:register(long,long,long)",468,471,"/**
 * Processes a block region using the dataIndex.
 * @param raw Raw data offset.
 * @param begin Start of the block region.
 * @param end End of the block region.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getBlockIndexNear,org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockIndexNear(long),745,756,"/**
 * Finds the index of a block region by offset.
 * @param offset The offset to search for.
 * @return Index of the region, or -1 if not found.
 */","* Find the smallest Block index whose starting offset is greater than or
     * equal to the specified offset.
     * 
     * @param offset
     *          User-specific offset.
     * @return the index to the data Block if such block exists; or -1
     *         otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:lowerBound(org.apache.hadoop.io.file.tfile.RawComparable),2187,2201,"/**
 * Finds the index of a key using the comparator.
 * @param key The key to search for.
 * @return The index or -1 if not found.
 */","* @param key
     *          input key.
     * @return the ID of the first block that contains key >= input key. Or -1
     *         if no such block exists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,write,org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(int),45,51,"/**
* Writes a byte to the buffer. Expands buffer if full.
* @param b The byte to write.
* @throws IOException if an I/O error occurs.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,write,"org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(byte[],int,int)",53,65,"/**
* Copies data to internal buffer, potentially calling m1().
* @param b input byte array
* @param off offset in the byte array
* @param len number of bytes to copy
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,flush,org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:flush(),67,71,"/**
* Calls m1 and out.m2.
* Calls m1 and then delegates to out.m2.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:upperBound(org.apache.hadoop.io.file.tfile.RawComparable),2209,2223,"/**
 * Finds the index of a key using the comparator.
 * @param key The key to search for.
 * @return Index of the key or -1 if not found.
 */","* @param key
     *          input key.
     * @return the ID of the first block that contains key > input key. Or -1
     *         if no such block exists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumByLocation,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),2244,2248,"/**
 * Calculates the record number based on location data.
 * @param location Location object containing block index and offset.
 * @return The calculated record number.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),741,744,"/**
 * Calls m1 with block and record indices from the Location.
 * @param other The Location object to extract indices from.
 */",* @see java.lang.Comparable#compareTo(java.lang.Object),,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(int,long)",705,707,"/**
 * Constructs a Location object with given block and record indices.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,set,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:set(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),734,736,"/**
* Calls m1 with block and record indices from the Location object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(byte[]),1775,1777,"/**
 * Calls overloaded method with offset 0.
 * @param buf byte array to process
 * @throws IOException if an I/O error occurs
 */
","* Copy the key into user supplied buffer.
         * 
         * @param buf
         *          The buffer supplied by user. The length of the buffer must
         *          not be shorter than the key length.
         * @return The length of the key.
         * 
         * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[],int)",1859,1890,"/**
* Reads data into the buffer, handling potential buffer overflows.
* @param buf buffer to read into
* @param offset starting offset in the buffer
* @return number of bytes read
*/
","* Copy value into user-supplied buffer. User supplied buffer must be
         * large enough to hold the whole value (starting from the offset). The
         * value part of the key-value pair pointed by the current cursor is not
         * cached and can only be examined once. Calling any of the following
         * functions more than once without moving the cursor will result in
         * exception: {@link #getValue(byte[])}, {@link #getValue(byte[], int)},
         * {@link #getValueStream}.
         *
         * @param buf buf.
         * @param offset offset.
         * @return the length of the value. Does not require
         *         isValueLengthKnown() to be true.
         * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:<init>(org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState),549,552,"/**
 * Initializes a BlockReader with the provided RBlockState.
 * @param rbs The RBlockState containing the input stream.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getRawSize,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getRawSize(),585,587,"/**
* Delegates to rBlkState.m1().m2() and returns the result.
*/","* Get the uncompressed size of the block.
       * 
       * @return uncompressed size of the block.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressedSize(),594,596,"/**
* Delegates to rBlkState.m1().m2() and returns the result.
*/","* Get the compressed size of the block.
       * 
       * @return compressed size of the block.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getStartPos,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getStartPos(),603,605,"/**
* Retrieves a value from the rBlkState object.
*/","* Get the starting position of the block in the file.
       * 
       * @return the starting position of the block in the file.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(byte[]),394,397,"/**
* Calls m1 with the provided byte array and full length.
* @param b The byte array to process.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputOutputStream.java,constructOutputStream,org.apache.hadoop.io.DataOutputOutputStream:constructOutputStream(java.io.DataOutput),43,49,"/**
 * Returns the OutputStream if possible, otherwise wraps it.
 */","* Construct an OutputStream from the given DataOutput. If 'out'
   * is already an OutputStream, simply returns it. Otherwise, wraps
   * it in an OutputStream.
   * @param out the DataOutput to wrap
   * @return an OutputStream instance that outputs to 'out'",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FloatWritable.java,<init>,org.apache.hadoop.io.FloatWritable:<init>(float),34,34,"/**
* Constructs a FloatWritable with the given float value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FastByteComparisons.java,compareTo,"org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:compareTo(byte[],int,int,byte[],int,int)",188,242,"/**
* Compares two byte buffers for equality, handling stride and offsets.
*/","* Lexicographically compare two arrays.
       *
       * @param buffer1 left operand
       * @param buffer2 right operand
       * @param offset1 Where to start comparing in the left buffer
       * @param offset2 Where to start comparing in the right buffer
       * @param length1 How much to compare from the left buffer
       * @param length2 How much to compare from the right buffer
       * @return 0 if equal, < 0 if left is less than right, etc.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,<init>,org.apache.hadoop.io.DataOutputBuffer:<init>(),89,91,"/**
 * Constructs a DataOutputBuffer with a default Buffer.
 */",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,<init>,org.apache.hadoop.io.DataOutputBuffer:<init>(int),93,95,"/**
 * Creates a DataOutputBuffer with the specified initial size.
 * @param size Initial buffer capacity.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,getData,org.apache.hadoop.io.DataOutputBuffer:getData(),108,108,"/**
* Delegates to buffer's m1() method and returns the result.
*/","* Returns the current contents of the buffer.
   *  Data is only valid to {@link #getLength()}.
   *
   * @return data byte.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,getLength,org.apache.hadoop.io.DataOutputBuffer:getLength(),114,114,"/**
* Delegates to the buffer's m1() method and returns the result.
*/","* Returns the length of the valid data currently in the buffer.
   * @return length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,writeInt,"org.apache.hadoop.io.DataOutputBuffer:writeInt(int,int)",154,164,"/**
* Writes an integer to the buffer at the given offset.
* @param v integer to write
* @param offset offset in the buffer
*/","* Overwrite an integer into the internal buffer. Note that this call can only
   * be used to overwrite existing data in the buffer, i.e., buffer#count cannot
   * be increased, and DataOutputStream#written cannot be increased.
   *
   * @param v v.
   * @param offset offset.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNSDomainNameResolver.java,getHostnameByIP,org.apache.hadoop.net.DNSDomainNameResolver:getHostnameByIP(java.net.InetAddress),44,62,"/**
* Extracts a hostname from an InetAddress, or performs reverse DNS lookup.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDistance,"org.apache.hadoop.net.NetworkTopology:getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",324,365,"/**
* Calculates distance between nodes. Returns MAX_VALUE on error.
*/","Return the distance between two nodes
   * It is assumed that the distance from one node to its parent is 1
   * The distance between two nodes is calculated by summing up their distances
   * to their closest common ancestor.
   * @param node1 one node
   * @param node2 another node
   * @return the distance between node1 and node2 which is zero if they are the same
   *  or {@link Integer#MAX_VALUE} if node1 or node2 do not belong to the cluster",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,isNodeInScope,"org.apache.hadoop.net.NetworkTopology:isNodeInScope(org.apache.hadoop.net.Node,java.lang.String)",1023,1029,"/**
 * Checks if a scope matches a node's location.
 * @param node The Node to check.
 * @param scope The scope string.
 * @return True if the scope matches, false otherwise.
 */","* Checks whether a node belongs to the scope.
   * @param node  the node to check.
   * @param scope scope to check.
   * @return true if node lies within the scope",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,getPathComponents,org.apache.hadoop.net.NodeBase:getPathComponents(org.apache.hadoop.net.Node),124,126,"/**
* Calls m1 and m2 to process a Node.
* @param node The Node to process.
* @return String array result of the chained calls.
*/
","* Get the path components of a node.
   * @param node a non-null node
   * @return the path of a node",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,equals,org.apache.hadoop.net.NodeBase:equals(java.lang.Object),128,137,"/**
 * Checks if this NodeBase is equivalent to another NodeBase.
 * @param to The NodeBase to compare with.
 * @return True if equivalent, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,hashCode,org.apache.hadoop.net.NodeBase:hashCode(),139,142,"/**
* Delegates m2() call to the superclass's m2() method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,toString,org.apache.hadoop.net.NodeBase:toString(),145,148,"/**
* Delegates to m1 with the current object as an argument.
*/",@return this node's path as its string representation,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,remove,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:remove(org.apache.hadoop.net.Node),228,254,"/**
 * Removes a node from the cluster map, handling inner nodes and locks.
 */","Remove a node
   * Update node counter and rack counter if necessary
   * @param node node to be removed; can be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDatanodesInRack,org.apache.hadoop.net.NetworkTopology:getDatanodesInRack(java.lang.String),202,215,"/**
* Retrieves a list of nodes from the cluster map by location.
* @param loc Location string to search for nodes.
* @return List of nodes or an empty list if not found.
*/","* Given a string representation of a rack, return its children
   * @param loc a path-like string representation of a rack
   * @return a newly allocated list with all the node's children",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getNode,org.apache.hadoop.net.NetworkTopology:getNode(java.lang.String),271,281,"/**
* Retrieves a Node from the cluster map using a location string.
*/","Given a string representation of a node, return its reference
   * 
   * @param loc
   *          a path-like string representation of a node
   * @return a reference to the node; null if the node is not in the tree",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,locationToDepth,org.apache.hadoop.net.NodeBase:locationToDepth(java.lang.String),209,219,"/**
 * Calculates the directory depth of a location string.
 * @param location The location string to analyze.
 * @return The directory depth.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,toString,org.apache.hadoop.net.NetworkTopology:toString(),714,732,"/**
 * Generates a string representation of rack and leaf information.
 * Returns a formatted string with rack count, leaf count, and leaf details.
 */",convert a network tree to a string.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,isOnSameRack,"org.apache.hadoop.net.NetworkTopology:isOnSameRack(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",409,415,"/**
 * Checks if two nodes satisfy a condition (delegates to m1).
 * @param node1 The first node.
 * @param node2 The second node.
 */
","Check if two nodes are on the same rack
   * @param node1 one node (can be null)
   * @param node2 another node (can be null)
   * @return true if node1 and node2 are on the same rack; false otherwise
   * @exception IllegalArgumentException when either node1 or node2 is null, or
   * node1 or node2 do not belong to the cluster",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int)",569,637,"/**
 * Selects a Node, considering exclusions and availability.
 * @param parentNode Parent Node.
 * @return Selected Node or null if no valid Node found.
 */","* Randomly choose one node under <i>parentNode</i>, considering the exclude
   * nodes and scope. Should be called with {@link #netlock}'s readlock held.
   *
   * @param parentNode        the parent node
   * @param excludedScopeNode the node corresponding to the exclude scope.
   * @param excludedNodes     a collection of nodes to be excluded from
   * @param totalInScopeNodes total number of nodes under parentNode, excluding
   *                          the excludedScopeNode
   * @param availableNodes    number of available nodes under parentNode that
   *                          could be chosen, excluding excludedNodes
   * @return the chosen node, or null if none can be chosen",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getWeightUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:getWeightUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",807,845,"/**
 * Calculates a weight based on reader and node paths.
 * Returns weight, considering path similarity.
 */","* Returns an integer weight which specifies how far away <i>node</i> is
   * from <i>reader</i>. A lower value signifies that a node is closer.
   * It uses network location to calculate the weight
   *
   * @param reader Node where data will be read
   * @param node Replica of data
   * @return weight",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,interAddNodeWithEmptyRack,org.apache.hadoop.net.NetworkTopology:interAddNodeWithEmptyRack(org.apache.hadoop.net.Node),1083,1097,"/**
* Adds node to rack's node set, if not decommissioned.
* @param node The Node object to add.
*/","* Internal function for update empty rack number
   * for add or recommission a node.
   * @param node node to be added; can be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,<init>,"org.apache.hadoop.net.SocketIOWithTimeout:<init>(java.nio.channels.SelectableChannel,long)",58,66,"/**
 * Initializes SocketIO with a SelectableChannel and timeout.
 * @param channel SelectableChannel for IO operations.
 * @param timeout Timeout duration in milliseconds.
 * @throws IOException if channel is invalid.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,write,"org.apache.hadoop.net.SocketOutputStream:write(byte[],int,int)",111,129,"/**
 * Reads data from a byte buffer until it's empty, handling IO exceptions.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,transferToFully,"org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.LongWritable)",202,251,"/**
* Transfers data from a FileChannel, tracking wait & transfer times.
* @param fileCh channel to transfer from, position, count, times.
*/","* Transfers data from FileChannel using 
   * {@link FileChannel#transferTo(long, long, WritableByteChannel)}.
   * Updates <code>waitForWritableTime</code> and <code>transferToTime</code>
   * with the time spent blocked on the network and the time spent transferring
   * data from disk to network respectively.
   * 
   * Similar to readFully(), this waits till requested amount of 
   * data is transfered.
   * 
   * @param fileCh FileChannel to transfer data from.
   * @param position position within the channel where the transfer begins
   * @param count number of bytes to transfer.
   * @param waitForWritableTime nanoseconds spent waiting for the socket 
   *        to become writable
   * @param transferToTime nanoseconds spent transferring data
   * 
   * @throws EOFException 
   *         If end of input file is reached before requested number of 
   *         bytes are transfered.
   *
   * @throws SocketTimeoutException 
   *         If this channel blocks transfer longer than timeout for 
   *         this stream.
   *          
   * @throws IOException Includes any exception thrown by 
   *         {@link FileChannel#transferTo(long, long, WritableByteChannel)}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,normalizeHostNames,org.apache.hadoop.net.NetUtils:normalizeHostNames(java.util.Collection),662,668,"/**
 * Transforms a collection of names, adding a modified version.
 * @param names Collection of names to transform.
 * @return List of transformed names.
 */
","* Given a collection of string representation of hosts, return a list of
   * corresponding IP addresses in the textual representation.
   * 
   * @param names a collection of string representations of hosts
   * @return a list of corresponding IP addresses in the string format
   * @see #normalizeHostName(String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getHostDetailsAsString,"org.apache.hadoop.net.NetUtils:getHostDetailsAsString(java.lang.String,int,java.lang.String)",977,988,"/**
 * Constructs a string containing host and port details.
 * @param destHost Destination host string.
 * @param destPort Destination port (int).
 * @param localHost Local host string.
 * @return String with formatted host and port information.
 */
","* Get the host details as a string
   * @param destHost destinatioon host (nullable)
   * @param destPort destination port
   * @param localHost local host (nullable)
   * @return a string describing the destination host:port and the local host",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getIPs,"org.apache.hadoop.net.NetUtils:getIPs(java.lang.String,boolean)",1041,1068,"/**
 * Finds InetAddresses within a subnet, optionally including subinterfaces.
 * @param subnet Subnet string (e.g., ""192.168.1.0/24"")
 * @param returnSubinterfaces Whether to include subinterfaces.
 * @return List of InetAddress objects.
 */
","* Return an InetAddress for each interface that matches the
   * given subnet specified using CIDR notation.
   *
   * @param subnet subnet specified using CIDR notation
   * @param returnSubinterfaces
   *            whether to return IPs associated with subinterfaces
   * @throws IllegalArgumentException if subnet is invalid
   * @return ips.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getFreeSocketPorts,org.apache.hadoop.net.NetUtils:getFreeSocketPorts(int),1098,1113,"/**
* Generates a set of free ports.
* @param numOfPorts number of ports to acquire
* @throws IllegalStateException if not enough ports are available
*/","* Return free ports. There is no guarantee they will remain free, so
   * ports should be used immediately. The number of free ports returned by
   * this method should match argument {@code numOfPorts}. Num of ports
   * provided in the argument should not exceed 25.
   *
   * @param numOfPorts Number of free ports to acquire.
   * @return Free ports for binding a local socket.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,getConf,org.apache.hadoop.net.TableMapping:getConf(),71,74,"/**
* Delegates the call to the m2() method of the object returned by m1().
* @return Configuration object returned by m1().m2()
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,setConf,org.apache.hadoop.net.TableMapping:setConf(org.apache.hadoop.conf.Configuration),76,80,"/**
 * Calls super.m1 and then m2().m1 with the given configuration.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:<init>(),172,172,"/**
 * Default constructor for RawScriptBasedMapping class.
 */
","* Constructor. The mapping is not ready to use until
     * {@link #setConf(Configuration)} has been called",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/CachedDNSToSwitchMapping.java,<init>,org.apache.hadoop.net.CachedDNSToSwitchMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping),50,52,"/**
 * Constructs a CachedDNSToSwitchMapping with the given raw mapping.
 */","* cache a raw DNS mapping
   * @param rawMapping the raw mapping to cache",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,org.apache.hadoop.net.NodeBase:<init>(java.lang.String),53,61,"/**
 * Constructs a NodeBase with a normalized path.
 * @param path The path to be normalized and set.
 */
","Construct a node from its path
   * @param path 
   *   a concatenation of this node's location, the path separator, and its name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,"org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String)",67,69,"/**
 * Constructs a NodeBase with the given name and normalized location.
 */","Construct a node from its name and its location
   * @param name this node's name (can be null, must not contain {@link #PATH_SEPARATOR})
   * @param location this node's location",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,"org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.Node,int)",77,81,"/**
 * Constructs a new NodeBase with the given name, location, parent, and level.
 */","Construct a node from its name and its location
   * @param name this node's name (can be null, must not contain {@link #PATH_SEPARATOR})
   * @param location this node's location 
   * @param parent this node's parent node
   * @param level this node's level in the tree",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,getConf,org.apache.hadoop.net.ScriptBasedMapping:getConf(),116,119,"/**
* Delegates the call to the m2() method of the object returned by m1().
* @return Configuration object returned by m1().m2()
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,toString,org.apache.hadoop.net.ScriptBasedMapping:toString(),121,124,"/**
* Concatenates a string with the result of m1().m2().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:read(java.nio.ByteBuffer),602,628,"/**
 * Reads data into a ByteBuffer. Returns the number of bytes read.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,write,org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(int),563,575,"/**
 * Sends a single byte to the domain socket.
 * @param val The byte value to send.
 * @throws IOException if an I/O error occurs.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,write,"org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(byte[],int,int)",577,587,"/**
* Reads data from the domain socket.
* @param b byte array to store data, off offset, len length
* @throws IOException if an I/O error occurs
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read(),507,519,"/**
 * Reads a single byte from the domain socket.
 * @return Byte read or -1 on error.
 * @throws IOException if an I/O error occurs.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,"org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read(byte[],int,int)",521,532,"/**
 * Reads data from the domain socket.
 * @param b buffer to read into, @param off offset, @param len length
 * @return Number of bytes read or -1 on error.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,available,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:available(),534,545,"/**
 * Gets the number of bytes available for reading from the socket.
 * @return Number of available bytes, or negative value on error.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,<init>,"org.apache.hadoop.net.unix.DomainSocket:<init>(java.lang.String,int)",168,172,"/**
 * Constructs a DomainSocket with the given file descriptor and path.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,sendCallback,"org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)",386,424,"/**
* Sends a callback, potentially closing the file descriptor.
* @param caller Caller of the method, for logging.
*/","* Send callback and return whether or not the domain socket was closed as a
   * result of processing.
   *
   * @param caller reason for call
   * @param entries mapping of file descriptor to entry
   * @param fdSet set of file descriptors
   * @param fd file descriptor
   * @return true if the domain socket was closed as a result of processing",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,isOpen,org.apache.hadoop.net.unix.DomainSocket:isOpen(),268,270,"/**
* Delegates to the internal refCount's m1() method.
*/","* Return true if the file descriptor is currently open.
   *
   * @return                 True if the file descriptor is currently open.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket:close(),344,388,"/**
* Decrements refCount, shuts down, and releases the file descriptor.
*/",* Close the Socket.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,addNotificationSocket,"org.apache.hadoop.net.unix.DomainSocketWatcher:addNotificationSocket(java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet)",546,561,"/**
* Adds a notification socket to the entry set and fdSet.
* @param entries TreeMap of entries
* @param fdSet Set of file descriptors
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,trimIdleSelectors,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:trimIdleSelectors(long),426,444,"/**
 * Removes idle selectors from provider queues based on timeout.
 * @param now Current timestamp to determine idle selectors.
 */","* Closes selectors that are idle for IDLE_TIMEOUT (10 sec). It does not
     * traverse the whole list, just over the one that have crossed 
     * the timeout.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,isLeafParent,org.apache.hadoop.net.InnerNodeImpl:isLeafParent(),302,304,"/**
 * Delegates to m1() and returns its boolean result.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,getNextAncestorName,org.apache.hadoop.net.InnerNodeImpl:getNextAncestorName(org.apache.hadoop.net.Node),113,127,"/**
 * Extracts a name from a Node, handling path separators.
 * @param n The Node to extract the name from.
 * @return The extracted name string.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int)",70,76,"/**
 * Creates a socket and connects it to the specified address and port.
 * @param addr The address to connect to.
 * @param port The port to connect to.
 * @return The connected Socket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",78,86,"/**
 * Creates a socket, binds it, and connects to the specified address.
 * @param addr Remote address to connect to.
 * @param port Remote port to connect to.
 * @return Socket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int)",88,95,"/**
 * Creates a socket and connects to the specified host and port.
 * @param host The hostname to connect to.
 * @param port The port number to connect to.
 * @return A connected Socket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",97,106,"/**
 * Creates a socket, binds it, and connects to host:port.
 * @param host Hostname or IP address.
 * @param port Port number.
 * @return Socket object.
 * @throws IOException, UnknownHostException
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int)",65,71,"/**
 * Creates a socket and connects to the specified address and port.
 * @param addr The address to connect to.
 * @param port The port to connect to.
 * @return A connected Socket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",73,81,"/**
 * Creates a socket, binds it, and connects to the specified address.
 * @param addr Remote address to connect to.
 * @param port Remote port to connect to.
 * @return Socket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int)",83,90,"/**
 * Creates a socket and connects to the specified host and port.
 * @param host The hostname to connect to.
 * @param port The port number to connect to.
 * @return A connected Socket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",92,101,"/**
 * Creates a socket bound to local address and connected to host.
 * @param host Hostname or IP address.
 * @param port Port number.
 * @return Socket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,toString,org.apache.hadoop.net.ScriptBasedMappingWithDependency:toString(),71,74,"/**
* Concatenates a string with the result of m1().m2().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,getDependency,org.apache.hadoop.net.ScriptBasedMappingWithDependency:getDependency(java.lang.String),96,115,"/**
 * Retrieves dependencies for a name, caching the result.
 * @param name The name to retrieve dependencies for.
 * @return List of dependencies or an empty list if none.
 */","* Get dependencies in the topology for a given host
   * @param name - host name for which we are getting dependency
   * @return a list of hosts dependent on the provided host name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,setTimeout,org.apache.hadoop.net.SocketInputWrapper:setTimeout(long),69,75,"/**
 * Sets a timeout value for the socket operation.
 * @param timeoutMs Timeout in milliseconds.
 */","* Set the timeout for reads from this stream.
   * 
   * Note: the behavior here can differ subtly depending on whether the
   * underlying socket has an associated Channel. In particular, if there is no
   * channel, then this call will affect the socket timeout for <em>all</em>
   * readers of this socket. If there is a channel, then this call will affect
   * the timeout only for <em>this</em> stream. As such, it is recommended to
   * only create one {@link SocketInputWrapper} instance per socket.
   * 
   * @param timeoutMs
   *          the new timeout, 0 for no timeout
   * @throws SocketException
   *           if the timeout cannot be set",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPs,"org.apache.hadoop.net.DNS:getIPs(java.lang.String,boolean)",175,209,"/**
 * Gets IP addresses for a network interface.
 * @param strInterface Interface name.
 * @return Array of IP addresses.
 */","* Returns all the IPs associated with the provided interface, if any, in
   * textual form.
   * 
   * @param strInterface
   *            The name of the network interface or sub-interface to query
   *            (eg eth0 or eth0:0) or the string ""default""
   * @param returnSubinterfaces
   *            Whether to return IPs associated with subinterfaces of
   *            the given interface
   * @return A string vector of all the IPs associated with the provided
   *         interface. The local host IP is returned if the interface
   *         name ""default"" is specified or there is an I/O error looking
   *         for the given interface.
   * @throws UnknownHostException
   *             If the given interface is invalid
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPsAsInetAddressList,"org.apache.hadoop.net.DNS:getIPsAsInetAddressList(java.lang.String,boolean)",428,457,"/**
 * Gets a list of InetAddresses for a network interface.
 * @param strInterface Interface name, ""default"" uses cached address.
 * @param returnSubinterfaces Whether to include sub-interfaces.
 * @return List of InetAddress objects.
 */
","* Returns all the IPs associated with the provided interface, if any, as
   * a list of InetAddress objects.
   *
   * @param strInterface
   *            The name of the network interface or sub-interface to query
   *            (eg eth0 or eth0:0) or the string ""default""
   * @param returnSubinterfaces
   *            Whether to return IPs associated with subinterfaces of
   *            the given interface
   * @return A list of all the IPs associated with the provided
   *         interface. The local host IP is returned if the interface
   *         name ""default"" is specified or there is an I/O error looking
   *         for the given interface.
   * @throws UnknownHostException
   *             If the given interface is invalid
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,read,"org.apache.hadoop.net.SocketInputStream:read(byte[],int,int)",129,132,"/**
* Delegates to a ByteBuffer-based m2 method.
* @param b byte array, off offset, len length
* @throws IOException if I/O error occurs
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getRack,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getRack(java.lang.String),57,80,"/**
* Processes a location string, returning a value based on node state.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getNodeGroup,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeGroup(java.lang.String),90,118,"/**
* Resolves a location string, recursing if necessary.
*/","* Given a string representation of a node group for a specific network
   * location
   * 
   * @param loc
   *            a path-like string representation of a network location
   * @return a node group string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,dumpTopology,org.apache.hadoop.net.AbstractDNSToSwitchMapping:dumpTopology(),112,133,"/**
* Generates a string representation of rack topology data.
* Returns formatted string or ""No topology information"" if null.
*/","* Generate a string listing the switch mapping implementation,
   * the mapping for every known node and the number of nodes and
   * unique switches known about -each entry to a separate line.
   * @return a string that can be presented to the ops team or used in
   * debug messages.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,isMappingSingleSwitch,org.apache.hadoop.net.AbstractDNSToSwitchMapping:isMappingSingleSwitch(org.apache.hadoop.net.DNSToSwitchMapping),150,153,"/**
 * Checks if mapping is valid AbstractDNSToSwitchMapping and m1() returns true.
 */","* Query for a {@link DNSToSwitchMapping} instance being on a single
   * switch.
   * <p>
   * This predicate simply assumes that all mappings not derived from
   * this class are multi-switch.
   * @param mapping the mapping to query
   * @return true if the base class says it is single switch, or the mapping
   * is not derived from this class.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getWeight,"org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getWeight(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",256,271,"/**
 * Calculates a weight based on reader and node conditions.
 * Returns a weight (0-3) based on reader's methods.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,writeAttribute,"org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)",330,388,"/**
 * Retrieves and serializes attribute value using JsonGenerator.
 * @param jg JsonGenerator for serialization
 * @param oname ObjectName of the MBean
 * @param attr MBeanAttributeInfo for the attribute
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,writeObject,"org.apache.hadoop.jmx.JMXJsonServlet:writeObject(com.fasterxml.jackson.core.JsonGenerator,java.lang.Object,java.lang.String)",395,436,"/**
 * Serializes an object to a JsonGenerator, handling various types.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,getCurrentStats,"org.apache.hadoop.log.LogThrottlingHelper:getCurrentStats(java.lang.String,int)",290,297,"/**
 * Retrieves statistics for a recorder at a given index.
 * @param recorderName Recorder name.
 * @param idx Index of the statistic.
 * @return SummaryStatistics object or null if not found.
 */
","* Return the summary information for given index.
   *
   * @param recorderName The name of the recorder.
   * @param idx The index value.
   * @return The summary information.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseProtocolArgs,"org.apache.hadoop.log.LogLevel$CLI:parseProtocolArgs(java.lang.String[],int)",209,228,"/**
 * Parses the protocol from arguments.
 * @param args Command-line arguments.
 * @param index Current argument index.
 * @return Next argument index after parsing.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,printUsage,org.apache.hadoop.log.LogLevel:printUsage(),87,90,"/**
* Prints usage information and parses generic options.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,printGenericCommandUsage,org.apache.hadoop.util.ToolRunner:printGenericCommandUsage(java.io.PrintStream),105,107,"/**
 * Delegates printing to GenericOptionsParser's m1 method.
 * @param out PrintStream to use for output.
 */","* Prints generic command-line argurments and usage information.
   * 
   *  @param out stream to write usage information to.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericsUtil.java,isLog4jLogger,org.apache.hadoop.util.GenericsUtil:isLog4jLogger(java.lang.Class),95,100,"/**
* Recursively checks a class and its superclass.
* Returns true if not null, otherwise false.
*/","* Determine whether the log of <code>clazz</code> is Log4j implementation.
   * @param clazz a class to be determined
   * @return true if the log of <code>clazz</code> is Log4j implementation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,<init>,"org.apache.hadoop.log.LogThrottlingHelper:<init>(long,java.lang.String)",159,161,"/**
 * Constructs a LogThrottlingHelper with a Timer.
 * @param minLogPeriodMs Minimum logging period in milliseconds.
 * @param primaryRecorderName Name of the primary recorder.
 */
","* Create a log helper with a specified primary recorder name; this can be
   * used in conjunction with {@link #record(String, long, double...)} to set up
   * primary and dependent recorders. See
   * {@link #record(String, long, double...)} for more details.
   *
   * @param minLogPeriodMs The minimum period with which to log; do not log
   *                       more frequently than this.
   * @param primaryRecorderName The name of the primary recorder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,record,"org.apache.hadoop.log.LogThrottlingHelper:record(java.lang.String,long,double[])",247,281,"/**
 * Logs action data for a recorder. Creates a new log if needed.
 * @param recorderName Name of the recorder.
 * @param currentTimeMs Current timestamp in milliseconds.
 */","* Record some set of values at the specified time into this helper. This can
   * be useful to avoid fetching the current time twice if the caller has
   * already done so for other purposes. This additionally allows the caller to
   * specify a name for this recorder. When multiple names are used, one is
   * denoted as the primary recorder. Only recorders named as the primary
   * will trigger logging; other names not matching the primary can <i>only</i>
   * be triggered by following the primary. This is used to coordinate multiple
   * logging points. A primary can be set via the
   * {@link #LogThrottlingHelper(long, String)} constructor. If no primary
   * is set in the constructor, then the first recorder name used becomes the
   * primary.
   *
   * If multiple names are used, they maintain entirely different sets of values
   * and summary information. For example:
   * <pre>{@code
   *   // Initialize ""pre"" as the primary recorder name
   *   LogThrottlingHelper helper = new LogThrottlingHelper(1000, ""pre"");
   *   LogAction preLog = helper.record(""pre"", Time.monotonicNow());
   *   if (preLog.shouldLog()) {
   *     // ...
   *   }
   *   double eventsProcessed = ... // perform some action
   *   LogAction postLog =
   *       helper.record(""post"", Time.monotonicNow(), eventsProcessed);
   *   if (postLog.shouldLog()) {
   *     // ...
   *     // Can use postLog.getStats(0) to access eventsProcessed information
   *   }
   * }</pre>
   * Since ""pre"" is the primary recorder name, logging to ""pre"" will trigger a
   * log action if enough time has elapsed. This will indicate that ""post""
   * should log as well. This ensures that ""post"" is always logged in the same
   * iteration as ""pre"", yet each one is able to maintain its own summary
   * information.
   *
   * <p>Other behavior is the same as {@link #record(double...)}.
   *
   * @param recorderName The name of the recorder. This is used to check if the
   *                     current recorder is the primary. Other names are
   *                     arbitrary and are only used to differentiate between
   *                     distinct recorders.
   * @param currentTimeMs The current time.
   * @param values The values to log.
   * @return The LogAction for the specified recorder.
   *
   * @see #record(double...)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,fromInternalName,org.apache.hadoop.http.ProfileServlet$Event:fromInternalName(java.lang.String),148,156,"/**
 * Finds an event by name.
 * @param name The name of the event to find.
 * @return The event with the given name, or null.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfilerDisabledServlet.java,doGet,"org.apache.hadoop.http.ProfilerDisabledServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",34,48,"/**
 * Sends an internal server error response, disables profiler.
 * Sends error code and message to the response object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,<init>,org.apache.hadoop.http.ProfileServlet:<init>(),177,181,"/**
 * Initializes the servlet, setting process ID and async profiler home.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,needsQuoting,org.apache.hadoop.http.HtmlQuoting:needsQuoting(java.lang.String),68,74,"/**
 * Checks if a string is valid.
 * @param str The string to validate. Returns false if null.
 */
","* Does the given string need to be quoted?
   * @param str the string to check
   * @return does the string contain any of the active html characters?",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,quoteHtmlChars,org.apache.hadoop.http.HtmlQuoting:quoteHtmlChars(java.lang.String),114,131,"/**
* Processes a string, returning a modified string or the original.
* @param item The string to process.
* @return Modified string or original if processing fails.
*/","* Quote the given item to make it html-safe.
   * @param item the string to quote
   * @return the quoted string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addJerseyResourcePackage,"org.apache.hadoop.http.HttpServer2:addJerseyResourcePackage(java.lang.String,java.lang.String)",1018,1022,"/**
* Calls m2 with default metadata.
* @param packageName Package name to process.
* @param pathSpec Path specification to use.
*/","* Add a Jersey resource package.
   * @param packageName The Java package name containing the Jersey resource.
   * @param pathSpec The path spec for the servlet",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addServlet,"org.apache.hadoop.http.HttpServer2:addServlet(java.lang.String,java.lang.String,java.lang.Class)",1050,1053,"/**
 * Registers a servlet with the given name and path spec.
 * @param name servlet name
 * @param pathSpec servlet path
 * @param clazz servlet class
 */
","* Add a servlet in the server.
   * @param name The name of the servlet (can be passed as null)
   * @param pathSpec The path spec for the servlet
   * @param clazz The servlet class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addInternalServlet,"org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class)",1065,1068,"/**
 * Overloads m1 with default 'false' for the 'secure' parameter.
 * @param name servlet name, pathSpec servlet path, clazz servlet class
 */","* Add an internal servlet in the server.
   * Note: This method is to be used for adding servlets that facilitate
   * internal communication and not for user facing functionality. For
   * servlets added using this method, filters are not enabled.
   *
   * @param name The name of the servlet (can be passed as null)
   * @param pathSpec The path spec for the servlet
   * @param clazz The servlet class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addFilter,"org.apache.hadoop.http.HttpServer2:addFilter(java.lang.String,java.lang.String,java.util.Map)",1170,1193,"/**
 * Adds a filter to the web app context and default contexts.
 * @param name Filter name
 * @param classname Filter class name
 * @param parameters Filter parameters
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addGlobalFilter,"org.apache.hadoop.http.HttpServer2:addGlobalFilter(java.lang.String,java.lang.String,java.util.Map)",1195,1206,"/**
 * Adds a global filter to the web application context.
 * @param name Filter name.
 * @param classname Filter class name.
 * @param parameters Filter parameters.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,defineFilter,"org.apache.hadoop.http.HttpServer2:defineFilter(org.eclipse.jetty.servlet.ServletContextHandler,java.lang.String,java.lang.String,java.util.Map,java.lang.String[])",1217,1222,"/**
 * Registers a filter with the given ServletContextHandler.
 * @param ctx Handler, name, classname, params, URLs for filter.
 */
","* Define a filter for a context and set up default url mappings.
   *
   * @param ctx ctx.
   * @param name name.
   * @param classname classname.
   * @param parameters parameters.
   * @param urls urls.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,bindForSinglePort,"org.apache.hadoop.http.HttpServer2:bindForSinglePort(org.eclipse.jetty.server.ServerConnector,int)",1478,1493,"/**
 * Attempts to connect a listener to a port, retrying if needed.
 * @param listener ServerConnector to connect
 * @param port initial port to try
 */","* Bind using single configured port. If findPort is true, we will try to bind
   * after incrementing port till a free port is found.
   * @param listener jetty listener.
   * @param port port which is set in the listener.
   * @throws Exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,toString,org.apache.hadoop.http.HttpServer2:toString(),1631,1642,"/**
 * Generates a string describing the server's state and listeners.
 * Returns a server status string.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getEnum,org.apache.hadoop.http.HttpServer2$XFrameOption:getEnum(java.lang.String),1946,1954,"/**
 * Finds an XFrameOption matching the given value.
 * @param value The value to match.
 * @throws IllegalArgumentException if no match is found.
 */
","* We cannot use valueOf since the AllowFrom enum differs from its value
     * Allow-From. This is a helper method that does exactly what valueof does,
     * but allows us to handle the AllowFrom issue gracefully.
     *
     * @param value - String must be DENY, SAMEORIGIN or ALLOW-FROM.
     * @return XFrameOption or throws IllegalException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,init,org.apache.hadoop.http.HttpServer2$QuotingInputFilter:init(javax.servlet.FilterConfig),1860,1864,"/**
* Initializes the filter with the provided configuration.
* @param config Filter configuration object.
* @throws ServletException if initialization fails.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,doFilter,"org.apache.hadoop.http.HttpServer2$QuotingInputFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",1870,1893,"/**
* Filters a request, sets content type, and passes to chain.
* @param request ServletRequest
* @param response ServletResponse
* @param chain FilterChain
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,init,org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter:init(javax.servlet.FilterConfig),114,118,"/**
 * Initializes username and user based on FilterConfig.
 * @param conf FilterConfig object containing user information.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileMonitoringTimerTask.java,<init>,"org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.util.List,java.util.function.Consumer,java.util.function.Consumer)",75,89,"/**
 * Creates a FileMonitoringTimerTask.
 * @param filePaths Paths to monitor.
 * @param onFileChange Action on file change.
 * @param onChangeFailure Action on failure.
 */
","* Create file monitoring task to be scheduled using a standard
   * Java {@link java.util.Timer} instance.
   *
   * @param filePaths The path to the file to monitor.
   * @param onFileChange The function to call when the file has changed.
   * @param onChangeFailure The function to call when an exception is
   *                       thrown during the file change processing.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getNonNegative,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNonNegative(java.lang.String,int)",408,417,"/**
 * Retrieves property value, validating it's non-negative.
 * @param key Property key. @param defaultValue Default value.
 * @return Property value as a long.
 */","* Return the property value if it's non-negative and throw an exception if
   * it's not.
   *
   * @param key the property key
   * @param defaultValue the default value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkIfPropertyExists,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkIfPropertyExists(java.lang.String),424,429,"/**
 * Throws exception if property key is missing from properties.
 * @param key The property key to check.
 */","* Throw a {@link MetricsException} if the given property is not set.
   *
   * @param key the key to validate",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkForErrors,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkForErrors(java.lang.String),905,910,"/**
 * Throws a MetricsException if currentOutStream.m1() returns true.
 * @param message Exception message to include.
 */","* If the sink isn't set to ignore errors, throw a {@link MetricsException}
   * if the stream encountered an exception.  The message parameter will be used
   * as the new exception's message with the current file name
   * ({@link #currentFilePath}) appended to it.
   *
   * @param message the exception message. The message will have a colon and
   * the current file name ({@link #currentFilePath}) appended to it.
   * @throws MetricsException thrown if there was an error and the sink isn't
   * ignoring errors",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,throwMetricsException,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String),940,944,"/**
 * Throws a MetricsException if ignoreError is false.
 * @param message Error message to include in exception.
 */","* If the sink isn't set to ignore errors, throw a new
   * {@link MetricsException}.  The message parameter will be used  as the
   * new exception's message with the current file name
   * ({@link #currentFilePath}) appended to it.
   *
   * @param message the exception message. The message will have a colon and
   * the current file name ({@link #currentFilePath}) appended to it.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String),29,31,"/**
 * Constructs a MetricsConfigException with the given error message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,checkMetricName,org.apache.hadoop.metrics2.lib.MetricsRegistry:checkMetricName(java.lang.String),434,452,"/**
 * Validates metric name: checks for whitespace & existence.
 * @param name The metric name to validate.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,checkTagName,org.apache.hadoop.metrics2.lib.MetricsRegistry:checkTagName(java.lang.String),454,458,"/**
 * Throws exception if tag already exists in the tag map.
 * @param name The tag name to check for existence.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsFactory.java,getInstance,org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getInstance(java.lang.Class),37,46,"/**
 * Returns a metrics factory instance or throws exception if unknown.
 * @param cls Metrics factory class.
 * @return Factory instance or throws MetricsException.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,build,org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:build(),76,92,"/**
 * Returns a MetricsSource, either from the source or a new one.
 * Throws exception if hybrid or no @Metric annotation is present.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newTag,org.apache.hadoop.metrics2.lib.MethodMetric:newTag(java.lang.Class),125,139,"/**
 * Creates a MutableMetric for String results.
 * @param resType The result type (must be String.class)
 * @return MutableMetric instance or throws exception.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getRollInterval,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getRollInterval(),341,399,"/**
 * Parses roll interval from properties and returns millis.
 * Throws MetricsException for invalid format or value.
 */","* Extract the roll interval from the configuration and return it in
   * milliseconds.
   *
   * @return the roll interval in millis",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,throwMetricsException,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String,java.lang.Throwable)",924,929,"/**
 * Throws a MetricsException if ignoreError is false.
 * @param message Error message.
 * @param t The exception that caused the error.
 */
","* If the sink isn't set to ignore errors, wrap the Throwable in a
   * {@link MetricsException} and throw it.  The message parameter will be used
   * as the new exception's message with the current file name
   * ({@link #currentFilePath}) and the Throwable's string representation
   * appended to it.
   *
   * @param message the exception message. The message will have a colon, the
   * current file name ({@link #currentFilePath}), and the Throwable's string
   * representation (wrapped in square brackets) appended to it.
   * @param t the Throwable to wrap",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/FileSink.java,init,org.apache.hadoop.metrics2.sink.FileSink:init(org.apache.commons.configuration2.SubsetConfiguration),45,55,"/**
 * Creates a print stream for metrics output, using filename from config.
 * @param conf SubsetConfiguration object containing filename.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String,java.lang.Throwable)",33,35,"/**
 * Constructs a MetricsConfigException with a message and cause.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.Throwable),37,39,"/**
 * Constructs a MetricsConfigException with a given cause.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,equals,org.apache.hadoop.metrics2.MetricsTag:equals(java.lang.Object),71,78,"/**
 * Checks if an object is a MetricsTag and matches based on info & value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,toString,org.apache.hadoop.metrics2.MetricsTag:toString(),84,89,"/**
 * Creates a formatted string with info and value from internal methods.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,appendPrefix,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:appendPrefix(org.apache.hadoop.metrics2.MetricsRecord,java.lang.StringBuilder)",86,106,"/**
 * Appends relevant metrics tags to a StringBuilder.
 * @param record MetricsRecord containing the data.
 * @param sb StringBuilder to append to.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,accepts,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(org.apache.hadoop.metrics2.MetricsTag),102,119,"/**
* Checks if a tag matches include/exclude patterns.
* @param tag The MetricsTag to check. Returns true if included.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,accepts,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.Iterable),121,142,"/**
* Checks if tags match include/exclude patterns.
* @param tags Iterable of MetricsTag objects to check.
* @return True if tags match, false otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordImpl.java,context,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:context(),72,80,"/**
 * Finds the context tag or returns the default context.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,"org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsInfo,java.lang.Object)",61,63,"/**
* Creates a MetricStringBuilder using provided info and value.
* @param info MetricsInfo object.
* @param value Object to be used in the metric.
* @return MetricStringBuilder object.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsTag),86,89,"/**
* Creates a MetricsRecordBuilder with the given tag's data.
* @param tag The MetricsTag to use for building the record.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,setContext,org.apache.hadoop.metrics2.MetricStringBuilder:setContext(java.lang.String),97,100,"/**
* Creates a MetricsRecordBuilder with the given context value.
* @param value The context value to set.
* @return A MetricsRecordBuilder instance.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/AbstractMetric.java,equals,org.apache.hadoop.metrics2.AbstractMetric:equals(java.lang.Object),75,82,"/**
 * Checks if the object is an AbstractMetric and compares info/m3.
 * @param obj Object to check, must be an AbstractMetric.
 * @return True if objects are equivalent, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/FileSink.java,putMetrics,org.apache.hadoop.metrics2.sink.FileSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),57,80,"/**
* Writes a MetricsRecord to the writer, formatting tags and metrics.
* @param record The MetricsRecord to write.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,update,"org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord,boolean)",154,177,"/**
 * Creates or retrieves a Record based on MetricsRecord data.
 * @param mr MetricsRecord to process.
 * @param includingTags Whether to include tags in the Record.
 * @return The created or retrieved Record.
 */
","* Update the cache and return the current cached record
   * @param mr the update record
   * @param includingTags cache tag values (for later lookup by name) if true
   * @return the updated cache record",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,loadGangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:loadGangliaConf(org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaConfType),185,218,"/**
 * Processes Ganglia configuration properties for a given type.
 * @param gtype Ganglia configuration type to process.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,xdr_string,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:xdr_string(java.lang.String),245,252,"/**
 * Copies a string to the buffer, updates offset, and performs actions.
 * @param s The string to be copied.
 */
","* Puts a string into the buffer by first writing the size of the string as an
   * int, followed by the bytes of the string, padded if necessary to a multiple
   * of 4.
   * @param s the string to be written to buffer at offset location",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.java,emitMetric,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",47,104,"/**
 * Emits a metric to Ganglia, handling null checks and logging.
 * @param groupName Metric group name.
 */","* The method sends metrics to Ganglia servers. The method has been taken from
   * org.apache.hadoop.metrics.ganglia.GangliaContext31 with minimal changes in
   * order to keep it in sync.
   * @param groupName The group name of the metric
   * @param name The metric name
   * @param type The type of the metric
   * @param value The value of the metric
   * @param gConf The GangliaConf for this metric
   * @param gSlope The slope for this metric
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,emitMetric,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",221,252,"/**
 * Emits a Ganglia metric with given name, type, value, and config.
 * @param groupName Metric group name.
 * @param name Metric name.
 * @param type Metric type.
 * @param value Metric value.
 */","* The method sends metrics to Ganglia servers. The method has been taken from
   * org.apache.hadoop.metrics.ganglia.GangliaContext30 with minimal changes in
   * order to keep it in sync.
   * @param groupName The group name of the metric
   * @param name The metric name
   * @param type The type of the metric
   * @param value The value of the metric
   * @param gConf The GangliaConf for this metric
   * @param gSlope The slope for this metric
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,calculateSlope,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:calculateSlope(org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",196,207,"/**
 * Returns the GangliaSlope, prioritizing gConf.m1() or slopeFromMetric.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,flush,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:flush(),176,180,"/**
* Delegates m2() call to the writer if m1() returns true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,connect,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:connect(),143,165,"/**
 * Establishes a connection to Graphite server. Throws exception on failure.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,close,org.apache.hadoop.metrics2.sink.GraphiteSink:close(),124,127,"/**
 * Delegates the m1 method call to the graphite object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,init,org.apache.hadoop.metrics2.sink.StatsDSink:init(org.apache.commons.configuration2.SubsetConfiguration),78,95,"/**
 * Initializes configuration based on SubsetConfiguration.
 * @param conf Configuration object containing initialization values.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,close,org.apache.hadoop.metrics2.sink.StatsDSink:close(),163,166,"/**
 * Delegates the m1 method call to the statsd object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,putMetrics,org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),68,82,"/**
 * Processes metrics records, adding compatible metrics to the nextPromMetrics.
 * @param metricsRecord The record containing metrics to process.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,getMetricKey,"org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:getMetricKey(java.lang.String,org.apache.hadoop.metrics2.AbstractMetric,java.util.List)",165,174,"/**
 * Masks a Prometheus metric key based on a pattern.
 * @param promMetricKey Metric key to mask.
 * @param metric Metric object.
 * @param extendTags List to add extended tags.
 * @return Masked metric key or original if no match.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,init,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:init(org.apache.commons.configuration2.SubsetConfiguration),54,84,"/**
* Processes configuration to apply include/exclude rules.
* Applies tag patterns from configuration, validating them.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsBufferBuilder.java,add,"org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:add(java.lang.String,java.lang.Iterable)",29,31,"/**
* Delegates to overloaded method with a MetricsBuffer.Entry.
* @param name Metric name.
* @param records Iterable of MetricsRecordImpl.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsBufferBuilder.java,get,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:get(),33,35,"/**
 * Creates a new MetricsBuffer, copying data from the current buffer.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:<init>(org.apache.hadoop.metrics2.impl.MetricsBuffer),240,242,"/**
 * Constructs a WaitableMetricsBuffer using the provided MetricsBuffer.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,dequeue,org.apache.hadoop.metrics2.impl.SinkQueue:dequeue(),101,108,"/**
 * Retrieves a value after waiting for a condition to be met.
 * Calls m1(), waits if size is 0, then returns the result of m3().
 */","* Dequeue one element from head of the queue, will block if queue is empty
   * @return  the first element
   * @throws InterruptedException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,clear,org.apache.hadoop.metrics2.impl.SinkQueue:clear(),154,161,"/**
 * Resets the data structure by clearing all elements and size.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,waitForData,org.apache.hadoop.metrics2.impl.SinkQueue:waitForData(),110,118,"/**
 * Retrieves a value after waiting for a condition to be met.
 * Calls m1, waits if size is 0, calls m3, then returns m4().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(org.apache.hadoop.metrics2.MetricsSystem$Callback),308,311,"/**
* Processes a callback, adding it to the callbacks list.
* @param callback The callback to be processed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)",313,315,"/**
* Registers a callback with the given name.
* @param name Callback name.
* @param callback The callback to register.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,incr,org.apache.hadoop.metrics2.lib.MutableCounterLong:incr(),42,45,"/**
* Calls m1 with default argument value of 1.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrSentBytes,org.apache.hadoop.ipc.metrics.RpcMetrics:incrSentBytes(int),256,258,"/**
* Updates the sentBytes counter by the given count.
* @param count The number of bytes to add to the counter.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrReceivedBytes,org.apache.hadoop.ipc.metrics.RpcMetrics:incrReceivedBytes(int),265,267,"/**
* Sets the size of the received bytes buffer.
* @param count The size of the buffer to set.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordImpl.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsRecordImpl:<init>(org.apache.hadoop.metrics2.MetricsInfo,long,java.util.List,java.lang.Iterable)",47,54,"/**
 * Creates a MetricsRecordImpl with the given info, timestamp, tags, and metrics.
 */
","* Construct a metrics record
   * @param info  {@link MetricsInfo} of the record
   * @param timestamp of the record
   * @param tags  of the record
   * @param metrics of the record",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,toString,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:toString(),46,54,"/**
 * Creates a formatted string from data retrieved by other methods.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,hashCode,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:hashCode(),42,44,"/**
* Returns the minimum value among the results of m1, m2, and m3.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,equals,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:equals(java.lang.Object),29,39,"/**
 * Checks if an object is a MetricsRecord and equals this one.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,newAttrInfo,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(java.lang.String,java.lang.String,java.lang.String)",53,56,"/**
* Creates an MBeanAttributeInfo with adjusted name.
* @param name Attribute name.
* @param desc Description of the attribute.
* @param type Data type of the attribute.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,setAttrCacheTag,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheTag(org.apache.hadoop.metrics2.MetricsTag,int)",279,282,"/**
 * Caches an attribute based on a tag and record number.
 * @param tag The tag containing attribute data.
 * @param recNo Record number for attribute key generation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,setAttrCacheMetric,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheMetric(org.apache.hadoop.metrics2.AbstractMetric,int)",296,299,"/**
* Caches an attribute for a given metric and record number.
* @param metric The metric object.
* @param recNo Record number for the attribute.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,refreshQueueSizeGauge,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:refreshQueueSizeGauge(),168,170,"/**
* Calculates a value using queue size and element.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/UniqueNames.java,uniqueName,org.apache.hadoop.metrics2.lib.UniqueNames:uniqueName(java.lang.String),47,65,"/**
 * Generates a unique name by incrementing a counter.
 * @param name The base name to generate a unique version of.
 * @return A unique name based on the input name.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,incr,org.apache.hadoop.metrics2.lib.MutableCounterInt:incr(),41,44,"/**
* Calls m1 with default argument value of 1.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableCounterInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",59,65,"/**
* Records metrics based on 'all' flag and m1() result.
* @param builder MetricsRecordBuilder to populate.
* @param all If true, always record; otherwise, depends on m1().
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,<init>,org.apache.hadoop.metrics2.lib.MutableQuantiles$RolloverSample:<init>(org.apache.hadoop.metrics2.lib.MutableQuantiles),236,238,"/**
 * Constructs a RolloverSample with a parent MutableQuantiles object.
 * @param parent The parent MutableQuantiles instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableQuantiles:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",129,146,"/**
 * Records metrics data using a builder, conditionally.
 * @param builder MetricsRecordBuilder to populate.
 * @param all If true, always records all metrics.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addGetGroups,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:addGetGroups(long),155,162,"/**
 * Updates group and quantile latency values.
 * @param latency The latency value to update with.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcEnQueueTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcEnQueueTime(long),277,284,"/**
 * Records enQTime to rpcEnQueueTime and quantiles (if enabled).
 * @param enQTime The enqueue time to record.
 */
","* Sometimes, the request time observed by the client is much longer than
   * the queue + process time on the RPC server.Perhaps the RPC request
   * 'waiting enQueue' took too long on the RPC server, so we should add
   * enQueue time to RpcMetrics. See HADOOP-18840 for details.
   * Add an RPC enqueue time sample
   * @param enQTime the queue time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcQueueTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcQueueTime(long),290,297,"/**
 * Records queue time. Updates rpcQueueTime and quantiles if enabled.
 */","* Add an RPC queue time sample
   * @param qTime the queue time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcLockWaitTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcLockWaitTime(long),299,306,"/**
 * Updates lock wait time statistics.
 * @param waitTime Wait time in milliseconds.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcProcessingTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcProcessingTime(long),312,319,"/**
 * Records processing time. Updates rpcProcessingTime and quantiles.
 */","* Add an RPC processing time sample
   * @param processingTime the processing time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcResponseTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcResponseTime(long),321,328,"/**
 * Records response time. Updates RPC response time and quantiles.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addDeferredRpcProcessingTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addDeferredRpcProcessingTime(long),330,337,"/**
 * Records RPC processing time.
 * @param processingTime The time taken for RPC processing.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,addWriteFileLatency,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addWriteFileLatency(long),110,116,"/**
 * Updates quantiles with write latency.
 * @param writeLatency the write latency value to update with
 */","* Add the file write latency to {@link MutableQuantiles} metrics.
   *
   * @param writeLatency file write latency in microseconds",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,addReadFileLatency,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addReadFileLatency(long),123,129,"/**
* Updates quantiles with read latency.
* @param readLatency The read latency value to update with.
*/","* Add the file read latency to {@link MutableQuantiles} metrics.
   *
   * @param readLatency file read latency in microseconds",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,<init>,org.apache.hadoop.metrics2.lib.MutableInverseQuantiles$InversePercentile:<init>(double),41,43,"/**
 * Constructs a new InversePercentile object.
 * @param inversePercentile percentile value (0-100)
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,initialize,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:initialize(java.lang.String),57,59,"/**
 * Returns a MetricsSystem instance with the given prefix.
 * @param prefix Prefix to be used for metrics.
 * @return MetricsSystem instance.
 */
","* Convenience method to initialize the metrics system
   * @param prefix  for the metrics system configuration
   * @return the metrics system instance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,instance,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:instance(),68,70,"/**
 * Returns the MetricsSystem instance.
 * Returns a MetricsSystem object.
 */
",* @return the metrics system object,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,shutdown,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:shutdown(),75,77,"/**
 * Calls the m1() method of the INSTANCE object.
 */",* Shutdown the metrics system,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,setInstance,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:setInstance(org.apache.hadoop.metrics2.MetricsSystem),87,90,"/**
 * Delegates to a private instance for metrics processing.
 * @param ms The MetricsSystem to process.
 * @return Processed MetricsSystem.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,removeMBeanName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeMBeanName(javax.management.ObjectName),113,116,"/**
* Calls m2 on INSTANCE with the result of name.m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,removeSourceName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeSourceName(java.lang.String),118,121,"/**
 * Calls the m1 method of the INSTANCE object with the given name.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTag,org.apache.hadoop.ipc.metrics.RpcMetrics:getTag(java.lang.String),449,452,"/**
 * Retrieves a MetricsTag from the registry by tag name.
 * @param tagName The name of the tag to retrieve.
 * @return The MetricsTag object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,snapshot,"org.apache.hadoop.metrics2.lib.MetricsRegistry:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",465,472,"/**
 * Populates a MetricsRecordBuilder with data from tags and metrics.
 * @param builder Builder to populate, all flag for metric processing.
 */","* Sample all the mutable metrics and put the snapshot in the builder
   * @param builder to contain the metrics snapshot
   * @param all get all the metrics even if the values are not changed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,toString,org.apache.hadoop.metrics2.lib.MetricsRegistry:toString(),474,481,"/**
 * Constructs a formatted string with data from related methods.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,getStats,org.apache.hadoop.metrics2.lib.MutableRollingAverages:getStats(long),292,314,"/**
 * Calculates averages for metrics with sufficient samples.
 * @param minSamples minimum number of samples required
 * @return Map of metric names to their calculated averages.
 */","* Retrieve a map of metric name {@literal ->} (aggregate).
   * Filter out entries that don't have at least minSamples.
   *
   * @param minSamples input minSamples.
   * @return a map of peer DataNode Id to the average latency to that
   *         node seen over the measurement period.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingSampleCount,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingSampleCount(),396,398,"/**
* Retrieves the masked RPC processing time.
*/","* Returns the number of samples that we have seen so far.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingSampleCount,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingSampleCount(),437,439,"/**
* Returns the deferred RPC processing time.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,rollOverAvgs,org.apache.hadoop.metrics2.lib.MutableRollingAverages:rollOverAvgs(),247,274,"/**
* Processes rates in the current snapshot, updating averages.
*/","* Iterates over snapshot to capture all Avg metrics into rolling structure
   * {@link MutableRollingAverages#averages}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",83,89,"/**
* Records metrics based on 'all' flag and m1() result.
* @param builder MetricsRecordBuilder to populate
* @param all If true, always record; otherwise, depends on m1()
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeLong:incr(),46,49,"/**
 * Calls m1 with default argument value of 1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeLong:decr(),60,63,"/**
 * Calls m1 with default argument value of 1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,info,"org.apache.hadoop.metrics2.lib.Interns:info(java.lang.String,java.lang.String)",117,119,"/**
* Creates a MetricsInfo object and caches it.
* @param name Metrics name.
* @param description Metrics description.
*/
","* Get a metric info object.
   * @param name Name of metric info object
   * @param description Description of metric info object
   * @return an interned metric info object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,tag,"org.apache.hadoop.metrics2.lib.Interns:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",151,153,"/**
 * Creates a MetricsTag using the provided MetricsInfo and value.
 */","* Get a metrics tag.
   * @param info  of the tag
   * @param value of the tag
   * @return an interned metrics tag",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeFloat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",52,58,"/**
* Adds metrics to builder if 'all' is true or m1() returns true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr(float),70,79,"/**
 * Iteratively updates a value until a condition is met.
 * @param delta Increment value for each iteration.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,add,"org.apache.hadoop.metrics2.lib.MutableStat:add(long,long)",123,126,"/**
 * Updates interval statistics and calls m2.
 * @param numSamples Number of samples.
 * @param sum The sum of the samples.
 */
","* Add a number of samples and their sum to the running stat
   *
   * Note that although use of this method will preserve accurate mean values,
   * large values for numSamples may result in inaccurate variance values due
   * to the use of a single step of the Welford method for variance calculation.
   * @param numSamples  number of samples
   * @param sum of the samples",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,add,org.apache.hadoop.metrics2.util.SampleStat:add(double),68,71,"/**
* Updates min/max and returns a SampleStat object.
* @param x The double value to be processed.
*/","* Add a sample the running stat.
   * @param x the sample number
   * @return  self",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingMean,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingMean(),404,406,"/**
* Calculates a value by chaining method calls on rpcProcessingTime.
*/","* Returns mean of RPC Processing Times.
   * @return double",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingMean,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingMean(),441,443,"/**
* Returns the deferred RPC processing time.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,min,org.apache.hadoop.metrics2.util.SampleStat:min(),134,136,"/**
* Delegates to minmax.m1() and returns the result.
*/",* @return  the minimum value of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,max,org.apache.hadoop.metrics2.util.SampleStat:max(),141,143,"/**
* Delegates to minmax.m1() and returns the result.
*/",* @return  the maximum value of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,org.apache.hadoop.metrics2.util.SampleStat$MinMax:reset(org.apache.hadoop.metrics2.util.SampleStat$MinMax),188,191,"/**
* Copies min and max values from the given MinMax object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,resetMinMax,org.apache.hadoop.metrics2.lib.MutableStat:resetMinMax(),177,179,"/**
* Calls the m1() method of the minMax object.
*/",* Reset the all time min max of the metric,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,org.apache.hadoop.metrics2.util.SampleStat:reset(),40,45,"/**
 * Resets internal state: numSamples, mean, s, and minmax.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,snapshot,"org.apache.hadoop.metrics2.lib.MethodMetric$1:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",141,143,"/**
* Delegates method call to the underlying implementation.
* @param builder MetricsRecordBuilder to populate
* @param all boolean flag to control record building
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newCounter,org.apache.hadoop.metrics2.lib.MethodMetric:newCounter(java.lang.Class),72,87,"/**
* Creates a MutableMetric for the given type.
* Throws MetricsException if type is unsupported.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,snapshot,"org.apache.hadoop.metrics2.lib.MethodMetric$2:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",141,143,"/**
* Delegates m1 call to the impl.
* @param builder MetricsRecordBuilder object
* @param all boolean flag to control behavior
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newGauge,org.apache.hadoop.metrics2.lib.MethodMetric:newGauge(java.lang.Class),106,123,"/**
 * Creates a MutableMetric for the given type. Throws exception if unsupported.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",83,89,"/**
 * Builds and records metrics based on 'all' flag and m1() result.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getNextTgtRenewalTime,"org.apache.hadoop.security.UserGroupInformation:getNextTgtRenewalTime(long,long,org.apache.hadoop.io.retry.RetryPolicy)",1110,1117,"/**
 * Calculates the next retry time based on target time, now, and policy.
 * @param tgtEndTime Target end time.
 * @param now Current time.
 * @param rp Retry policy.
 * @return Next retry time in milliseconds.
 */
","* Get time for next login retry. This will allow the thread to retry with
   * exponential back-off, until tgt endtime.
   * Last retry is {@link #kerberosMinSecondsBeforeRelogin} before endtime.
   *
   * @param tgtEndTime EndTime of the tgt.
   * @param now Current time.
   * @param rp The retry policy.
   * @return Time for next login retry.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeInt:incr(),46,49,"/**
* Calls m1 with default value 1.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeInt:decr(),60,63,"/**
 * Calls m1 with default value 1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,reattach,"org.apache.hadoop.metrics2.source.JvmMetrics:reattach(org.apache.hadoop.metrics2.MetricsSystem,org.apache.hadoop.metrics2.source.JvmMetrics)",125,127,"/**
 * Records JVM metrics using the provided MetricsSystem.
 * @param ms MetricsSystem to use for recording.
 * @param jvmMetrics JVM metrics to record.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getMemoryUsage,org.apache.hadoop.metrics2.source.JvmMetrics:getMemoryUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder),157,168,"/**
* Records memory metrics using the provided builder.
* Uses MemoryUsage and Runtime objects to populate metrics.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Metrics2Util.java,equals,org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:equals(java.lang.Object),56,62,"/**
 * Checks if the given object is equal to this object.
 * @param other The object to compare to.
 * @return True if equal, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,stddev,org.apache.hadoop.metrics2.util.SampleStat:stddev(),127,129,"/**
* Returns the result of applying Math.m2 to the value of m1().
*/",* @return  the standard deviation of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,compress,org.apache.hadoop.metrics2.util.SampleQuantiles:compress(),176,198,"/**
* Adjusts sample item values based on a condition.
* Iterates through samples, modifying 'g' values if criteria met.
*/","* Try to remove extraneous items from the set of sampled items. This checks
   * if an item is unnecessary based on the desired error bounds, and merges it
   * with the adjacent item if it is.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,query,org.apache.hadoop.metrics2.util.SampleQuantiles:query(double),206,228,"/**
* Estimates a value at a given quantile from the sample data.
* @param quantile quantile value between 0 and 1
* @return estimated value at the specified quantile
*/","* Get the estimated value at the specified quantile.
   * 
   * @param quantile Queried quantile, e.g. 0.50 or 0.99.
   * @return Estimated value at that quantile.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,insertBatch,org.apache.hadoop.metrics2.util.SampleQuantiles:insertBatch(),129,169,"/**
* Processes buffer data, inserts items into samples, and adjusts values.
*/","* Merges items from buffer into the samples array in one pass.
   * This is more efficient than doing an insert on every item.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Metrics2Util.java,offer,org.apache.hadoop.metrics2.util.Metrics2Util$TopN:offer(org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair),84,95,"/**
* Inserts an entry. If full, replaces smallest if larger.
* @param entry The NameValuePair to insert.
* @return True if inserted/replaced, false otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,<init>,org.apache.hadoop.metrics2.util.MetricsCache:<init>(),136,138,"/**
 * Default constructor, initializes with the default max records per name.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,tag,"org.apache.hadoop.metrics2.MetricsJsonBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",66,69,"/**
 * Creates a MetricsRecordBuilder with provided info and value.
 * @param info MetricsInfo object
 * @param value String value for the record
 * @return MetricsRecordBuilder instance
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,add,org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.MetricsTag),71,74,"/**
* Creates a MetricsRecordBuilder using provided tag data.
* @param tag tag containing metric data
* @return MetricsRecordBuilder instance
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,add,org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.AbstractMetric),76,79,"/**
* Creates a MetricsRecordBuilder from an AbstractMetric.
* @param metric The AbstractMetric to build the record from.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,setContext,org.apache.hadoop.metrics2.MetricsJsonBuilder:setContext(java.lang.String),81,84,"/**
* Creates a MetricsRecordBuilder with the given context value.
* @param value The context value to set.
* @return A MetricsRecordBuilder instance.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",86,89,"/**
* Creates a MetricsRecordBuilder with provided info and value.
* @param info MetricsInfo object
* @param value Integer value for the record
* @return MetricsRecordBuilder instance
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",91,94,"/**
* Creates a MetricsRecordBuilder with given info and value.
* @param info MetricsInfo object
* @param value The value to be associated with the metric
* @return MetricsRecordBuilder object
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",96,99,"/**
* Creates a MetricsRecordBuilder using provided info and value.
* @param info MetricsInfo object
* @param value int value to be used
* @return MetricsRecordBuilder object
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",101,104,"/**
* Creates a MetricsRecordBuilder with given info and value.
* @param info MetricsInfo object
* @param value The value to be associated with the metric
* @return A MetricsRecordBuilder instance
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",106,109,"/**
* Creates a MetricsRecordBuilder with given info and value.
* @param info MetricsInfo object
* @param value The float value to be recorded
* @return MetricsRecordBuilder object
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",111,114,"/**
* Creates a MetricsRecordBuilder with provided info and value.
* @param info MetricsInfo object
* @param value The value to be associated with the metric
* @return MetricsRecordBuilder instance
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolServerSideTranslatorPB.java,getProtocolVersion,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)",68,72,"/**
* Calls RPC.m1 with ZKFCProtocolPB.class and returns a long.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getProtocolVersion,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)",180,184,"/**
 * Calls RPC.m1 with HAServiceProtocolPB. Returns a long value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/NetgroupCache.java,getNetgroupNames,org.apache.hadoop.security.NetgroupCache:getNetgroupNames(),63,65,"/**
* Returns a new list containing the elements from m1().
*/","* Get the list of cached netgroups
   *
   * @return list of cached groups",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/NetgroupCache.java,isCached,org.apache.hadoop.security.NetgroupCache:isCached(java.lang.String),81,83,"/**
 * Delegates group processing to m1().m2().
 * @param group The group string to process.
 * @return Result of m1().m2(group).
 */
","* Returns true if a given netgroup is cached
   *
   * @param group check if this group is cached
   * @return true if group is cached, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getDefaults,org.apache.hadoop.security.UserGroupInformation$LoginParams:getDefaults(),2097,2103,"/**
 * Creates and populates LoginParams with default Kerberos settings.
 * @return LoginParams object containing Kerberos configuration.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,toLowerCase,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:toLowerCase(javax.servlet.http.HttpServletRequest),132,196,"/**
 * Wraps request, transforming parameters into a List<String>.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getServerProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getServerProperties(java.net.InetAddress,int)",103,106,"/**
* Calls m1 with only the client address.
* @param clientAddress The InetAddress of the client.
* @return A map of string key-value pairs.
*/
","* Identify the Sasl Properties to be used for a connection with a  client.
   * @param clientAddress  client's address
   * @param ingressPort the port that the client is connecting
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getClientProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getClientProperties(java.net.InetAddress,int)",123,126,"/**
 * Delegates to another m1 method.
 * @param serverAddress The server address.
 * @param ingressPort The ingress port.
 * @return A map of strings to strings.
 */
","* Identify the Sasl Properties to be used for a connection with a server.
   * @param serverAddress server's address
   * @param ingressPort the port that is used to connect to server
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,getPassword,org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:getPassword(org.apache.hadoop.security.token.TokenIdentifier),289,292,"/**
 * Masks a token using the secret manager.
 * @param tokenid Token identifier.
 * @return Masked char array.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:getGroupsSet(java.lang.String),65,68,"/**
 * Delegates the call to impl.m1(user).
 * @param user The user string.
 * @return A set of strings.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:getGroupsSet(java.lang.String),64,67,"/**
 * Delegates the call to impl.m1(user).
 * @param user The user string.
 * @return Set of strings returned by impl.m1(user)
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.CompositeGroupsMapping:getGroupsSet(java.lang.String),110,131,"/**
* Retrieves a set of groups for a user from multiple providers.
* @param user The user to fetch groups for.
* @return A set of group names.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,getEnabledConfigKey,org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getEnabledConfigKey(),71,73,"/**
* Concatenates m1() result with ENABLED_SUFFIX.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCredentialsInternal,org.apache.hadoop.security.UserGroupInformation:getCredentialsInternal(),1759,1770,"/**
 * Retrieves credentials, creating new ones if none exist.
 * Returns the Credentials object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/User.java,<init>,"org.apache.hadoop.security.User:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,javax.security.auth.login.LoginContext)",46,57,"/**
 * Creates a User object with the given name, auth method, and login context.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getHostFromPrincipal,org.apache.hadoop.security.SecurityUtil:getHostFromPrincipal(java.lang.String),353,355,"/**
 * Extracts a masked Hadoop Kerberos name from a principal name.
 */","* Get the host name from the principal name of format {@literal <}service
   * {@literal >}/host@realm.
   * @param principalName principal name of format as described above
   * @return host name if the the string conforms to the above format, else null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroupInternal,org.apache.hadoop.security.Groups:getGroupInternal(java.lang.String),242,264,"/**
* Retrieves a set of strings associated with a user.
* Returns cached or computed set; throws exception if user is blocked.
*/","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * @param user User's name
   * @return the group memberships of the user as Set
   * @throws IOException if user does not exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,refresh,org.apache.hadoop.security.Groups:refresh(),430,441,"/**
 * Clears user-to-groups cache and related caches.
 * Refreshes groups and clears negative cache if needed.
 */",* Refresh all user-to-groups mappings.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMapping.java,getGroups,org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroups(java.lang.String),79,82,"/**
* Transforms user input and returns a list of strings.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroupsSet(java.lang.String),84,90,"/**
 * Retrieves a set of group names for a given user.
 * @param user The user identifier.
 * @return A set of group names.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,close,org.apache.hadoop.security.KDiag:close(),189,195,"/**
 * Calls m1() and calls m2() on 'out' if it exists.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,println,"org.apache.hadoop.security.KDiag:println(java.lang.String,java.lang.Object[])",854,863,"/**
 * Logs a formatted message to either 'out' or System.out.
 * @param format Format string for the message.
 * @param args Arguments to format into the message.
 */
","* Print a line of output. This goes to any output file, or
   * is logged at info. The output is flushed before and after, to
   * try and stay in sync with JRE logging.
   *
   * @param format format string
   * @param args any arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,usage,org.apache.hadoop.security.KDiag:usage(),246,263,"/**
* Generates a help string describing KDiag command-line arguments.
* Returns a formatted string with argument descriptions.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapInternal,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapInternal(org.apache.hadoop.thirdparty.com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)",224,281,"/**
 * Updates a BiMap with data from a bash command's output.
 * @param map BiMap to update.
 * @param mapName Name of the map being updated.
 * @return True if the map was updated, false otherwise.
 */","* Get the list of users or groups returned by the specified command,
   * and save them in the corresponding map.
   *
   * @param map map.
   * @param mapName mapName.
   * @param command command.
   * @param staticMapping staticMapping.
   * @param regex regex.
   * @throws IOException raised on errors performing I/O.
   * @return updateMapInternal.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getRunScriptCommand,org.apache.hadoop.util.Shell:getRunScriptCommand(java.io.File),443,448,"/**
 * Executes a script based on the OS.
 * @param script The script file to execute.
 * @return Array of command to execute the script.
 */","* Returns a command to run the given script.  The script interpreter is
   * inferred by platform: cmd on Windows or bash otherwise.
   *
   * @param script File script to run
   * @return String[] command to run the script",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,valueOf,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:valueOf(org.apache.hadoop.security.SaslRpcServer$AuthMethod),1504,1512,"/**
 * Finds authentication method matching the given AuthMethod.
 * @param authMethod The AuthMethod to search for.
 * @throws IllegalArgumentException if no match is found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,switchBindUser,org.apache.hadoop.security.LdapGroupsMapping:switchBindUser(javax.naming.AuthenticationException),644,651,"/**
 * Switches bind user after an AuthenticationException.
 * Logs the switch if the new user is different.
 */","* Switch to the next available user to bind to.
   * @param e AuthenticationException encountered when contacting LDAP",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,"org.apache.hadoop.security.SaslOutputStream:write(byte[],int,int)",166,193,"/**
 * Processes input data, potentially wrapping with SASL tokens.
 * @param inBuf Input byte array.
 * @param off Offset in the byte array.
 * @param len Length of data to process.
 */","* Writes <code>len</code> bytes from the specified byte array starting at
   * offset <code>off</code> to this output stream.
   * 
   * @param inBuf
   *          the data.
   * @param off
   *          the start offset in the data.
   * @param len
   *          the number of bytes to write.
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,close,org.apache.hadoop.security.SaslOutputStream:close(),213,217,"/**
* Calls m1() and delegates to outStream's m2(), throwing IOException.
*/","* Closes this output stream and releases any system resources associated with
   * this stream.
   * 
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,init,org.apache.hadoop.security.SaslRpcServer:init(org.apache.hadoop.conf.Configuration),175,182,"/**
 * Initializes the Sasl server factory if it's not already initialized.
 * @param conf Hadoop configuration object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,createSaslServer,"org.apache.hadoop.security.SaslPlainServer$SaslPlainServerFactory:createSaslServer(java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)",48,53,"/**
 * Creates a SaslServer based on the mechanism.
 * @param mechanism Sasl mechanism name.
 * @return SaslServer or null if not PLAIN.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,getAuthorizationID,org.apache.hadoop.security.SaslPlainServer:getAuthorizationID(),127,131,"/**
 * Retrieves the authorization value after executing m1().
 * @return The authorization value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,getNegotiatedProperty,org.apache.hadoop.security.SaslPlainServer:getNegotiatedProperty(java.lang.String),133,137,"/**
 * Returns ""auth"" if QOP supports the property, otherwise null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,wrap,"org.apache.hadoop.security.SaslPlainServer:wrap(byte[],int,int)",139,145,"/**
 * Throws an exception as PLAIN doesn't support integrity/privacy.
 * @param outgoing Outgoing byte array (unused).
 * @param offset Offset in the array (unused).
 * @param len Length of the data (unused).
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,unwrap,"org.apache.hadoop.security.SaslPlainServer:unwrap(byte[],int,int)",147,153,"/**
* Throws an exception indicating PLAIN does not support integrity/privacy.
* @param incoming Input byte array (unused)
* @param offset Offset in the byte array (unused)
* @param len Length of the byte array (unused)
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createKeyStore,"org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyStore(java.lang.String,java.lang.String)",1102,1109,"/**
 * Loads a KeyStore from a file.
 * @param location Path to the KeyStore file.
 * @param password KeyStore password.
 * @return KeyStore object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,init,org.apache.hadoop.security.http.RestCsrfPreventionFilter:init(javax.servlet.FilterConfig),71,93,"/**
 * Initializes filter configuration from FilterConfig.
 * Retrieves and sets custom header, ignored methods, and agents.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,handleHttpInteraction,org.apache.hadoop.security.http.RestCsrfPreventionFilter:handleHttpInteraction(org.apache.hadoop.security.http.RestCsrfPreventionFilter$HttpInteraction),193,203,"/**
 * Handles CSRF protection: responds with error or continues.
 * @param httpInteraction The HTTP interaction object.
 */","* Handles an {@link HttpInteraction} by applying the filtering logic.
   *
   * @param httpInteraction caller's HTTP interaction
   * @throws IOException if there is an I/O error
   * @throws ServletException if the implementation relies on the servlet API
   *     and a servlet API call has failed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,initializeAllowedMethods,org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedMethods(javax.servlet.FilterConfig),165,174,"/**
 * Sets allowed HTTP methods based on config or default.
 * @param filterConfig Filter configuration object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,doCrossFilter,"org.apache.hadoop.security.http.CrossOriginFilter:doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",107,153,"/**
 * Validates and sets CORS response headers based on request.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,initializeAllowedHeaders,org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedHeaders(javax.servlet.FilterConfig),176,185,"/**
 * Configures allowed headers based on filter config, defaulting if needed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,parsePartialGroupNames,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:parsePartialGroupNames(java.lang.String,java.lang.String)",242,269,"/**
 * Extracts group names, validates against IDs, returns a set.
 * @param groupNames Comma-separated group names.
 * @param groupIDs Comma-separated group IDs.
 * @return Set of group names.
 */","* Attempt to parse group names given that some names are not resolvable.
   * Use the group id list to identify those that are not resolved.
   *
   * @param groupNames a string representing a list of group names
   * @param groupIDs a string representing a list of group ids
   * @return a linked list of group names
   * @throws PartialGroupNameException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,newLoginContext,"org.apache.hadoop.security.UserGroupInformation:newLoginContext(java.lang.String,javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration)",503,518,"/**
 * Creates a HadoopLoginContext with provided app name, subject, and config.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,login,org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:login(),2142,2155,"/**
 * Executes login, tracks login success/failure metrics.
 * Throws LoginException if login fails.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logout,org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:logout(),2157,2164,"/**
 * Executes m2, conditionally, within a synchronized block.
 * Checks login status before calling super.m2().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,createSecretKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createSecretKey(byte[]),692,694,"/**
* Delegates SecretKey generation to SecretManager.
* @param key The key to use for SecretKey generation.
* @return A SecretKey object.
*/
","* Convert the byte[] to a secret key
   * @param key the byte[] to create the secret key from
   * @return the secret key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,formatTokenId,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:formatTokenId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),83,90,"/**
 * Formats a TokenIdent object into a string representation.
 * Returns a formatted string or a default if an error occurs.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeStoredToken,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),204,211,"/**
* Removes a token from the SQL secret manager.
* @param ident TokenIdent object containing token details.
* @throws IOException if an I/O error occurs.
*/
","* Removes the existing TokenInformation from the SQL database to
   * invalidate it.
   * @param ident TokenInformation to remove from the SQL database.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Daemon.java,newThread,org.apache.hadoop.util.Daemon$DaemonFactory:newThread(java.lang.Runnable),42,45,"/**
* Creates and returns a daemon thread running the given runnable.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceStart,org.apache.hadoop.util.JvmPauseMonitor:serviceStart(),81,86,"/**
* Starts the monitor thread, calls its method, and calls super.m2().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,reset,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:reset(),182,187,"/**
 * Executes a sequence of operations: m1, allKeys.m2, m3, currentTokens.m2.
 */",* Reset all data structures and mutable state.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),350,352,"/**
* Calls m2 on allKeys with the key's m1 value and the key.
*/","* For subclasses externalizing the storage, for example Zookeeper
   * based implementations.
   *
   * @param key DelegationKey.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeStoredMasterKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey),370,377,"/**
 * Removes a delegation key using the provided key. Logs errors.
 */","* Removes the existing DelegationKey from the SQL database to
   * invalidate it.
   * @param key DelegationKey to remove from the SQL database.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addKey(org.apache.hadoop.security.token.delegation.DelegationKey),213,220,"/**
* Adds a delegation key. Throws exception if SecretManager is running.
* Updates m3 if key.m1() exceeds m2().
*/","* Add a previously used master key to cache (when NN restarts), 
   * should be called before activate().
   *
   * @param key delegation key.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),338,341,"/**
* Processes a delegation key, invoking allKeys.m2 and m3.
* @param key The delegation key to process.
* @throws IOException if an I/O error occurs.
*/
","* For subclasses externalizing the storage, for example Zookeeper
   * based implementations.
   *
   * @param key DelegationKey.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(long,byte[])",707,709,"/**
 * Constructs a DelegationTokenInformation with a password.
 * @param renewDate Renewal date for the delegation token.
 * @param password Password used for delegation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredKeys,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredKeys(),478,491,"/**
 * Removes expired delegation keys from the collection.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTokenTrackingId,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenTrackingId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),561,567,"/**
 * Retrieves delegation token information from identifier.
 * @param identifier TokenIdent object
 * @return Delegation token string or null if not found
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredStoredToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),795,797,"/**
 * Calls m1 with the provided TokenIdent.
 * @param ident The TokenIdent to pass to m1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,setExternalDelegationTokenSecretManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager),136,141,"/**
 * Replaces the current secret manager with a new one.
 * @param secretManager The new secret manager to use.
 */
","* Sets an external <code>DelegationTokenSecretManager</code> instance to
   * manage creation and verification of Delegation Tokens.
   * <p>
   * This is useful for use cases where secrets must be shared across multiple
   * services.
   *
   * @param secretManager a <code>DelegationTokenSecretManager</code> instance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,destroy,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:destroy(),154,158,"/**
* Calls secretManager.m1() if managedSecretManager is true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,stopThreads,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:stopThreads(),437,475,"/**
* Stops various token and key management components gracefully.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationTokenLoadingCache.java,isEmpty,org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:isEmpty(),57,60,"/**
* Checks if m1() returns 0.
* @return True if m1() is 0, false otherwise.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,<init>,"org.apache.hadoop.security.token.delegation.DelegationKey:<init>(int,long,javax.crypto.SecretKey)",51,53,"/**
 * Constructs a DelegationKey with an encoded SecretKey or null.
 * @param keyId Delegation key identifier.
 * @param expiryDate Expiry date of the delegation.
 * @param key The SecretKey, encoded if not null.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,getConfiguration,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getConfiguration(java.lang.String,javax.servlet.FilterConfig)",114,120,"/**
 * Loads properties, initializes them, and returns the Properties object.
 */","* It delegates to
   * {@link AuthenticationFilter#getConfiguration(String, FilterConfig)} and
   * then overrides the {@link AuthenticationHandler} to use if authentication
   * type is set to <code>simple</code> or <code>kerberos</code> in order to use
   * the corresponding implementation with delegation token support.
   *
   * @param configPrefix parameter not used.
   * @param filterConfig parameter not used.
   * @return hadoop-auth de-prefixed configuration for the filter and handler.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,initializeAuthHandler,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:initializeAuthHandler(java.lang.String,javax.servlet.FilterConfig)",204,215,"/**
 * Initializes delegation token secret manager.
 * Sets and clears the CuratorFramework instance.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/HttpUserGroupInformation.java,get,org.apache.hadoop.security.token.delegation.web.HttpUserGroupInformation:get(),36,39,"/**
 * Returns the UserGroupInformation obtained from DelegationTokenAuthenticationFilter.
 */","* Returns the remote {@link UserGroupInformation} in context for the current
   * HTTP request, taking into account proxy user requests.
   *
   * @return the remote {@link UserGroupInformation}, <code>NULL</code> if none.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:<init>(),89,92,"/**
 * Constructs a MultiSchemeDelegationTokenAuthenticationHandler.
 * Delegates to MultiSchemeAuthenticationHandler for auth.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:<init>(),49,52,"/**
 * Constructs a KerberosDelegationTokenAuthenticationHandler.
 * Initializes the parent authentication handler.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:<init>(),50,53,"/**
 * Constructs a PseudoDelegationTokenAuthenticationHandler.
 * Initializes the underlying authentication handler.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,obtainDelegationTokenAuthenticator,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:obtainDelegationTokenAuthenticator(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",128,140,"/**
 * Returns a DelegationTokenAuthenticator, creating one if null.
 * @param dta The authenticator, or null to create a new one.
 * @param connConfigurator Configures the authenticator.
 * @return A configured DelegationTokenAuthenticator.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticator.java,<init>,org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:<init>(),41,52,"/**
 * Creates an authenticator using a PseudoAuthenticator for username.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticator.java,<init>,org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator:<init>(),38,45,"/**
 * Constructs a KerberosDelegationTokenAuthenticator.
 * Delegates to KerberosAuthenticator, falls back to PseudoDelegationTokenAuthenticator.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,isManagementOperation,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:isManagementOperation(javax.servlet.http.HttpServletRequest),211,218,"/**
 * Checks if delegation token operation is allowed.
 * @param request HttpServletRequest object
 * @return True if operation is allowed, false otherwise.
 */
","* This method checks if the given HTTP request corresponds to a management
   * operation.
   *
   * @param request The HTTP request
   * @return true if the given HTTP request corresponds to a management
   *         operation false otherwise
   * @throws IOException In case of I/O error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,getDelegationToken,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getDelegationToken(javax.servlet.http.HttpServletRequest),412,421,"/**
 * Retrieves the delegation token from request header or parameter.
 * @param request HttpServletRequest object
 * @return Delegation token string, or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,hasDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:hasDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",115,132,"/**
* Checks if a delegation token is present in the URL or token.
* @param url The URL to check.
* @param token The authentication token.
* @return True if a delegation token is found, false otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,org.apache.hadoop.crypto.key.kms.KMSClientProvider$TokenSelector:<init>(),167,169,"/**
 * Default constructor. Initializes the TokenSelector with TOKEN_KIND.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,incrementDelegationTokenSeqNum,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementDelegationTokenSeqNum(),504,531,"/**
* Increments the sequence number and fetches a new range if needed.
* Returns the incremented sequence number.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,incrementCurrentKeyId,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementCurrentKeyId(),547,559,"/**
 * Increments the keyId sequence counter and returns the new value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredMasterKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey),727,754,"/**
 * Removes a ZKDTSMDelegationKey node from Zookeeper.
 * @param key The delegation key to remove.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,equals,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:equals(java.lang.Object),166,182,"/**
* Checks if two AbstractDelegationTokenIdentifiers are equal.
* Compares sequenceNumber, issueDate, maxDate, masterKeyId, owner,
* renewer, and realUser. Returns true if equal, false otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,isManaged,org.apache.hadoop.security.token.Token:isManaged(),487,489,"/**
* Delegates to m1().m2(this) and returns the result.
*/","* Is this token managed so that it can be renewed or cancelled?
   * @return true, if it can be renewed and cancelled.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,renew,org.apache.hadoop.security.token.Token:renew(org.apache.hadoop.conf.Configuration),498,501,"/**
* Delegates m2 call to the inner object, passing conf.
* @param conf Configuration object for the method.
* @return Long value returned by the inner m2 method.
*/
","* Renew this delegation token.
   * @param conf configuration.
   * @return the new expiration time
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,cancel,org.apache.hadoop.security.token.Token:cancel(org.apache.hadoop.conf.Configuration),510,513,"/**
* Delegates method execution to m1().m2 with provided config.
* @param conf Configuration object passed to the delegate.
*/","* Cancel this delegation token.
   *
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,validate,org.apache.hadoop.security.token.DtUtilShell$Get:validate(),225,239,"/**
 * Validates service URL. Returns true if valid, false otherwise.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,getCommandUsage,org.apache.hadoop.security.token.DtUtilShell:getCommandUsage(),179,187,"/**
* Generates a formatted string with various operation details.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,"org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[],int,int)",580,593,"/**
 * Reads data into the provided buffer.
 * @param buf buffer to read into, off offset, len length to read
 * @return number of bytes read
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,isValidAuthType,org.apache.hadoop.security.SaslRpcClient:isValidAuthType(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),189,199,"/**
* Checks if an authentication method is supported for the auth type.
* @param authType SaslAuth object; determines auth method compatibility.
* @return True if the auth method supports the auth type.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getInputStream,org.apache.hadoop.security.SaslRpcClient:getInputStream(java.io.InputStream),533,538,"/**
 * Wraps the input stream if m1() returns true.
 * @param in The input stream to potentially wrap.
 * @return The (potentially wrapped) input stream.
 */
","* Get SASL wrapped InputStream if SASL QoP requires unwrapping,
   * otherwise return original stream.  Can be called only after
   * saslConnect() has been called.
   * 
   * @param in - InputStream used to make the connection
   * @return InputStream that may be using SASL unwrap
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getOutputStream,org.apache.hadoop.security.SaslRpcClient:getOutputStream(java.io.OutputStream),549,558,"/**
 * Wraps OutputStream with BufferedOutputStream if m1() returns true.
 * @param out The OutputStream to wrap.
 * @return Wrapped OutputStream or original if m1() is false.
 */","* Get SASL wrapped OutputStream if SASL QoP requires wrapping,
   * otherwise return original stream.  Can be called only after
   * saslConnect() has been called.
   * 
   * @param out - OutputStream used to make the connection
   * @return OutputStream that may be using wrapping
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,disposeSasl,org.apache.hadoop.ipc.Client$Connection:disposeSasl(),546,554,"/**
 * Releases the SASL RPC client, attempting to perform m1().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,noPasswordWarning,"org.apache.hadoop.security.ProviderUtils:noPasswordWarning(java.lang.String,java.lang.String)",244,247,"/**
* Combines prefixes/suffixes with a masked key.
* @param envKey Environment key.
* @param fileKey File key.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,noPasswordError,"org.apache.hadoop.security.ProviderUtils:noPasswordError(java.lang.String,java.lang.String)",249,251,"/**
 * Concatenates NO_PASSWORD_ERROR with the result of m1.
 * @param envKey Environment key.
 * @param fileKey File key.
 * @return Combined string.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,readMoreData,org.apache.hadoop.security.SaslInputStream:readMoreData(),95,125,"/**
 * Reads and processes SASL token from input stream.
 * Returns token length or -1 if EOF is reached.
 */","* Read more data and get them processed <br>
   * Entry condition: ostart = ofinish <br>
   * Exit condition: ostart <= ofinish <br>
   * 
   * return (ofinish-ostart) (we have this many bytes for you), 0 (no data now,
   * but could have more later), or -1 (absolutely no more data)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,close,org.apache.hadoop.security.SaslInputStream:close(),341,348,"/**
* Releases resources and closes the stream.
*/","* Closes this input stream and releases any system resources associated with
   * the stream.
   * <p>
   * The <code>close</code> method of <code>SASLInputStream</code> calls the
   * <code>close</code> method of its underlying input stream.
   * 
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(),37,39,"/**
 * Constructs a new AuthorizationException with default constructor.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.Throwable),54,56,"/**
 * Constructs an AuthorizationException with a root cause.
 * @param cause The underlying exception that caused this one.
 */
","* Constructs a new exception with the specified cause and a detail
   * message of <tt>(cause==null ? null : cause.toString())</tt> (which
   * typically contains the class and detail message of <tt>cause</tt>).
   * @param  cause the cause (which is saved for later retrieval by the
   *         {@link #getCause()} method).  (A <tt>null</tt> value is
   *         permitted, and indicates that the cause is nonexistent or
   *         unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,<init>,org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:<init>(org.apache.hadoop.security.token.Token),664,667,"/**
 * Initializes with a token to extract username and password.
 * @param token Token containing user credentials.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reset,org.apache.hadoop.security.UserGroupInformation:reset(),368,379,"/**
 * Resets authentication-related configuration to default values.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getLogin,org.apache.hadoop.security.UserGroupInformation:getLogin(),522,526,"/**
 * Retrieves a HadoopLoginContext from user, or null if not.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginSuccess,org.apache.hadoop.security.UserGroupInformation:isLoginSuccess(),537,542,"/**
* Delegates m2() call to HadoopLoginContext if applicable, else returns true.
*/","This method checks for a successful Kerberos login
    * and returns true by default if it is not using Kerberos.
    *
    * @return true on successful login",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setLogin,org.apache.hadoop.security.UserGroupInformation:setLogin(javax.security.auth.login.LoginContext),528,530,"/**
 * Delegates a method call to the user object's m1 method.
 * @param login The LoginContext object to pass to user.m1()
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setLastLogin,org.apache.hadoop.security.UserGroupInformation:setLastLogin(long),548,550,"/**
* Delegates login time update to the user object.
* @param loginTime The login timestamp to be updated.
*/","* Set the last login time for logged in user
   * @param loginTime the number of milliseconds since the beginning of time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,<init>,org.apache.hadoop.security.UserGroupInformation:<init>(javax.security.auth.Subject),559,568,"/**
 * Constructs UserGroupInformation from a Subject, extracting the User principal.
 * @param subject The Subject containing the User principal.
 */
","* Create a UserGroupInformation for the given subject.
   * This does not change the subject or acquire new credentials.
   *
   * The creator of subject is responsible for renewing credentials.
   * @param subject the user's subject",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUserName,org.apache.hadoop.security.UserGroupInformation:getUserName(),1667,1671,"/**
* Delegates to user's m1() method and returns the result.
*/","* Get the user's full principal name.
   * @return the user's full principal name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hasKerberosCredentials,org.apache.hadoop.security.UserGroupInformation:hasKerberosCredentials(),574,576,"/**
* Checks if user authentication method is Kerberos.
* @return True if Kerberos, false otherwise.
*/
","* checks if logged in using kerberos
   * @return true if the subject logged via keytab or has a Kerberos TGT",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getAuthenticationMethod(),1853,1855,"/**
* Delegates authentication method call to the user object.
*/","* Get the authentication method from the subject
   * 
   * @return AuthenticationMethod in the subject, null if not present.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,fixKerberosTicketOrder,org.apache.hadoop.security.UserGroupInformation:fixKerberosTicketOrder(),1204,1233,"/**
 * Removes invalid or non-TGT Kerberos tickets from the set.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hasSufficientTimeElapsed,org.apache.hadoop.security.UserGroupInformation:hasSufficientTimeElapsed(long),1401,1410,"/**
 * Checks if Kerberos relogin is needed based on last login time.
 * @param now Current timestamp in milliseconds.
 * @return True if relogin is needed, false otherwise.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealUser,org.apache.hadoop.security.UserGroupInformation:getRealUser(),1543,1550,"/**
 * Returns the UserGroupInformation of the first RealUser found.
 * Returns null if no RealUser is found.
 */","* get RealUser (vs. EffectiveUser)
   * @return realUser running over proxy user",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getShortUserName,org.apache.hadoop.security.UserGroupInformation:getShortUserName(),1651,1653,"/**
* Delegates to user's m1() method and returns the result.
*/","* Get the user's login name.
   * @return the user's name up to the first '/' or '@'.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod),1834,1837,"/**
 * Delegates m1 call to the user object.
 * @param authMethod Authentication method to use.
 */
","* Sets the authentication method in the subject
   * 
   * @param authMethod authMethod.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.security.cert.X509Certificate)",349,360,"/**
 * Validates hostnames against certificate, re-throwing SSLException.
 * @param host Hostnames to validate.
 * @param cert X509Certificate to validate against.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.lang.String[],java.lang.String[],boolean,boolean)",362,456,"/**
* Validates certificate hostnames against provided hosts, CNs, and subjectAlts.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509TrustManager.java,<init>,"org.apache.hadoop.security.ssl.ReloadingX509TrustManager:<init>(java.lang.String,java.lang.String,java.lang.String)",72,78,"/**
 * Creates a ReloadingX509TrustManager with given type, location, and password.
 */","* Creates a reloadable trustmanager. The trustmanager reloads itself
   * if the underlying trustore file has changed.
   *
   * @param type type of truststore file, typically 'jks'.
   * @param location local path to the truststore file.
   * @param password password of the truststore file.
   * changed, in milliseconds.
   * @throws IOException thrown if the truststore could not be initialized due
   * to an IO error.
   * @throws GeneralSecurityException thrown if the truststore could not be
   * initialized due to a security error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509TrustManager.java,loadFrom,org.apache.hadoop.security.ssl.ReloadingX509TrustManager:loadFrom(java.nio.file.Path),115,123,"/**
* Reloads trust manager with certificates from the given path.
* @param path Path to certificate files; returns this object.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509KeystoreManager.java,<init>,"org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",69,77,"/**
 * Creates a ReloadingX509KeystoreManager.
 * @param type keystore type (e.g., ""JKS"", ""PKCS12"")
 * @param location keystore file location
 */
","* Construct a <code>Reloading509KeystoreManager</code>
   *
   * @param type type of keystore file, typically 'jks'.
   * @param location local path to the keystore file.
   * @param storePassword password of the keystore file.
   * @param keyPassword The password of the key.
   * @throws IOException raised on errors performing I/O.
   * @throws GeneralSecurityException thrown if create encryptor error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509KeystoreManager.java,loadFrom,org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:loadFrom(java.nio.file.Path),123,131,"/**
* Reloads the X509 keystore manager with data from the given path.
* @param path Path to the keystore file.
* @return The ReloadingX509KeystoreManager instance.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,getResource,org.apache.hadoop.util.FindClass:getResource(java.lang.String),163,165,"/**
 * Delegates URL retrieval to m1().m2(name).
 * @param name URL name; passed to m1().m2()
 * @return URL object obtained from m1().m2()
 */
","* Get the resource
   * @param name resource name
   * @return URL or null for not found",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getConfResourceAsInputStream,org.apache.hadoop.conf.Configuration:getConfResourceAsInputStream(java.lang.String),2893,2908,"/**
 * Retrieves an InputStream for the given resource name.
 * @param name Resource name to locate.
 * @return InputStream or null if resource not found/error.
 */","* Get an input stream attached to the configuration resource with the
   * given <code>name</code>.
   * 
   * @param name configuration resource name.
   * @return an input stream attached to the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getConfResourceAsReader,org.apache.hadoop.conf.Configuration:getConfResourceAsReader(java.lang.String),2917,2932,"/**
 * Creates a Reader from a resource URL.
 * @param name resource name; returns Reader or null on error.
 */","* Get a {@link Reader} attached to the configuration resource with the
   * given <code>name</code>.
   * 
   * @param name configuration resource name.
   * @return a reader attached to the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,createSSLEngine,org.apache.hadoop.security.ssl.SSLFactory:createSSLEngine(),256,268,"/**
 * Creates and configures an SSLEngine based on the current mode.
 * @return Configured SSLEngine instance.
 */","* Returns a configured SSLEngine.
   *
   * @return the configured SSLEngine.
   * @throws GeneralSecurityException thrown if the SSL engine could not
   * be initialized.
   * @throws IOException thrown if and IO error occurred while loading
   * the server keystore.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,configure,org.apache.hadoop.security.ssl.SSLFactory:configure(java.net.HttpURLConnection),358,372,"/**
 * Applies SSL configuration to HttpsURLConnection if applicable.
 * @param conn HttpURLConnection to configure, returns the same.
 */","* If the given {@link HttpURLConnection} is an {@link HttpsURLConnection}
   * configures the connection with the {@link SSLSocketFactory} and
   * {@link HostnameVerifier} of this SSLFactory, otherwise does nothing.
   *
   * @param conn the {@link HttpURLConnection} instance to configure.
   * @return the configured {@link HttpURLConnection} instance.
   *
   * @throws IOException if an IO error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,configureConnection,org.apache.hadoop.crypto.key.kms.KMSClientProvider:configureConnection(java.net.HttpURLConnection),488,500,"/**
 * Applies SSL parameters from factory to HttpsURLConnection.
 * @param conn The HttpURLConnection to configure.
 * @throws IOException if SSL configuration fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,initializeSSLContext,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),153,185,"/**
 * Initializes SSL context based on the preferred channel mode.
 * @param preferredChannelMode Desired SSL channel mode.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(),236,239,"/**
* Creates an SSL socket using the provided factory.
* @return An SSLSocket object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.Socket,java.lang.String,int,boolean)",241,248,"/**
 * Creates an SSLSocket using the given socket and parameters.
 * @param s Socket to wrap, host, port, autoClose flag.
 * @return SSLSocket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",250,257,"/**
 * Creates an SSLSocket connected to the given address and port.
 * @param address Remote address
 * @param port Remote port
 * @param localAddress Local address
 * @param localPort Local port
 * @return SSLSocket object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",259,266,"/**
 * Creates an SSLSocket connected to the specified host and port.
 * @param host Hostname. @param port Port number.
 * @return SSLSocket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int)",268,273,"/**
* Creates an SSLSocket connected to the given host and port.
* @param host The host address.
* @param port The port number.
* @return An SSLSocket object.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int)",275,280,"/**
 * Creates an SSLSocket connected to the specified host and port.
 * @param host The hostname. @param port The port number.
 * @return An SSLSocket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configured.java,<init>,org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration),39,41,"/**
 * Initializes the Configured object with the provided configuration.
 * @param conf The configuration object to use.
 */
","Construct a Configured.
   * @param conf the Configuration object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,handleExecutorTimeout,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:handleExecutorTimeout(org.apache.hadoop.util.Shell$ShellCommandExecutor,java.lang.String)",174,192,"/**
 * Checks if shell group lookup command exceeded the timeout.
 * @param executor ShellCommandExecutor instance
 * @param user User for whom groups are being retrieved
 * @return True if timeout exceeded, false otherwise.
 */","* Check if the executor had a timeout and logs the event.
   * @param executor to check
   * @param user user to log
   * @return true if timeout has occurred",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,toString,org.apache.hadoop.util.Shell$ShellCommandExecutor:toString(),1312,1325,"/**
 * Constructs a string from m1() arguments, quoting if needed.
 */","* Returns the commands of this instance.
     * Arguments with spaces in are presented with quotes round; other
     * arguments are presented raw
     *
     * @return a string representation of the object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,read,org.apache.hadoop.security.SaslRpcServer$AuthMethod:read(java.io.DataInput),262,264,"/**
* Delegates to m2 with input from DataInput.
* @param in DataInput object to read from
* @return AuthMethod object obtained from m2
*/
","* Read from in.
     *
     * @param in DataInput.
     * @throws IOException raised on errors performing I/O.
     * @return AuthMethod.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByExactName,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByExactName(java.lang.String),687,704,"/**
 * Resolves a host address to an InetAddress object.
 * @param host The hostname or IP address to resolve.
 * @return An InetAddress object or null if resolution fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKerberosEntry,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getKerberosEntry(),2232,2285,"/**
 * Creates an AppConfigurationEntry based on login parameters.
 * Configures options and control flag based on params.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,"org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:<init>(java.util.Map,java.util.Map)",572,576,"/**
 * Constructs a StaticMapping with provided UID and GID mappings.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:<init>(),545,547,"/**
 * Constructs a PassThroughMap with no initial mappings.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,addUser,org.apache.hadoop.security.authorize.AccessControlList:addUser(java.lang.String),152,159,"/**
 * Adds a user if conditions are met.
 * @param user The user to be added.
 */
","* Add user to the names of users allowed for this service.
   * 
   * @param user
   *          The user name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,addGroup,org.apache.hadoop.security.authorize.AccessControlList:addGroup(java.lang.String),167,177,"/**
* Adds a group if valid. Adds to lists if m2() returns false.
*/","* Add group to the names of groups allowed for this service.
   * 
   * @param group
   *          The group name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,removeUser,org.apache.hadoop.security.authorize.AccessControlList:removeUser(java.lang.String),185,192,"/**
* Removes a user if allowed. Throws exception if removal is restricted.
*/","* Remove user from the names of users allowed for this service.
   * 
   * @param user
   *          The user name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,removeGroup,org.apache.hadoop.security.authorize.AccessControlList:removeGroup(java.lang.String),200,208,"/**
 * Removes a group if allowed; otherwise, throws an exception.
 * @param group The group to remove.
 */
","* Remove group from the names of groups allowed for this service.
   * 
   * @param group
   *          The group name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getUsersString,org.apache.hadoop.security.authorize.AccessControlList:getUsersString(),337,339,"/**
* Delegates user processing to m1, returning the result.
*/","* Returns comma-separated concatenated single String of the set 'users'
   *
   * @return comma separated list of users",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getGroupsString,org.apache.hadoop.security.authorize.AccessControlList:getGroupsString(),346,348,"/**
* Delegates to m1 with the 'groups' parameter.
*/","* Returns comma-separated concatenated single String of the set 'groups'
   *
   * @return comma separated list of groups",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,printStackTrace,org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace(),65,68,"/**
* Calls m1 with System.err as the output stream.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getProxyGroups,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyGroups(),175,182,"/**
 * Creates a map of proxy groups from the proxy user ACL.
 * @return Map of proxy group names to user lists.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getProxyHosts,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyHosts(),184,193,"/**
 * Creates a map of proxy host names to their associated hostnames.
 * @return Map of proxy host names to their hostnames.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,innerSetCredential,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:innerSetCredential(java.lang.String,char[])",266,281,"/**
* Stores a credential entry in the keystore.
* @param alias Credential alias.
* @param material Credential material.
* @throws IOException if storing fails.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$CheckCommand:validate(),321,346,"/**
* Validates alias and provider setup; returns false if issues found.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$CreateCommand:validate(),411,436,"/**
* Validates alias and provider configuration.
* Returns false if alias is missing or provider fails.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,execute,org.apache.hadoop.security.alias.CredentialShell$CheckCommand:execute(),348,386,"/**
 * Checks the password for a credential provider alias.
 * Throws IOException if console unavailable or error occurs.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,promptForCredential,org.apache.hadoop.security.alias.CredentialShell:promptForCredential(),473,499,"/**
* Prompts user for alias password, ensuring they match.
* @return char[] Password entered by the user.
* @throws IOException If no console is available.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,warnIfTransientProvider,org.apache.hadoop.security.alias.CredentialShell$Command:warnIfTransientProvider(),172,176,"/**
* Logs a warning if the provider is transient.
* Checks provider state and logs a warning message.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,createPermissions,org.apache.hadoop.security.alias.LocalKeyStoreProvider:createPermissions(java.lang.String),80,90,"/**
 * Sets permissions based on the provided string.
 * @param perms Permissions mode string (e.g., ""700"").
 * @throws IOException if permissions string is invalid.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,isOriginalTGT,org.apache.hadoop.security.SecurityUtil:isOriginalTGT(javax.security.auth.kerberos.KerberosTicket),168,170,"/**
* Checks a Kerberos ticket's validity based on its m1 value.
* @param ticket The Kerberos ticket to validate.
*/","* Check whether the server principal is the TGS's principal
   * @param ticket the original TGT (the ticket that is obtained when a 
   * kinit is done)
   * @return true or false",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setSslConfiguration,"org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore,org.apache.zookeeper.common.ClientX509Util)",825,850,"/**
 * Configures ZK client for SSL/TLS using provided keystore/truststore.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KerberosAuthException.java,<init>,"org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.String,java.lang.Throwable)",51,54,"/**
* Constructs KerberosAuthException with message and cause.
* @param initialMsg Initial error message.
* @param cause The underlying Throwable cause.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,getServerProperties,org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.net.InetAddress),116,122,"/**
 * Returns SASL properties based on client address whitelist.
 * @param clientAddress Client's InetAddress; null uses saslProps.
 * @return Map of SASL properties.
 */
","* Identify the Sasl Properties to be used for a connection with a client.
   * @param clientAddress client's address
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,getIdentifier,"org.apache.hadoop.security.SaslRpcServer:getIdentifier(java.lang.String,org.apache.hadoop.security.token.SecretManager)",192,204,"/**
 * Retrieves a TokenIdentifier from a string ID using a SecretManager.
 * @param id Token identifier string.
 * @param secretManager SecretManager to retrieve the token.
 * @return TokenIdentifier object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,checkCodec,org.apache.hadoop.crypto.CryptoStreamUtils:checkCodec(org.apache.hadoop.crypto.CryptoCodec),75,81,"/**
 * Validates codec cipher suite; throws exception if unsupported.
 */","* AES/CTR/NoPadding or SM4/CTR/NoPadding is required.
   *
   * @param codec crypto codec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,<init>,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:<init>(),35,40,"/**
 * Constructs an OpensslAesCtrCryptoCodec, throwing exception on load failure.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getPos,org.apache.hadoop.crypto.CryptoInputStream:getPos(),580,585,"/**
* Calculates the masked function value.
* Uses m1(), subtracts outBuffer.m2() from streamOffset.
*/",Get underlying stream position.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,available,org.apache.hadoop.crypto.CryptoInputStream:available(),672,677,"/**
* Calculates a combined value by calling m1 and summing results.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFromUnderlyingStream,org.apache.hadoop.crypto.CryptoInputStream:readFromUnderlyingStream(java.nio.ByteBuffer),223,231,"/**
 * Reads data from input buffer into a temporary byte array.
 * @param inBuffer Input buffer to read from.
 * @return Number of bytes read, or -1 if an error occurred.
 */
",Read data from underlying stream.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:<init>(int,java.lang.String,org.apache.hadoop.crypto.CipherSuite,java.lang.String)",115,126,"/**
 * Creates a JceCtrCipher with specified mode, provider, suite, and name.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherSuite.java,convert,org.apache.hadoop.crypto.CipherSuite:convert(java.lang.String),84,92,"/**
 * Finds a CipherSuite by name.
 * @param name The name of the cipher suite to find.
 * @throws IllegalArgumentException if no matching suite is found.
 */
","* Convert to CipherSuite from name, {@link #algoBlockSize} is fixed for
   * certain cipher suite, just need to compare the name.
   * @param name cipher suite name
   * @return CipherSuite cipher suite",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,encrypt,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",140,143,"/**
* Delegates processing to m1, copying data between buffers.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,decrypt,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",145,148,"/**
* Delegates processing to m1, copying data between buffers.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoProtocolVersion.java,supports,org.apache.hadoop.crypto.CryptoProtocolVersion:supports(org.apache.hadoop.crypto.CryptoProtocolVersion),54,64,"/**
 * Checks if a CryptoProtocolVersion is a known supported version.
 */","* Returns if a given protocol version is supported.
   *
   * @param version version number
   * @return true if the version is supported, else false",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherOption.java,<init>,org.apache.hadoop.crypto.CipherOption:<init>(org.apache.hadoop.crypto.CipherSuite),34,36,"/**
 * Constructs a CipherOption with the provided CipherSuite.
 * @param suite the CipherSuite for this option
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,tokenizeTransformation,org.apache.hadoop.crypto.OpensslCipher:tokenizeTransformation(java.lang.String),155,179,"/**
* Parses a transformation string into a Transform object.
* @param transformation Transformation string (e.g., ""alg/mode/format"")
* @throws NoSuchAlgorithmException if transformation is invalid.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,finalize,org.apache.hadoop.crypto.OpensslCipher:finalize(),293,296,"/**
* Calls the m1 method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OpensslSecureRandom.java,next,org.apache.hadoop.crypto.random.OpensslSecureRandom:next(int),105,118,"/**
* Calculates a masked integer value from a given number of bits.
* @param numBits The number of bits to mask (0-32).
* @return The masked integer value.
*/","* Generates an integer containing the user-specified number of
   * random bits (right justified, with leading zeros).
   *
   * @param numBits number of random bits to be generated, where
   * 0 {@literal <=} <code>numBits</code> {@literal <=} 32.
   *
   * @return int an <code>int</code> containing the user-specified number
   * of random bits (right justified, with leading zeros).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersion(java.lang.String),327,350,"/**
 * Retrieves a KeyVersion for the given versionName.
 * @param versionName Version name to retrieve key for.
 * @return KeyVersion object or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,innerSetKeyVersion,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:innerSetKeyVersion(java.lang.String,java.lang.String,byte[],java.lang.String)",495,506,"/**
 * Stores a key version in the keystore and returns a KeyVersion object.
 * @param name Key name, versionName version name, material key material.
 * @param cipher Cipher algorithm used.
 * @return KeyVersion object representing the stored key.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSKeyVersion:<init>(java.lang.String,java.lang.String,byte[])",611,613,"/**
 * Constructs a KMSKeyVersion with the given key name, version, and material.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,createKeyProviderCryptoExtension,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider),605,621,"/**
 * Creates a KeyProviderCryptoExtension, extracting CryptoExtension if available.
 * @param keyProvider The KeyProvider to use.
 * @return A KeyProviderCryptoExtension instance.
 */","* Creates a <code>KeyProviderCryptoExtension</code> using a given
   * {@link KeyProvider}.
   * <p>
   * If the given <code>KeyProvider</code> implements the
   * {@link CryptoExtension} interface the <code>KeyProvider</code> itself
   * will provide the extension functionality.
   * If the given <code>KeyProvider</code> implements the
   * {@link KeyProviderExtension} interface and the KeyProvider being
   * extended by the <code>KeyProvider</code> implements the
   * {@link CryptoExtension} interface, the KeyProvider being extended will
   * provide the extension functionality. Otherwise, a default extension
   * implementation will be used.
   *
   * @param keyProvider <code>KeyProvider</code> to use to create the
   * <code>KeyProviderCryptoExtension</code> extension.
   * @return a <code>KeyProviderCryptoExtension</code> instance using the
   * given <code>KeyProvider</code>.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,close,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:close(),623,629,"/**
 * Delegates m2() call to the provider if it's valid.
 * @throws IOException if provider's m2() throws IOException
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,readObject,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:readObject(java.io.ObjectInputStream),700,705,"/**
* Reads metadata from the input stream.
* @param in Input stream containing metadata.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSMetadata:<init>(java.lang.String,int,java.lang.String,java.util.Map,java.util.Date,int)",647,650,"/**
 * Constructs a KMSMetadata object.
 * @param cipher Cipher used, bitLength, description, attributes, created, versions
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,writeObject,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:writeObject(java.io.ObjectOutputStream),694,698,"/**
* Writes metadata to the output stream.
* @param out Output stream to write the metadata to.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,printException,org.apache.hadoop.crypto.key.KeyShell:printException(java.lang.Exception),533,537,"/**
* Logs an exception message using the configured logger.
* @param e The exception to log.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$ListCommand:execute(),252,271,"/**
* Lists keys from a KeyProvider, optionally with metadata.
* Throws IOException if listing fails.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,getKeysMetadata,org.apache.hadoop.crypto.key.KeyProviderExtension:getKeysMetadata(java.lang.String[]),61,64,"/**
* Delegates metadata retrieval to keyProvider.
* @param names Metadata keys to retrieve.
* @return Array of Metadata objects.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,cleanupNewAndOld,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:cleanupNewAndOld(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",600,605,"/**
* Moves a file or directory from oldPath to newPath.
* @param newPath Destination path.
* @param oldPath Source path.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,backupToOld,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:backupToOld(org.apache.hadoop.fs.Path),622,630,"/**
 * Attempts to process a path. Returns true on success, false otherwise.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,revertFromOld,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:revertFromOld(org.apache.hadoop.fs.Path,boolean)",632,637,"/**
 * Deletes the old file if it existed.
 * @param oldPath Path to the file to delete.
 * @param fileExisted True if the file existed previously.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,deleteKey,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:deleteKey(java.lang.String),462,493,"/**
 * Removes all versions of a key and the key itself from the keystore.
 * @param name The name of the key to remove.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getAlgorithm,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:getAlgorithm(),679,682,"/**
* Returns a masked value from the metadata.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.KeyProvider:getCurrentKey(java.lang.String),496,502,"/**
 * Retrieves a KeyVersion by name.
 * @param name Key name, used to locate the version.
 * @return KeyVersion object or null if not found.
 */
","* Get the current version of the key, which should be used for encrypting new
   * data.
   * @param name the base name of the key
   * @return the version name of the current version of the key or null if the
   *    key version doesn't exist
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,generateKey,"org.apache.hadoop.crypto.key.KeyProvider:generateKey(int,java.lang.String)",545,552,"/**
 * Generates a key of specified size using the given algorithm.
 * @param size Key size in bits.
 * @param algorithm Algorithm to use for key generation.
 */","* Generates a key material.
   *
   * @param size length of the key.
   * @param algorithm algorithm to use for generating the key.
   * @return the generated key.
   * @throws NoSuchAlgorithmException no such algorithm exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,<init>,"org.apache.hadoop.crypto.key.CachingKeyProvider:<init>(org.apache.hadoop.crypto.key.KeyProvider,long,long)",91,95,"/**
 * Constructs a CachingKeyProvider with a KeyProvider and timeouts.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,invalidateCache,org.apache.hadoop.crypto.key.CachingKeyProvider:invalidateCache(java.lang.String),156,164,"/**
 * Executes a sequence of operations related to a given name.
 * @param name The name to operate on.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,invalidateCache,org.apache.hadoop.crypto.key.KeyProviderExtension:invalidateCache(java.lang.String),120,123,"/**
* Delegates the m1 call to the keyProvider.
* @param name The name to pass to keyProvider.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:execute(),511,525,"/**
 * Invalidates cache for a key within a KeyProvider.
 * Logs success/failure; re-throws IOException if occurs.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,toJSON,org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProvider$KeyVersion),95,108,"/**
 * Creates a JSON map from a KeyVersion object.
 * @param keyVersion The KeyVersion to convert.
 * @return A Map representing the KeyVersion data.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,generateEncryptedKey,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(org.apache.hadoop.crypto.Encryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,byte[],byte[])",307,325,"/**
 * Encrypts a key using the provided encryptor, key, and IV.
 * @param encryptor encryptor object
 * @param encryptionKey KeyVersion object
 * @param key key to encrypt
 * @param iv initialization vector
 * @return EncryptedKeyVersion object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,createForDecryption,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:createForDecryption(java.lang.String,java.lang.String,byte[],byte[])",103,110,"/**
 * Creates an EncryptedKeyVersion object with provided data.
 * @param keyName Key name.
 * @param encryptionKeyVersionName Version name.
 * @return EncryptedKeyVersion object.
 */
","* Factory method to create a new EncryptedKeyVersion that can then be
     * passed into {@link #decryptEncryptedKey}. Note that the fields of the
     * returned EncryptedKeyVersion will only partially be populated; it is not
     * necessarily suitable for operations besides decryption.
     *
     * @param keyName Key name of the encryption key use to encrypt the
     *                encrypted key.
     * @param encryptionKeyVersionName Version name of the encryption key used
     *                                 to encrypt the encrypted key.
     * @param encryptedKeyIv           Initialization vector of the encrypted
     *                                 key. The IV of the encryption key used to
     *                                 encrypt the encrypted key is derived from
     *                                 this IV.
     * @param encryptedKeyMaterial     Key material of the encrypted key.
     * @return EncryptedKeyVersion suitable for decryption.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,decryptEncryptedKey,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.Decryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",410,431,"/**
 * Decrypts a key version using the provided decryptor and key.
 * @param decryptor Decryptor to use for decryption.
 * @param encryptionKey Encryption key.
 * @param encryptedKeyVersion Encrypted key version.
 * @return Decrypted KeyVersion object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension.java,createKeyProviderDelegationTokenExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:createKeyProviderDelegationTokenExtension(org.apache.hadoop.crypto.key.KeyProvider),138,148,"/**
 * Creates a KeyProviderDelegationTokenExtension.
 * @param keyProvider The KeyProvider to delegate to.
 * @return A KeyProviderDelegationTokenExtension instance.
 */
","* Creates a <code>KeyProviderDelegationTokenExtension</code> using a given 
   * {@link KeyProvider}.
   * <p>
   * If the given <code>KeyProvider</code> implements the 
   * {@link DelegationTokenExtension} interface the <code>KeyProvider</code> 
   * itself will provide the extension functionality, otherwise a default 
   * extension implementation will be used.
   * 
   * @param keyProvider <code>KeyProvider</code> to use to create the 
   * <code>KeyProviderDelegationTokenExtension</code> extension.
   * @return a <code>KeyProviderDelegationTokenExtension</code> instance 
   * using the given <code>KeyProvider</code>.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$CreateCommand:validate(),431,454,"/**
* Executes a provider-dependent action, returning success/failure.
* Returns true if successful, false otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,writeJson,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:writeJson(java.lang.Object,java.io.OutputStream)",253,257,"/**
 * Serializes an object to a JSON string and writes to an OutputStream.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,checkNotEmpty,"org.apache.hadoop.util.KMSUtil:checkNotEmpty(java.lang.String,java.lang.String)",133,141,"/**
 * Validates input string 's'. Throws exception if empty.
 * @param s String to validate.
 * @param name Name of the parameter being validated.
 * @return The input string 's'.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,warmUpEncryptedKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:warmUpEncryptedKeys(java.lang.String[]),951,959,"/**
 * Processes key names, potentially throwing an IOException.
 * @param keyNames Array of key names to process.
 * @throws IOException if an error occurs during processing.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,close,org.apache.hadoop.crypto.key.kms.KMSClientProvider:close(),1195,1207,"/**
* Executes a task, handling exceptions and releasing resources.
*/",* Shutdown valueQueue executor threads,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,submitRefillTask,"org.apache.hadoop.crypto.key.kms.ValueQueue:submitRefillTask(java.lang.String,java.util.Queue)",401,443,"/**
 * Processes a key by executing a named runnable task.
 * @param keyName The name of the key being processed.
 * @param keyQueue Queue of keys to be processed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,deleteByName,org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:deleteByName(java.lang.String),187,194,"/**
 * Retrieves and executes a NamedRunnable by name.
 * @param name The name of the NamedRunnable to retrieve.
 * @return The NamedRunnable object or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getLock,org.apache.hadoop.crypto.key.kms.ValueQueue:getLock(java.lang.String),136,138,"/**
* Retrieves a ReadWriteLock for the given key.
* @param keyName The key to get the lock for.
* @return A ReadWriteLock object.
*/
","* Get the stripped lock given a key name.
   *
   * @param keyName The key name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,flush,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:flush(),557,567,"/**
 * Flushes each KMSClientProvider, logging errors if any occur.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,isTransient,org.apache.hadoop.crypto.key.KeyProviderExtension:isTransient(),56,59,"/**
 * Delegates m1() call to the key provider.
 * @return True if the key provider's m1() returns true.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,warnIfTransientProvider,org.apache.hadoop.crypto.key.KeyShell$Command:warnIfTransientProvider(),219,223,"/**
* Logs a warning if the provider is transient.
* Checks provider state and logs a warning if needed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,createKeyProviderFromUri,"org.apache.hadoop.util.KMSUtil:createKeyProviderFromUri(org.apache.hadoop.conf.Configuration,java.net.URI)",81,93,"/**
 * Instantiates a KeyProvider from a URI and configuration.
 * @param conf Configuration object
 * @param providerUri URI of the KeyProvider
 * @return KeyProvider instance
 * @throws IOException if KeyProvider instantiation fails
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,append,org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String),207,215,"/**
* Appends field to StringBuilder if field exists.
* @param field The field to append.
* @return The Builder instance.
*/
","* Append new field to the context.
     * @param field one of fields to append.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,append,"org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String,java.lang.String)",223,231,"/**
 * Adds key-value pair to the StringBuilder if both key & value exist.
 * @param key The key to add.
 * @param value The value to add.
 * @return The Builder object for chaining.
 */
","* Append new field which contains key and value to the context.
     * @param key the key of field.
     * @param value the value of field.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,appendIfAbsent,"org.apache.hadoop.ipc.CallerContext$Builder:appendIfAbsent(java.lang.String,java.lang.String)",240,251,"/**
 * Adds a key-value pair to the builder, conditionally.
 * @param key The key to add.
 * @param value The value associated with the key.
 */
","* Append new field which contains key and value to the context
     * if the key(""key:"") is absent.
     * @param key the key of field.
     * @param value the value of field.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,"org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,java.lang.String)",148,154,"/**
 * Initializes a new Builder with a context and field separator.
 * @param context Initial context string (if valid).
 * @param separator Field separator string.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RefreshResponse.java,successResponse,org.apache.hadoop.ipc.RefreshResponse:successResponse(),37,39,"/**
 * Creates and returns a RefreshResponse indicating success.
 * Returns a RefreshResponse object with status 0 and message ""Success"".
 */
","* Convenience method to create a response for successful refreshes.
   * @return void response",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,unpack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto),82,104,"/**
 * Creates a RefreshResponse from a GenericRefreshResponseProto.
 * Extracts data from the proto to populate the response object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolServerSideTranslatorPB.java,pack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:pack(java.util.Collection),66,83,"/**
 * Converts a collection of RefreshResponse to a GenericProto.
 * @param responses Collection of RefreshResponse objects.
 * @return GenericRefreshResponseCollectionProto object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ObserverRetryOnActiveException.java,<init>,org.apache.hadoop.ipc.ObserverRetryOnActiveException:<init>(java.lang.String),32,34,"/**
 * Constructs a new ObserverRetryOnActiveException with the given message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientId.java,toString,org.apache.hadoop.ipc.ClientId:toString(byte[]),52,62,"/**
 * Generates a UUID from a byte array.
 * @param clientId byte array representing the client ID
 * @return UUID string representation
 */
","* @return Convert a clientId byte[] to string.
   * @param clientId input clientId.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long)",72,82,"/**
 * Constructs a CacheEntry with client ID, call ID, and expiration time.
 * @param clientId Client identifier (16 bytes).
 * @param callId Call identifier.
 * @param expirationTime Expiration timestamp.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,isServerFailOverEnabledByQueue,org.apache.hadoop.ipc.CallQueueManager:isServerFailOverEnabledByQueue(),242,249,"/**
 * Checks if the BlockingQueue is a FairCallQueue and returns its status.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.ipc.Schedulable),722,725,"/**
* Delegates to callQueue.m1(e).
* @param e The schedulable element.
* @return Result from callQueue.m1(e).
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isClientBackoffEnabled,org.apache.hadoop.ipc.Server:isClientBackoffEnabled(),3872,3874,"/**
* Delegates the call to the internal callQueue's m1 method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,addInternal,"org.apache.hadoop.ipc.CallQueueManager:addInternal(org.apache.hadoop.ipc.Schedulable,boolean)",306,321,"/**
 * Attempts to put a reference, handling backoff and exceptions.
 * @param e reference to put
 * @param checkBackoff whether to check backoff conditions
 * @return true on success, throws exception on failure.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,offer,org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object),335,338,"/**
* Delegates the operation to the putRef's m2 method.
* @param e the element to be processed
* @return the result of the delegated operation
*/
","* Insert e into the backing queue.
   * Return true if e is queued.
   * Return false if the queue is full.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,offer,"org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)",340,344,"/**
* Delegates putRef.m2() call.
* @param e element to put, timeout, unit - passed to m2()
* @return result of putRef.m2()
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getCallQueueLen,org.apache.hadoop.ipc.Server:getCallQueueLen(),3868,3870,"/**
* Calls the m1 method of the callQueue and returns the result.
*/","* The number of rpc calls in the queue.
   * @return The number of rpc calls in the queue.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolInterfaces,org.apache.hadoop.ipc.RPC:getProtocolInterfaces(java.lang.Class),144,147,"/**
* Retrieves and processes interfaces from a protocol class.
* @param protocol Class containing interface definitions
* @return Array of processed interface classes
*/
","* Get all interfaces that the given protocol implements or extends
   * which are assignable from VersionedProtocol.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getServerAddress,org.apache.hadoop.ipc.RPC:getServerAddress(java.lang.Object),740,742,"/**
 * Retrieves an InetSocketAddress from a proxy object.
 * Uses m1 and m2 methods on the proxy.
 */
","* @return Returns the server address for a given proxy.
   * @param proxy input proxy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,org.apache.hadoop.ipc.CallerContext:<init>(org.apache.hadoop.ipc.CallerContext$Builder),71,74,"/**
 * Constructs a CallerContext using values from the Builder.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,toString,org.apache.hadoop.ipc.CallerContext:toString(),112,123,"/**
 * Generates a string based on context and signature.
 * @return Combined string or empty string if m1() fails.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,sendPing,org.apache.hadoop.ipc.Client$Connection:sendPing(),1071,1080,"/**
* Sends a ping request if the activity interval has elapsed.
* Updates lastActivity time.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,registerProtocolEngine,"org.apache.hadoop.ipc.Server:registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)",288,300,"/**
* Registers an RPC kind with a wrapper class and invoker.
* @param rpcKind RPC kind to register.
* @param rpcRequestWrapperClass Wrapper class for the RPC request.
* @param rpcInvoker Invoker for the RPC request.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ExternalCall.java,get,org.apache.hadoop.ipc.ExternalCall:get(),47,53,"/**
 * Executes m1(), throws ExecutionException on error, returns result.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,monotonicNowNanos,org.apache.hadoop.util.Timer:monotonicNowNanos(),58,60,"/**
 * Delegates to Time.m1() and returns the result.
 */","* Same as {@link #monotonicNow()} but returns its result in nanoseconds.
   * Note that this is subject to the same resolution constraints as
   * {@link System#nanoTime()}.
   * @return a monotonic clock that counts in nanoseconds.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getUserGroupInformation,org.apache.hadoop.ipc.Server$Call:getUserGroupInformation(),1112,1115,"/**
* Delegates to m1() and returns the UserGroupInformation.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteUser,org.apache.hadoop.ipc.Server:getRemoteUser(),445,448,"/**
* Retrieves UserGroupInformation from the current Call object.
* Returns null if the current Call object is null.
*/","Returns the RPC remote user when invoked inside an RPC.  Note this
   *  may be different than the current user if called within another doAs
   *  @return connection's UGI or null if not an RPC",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doResponse,org.apache.hadoop.ipc.Server$Call:doResponse(java.lang.Throwable),1105,1107,"/**
* Calls m1 with a fatal status code.
* @param t The Throwable to be handled.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler$DecayTask:<init>(org.apache.hadoop.ipc.DecayRpcScheduler,java.util.Timer)",210,213,"/**
 * Constructs a DecayTask with a scheduler and timer.
 * @param scheduler DecayRpcScheduler reference
 * @param timer Timer instance for scheduling
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,putVersionSignatureMap,"org.apache.hadoop.ipc.RpcClientUtil:putVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String,java.util.Map)",86,89,"/**
* Updates protocol signature map with address, protocol, and RPC kind.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,getVersionSignatureMap,"org.apache.hadoop.ipc.RpcClientUtil:getVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String)",91,94,"/**
 * Retrieves protocol signature from cache using provided key.
 * @param addr Socket address, protocol, and RPC kind for lookup.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getFingerprints,org.apache.hadoop.ipc.ProtocolSignature:getFingerprints(java.lang.reflect.Method[]),121,130,"/**
 * Calculates hash codes for an array of methods.
 * @param methods Array of methods to process.
 * @return Array of integer hash codes.
 */
","* Convert an array of Method into an array of hash codes
   * 
   * @param methods
   * @return array of hash codes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,convertProtocolSignatureProtos,org.apache.hadoop.ipc.RpcClientUtil:convertProtocolSignatureProtos(java.util.List),149,161,"/**
 * Converts ProtocolSignatureProto list to a map of signatures.
 * @param protoList List of ProtocolSignatureProto objects.
 * @return Map of Long to ProtocolSignature.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,methodExists,"org.apache.hadoop.ipc.RpcClientUtil:methodExists(int,long,java.util.Map)",163,174,"/**
 * Checks if a method hash is present in the protocol signature.
 * @param methodHash Hash of the method to check.
 * @param version Protocol version.
 * @param versionMap Map of versions to signatures.
 * @return True if the method hash is found, false otherwise.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RefreshRegistry.java,dispatch,"org.apache.hadoop.ipc.RefreshRegistry:dispatch(java.lang.String,java.lang.String[])",94,131,"/**
 * Retrieves refresh responses for a given identifier.
 * @param identifier Identifier for refresh requests.
 * @param args Arguments passed to the handlers.
 * @return Collection of RefreshResponse objects.
 */
","* Lookup the responsible handler and return its result.
   * This should be called by the RPC server when it gets a refresh request.
   * @param identifier the resource to refresh
   * @param args the arguments to pass on, not including the program name
   * @throws IllegalArgumentException on invalid identifier
   * @return the response from the appropriate handler",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,<init>,"org.apache.hadoop.ipc.RemoteException:<init>(java.lang.String,java.lang.String)",40,42,"/**
* Constructs a RemoteException with a class name and message.
* @param className Name of the remote class.
* @param msg Error message.
*/
","* @param className wrapped exception, may be null
   * @param msg may be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,unwrapRemoteException,org.apache.hadoop.ipc.RemoteException:unwrapRemoteException(java.lang.Class[]),81,96,"/**
 * Attempts to find and return an IOException handler.
 * Returns 'this' if no suitable handler is found.
 */","* If this remote exception wraps up one of the lookupTypes
   * then return this exception.
   * <p>
   * Unwraps any IOException.
   * 
   * @param lookupTypes the desired exception class. may be null.
   * @return IOException, which is either the lookupClass exception or this.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,unwrapRemoteException,org.apache.hadoop.ipc.RemoteException:unwrapRemoteException(),107,115,"/**
* Retrieves an IOException class from a class object.
* Returns 'this' if an exception occurs.
*/","* Instantiate and return the exception wrapped up by this remote exception.
   * 
   * <p> This unwraps any <code>Throwable</code> that has a constructor taking
   * a <code>String</code> as a parameter.
   * Otherwise it returns this.
   * 
   * @return <code>Throwable</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getNumInProcessHandler,org.apache.hadoop.ipc.metrics.RpcMetrics:getNumInProcessHandler(),157,160,"/**
* Returns the number of in-process handlers from the server.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTotalRequests,org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequests(),175,178,"/**
* Delegates to server.m1() and returns the result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTotalRequestsPerSecond,org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequestsPerSecond(),180,183,"/**
* Returns the number of requests per second from the server.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[],org.apache.hadoop.tracing.Span,org.apache.hadoop.ipc.CallerContext)",1000,1012,"/**
 * Initializes a new RPC call with provided parameters.
 * @param id Call ID, retry count, RPC kind, client ID, span, context.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,get,"org.apache.hadoop.ipc.ProcessingDetails:get(org.apache.hadoop.ipc.ProcessingDetails$Timing,java.util.concurrent.TimeUnit)",73,75,"/**
* Calculates a time duration based on the given type and unit.
* @param type Timing type; @param timeUnit TimeUnit for duration
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,toString,org.apache.hadoop.ipc.ProcessingDetails:toString(),97,108,"/**
 * Formats timing data into a string.
 * Iterates through timings and appends formatted data.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedTimeCostProvider.java,getCost,org.apache.hadoop.ipc.WeightedTimeCostProvider:getCost(org.apache.hadoop.ipc.ProcessingDetails),100,109,"/**
 * Calculates a cost based on processing details and weights.
 * @param details ProcessingDetails object containing data.
 * @return The calculated cost as a long.
 */
","* Calculates a weighted sum of the times stored on the provided processing
   * details to be used as the cost in {@link DecayRpcScheduler}.
   *
   * @param details Processing details
   * @return The weighted sum of the times. The returned unit is the same
   *         as the default unit used by the provided processing details.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,set,"org.apache.hadoop.ipc.ProcessingDetails:set(org.apache.hadoop.ipc.ProcessingDetails$Timing,long,java.util.concurrent.TimeUnit)",81,83,"/**
* Calls m2 with the converted value using TimeUnit.
* @param type Timing type
* @param value Value to be converted
* @param timeUnit Time unit of the value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getSchedulingDecisionSummary,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getSchedulingDecisionSummary(),904,912,"/**
 * Delegates to the delegate's m2() method.
 * Returns ""No Active Scheduler"" if delegate's m1() returns null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getUniqueIdentityCount,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getUniqueIdentityCount(),924,932,"/**
 * Delegates to the delegate's m2() method. Returns -1 if delegate's m1() returns null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getTotalCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getTotalCallVolume(),934,942,"/**
 * Delegates to the delegate's m2() method.
 * Returns the result or -1 if the delegate's m2() is null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getAverageResponseTime,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getAverageResponseTime(),944,952,"/**
 * Delegates to the delegate's m2() or returns a default value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getResponseTimeCountInLastWindow,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getResponseTimeCountInLastWindow(),954,961,"/**
 * Retrieves call count data, using delegate or default if null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumDroppedConnections,org.apache.hadoop.ipc.Server:getNumDroppedConnections(),3859,3862,"/**
 * Delegates to connectionManager.m1() and returns the result.
 */","* The number of RPC connections dropped due to
   * too many connections.
   * @return the number of dropped rpc connections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isFull,org.apache.hadoop.ipc.Server$ConnectionManager:isFull(),4088,4091,"/**
* Checks if m1() exceeds the maximum allowed connections.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumOpenConnections,org.apache.hadoop.ipc.Server:getNumOpenConnections(),3837,3839,"/**
 * Delegates to connectionManager's m1() method and returns the result.
 */","* The number of open RPC conections
   * @return the number of open rpc connections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getConnections,org.apache.hadoop.ipc.Server:getConnections(),756,759,"/**
 * Retrieves an array of connections from the connection manager.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,startIdleScan,org.apache.hadoop.ipc.Server$ConnectionManager:startIdleScan(),4159,4161,"/**
 * Calls the m1 method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,putQueue,"org.apache.hadoop.ipc.FairCallQueue:putQueue(int,org.apache.hadoop.ipc.Schedulable)",229,233,"/**
 * Adds an element to the queue with given priority and processes.
 * @param priority Element priority.
 * @param e The element to be added.
 */
","* Put the element in a queue of a specific priority.
   * @param priority - queue priority
   * @param e - element to add",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offerQueue,"org.apache.hadoop.ipc.FairCallQueue:offerQueue(int,org.apache.hadoop.ipc.Schedulable)",241,248,"/**
 * Executes a task based on priority and element.
 * @param priority Task priority.
 * @param e Element to be processed.
 * @return True if task executed successfully, false otherwise.
 */
","* Offer the element to queue of a specific priority.
   * @param priority - queue priority
   * @param e - element to add
   * @return boolean if added to the given queue",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,"org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable,long,java.util.concurrent.TimeUnit)",269,279,"/**
 * Offers an element to a queue with a timeout.
 * @param e element to offer, timeout duration, unit of time
 * @return True if offer successful, false if timeout.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable),281,290,"/**
 * Adds element to the appropriate queue based on priority.
 * @param e element to add; priority determined by e.m1()
 * @return True if added successfully, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,drainTo,org.apache.hadoop.ipc.FairCallQueue:drainTo(java.util.Collection),366,369,"/**
 * Delegates to overloaded method with default maxValue.
 * @param c Collection to process; delegates to another method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addTerseExceptions,org.apache.hadoop.ipc.Server:addTerseExceptions(java.lang.Class[]),174,176,"/**
* Delegates exception class array to exceptionsHandler for processing.
*/","* Add exception classes for which server won't log stack traces.
   *
   * @param exceptionClass exception classes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addSuppressedLoggingExceptions,org.apache.hadoop.ipc.Server:addSuppressedLoggingExceptions(java.lang.Class[]),183,185,"/**
* Delegates exception class handling to exceptionsHandler.
* @param exceptionClass Array of exception classes to handle.
*/","* Add exception classes which server won't log at all.
   *
   * @param exceptionClass exception classes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,logException,"org.apache.hadoop.ipc.Server:logException(org.slf4j.Logger,java.lang.Throwable,org.apache.hadoop.ipc.Server$Call)",3244,3262,"/**
* Logs exceptions based on handler rules.
* @param logger Logger instance for logging.
* @param e Throwable exception to log.
* @param call Call object related to the exception.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getSupportedProtocolVersions,"org.apache.hadoop.ipc.RPC$Server:getSupportedProtocolVersions(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",1146,1164,"/**
* Filters VerProtocolImpl based on protocol name.
* @param rpcKind RPC kind.
* @param protocolName Protocol name to filter by.
* @return Filtered VerProtocolImpl array or null.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getHighestSupportedProtocol,"org.apache.hadoop.ipc.RPC$Server:getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",1166,1187,"/**
 * Finds the highest version protocol for a given RPC kind and name.
 * @param rpcKind RPC kind.
 * @param protocolName Protocol name.
 * @return VerProtocolImpl object or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UnexpectedServerException.java,<init>,org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String),32,34,"/**
* Constructs an UnexpectedServerException with the given message.
*/
","* Constructs exception with the specified detail message.
   * 
   * @param messages detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcServerException.java,<init>,org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String),33,35,"/**
 * Constructs a new RpcServerException with the given error message.
 */
","* Constructs exception with the specified detail message.
   * @param message detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientException.java,<init>,org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String),31,33,"/**
 * Constructs an RPC client exception with the given error message.
 */
","* Constructs exception with the specified detail message.
   * 
   * @param messages detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UnexpectedServerException.java,<init>,"org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String,java.lang.Throwable)",45,47,"/**
* Constructs an UnexpectedServerException with a message and cause.
*/
","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause that cause this exception
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcServerException.java,<init>,"org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String,java.lang.Throwable)",45,47,"/**
 * Constructs a new RpcServerException with a message and cause.
 */
","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientException.java,<init>,"org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String,java.lang.Throwable)",44,46,"/**
* Constructs an RpcClientException with a message and cause.
*/
","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause that cause this exception
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,setCapacity,org.apache.hadoop.ipc.ResponseBuffer:setCapacity(int),60,62,"/**
* Delegates m1 call to the FramedBuffer object.
* @param capacity The capacity value to be passed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,reset,org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:reset(),98,102,"/**
* Resets the count to FRAMING_BYTES and calls m1(0).
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,getFramedBuffer,org.apache.hadoop.ipc.ResponseBuffer:getFramedBuffer(),42,46,"/**
* Sets the 'written' value on the output FramedBuffer.
* @return The FramedBuffer object with the value set.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addCost,"org.apache.hadoop.ipc.DecayRpcScheduler:addCost(java.lang.Object,long)",568,600,"/**
* Updates call costs based on identity and cost delta.
* @param identity Object identifying the call.
* @param costDelta Amount to adjust call costs by.
*/","* Adjust the stored cost for a given identity.
   *
   * @param identity the identity of the user whose cost should be adjusted
   * @param costDelta the cost to add for the given identity",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,computePriorityLevel,"org.apache.hadoop.ipc.DecayRpcScheduler:computePriorityLevel(long,java.lang.Object)",609,634,"/**
* Calculates a priority level based on cost, identity, and thresholds.
*/","* Given the cost for an identity, compute a scheduling decision.
   *
   * @param cost the cost for an identity
   * @param identity the identity of the user
   * @return scheduling decision from 0 to numLevels - 1",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,setPriorityLevel,"org.apache.hadoop.ipc.DecayRpcScheduler:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",693,699,"/**
 * Sets user priority, logs action, and stores it.
 * @param ugi UserGroupInformation object.
 * @param priority Initial priority value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getCallVolumeSummary,org.apache.hadoop.ipc.DecayRpcScheduler:getCallVolumeSummary(),1127,1133,"/**
 * Calls m2(), then WRITER.m3() and handles exceptions.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,wrap,org.apache.hadoop.ipc.RpcWritable$Buffer:wrap(java.nio.ByteBuffer),145,147,"/**
 * Creates a new Buffer from the given ByteBuffer.
 * @param bb The ByteBuffer to wrap.
 * @return A new Buffer object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>(),514,515,"/**
 * Default constructor for the RpcProtobufRequest class.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto,com.google.protobuf.Message)",517,520,"/**
 * Constructs an RpcProtobufRequest with a header and payload.
 * @param header The request header.
 * @param payload The request payload.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>(),652,653,"/**
 * Default constructor for the RpcProtobufRequest class.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto,org.apache.hadoop.thirdparty.protobuf.Message)",655,658,"/**
 * Constructs an RpcProtobufRequest with a header and payload.
 * @param header Request header information.
 * @param payload Protobuf message payload.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getRemoteException,org.apache.hadoop.ipc.ProtobufHelper:getRemoteException(org.apache.hadoop.thirdparty.protobuf.ServiceException),56,58,"/**
 * Delegates exception handling to ShadedProtobufHelper.
 * @param se The ServiceException to handle.
 * @return An IOException representing the handled exception.
 */
","* Return the IOException thrown by the remote server wrapped in
   * ServiceException as cause.
   * @param se ServiceException that wraps IO exception thrown by the server
   * @return Exception wrapped in ServiceException or
   *         a new IOException that wraps the unexpected ServiceException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,ipc,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:ipc(org.apache.hadoop.ipc.internal.ShadedProtobufHelper$IpcCall),158,164,"/**
* Executes an IpcCall and handles ServiceException.
* @param call The IpcCall to execute.
* @return Result of the call or throws an IOException.
*/
","* Evaluate a protobuf call, converting any ServiceException to an IOException.
   * @param call invocation to make
   * @return the result of the call
   * @param <T> type of the result
   * @throws IOException any translated protobuf exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(java.lang.String),94,96,"/**
* Delegates to ShadedProtobufHelper.m1 to generate a ByteString.
* @param key The key used by the helper method.
* @return The generated ByteString.
*/
","* Get the ByteString for frequently used fixed and small set strings.
   * @param key string
   * @return ByteString for frequently used fixed and small set strings.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getByteString,org.apache.hadoop.ipc.ProtobufHelper:getByteString(byte[]),104,107,"/**
* Delegates ByteString creation to ShadedProtobufHelper.
* @param bytes The byte array to create ByteString from.
* @return A ByteString object.
*/
","* Get the byte string of a non-null byte array.
   * If the array is 0 bytes long, return a singleton to reduce object allocation.
   * @param bytes bytes to convert.
   * @return a value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,skipRetryCache,"org.apache.hadoop.ipc.RetryCache:skipRetryCache(byte[],int)",206,211,"/**
 * Checks conditions: server unavailable, invalid callId, or dummy clientId.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,setState,"org.apache.hadoop.ipc.RetryCache:setState(org.apache.hadoop.ipc.RetryCache$CacheEntry,boolean)",382,387,"/**
 * Updates the CacheEntry's status based on the success flag.
 * @param e CacheEntry to update; null safe.
 * @param success Boolean indicating success/failure.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,toString,org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:toString(),538,547,"/**
 * Constructs a string from header fields.
 * Returns concatenated string of m2 and m3 from header.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setCallIdAndRetryCount,"org.apache.hadoop.ipc.Client:setCallIdAndRetryCount(int,int,java.lang.Object)",122,128,"/**
* Executes a function with given ID, retry count, and handler.
* @param cid Call ID
* @param rc Retry count
* @param externalHandler External handler object
*/
","* Set call id and retry count for the next call.
   * @param cid input cid.
   * @param rc input rc.
   * @param externalHandler input externalHandler.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client:close(),1881,1885,"/**
 * Calls the m1 method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,checkAsyncCall,org.apache.hadoop.ipc.Client:checkAsyncCall(),1430,1442,"/**
 * Checks async call limit and throws exception if exceeded.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getListenerAddress,org.apache.hadoop.ipc.Server:getListenerAddress(),3751,3753,"/**
 * Retrieves the address of the listener.
 * @return InetSocketAddress representing the listener's address.
 */","* Return the socket (ip+port) on which the RPC server is listening to.
   * @return the socket (ip+port) on which the RPC server is listening to.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuxiliaryListenerAddresses,org.apache.hadoop.ipc.Server:getAuxiliaryListenerAddresses(),3762,3770,"/**
 * Retrieves a set of auxiliary listener addresses.
 * Returns an empty set if no listeners are configured.
 */","* Return the set of all the configured auxiliary socket addresses NameNode
   * RPC is listening on. If there are none, or it is not configured at all, an
   * empty set is returned.
   * @return the set of all the auxiliary addresses on which the
   *         RPC server is listening on.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doStop,org.apache.hadoop.ipc.Server$Listener:doStop(),1673,1688,"/**
 * Releases resources: calls selector, closes channel, and closes readers.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,stopClient,org.apache.hadoop.ipc.ClientCache:stopClient(org.apache.hadoop.ipc.Client),99,120,"/**
* Removes a client from the cache if its reference count is zero.
* @param client The client to potentially remove.
*/","* Stop a RPC client connection 
   * A RPC client is closed only when its reference count becomes zero.
   *
   * @param client input client.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,clearClientCache,org.apache.hadoop.ipc.ProtobufRpcEngine2:clearClientCache(),392,395,"/**
 * Calls the m1 method on the CLIENTS object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getHostInetAddress,org.apache.hadoop.ipc.Server$RpcCall:getHostInetAddress(),1224,1227,"/**
 * Delegates the call to connection.m1().
 * @return InetAddress object returned by connection.m1()
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemotePort,org.apache.hadoop.ipc.Server$RpcCall:getRemotePort(),1229,1232,"/**
 * Delegates the call to the wrapped connection's m1 method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setDeferredResponse,org.apache.hadoop.ipc.Server$RpcCall:setDeferredResponse(org.apache.hadoop.io.Writable),1347,1365,"/**
 * Sends a successful response if the connection is running.
 * Sends an error log if setup fails.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,toString,org.apache.hadoop.ipc.Server$RpcCall:toString(),1404,1407,"/**
* Appends rpcRequest and connection to the superclass's m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,waitForWork,org.apache.hadoop.ipc.Client$Connection:waitForWork(),1036,1062,"/**
 * Determines if the connection should be kept alive based on conditions.
 * Returns true to keep alive, false otherwise.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleConnectionTimeout,"org.apache.hadoop.ipc.Client$Connection:handleConnectionTimeout(int,int,java.io.IOException)",921,932,"/**
 * Retries connection if retries are less than maxRetries, logs attempt.
 * @param curRetries Current retry count.
 * @param maxRetries Maximum allowed retries.
 * @param ioe The IOException to throw if max retries are reached.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleConnectionFailure,"org.apache.hadoop.ipc.Client$Connection:handleConnectionFailure(int,java.io.IOException)",934,968,"/**
 * Retries connection with retry policy, logs failure/success.
 * @param curRetries Number of retries already attempted.
 * @param ioe The IOException that occurred.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,equals,org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:equals(java.lang.Object),168,171,"/**
* Delegates the call to the superclass's m1 method.
* @param obj The object passed to the super method.
* @return The result of the superclass's m1 method.
*/
",Override equals to avoid findbugs warnings,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getQueueSizes,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getQueueSizes(),438,446,"/**
 * Delegates to the FairCallQueue's m2 method.
 * Returns an empty array if the queue is null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getOverflowedCalls,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getOverflowedCalls(),448,456,"/**
 * Delegates to the FairCallQueue's m2() method.
 * Returns an empty array if the queue is null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufWrapperLegacy.java,<init>,org.apache.hadoop.ipc.ProtobufWrapperLegacy:<init>(java.lang.Object),51,56,"/**
 * Creates a ProtobufWrapperLegacy with the given message.
 * @param message The protobuf message to wrap.
 * @throws IllegalArgumentException if not an unshaded protobuf.
 */
","* Construct.
   * The type of the parameter is Object so as to keep the casting internal
   * to this class.
   * @param message message to wrap.
   * @throws IllegalArgumentException if the class is not a protobuf message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,switchToSimple,org.apache.hadoop.ipc.Server$Connection:switchToSimple(),2401,2405,"/**
 * Sets authentication protocol to NONE and calls method m1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,close,org.apache.hadoop.ipc.Server$Connection:close(),3082,3094,"/**
 * Releases resources: calls m1, shuts down socket, logs channel.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processSaslToken,org.apache.hadoop.ipc.Server$Connection:processSaslToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2387,2399,"/**
 * Processes a SASL message, evaluates the token, and returns a state.
 * @param saslMessage The SASL message to process.
 * @return SaslState representing the processing result.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,checkDataLength,org.apache.hadoop.ipc.Server$Connection:checkDataLength(int),2446,2459,"/**
 * Validates dataLength; throws IOException if invalid.
 * @param dataLength Length of data; must be non-negative & <= maxDataLength.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseOldVersionFatal,"org.apache.hadoop.ipc.Server:setupResponseOldVersionFatal(java.io.ByteArrayOutputStream,org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)",3627,3639,"/**
* Writes error response to output stream.
* @param response Output stream for response data.
* @param call RPC call context.
* @param rv Writable object (unused).
* @param errorClass Error class.
* @param error Error message.
*/","* Setup response for the IPC Call on Fatal Error from a 
   * client that is using old version of Hadoop.
   * The response is serialized using the previous protocol's response
   * layout.
   * 
   * @param response buffer to serialize the response into
   * @param call {@link Call} to which we are setting up the response
   * @param rv return value for the IPC Call, if the call was successful
   * @param errorClass error class, if the the call failed
   * @param error error message, if the call failed
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRpcRequestWrapper,org.apache.hadoop.ipc.Server:getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto),302,308,"/**
 * Gets the RPC request wrapper class for the given RPC kind.
 * @param rpcKind The RPC kind to look up.
 * @return RPC request wrapper class or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,toString,org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:toString(),676,685,"/**
 * Combines header fields from a request.
 * Returns a string of m2 and m3 separated by a dot.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,hashCode,org.apache.hadoop.ipc.RetryCache$CacheEntry:hashCode(),94,97,"/**
 * Combines clientId and callId to generate a unique hash code.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,advanceIndex,org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:advanceIndex(),121,132,"/**
* Checks remaining requests and triggers action if zero.
*/","* Advances the index, which will change the current index
   * if called enough times.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:<init>(),398,403,"/**
 * Constructor: Initializes the callback with server, call, and method name.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:<init>(),430,435,"/**
 * Constructor: Initializes callback with server, call, and method name.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,error,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable),412,418,"/**
 * Records processing time and exception details to the server.
 * @param t The exception that occurred during processing.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,error,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable),444,450,"/**
 * Records error metrics and calls the next handler.
 * @param t The exception to be processed.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,capacity,org.apache.hadoop.ipc.ResponseBuffer:capacity(),56,58,"/**
* Delegates m1() call to the FramedBuffer object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,ensureCapacity,org.apache.hadoop.ipc.ResponseBuffer:ensureCapacity(int),64,68,"/**
 * Adjusts the FramedBuffer's capacity if it's less than the given capacity.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setException,org.apache.hadoop.ipc.Client$Call:setException(java.io.IOException),341,344,"/**
 * Sets the error and calls method m1.
 * @param error The exception to be set as the error.
 */
","Set the exception when there is an error.
     * Notify the caller the call is done.
     * 
     * @param error exception thrown by the call; either local or remote",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setRpcResponse,org.apache.hadoop.ipc.Client$Call:setRpcResponse(org.apache.hadoop.io.Writable),351,354,"/**
* Sets the RPC response and calls m1().
* @param rpcResponse The response object to set.
*/
","Set the return value when there is no error. 
     * Notify the caller the call is done.
     * 
     * @param rpcResponse return value of the rpc call.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,read,org.apache.hadoop.ipc.Client$Connection$PingInputStream:read(),513,524,"/**
 * Retries super.m2() until successful, handling SocketTimeoutException.
 */","Read a byte from the stream.
       * Send a ping if timeout on read. Retries if no failure is detected
       * until a byte is read.
       * @throws IOException for any IO problem other than socket timeout",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,read,"org.apache.hadoop.ipc.Client$Connection$PingInputStream:read(byte[],int,int)",532,543,"/**
* Reads data from the input stream, retrying on timeout.
* @param buf buffer to store data, @param off offset, @param len length
* @throws IOException if an I/O error occurs
*/
","Read bytes into a buffer starting from offset <code>off</code>
       * Send a ping if timeout on read. Retries if no failure is detected
       * until a byte is read.
       * 
       * @return the total number of bytes read; -1 if the connection is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getHostAddress,org.apache.hadoop.ipc.Server$Call:getHostAddress(),1063,1066,"/**
 * Delegates to InetAddress.m2(), returning null if addr is null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteIp,org.apache.hadoop.ipc.Server:getRemoteIp(),385,388,"/**
 * Returns the InetAddress from the current call, or null.
 */","* @return Returns the remote side ip address when invoked inside an RPC
   *  Returns null in case of an error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getServerRpcInvoker,org.apache.hadoop.ipc.Server:getServerRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind),310,312,"/**
* Creates an RpcInvoker based on the provided RpcKind.
* @param rpcKind The type of RPC to create an invoker for.
* @return The created RpcInvoker.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemotePort,org.apache.hadoop.ipc.Server:getRemotePort(),394,397,"/**
* Retrieves a value from CurCall's m2() or returns 0 if null.
*/","* @return Returns the remote side port when invoked inside an RPC
   * Returns 0 in case of an error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuxiliaryPortEstablishedQOP,org.apache.hadoop.ipc.Server:getAuxiliaryPortEstablishedQOP(),411,423,"/**
 * Retrieves a value from an RpcCall connection if valid.
 * @return String value or null if the call is invalid.
 */","* Returns the SASL qop for the current call, if the current call is
   * set, and the SASL negotiation is done. Otherwise return null
   * Note this only returns established QOP for auxiliary port, and
   * returns null for primary (non-auxiliary) port.
   *
   * Also note that CurCall is thread local object. So in fact, different
   * handler threads will process different CurCall object.
   *
   * Also, only return for RPC calls, not supported for other protocols.
   * @return the QOP of the current connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getProtocol,org.apache.hadoop.ipc.Server:getProtocol(),450,453,"/**
* Retrieves a value from CurCall's m1() result, or null.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(),465,468,"/**
* Retrieves a value from CurCall.m1(). Returns 0 if CurCall.m1() is null.
*/","* @return Return the priority level assigned by call queue to an RPC
   * Returns 0 in case no priority is assigned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setLogSlowRPCThresholdTime,org.apache.hadoop.ipc.Server:setLogSlowRPCThresholdTime(long),556,560,"/**
 * Sets the threshold for logging slow RPC calls.
 * @param logSlowRPCThresholdMs Threshold in milliseconds.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setClientBackoffEnabled,org.apache.hadoop.ipc.Server:setClientBackoffEnabled(boolean),3876,3878,"/**
 * Delegates the boolean value to the callQueue's m1 method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addAuxiliaryListener,org.apache.hadoop.ipc.Server:addAuxiliaryListener(int),3427,3443,"/**
* Adds a listener for the given port. Throws IOException if port is in use.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseForProtobuf,"org.apache.hadoop.ipc.Server:setupResponseForProtobuf(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3585,3607,"/**
 * Creates a masked byte array from header and writable.
 * @param header RpcResponseHeaderProto object
 * @param rv Writable object, may be null
 * @return Masked byte array
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumOpenConnectionsPerUser,org.apache.hadoop.ipc.Server:getNumOpenConnectionsPerUser(),3844,3852,"/**
 * Parses connection data to String. Returns null on error.
 */",* @return Get the NumOpenConnections/User.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isServerFailOverEnabled,org.apache.hadoop.ipc.Server:isServerFailOverEnabled(),3880,3883,"/**
* Delegates to the callQueue's m1() method and returns the result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processResponse,"org.apache.hadoop.ipc.Server$Responder:processResponse(java.util.LinkedList,boolean)",1844,1922,"/**
 * Processes RPC calls from the response queue.
 * Returns true if done, handles errors, writes data.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,equals,org.apache.hadoop.ipc.Client$ConnectionId:equals(java.lang.Object),1823,1841,"/**
* Checks if two ConnectionId objects are equal.
* @param obj The object to compare to.
* @return True if objects are equal, false otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/Tracer.java,build,org.apache.hadoop.tracing.Tracer$Builder:build(),91,96,"/**
 * Returns the global Tracer instance, creating it if null.
 * @return Tracer instance for tracing operations.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/Tracer.java,newSpan,"org.apache.hadoop.tracing.Tracer:newSpan(java.lang.String,org.apache.hadoop.tracing.SpanContext)",55,57,"/**
 * Creates a new Span with the given description and SpanContext.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/NullTraceScope.java,<init>,org.apache.hadoop.tracing.NullTraceScope:<init>(),23,25,"/**
 * Creates a NullTraceScope with a null parent scope.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/TraceScope.java,close,org.apache.hadoop.tracing.TraceScope:close(),53,57,"/**
 * Calls m1() on the span if it exists.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,"org.apache.hadoop.util.MachineList:<init>(java.util.Collection,org.apache.hadoop.util.MachineList$InetAddressFactory)",95,136,"/**
 * Creates a MachineList from host entries and an address factory.
 * @param hostEntries Collection of host entries.
 * @param addressFactory Factory for resolving host addresses.
 */
","* Accepts a collection of ip/cidr/host addresses
   * 
   * @param hostEntries hostEntries.
   * @param addressFactory addressFactory to convert host to InetAddress",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,includes,org.apache.hadoop.util.MachineList:includes(java.lang.String),145,160,"/**
 * Checks if an IP address is valid, delegating to m2.
 * @param ipAddress The IP address to validate.
 * @return True if valid, false otherwise.
 */","* Accepts an ip address and return true if ipAddress is in the list.
   * {@link #includes(InetAddress)} should be preferred
   * to avoid possibly re-resolving the ip address.
   *
   * @param ipAddress ipAddress.
   * @return true if ipAddress is part of the list",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,checkConf,org.apache.hadoop.util.ConfTest:checkConf(java.io.InputStream),136,216,"/**
 * Validates XML configuration file, finds duplicated properties.
 * @param in Input stream for XML configuration file.
 * @return List of error strings, or empty list if valid.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,listFiles,org.apache.hadoop.util.ConfTest:listFiles(java.io.File),218,225,"/**
 * Filters files in a directory, returning only XML files.
 * @param dir The directory to filter.
 * @return An array of XML files found in the directory.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,<init>,"org.apache.hadoop.util.SysInfoLinux:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,long)",195,210,"/**
 * Constructs a SysInfoLinux object with provided procfs file paths.
 * @param jiffyLengthInMillis Jiffy length in milliseconds.
 */","* Constructor which allows assigning the /proc/ directories. This will be
   * used only in unit tests.
   * @param procfsMemFile fake file for /proc/meminfo
   * @param procfsCpuFile fake file for /proc/cpuinfo
   * @param procfsStatFile fake file for /proc/stat
   * @param procfsNetFile fake file for /proc/net/dev
   * @param procfsDisksFile fake file for /proc/diskstats
   * @param jiffyLengthInMillis fake jiffy length value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcMemInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(boolean),238,305,"/**
 * Reads memory information from /proc/meminfo and parses values.
 */","* Read /proc/meminfo, parse and compute memory information.
   * @param readAgain if false, read only on the first time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumProcessors,org.apache.hadoop.util.SysInfoLinux:getNumProcessors(),625,629,"/**
* Returns the number of processors available.
* Calls m1() first, then returns numProcessors.
*/
",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumCores,org.apache.hadoop.util.SysInfoLinux:getNumCores(),632,636,"/**
 * Returns the number of CPU cores available.
 * Calls m1() first.
 */",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCpuFrequency,org.apache.hadoop.util.SysInfoLinux:getCpuFrequency(),639,643,"/**
 * Returns the current CPU frequency.
 * Calls m1() before returning the value.
 */",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcStatFile,org.apache.hadoop.util.SysInfoLinux:readProcStatFile(),376,421,"/**
 * Reads CPU time from a file and updates the CPU time tracker.
 */","* Read /proc/stat file, parse and calculate cumulative CPU.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNetworkBytesRead,org.apache.hadoop.util.SysInfoLinux:getNetworkBytesRead(),675,679,"/**
* Reads network bytes and returns the count.
* Calls m1() and returns numNetBytesRead.
*/
",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNetworkBytesWritten,org.apache.hadoop.util.SysInfoLinux:getNetworkBytesWritten(),682,686,"/**
* Calculates and returns the number of bytes written.
* Calls m1() and returns numNetBytesWritten.
*/",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcDisksInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcDisksInfoFile(),483,543,"/**
 * Reads disk I/O statistics from procfsDisksFile.
 * Calculates total bytes read/written, ignoring loop/ram disks.
 */","* Read /proc/diskstats file, parse and calculate amount
   * of bytes read and written from/to disks.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,realloc,org.apache.hadoop.util.IdentityHashStore:realloc(int),74,90,"/**
* Resizes the buffer to the new capacity, copying existing elements.
* @param newCapacity The new capacity of the buffer (must be > 0).
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,get,org.apache.hadoop.util.IdentityHashStore:get(java.lang.Object),152,158,"/**
* Retrieves a value from the buffer based on key.
* @param k The key used to find the value.
* @return The value associated with the key, or null.
*/
","* Retrieve a value associated with a given key.
   *
   * @param k Generics Type k.
   * @return Generics Type V.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,remove,org.apache.hadoop.util.IdentityHashStore:remove(java.lang.Object),167,177,"/**
 * Retrieves a value from the buffer using the key.
 * @param k The key used to find the value.
 * @return The value associated with the key, or null.
 */
","* Retrieve a value associated with a given key, and delete the
   * relevant entry.
   *
   * @param k Generics Type k.
   * @return Generics Type V.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,ensureNext,org.apache.hadoop.util.LightWeightGSet$SetIterator:ensureNext(),312,327,"/**
 * Advances to the next element, checking for modification.
 * Throws ConcurrentModificationException if modification state is inconsistent.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,"org.apache.hadoop.util.LightWeightGSet:remove(int,java.lang.Object)",188,219,"/**
* Removes and returns the element associated with the given key.
* @param index index of the element
* @param key key of the element to remove
* @return the removed element or null if not found
*/","* Remove the element corresponding to the key,
   * given key.hashCode() == index.
   *
   * @param key key.
   * @param index index.
   * @return If such element exists, return it.
   *         Otherwise, return null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MergeSort.java,mergeSort,"org.apache.hadoop.util.MergeSort:mergeSort(int[],int[],int,int)",42,83,"/**
* Sorts a portion of an array using a merge-like approach.
* @param src source array, dest destination array, low start index, high end index
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,setOptionalSecureTransformerAttributes,org.apache.hadoop.util.XMLUtils:setOptionalSecureTransformerAttributes(javax.xml.transform.TransformerFactory),180,186,"/**
* Configures external DTD and stylesheet access.
* Uses m1 to set transformer access properties.
*/","* These attributes are recommended for maximum security but some JAXP transformers do
   * not support them. If at any stage, we fail to set these attributes, then we won't try again
   * for subsequent transformers.
   *
   * @param transformerFactory to update",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,string2long,org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:string2long(java.lang.String),906,927,"/**
 * Parses a string with a size prefix (k, m, g, t, p, e) to a long.
 */","* Convert a string to long.
     * The input string is first be trimmed
     * and then it is parsed with traditional binary prefix.
     *
     * For example,
     * ""-1230k"" will be converted to -1230 * 1024 = -1259520;
     * ""891g"" will be converted to 891 * 1024^3 = 956703965184;
     *
     * @param s input string
     * @return a long value represented by the input string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,long2String,"org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:long2String(long,java.lang.String,int)",937,977,"/**
 * Formats a long value with a unit and specified decimal places.
 * @param n The number to format.
 * @param unit The unit symbol.
 * @param decimalPlaces Decimal places for formatting.
 */","* Convert a long integer to a string with traditional binary prefix.
     * 
     * @param n the value to be converted
     * @param unit The unit, e.g. ""B"" for bytes.
     * @param decimalPlaces The number of decimal places.
     * @return a string with traditional binary prefix.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,formatPercent,"org.apache.hadoop.util.StringUtils:formatPercent(double,int)",153,155,"/**
* Formats a fraction as a percentage string with specified decimal places.
*/","* Format a percentage for presentation to the user.
   * @param fraction the percentage as a fraction, e.g. 0.1 = 10%
   * @param decimalPlaces the number of decimal places
   * @return a string representation of the percentage",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,"org.apache.hadoop.util.StringUtils:byteToHexString(byte[],int,int)",183,192,"/**
 * Converts a byte array slice to a hex string.
 * @param bytes byte array to convert
 * @param start start index (inclusive)
 * @param end end index (exclusive)
 */","* Given an array of bytes it will convert the bytes to a hex string
   * representation of the bytes
   * @param bytes bytes.
   * @param start start index, inclusively
   * @param end end index, exclusively
   * @return hex string representation of the byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,limitDecimalTo2,org.apache.hadoop.util.StringUtils:limitDecimalTo2(double),1033,1036,"/**
* Formats a double to a string with two decimal places.
* @param d The double value to format.
* @return Formatted string representation of the double.
*/
","* limitDecimalTo2.
   *
   * @param d double param.
   * @return string value (""%.2f"").
   * @deprecated use StringUtils.format(""%.2f"", d).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HeapSort.java,sort,"org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)",56,74,"/**
* Performs a sorting operation on a portion of the sorted data.
* @param s data structure, p start index, r end index, rep progress reporter
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,subtract,org.apache.hadoop.util.JvmPauseMonitor$GcTimes:subtract(org.apache.hadoop.util.JvmPauseMonitor$GcTimes),166,169,"/**
 * Subtracts another GcTimes object from this one.
 * @param other The GcTimes object to subtract.
 * @return A new GcTimes representing the difference.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,org.apache.hadoop.util.ExitUtil:terminate(org.apache.hadoop.util.ExitUtil$ExitException),233,272,"/**
 * Handles ExitException, logs error, and either exits or terminates.
 */","* Exits the JVM if exit is enabled, rethrow provided exception or any raised error otherwise.
   * Inner termination: either exit with the exception's exit code,
   * or, if system exits are disabled, rethrow the exception.
   * @param ee exit exception
   * @throws ExitException if {@link System#exit(int)} is disabled and not suppressed by an Error
   * @throws Error if {@link System#exit(int)} is disabled and one Error arise, suppressing
   * anything else, even <code>ee</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,org.apache.hadoop.util.ExitUtil:halt(org.apache.hadoop.util.ExitUtil$HaltException),286,326,"/**
 * Handles a HaltException, logs details, and either throws or handles it.
 */","* Halts the JVM if halt is enabled, rethrow provided exception or any raised error otherwise.
   * If halt is disabled, this method throws either the exception argument if no
   * error arise, the first error if at least one arise, suppressing <code>he</code>.
   * If halt is enabled, all throwables are caught, even errors.
   *
   * @param he the exception containing the status code, message and any stack
   * trace.
   * @throws HaltException if {@link Runtime#halt(int)} is disabled and not suppressed by an Error
   * @throws Error if {@link Runtime#halt(int)} is disabled and one Error arise, suppressing
   * anyuthing else, even <code>he</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,<init>,org.apache.hadoop.util.SysInfoWindows:<init>(),56,59,"/**
 * Initializes the SysInfoWindows object.
 * Sets lastRefreshTime to 0 and calls reset().
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,addShutdownHook,"org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int,long,java.util.concurrent.TimeUnit)",319,331,"/**
 * Adds a shutdown hook with priority and timeout.
 * @param shutdownHook Hook to execute during shutdown.
 */","*
   * Adds a shutdownHook with a priority and timeout the higher the priority
   * the earlier will run. ShutdownHooks with same priority run
   * in a non-deterministic order. The shutdown hook will be terminated if it
   * has not been finished in the specified period of time.
   *
   * @param shutdownHook shutdownHook <code>Runnable</code>
   * @param priority priority of the shutdownHook
   * @param timeout timeout of the shutdownHook
   * @param unit unit of the timeout <code>TimeUnit</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,removeShutdownHook,org.apache.hadoop.util.ShutdownHookManager:removeShutdownHook(java.lang.Runnable),340,350,"/**
 * Adds a shutdown hook. Throws exception if shutdown is in progress.
 * @param shutdownHook Hook to be added.
 * @return True if hook added, false otherwise.
 */
","* Removes a shutdownHook.
   *
   * @param shutdownHook shutdownHook to remove.
   * @return TRUE if the shutdownHook was registered and removed,
   * FALSE otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,hasShutdownHook,org.apache.hadoop.util.ShutdownHookManager:hasShutdownHook(java.lang.Runnable),358,363,"/**
* Registers a shutdown hook.
* @param shutdownHook Runnable to execute on shutdown.
* @return True if hook registered, false otherwise.
*/
","* Indicates if a shutdownHook is registered or not.
   *
   * @param shutdownHook shutdownHook to check if registered.
   * @return TRUE/FALSE depending if the shutdownHook is is registered.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,compareTo,org.apache.hadoop.util.ComparableVersion:compareTo(org.apache.hadoop.util.ComparableVersion),460,463,"/**
* Delegates to items' m1 method with the given ComparableVersion.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ThreadUtil.java,getResourceAsStream,org.apache.hadoop.util.ThreadUtil:getResourceAsStream(java.lang.String),91,99,"/**
 * Retrieves an input stream for the specified resource.
 * @param resourceName Name of the resource to load.
 * @throws IOException If resource cannot be loaded.
 */
","* Convenience method that returns a resource as inputstream from the
   * classpath.
   * <p>
   * Uses the Thread's context classloader to load resource.
   *
   * @param resourceName resource to retrieve.
   *
   * @throws IOException thrown if resource cannot be loaded
   * @return inputstream with the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,logWarning,"org.apache.hadoop.util.InstrumentedLock:logWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)",151,161,"/**
 * Logs a lock held time warning with associated details and stack trace.
 * @param lockHeldTime Lock held time in milliseconds.
 * @param stats Suppressed snapshot details.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,logWaitWarning,"org.apache.hadoop.util.InstrumentedLock:logWaitWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)",163,172,"/**
 * Logs a message indicating a lock acquisition wait exceeded threshold.
 * @param lockWaitTime Wait time in milliseconds.
 * @param stats Contains suppressed lock wait warning details.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sortInternal,"org.apache.hadoop.util.QuickSort:sortInternal(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable,int)",69,136,"/**
 * Sorts a portion of the indexed sortable using a recursive approach.
 * @param s sortable object, p/r bounds, rep progressable, depth recursion level
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,org.apache.hadoop.util.LineReader:<init>(java.io.InputStream),69,71,"/**
 * Constructs a LineReader with a default buffer size.
 * @param in The input stream to read from.
 */
","* Create a line reader that reads from the given stream using the
   * default buffer-size (64k).
   * @param in The input stream",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell$1:run(),951,960,"/**
 * Executes a process if enough time has passed since the last run.
 */","* Check to see if a command needs to be executed and execute if needed.
   *
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,newDaemonThreadFactory,org.apache.hadoop.util.BlockingThreadPoolExecutorService:newDaemonThreadFactory(java.lang.String),86,102,"/**
 * Creates a ThreadFactory with a given prefix, sets daemon and priority.
 * @param prefix Prefix for the created threads
 * @return A ThreadFactory with configured thread naming and priority.
 */
","* Get a named {@link ThreadFactory} that just builds daemon threads.
   *
   * @param prefix name prefix for all threads created from the factory
   * @return a thread factory that creates named, daemon threads with
   * the supplied exception handler and normal priority",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,"org.apache.hadoop.util.LightWeightResizableGSet:<init>(int,float)",66,81,"/**
 * Constructs a LightWeightResizableGSet with initial capacity and load factor.
 * @param initCapacity initial capacity of the set
 * @param loadFactor load factor for resizing
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,size,org.apache.hadoop.util.LightWeightResizableGSet:size(),108,111,"/**
 * Calls the superclass's m1() method and returns its result.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,getIterator,org.apache.hadoop.util.LightWeightResizableGSet:getIterator(java.util.function.Consumer),113,115,"/**
* Applies the consumer to an iterator derived from super.m1().m2().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,expandIfNecessary,org.apache.hadoop.util.LightWeightResizableGSet:expandIfNecessary(),148,152,"/**
* Doubles array capacity if size exceeds threshold.
* Expands the array if size > threshold and capacity < max.
*/","* Checks if we need to expand, and expands if necessary.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.util.Iterator),109,113,"/**
 * Converts an iterator to an ArrayList.
 * @param elements Iterator of elements to add to the list.
 * @return ArrayList containing elements from the iterator.
 */
","* Creates a <i>mutable</i> {@code ArrayList} instance containing the
   * given elements; a very thin shortcut for creating an empty list
   * and then calling Iterators#addAll.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,addAll,"org.apache.hadoop.util.Lists:addAll(java.util.Collection,java.lang.Iterable)",248,258,"/**
 * Adds elements from iterable to collection, handling nulls.
 * @param addTo Collection to add to.
 * @param elementsToAdd Iterable of elements to add.
 * @return True if all elements were added, otherwise false.
 */
","* Adds all elements in {@code iterable} to {@code collection}.
   *
   * @return {@code true} if {@code collection} was modified as a result of
   *     this operation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayListWithCapacity,org.apache.hadoop.util.Lists:newArrayListWithCapacity(int),128,132,"/**
 * Creates an ArrayList with the specified initial size.
 * @param initialArraySize the initial capacity of the ArrayList
 */
","* Creates an {@code ArrayList} instance backed by an array with the
   * specified initial size;
   * simply delegates to {@link ArrayList#ArrayList(int)}.
   *
   * @param <E> Generics Type E.
   * @param initialArraySize the exact size of the initial backing array for
   *     the returned array list
   *     ({@code ArrayList} documentation calls this value the ""capacity"").
   * @return a new, empty {@code ArrayList} which is guaranteed not to
   *     resize itself unless its size reaches {@code initialArraySize + 1}.
   * @throws IllegalArgumentException if {@code initialArraySize} is negative.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,computeArrayListCapacity,org.apache.hadoop.util.Lists:computeArrayListCapacity(int),192,195,"/**
 * Calculates a value based on arraySize, calls m1, and returns it.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,getResource,org.apache.hadoop.util.ApplicationClassLoader:getResource(java.lang.String),128,153,"/**
 * Resolves a resource URL by name, falling back to parent.
 * @param name Resource name to resolve.
 * @return URL object or null if not found.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,loadClass,"org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String,boolean)",160,204,"/**
 * Loads a class by name, resolving if requested.
 * @param name Class name to load.
 * @param resolve Whether to resolve the class.
 * @return Loaded Class object or throws ClassNotFoundException.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,writeJsonAsBytes,"org.apache.hadoop.util.JsonSerialization:writeJsonAsBytes(java.lang.Object,java.io.OutputStream)",305,312,"/**
 * Writes data derived from the instance to the output stream.
 * @param instance The object to process.
 * @param dataOutputStream Output stream to write data to.
 */
","* Write the JSON as bytes, then close the stream.
   * @param instance instance to write
   * @param dataOutputStream an output stream that will always be closed
   * @throws IOException on any failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,<init>,org.apache.hadoop.util.OperationDuration:<init>(),48,51,"/**
 * Initializes an OperationDuration with start and finish times.
 */","* Instantiate.
   * The start time and finished time are both set
   * to the current clock time.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,finished,org.apache.hadoop.util.OperationDuration:finished(),64,66,"/**
* Sets the 'finished' flag based on the result of m1().
*/",* Update the finished time with the current system time.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,asDuration,org.apache.hadoop.util.OperationDuration:asDuration(),114,116,"/**
* Calculates a duration by applying a mask to a value.
* Returns a Duration object.
*/
","* Get the duration of an operation as a java Duration
   * instance.
   * @return a duration.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,getDurationString,org.apache.hadoop.util.OperationDuration:getDurationString(),72,74,"/**
* Delegates to m1() and then m2, returning the result.
*/","* Return the duration as {@link #humanTime(long)}.
   * @return a printable duration.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,iterator,org.apache.hadoop.util.LightWeightCache:iterator(),234,256,"/**
 * Returns a new iterator that delegates to the superclass iterator.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,iterator,org.apache.hadoop.util.LightWeightGSet$Values:iterator(),240,243,"/**
* Delegates to the superclass's m1() method to get an iterator.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,getMonomial,"org.apache.hadoop.util.CrcUtil:getMonomial(long,int)",52,77,"/**
 * Calculates a masked product using exponentiation by squaring.
 * @param lengthBytes Length of bytes, must be positive.
 * @param mod Modulus for calculations.
 * @return Masked product.
 */","* Compute x^({@code lengthBytes} * 8) mod {@code mod}, where {@code mod} is
   * in ""reversed"" (little-endian) format such that {@code mod & 1} represents
   * x^31 and has an implicit term x^32.
   *
   * @param lengthBytes lengthBytes.
   * @param mod mod.
   * @return monomial.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,composeWithMonomial,"org.apache.hadoop.util.CrcUtil:composeWithMonomial(int,int,int,int)",88,91,"/**
* Calculates a masked CRC value.
* @param crcA Initial CRC value.
* @param crcB Second CRC value.
* @param monomial Monomial value.
* @param mod Modulus value.
*/
","* composeWithMonomial.
   *
   * @param crcA crcA.
   * @param crcB crcB.
   * @param monomial Precomputed x^(lengthBInBytes * 8) mod {@code mod}
   * @param mod mod.
   * @return compose with monomial.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,intToBytes,org.apache.hadoop.util.CrcUtil:intToBytes(int),113,125,"/**
 * Converts an integer to a byte array using m1.
 * @param value integer to convert
 * @return byte array representation of the integer
 */
","* @return 4-byte array holding the big-endian representation of
   *     {@code value}.
   *
   * @param value value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,toSingleCrcString,org.apache.hadoop.util.CrcUtil:toSingleCrcString(byte[]),182,190,"/**
 * Masks a byte array and returns a hex string representation.
 * @param bytes 4-byte array to mask; throws IOException if length != 4
 * @return Hex string representation of the masked byte array.
 */","* For use with debug statements; verifies bytes.length on creation,
   * expecting it to represent exactly one CRC, and returns a hex
   * formatted value.
   *
   * @param bytes bytes.
   * @throws IOException raised on errors performing I/O.
   * @return a list of hex formatted values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,toMultiCrcString,org.apache.hadoop.util.CrcUtil:toMultiCrcString(byte[]),201,218,"/**
 * Converts byte array to hex string representation, 4 bytes at a time.
 * @param bytes The byte array to convert.
 * @throws IOException if the byte array length is not divisible by 4.
 */","* For use with debug statements; verifies bytes.length on creation,
   * expecting it to be divisible by CRC byte size, and returns a list of
   * hex formatted values.
   *
   * @param bytes bytes.
   * @throws IOException raised on errors performing I/O.
   * @return a list of hex formatted values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.InputStream,java.io.File,java.util.regex.Pattern)",119,152,"/**
 * Extracts files from a JarInputStream to a directory, filtering by regex.
 * @param inputStream Input stream for the Jar file.
 * @param toDir Directory to extract files to.
 * @param unpackRegex Regex pattern for file extraction.
 */","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped.
   *
   * @param inputStream the jar stream to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File,java.util.regex.Pattern)",191,222,"/**
 * Extracts files from a JAR archive based on a regex pattern.
 * @param jarFile JAR file to extract from
 * @param toDir Directory to extract files to
 * @param unpackRegex Regex pattern for file selection
 */","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped.
   *
   * @param jarFile the .jar file to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newCachedThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newCachedThreadPool(java.util.concurrent.ThreadFactory),36,42,"/**
* Creates a HadoopThreadPoolExecutor with given thread factory.
* @param threadFactory ThreadFactory for new threads
* @return ExecutorService instance
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newFixedThreadPool,"org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int,java.util.concurrent.ThreadFactory)",44,50,"/**
 * Creates a HadoopThreadPoolExecutor with specified parameters.
 * @param nThreads Number of threads, also used as core pool size.
 * @param threadFactory Thread factory for creating new threads.
 * @return ExecutorService instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newFixedThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int),52,56,"/**
 * Creates a HadoopThreadPoolExecutor with specified thread count.
 * @param nThreads Number of threads for the executor.
 * @return HadoopThreadPoolExecutor instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newScheduledThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int),71,74,"/**
 * Creates a HadoopScheduledThreadPoolExecutor with specified core pool size.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newScheduledThreadPool,"org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int,java.util.concurrent.ThreadFactory)",76,79,"/**
 * Creates a HadoopScheduledThreadPoolExecutor.
 * @param corePoolSize Initial pool size.
 * @param threadFactory Thread factory for new threads.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopScheduledThreadPoolExecutor.java,afterExecute,"org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)",66,70,"/**
* Calls super.m1 and then calls ExecutorHelper.m2 with given params.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopThreadPoolExecutor.java,afterExecute,"org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)",87,91,"/**
* Calls super.m1 and then calls ExecutorHelper.m2 with same args.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,get,org.apache.hadoop.util.concurrent.AsyncGetFuture:get(),56,60,"/**
* Calls m1 with -1 and TimeUnit.MILLISECONDS, then calls super.m2().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,get,"org.apache.hadoop.util.concurrent.AsyncGetFuture:get(long,java.util.concurrent.TimeUnit)",62,67,"/**
* Calls m1, then calls super.m2(0, MILLISECONDS) and returns the result.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,isDone,org.apache.hadoop.util.concurrent.AsyncGetFuture:isDone(),69,73,"/**
* Calls m1 and then calls the superclass's m2 method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,<init>,org.apache.hadoop.util.StopWatch:<init>(),33,35,"/**
 * Constructs a StopWatch with a default Timer implementation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/UTF8ByteArrayUtils.java,findNthByte,"org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],int,int,byte,int)",78,89,"/**
 * Finds the last occurrence of a byte within a byte array segment.
 * @param utf byte array, start index, length, byte to find, count
 * @return Last position or -1 if not found.
 */","* Find the nth occurrence of the given byte b in a UTF-8 encoded string
   * @param utf a byte array containing a UTF-8 encoded string
   * @param start starting offset
   * @param length the length of byte array
   * @param b the byte to find
   * @param n the desired occurrence of the given byte
   * @return position that nth occurrence of the given byte if exists; otherwise -1",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,<init>,"org.apache.hadoop.util.CacheableIPList:<init>(org.apache.hadoop.util.FileBasedIPList,long)",33,37,"/**
 * Constructs a CacheableIPList with the given IP list and timeout.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,toString,org.apache.hadoop.util.WeakReferenceMap:toString(),108,115,"/**
 * Returns a string representation of the WeakReferenceMap.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,put,"org.apache.hadoop.util.WeakReferenceMap:put(java.lang.Object,java.lang.Object)",246,248,"/**
* Associates key with value in map, using WeakReference for value.
* @param key The key for the map.
* @param value The value to associate with the key.
*/
","* Put a value under the key.
   * A null value can be put, though on a get() call
   * a new entry is generated
   *
   * @param key key
   * @param value value
   * @return any old non-null reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,remove,org.apache.hadoop.util.WeakReferenceMap:remove(java.lang.Object),255,257,"/**
* Retrieves a value from the map using the given key.
* @param key The key to look up in the map.
* @return The associated value or null if not found.
*/
","* Remove any value under the key.
   * @param key key
   * @return any old non-null reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,containsKey,org.apache.hadoop.util.WeakReferenceMap:containsKey(java.lang.Object),266,269,"/**
 * Checks if a value exists for the given key.
 * @param key The key to check.
 * @return True if a value exists, false otherwise.
 */
","* Does the map have a valid reference for this object?
   * no-side effects: there's no attempt to notify or cleanup
   * if the reference is null.
   * @param key key to look up
   * @return true if there is a valid reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,create,org.apache.hadoop.util.WeakReferenceMap:create(java.lang.Object),197,235,"/**
* Retrieves or creates a value associated with a key.
* @param key The key to look up or create a value for.
* @return The value associated with the key.
*/
","* Create a new instance under a key.
   * <p>
   * The instance is created, added to the map and then the
   * map value retrieved.
   * This ensures that the reference returned is that in the map,
   * even if there is more than one entry being created at the same time.
   * If that race does occur, it will be logged the first time it happens
   * for this specific map instance.
   * <p>
   * HADOOP-18456 highlighted the risk of a concurrent GC resulting a null
   * value being retrieved and so returned.
   * To prevent this:
   * <ol>
   *   <li>A strong reference is retained to the newly created instance
   *       in a local variable.</li>
   *   <li>That variable is used after the resolution process, to ensure
   *       the JVM doesn't consider it ""unreachable"" and so eligible for GC.</li>
   *   <li>A check is made for the resolved reference being null, and if so,
   *       the put() is repeated</li>
   * </ol>
   * @param key key
   * @return the created value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,prune,org.apache.hadoop.util.WeakReferenceMap:prune(),288,300,"/**
 * Counts and removes entries with null values from the map.
 * Returns the number of removed entries.
 */","* Prune all null weak references, calling the referenceLost
   * callback for each one.
   *
   * non-atomic and non-blocking.
   * @return the number of entries pruned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,snapshot,org.apache.hadoop.util.InstrumentedLock$SuppressedStats:snapshot(),262,268,"/**
 * Creates and returns a SuppressedSnapshot with reset counters.
 * @return A new SuppressedSnapshot object.
 */","* Captures the current value of the counts into a SuppressedSnapshot object
     * and resets the values to zero.
     *
     * @return SuppressedSnapshot containing the current value of the counters",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,formatTimeDiff,"org.apache.hadoop.util.StringUtils:formatTimeDiff(long,long)",294,297,"/**
* Calculates a value based on the time difference.
* @param finishTime Finish time in milliseconds.
* @param startTime Start time in milliseconds.
*/
","* 
   * Given a finish and start time in long milliseconds, returns a 
   * String in the format Xhrs, Ymins, Z sec, for the time difference between two times. 
   * If finish time comes before start time then negative valeus of X, Y and Z wil return. 
   * 
   * @param finishTime finish time
   * @param startTime start time
   * @return a String in the format Xhrs, Ymins, Z sec,
   *         for the time difference between two times.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getTrimmedStringCollectionSplitByEquals,org.apache.hadoop.util.StringUtils:getTrimmedStringCollectionSplitByEquals(java.lang.String),505,525,"/**
 * Parses input string into key-value pairs and returns a map.
 */","* Splits an ""="" separated value <code>String</code>, trimming leading and
   * trailing whitespace on each value after splitting by comma and new line separator.
   *
   * @param str a comma separated <code>String</code> with values, may be null
   * @return a <code>Map</code> of <code>String</code> keys and values, empty
   * Collection if null String input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,split,"org.apache.hadoop.util.StringUtils:split(java.lang.String,char,char)",581,601,"/**
* Splits a string by a separator, handling escape characters.
* @param str String to split, escapeChar and separator chars.
* @return String array of split substrings.
*/","* Split a string using the given separator
   * @param str a string that may have escaped separator
   * @param escapeChar a char that be used to escape the separator
   * @param separator a separator char
   * @return an array of strings",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,"org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char[])",701,716,"/**
 * Masks characters in a string based on escapeChar and charsToEscape.
 * @param str input string
 * @param escapeChar escape character
 * @param charsToEscape array of chars to escape
 * @return masked string
 */
","* escapeString.
   *
   * @param str str.
   * @param escapeChar escapeChar.
   * @param charsToEscape array of characters to be escaped
   * @return escapeString.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,"org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char[])",748,782,"/**
 * Masks characters in a string based on escape char & chars.
 * @param str input string
 * @param escapeChar escape character
 * @param charsToEscape chars to escape
 * @return masked string
 */","* unEscapeString.
   * @param str str.
   * @param escapeChar escapeChar.
   * @param charsToEscape array of characters to unescape
   * @return escape string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getVersion,org.apache.hadoop.util.VersionInfo:getVersion(),105,107,"/**
 * Retrieves a masked value from the COMMON_VERSION_INFO object.
 */","* Get the Hadoop version.
   * @return the Hadoop version string, eg. ""0.6.3-dev""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getRevision,org.apache.hadoop.util.VersionInfo:getRevision(),113,115,"/**
 * Retrieves a masked value from the COMMON_VERSION_INFO object.
 */","* Get the Git commit hash of the repository when compiled.
   * @return the commit hash, eg. ""18f64065d5db6208daf50b02c1b5ed4ee3ce547a""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getBranch,org.apache.hadoop.util.VersionInfo:getBranch(),121,123,"/**
 * Retrieves a masked value from the COMMON_VERSION_INFO object.
 */","* Get the branch on which this originated.
   * @return The branch name, e.g. ""trunk"" or ""branches/branch-0.20""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getDate,org.apache.hadoop.util.VersionInfo:getDate(),129,131,"/**
* Returns a masked version string from COMMON_VERSION_INFO.
*/","* The date that Hadoop was compiled.
   * @return the compilation date in unix date format",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getUser,org.apache.hadoop.util.VersionInfo:getUser(),137,139,"/**
* Returns a masked version string from COMMON_VERSION_INFO.
*/","* The user that compiled Hadoop.
   * @return the username of the user",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getUrl,org.apache.hadoop.util.VersionInfo:getUrl(),145,147,"/**
 * Retrieves a masked value from the COMMON_VERSION_INFO object.
 */","* Get the URL for the Hadoop repository.
   * @return the URL of the Hadoop repository",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,_getBuildVersion,org.apache.hadoop.util.VersionInfo:_getBuildVersion(),85,90,"/**
* Combines strings from m1, m2, m3, and m4 into a single string.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getSrcChecksum,org.apache.hadoop.util.VersionInfo:getSrcChecksum(),153,155,"/**
 * Retrieves a masked value from the COMMON_VERSION_INFO object.
 */","* Get the checksum of the source files from which Hadoop was built.
   * @return the checksum of the source files",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getProtocVersion,org.apache.hadoop.util.VersionInfo:getProtocVersion(),170,172,"/**
* Returns a masked value from the COMMON_VERSION_INFO object.
*/","* Returns the protoc version used for the build.
   * @return the protoc version",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getCompilePlatform,org.apache.hadoop.util.VersionInfo:getCompilePlatform(),178,180,"/**
 * Retrieves a masked value from the COMMON_VERSION_INFO object.
 */","* Returns the OS platform used for the build.
   * @return the OS platform",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",604,610,"/**
 * Creates a FilteringRemoteIterator from a source iterator and filter.
 * @param source Iterator to filter.
 * @param filter Function to determine if an element is included.
 */
","* An iterator which combines filtering with transformation.
     * All source elements for which filter = true are returned,
     * transformed via the mapper.
     * @param source source iterator.
     * @param filter filter predicate.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",521,526,"/**
 * Creates a MappingRemoteIterator that maps elements from source.
 * @param source The source iterator.
 * @param mapper The function to map elements.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,java.io.Closeable)",677,682,"/**
 * Constructs a CloseRemoteIterator with a source iterator and closeable.
 * @param source Iterator to wrap; must not be null.
 * @param toClose Resource to close when iterator is closed.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.CallableRaisingIOE)",773,778,"/**
 * Constructs a HaltableRemoteIterator from a source iterator.
 * @param source The iterator to wrap.
 * @param continueWork Callable to determine if iteration continues.
 */
","* Wrap an iterator with one which adds a continuation probe.
     * The probe will be called in the {@link #hasNext()} method, before
     * the source iterator is itself checked and in {@link #next()}
     * before retrieval.
     * That is: it may be called multiple times per iteration.
     * @param source source iterator.
     * @param continueWork predicate which will trigger a fast halt if it returns false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator),553,556,"/**
 * Initializes the TypeCastingRemoteIterator with a source iterator.
 * @param source The iterator to wrap.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,hasNext,org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:hasNext(),634,640,"/**
* Checks if next is valid, otherwise calls m1().
* Returns true if successful, otherwise delegates.
*/","* Trigger a fetch if an entry is needed.
     * @return true if there was already an entry return,
     * or there was not but one could then be retrieved.set
     * @throws IOException failure in fetch operation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:<init>(java.lang.Object),723,725,"/**
 * Constructs a MaybeClose with an object and default close flag.
 * @param o The object to be potentially closed.
 */
","* Construct.
     * @param o object to close.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:close(),415,419,"/**
 * Delegates the m1 call to the sourceToClose object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:close(),454,457,"/**
* Delegates m1() call to the sourceToClose object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:next(),829,837,"/**
 * Returns the next value in a sequence.
 * Throws NoSuchElementException if sequence is empty.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,submit,"org.apache.hadoop.util.functional.CommonCallableSupplier:submit(java.util.concurrent.Executor,java.util.concurrent.Callable)",82,87,"/**
 * Creates a CompletableFuture that executes the Callable on the given executor.
 * @param executor Executor to run the Callable
 * @param call Callable to execute
 * @return CompletableFuture wrapping the Callable's result
 */
","* Submit a callable into a completable future.
   * RTEs are rethrown.
   * Non RTEs are caught and wrapped; IOExceptions to
   * {@code RuntimeIOException} instances.
   * @param executor executor.
   * @param call     call to invoke
   * @param <T>      type
   * @return the future to wait for",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,<init>,org.apache.hadoop.util.functional.LazyAutoCloseableReference:<init>(org.apache.hadoop.util.functional.CallableRaisingIOE),43,45,"/**
 * Creates a LazyAutoCloseableReference with the given constructor.
 */","* Constructor for this instance.
   * @param constructor method to invoke to actually construct the inner object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,lazyAtomicReferenceFromSupplier,org.apache.hadoop.util.functional.LazyAtomicReference:lazyAtomicReferenceFromSupplier(java.util.function.Supplier),148,151,"/**
 * Creates a LazyAtomicReference initialized with the given supplier.
 * @param supplier Supplier for the initial value.
 * @return A LazyAtomicReference.
 */
","* Create from a supplier.
   * This is not a constructor to avoid ambiguity when a lambda-expression is
   * passed in.
   * @param supplier supplier implementation.
   * @return a lazy reference.
   * @param <T> type of reference",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,eval,org.apache.hadoop.util.functional.LazyAutoCloseableReference:eval(),51,55,"/**
* Calls m2 with a boolean and then calls super.m3().
* Throws IOException if closed.
*/","* {@inheritDoc}
   * @throws IllegalStateException if the reference is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,apply,org.apache.hadoop.util.functional.LazyAtomicReference:apply(),104,107,"/**
* Delegates to the m1() method and returns its result.
*/","* Implementation of {@code CallableRaisingIOE.apply()}.
   * Invoke {@link #eval()}.
   * @return the value
   * @throws IOException on any evaluation failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,uncheckIOExceptions,org.apache.hadoop.util.functional.FunctionalIO:uncheckIOExceptions(org.apache.hadoop.util.functional.CallableRaisingIOE),45,47,"/**
 * Executes a CallableRaisingIOE and returns the result.
 * @param call CallableRaisingIOE to execute.
 * @return The result of the Callable.
 */
","* Invoke any operation, wrapping IOExceptions with
   * {@code UncheckedIOException}.
   * @param call callable
   * @param <T> type of result
   * @return result
   * @throws UncheckedIOException if an IOE was raised.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,toUncheckedIOExceptionSupplier,org.apache.hadoop.util.functional.FunctionalIO:toUncheckedIOExceptionSupplier(org.apache.hadoop.util.functional.CallableRaisingIOE),55,57,"/**
 * Adapts a CallableRaisingIOE to a Supplier, masking I/O exceptions.
 */","* Wrap a {@link CallableRaisingIOE} as a {@link Supplier}.
   * @param call call to wrap
   * @param <T> type of result
   * @return a supplier which invokes the call.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:next(),340,349,"/**
* Returns the singleton if m1() returns true, otherwise throws NoSuchElementException.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(org.apache.hadoop.fs.RemoteIterator),591,593,"/**
 * Creates a Builder for filtering RemoteIterator items.
 * @param items The iterator to filter.
 * @return A Builder instance.
 */
","* Create a task builder for the remote iterator.
   * @param items item source.
   * @param <I> type of result.
   * @return builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,throwOne,org.apache.hadoop.util.functional.TaskPool:throwOne(java.util.Collection),607,622,"/**
 * Processes a collection of exceptions, merging similar ones.
 * @param exceptions Collection of exceptions to process.
 */
","* Throw one exception, adding the others as suppressed
   * exceptions attached to the one thrown.
   * This method never completes normally.
   * @param exceptions collection of exceptions
   * @param <E> class of exceptions
   * @throws E an extracted exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,<init>,org.apache.hadoop.util.functional.TaskPool$Builder:<init>(java.lang.Iterable),161,163,"/**
 * Creates a builder from an iterable of items.
 * @param items iterable to initialize the builder with
 */
","* Create the builder.
     * @param items items to process",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,suppressExceptions,org.apache.hadoop.util.functional.TaskPool$Builder:suppressExceptions(),197,199,"/**
* Returns a new Builder instance, equivalent to m1(true).
*/","* Suppress exceptions from tasks.
     * RemoteIterator exceptions are not suppressable.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,raiseInnerCause,org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.ExecutionException),254,257,"/**
* Wraps and re-throws an ExecutionException as an IOException.
*/
","* From the inner cause of an execution exception, extract the inner cause
   * if it is an IOE or RTE.
   * This will always raise an exception, either the inner IOException,
   * an inner RuntimeException, or a new IOException wrapping the raised
   * exception.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,raiseInnerCause,org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.CompletionException),269,272,"/**
* Wraps and re-throws a CompletionException as an IOException.
*/
","* Extract the cause of a completion failure and rethrow it if an IOE
   * or RTE.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,setJobConf,"org.apache.hadoop.util.ReflectionUtils:setJobConf(java.lang.Object,org.apache.hadoop.conf.Configuration)",89,115,"/**
* Configures an object using a JobConf.
* @param theObject Object to configure.
* @param conf Configuration object.
*/
","* This code is to support backward compatibility and break the compile  
   * time dependency of core on mapred.
   * This should be made deprecated along with the mapred package HADOOP-1230. 
   * Should be removed when mapred package is removed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClassByName,org.apache.hadoop.conf.Configuration:getClassByName(java.lang.String),2638,2644,"/**
 * Loads a class by name.
 * @param name Class name to load.
 * @return The loaded Class object.
 * @throws ClassNotFoundException if class is not found.
 */
","* Load a class by name.
   * 
   * @param name the class name.
   * @return the class object.
   * @throws ClassNotFoundException if the class is not found.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,printThreadInfo,"org.apache.hadoop.util.ReflectionUtils:printThreadInfo(java.io.PrintStream,java.lang.String)",183,221,"/**
 * Prints a thread dump to the stream with the given title.
 * @param stream output stream
 * @param title dump title
 */
","* Print all of the thread's information and stack traces.
   * 
   * @param stream the stream to
   * @param title a string title for the stack trace",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(boolean),830,836,"/**
 * Constructs a Configuration object.
 * @param loadDefaults if true, loads default configuration values.
 */
","A new configuration where the behavior of reading from the default 
   * resources can be turned off.
   * 
   * If the parameter {@code loadDefaults} is false, the new instance
   * will not load resources from the default files. 
   * @param loadDefaults specifies whether to load from the default files",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,createServletExceptionResponse,"org.apache.hadoop.util.HttpExceptionUtils:createServletExceptionResponse(javax.servlet.http.HttpServletResponse,int,java.lang.Throwable)",72,86,"/**
* Sends an error response to the client with details of the exception.
* @param response HTTP response object
* @param status HTTP status code
* @param ex The exception to include in the response
*/","* Creates a HTTP servlet response serializing the exception in it as JSON.
   *
   * @param response the servlet response
   * @param status the error code to set in the response
   * @param ex the exception to serialize in the response
   * @throws IOException thrown if there was an error while creating the
   * response",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,createJerseyExceptionResponse,"org.apache.hadoop.util.HttpExceptionUtils:createJerseyExceptionResponse(javax.ws.rs.core.Response$Status,java.lang.Throwable)",95,104,"/**
 * Creates a response with error details from a Throwable.
 * @param status HTTP status code
 * @param ex The exception to extract error information from
 * @return Response object containing error details in JSON
 */","* Creates a HTTP JAX-RPC response serializing the exception in it as JSON.
   *
   * @param status the error code to set in the response
   * @param ex the exception to serialize in the response
   * @return the JAX-RPC response with the set error and JSON encoded exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,throwEx,org.apache.hadoop.util.HttpExceptionUtils:throwEx(java.lang.Throwable),119,121,"/**
 * Wraps the given exception as a RuntimeException using HttpExceptionUtils.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PureJavaCrc32C.java,<init>,org.apache.hadoop.util.PureJavaCrc32C:<init>(),41,43,"/**
 * Constructs a PureJavaCrc32C object and initializes it.
 */",Create a new PureJavaCrc32 object.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,remove,org.apache.hadoop.util.IntrusiveCollection:remove(java.lang.Object),326,338,"/**
 * Checks if an element is valid and processes it.
 * @param o The element to check, must be an Element.
 * @return True if valid, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,toArray,org.apache.hadoop.util.IntrusiveCollection:toArray(),256,264,"/**
 * Converts an iterator to an Object array.
 * @return An array of Objects from the iterator.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,retainAll,org.apache.hadoop.util.IntrusiveCollection:retainAll(java.util.Collection),372,384,"/**
 * Checks if the collection contains all elements of the iterator.
 * @param collection Collection to check against.
 * @return True if any element was removed from the iterator.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,clear,org.apache.hadoop.util.IntrusiveCollection:clear(),389,395,"/**
* Iterates through elements, calling m1() and m2() on each.
*/",* Remove all elements.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,containsAll,org.apache.hadoop.util.IntrusiveCollection:containsAll(java.util.Collection),340,348,"/**
 * Checks if all elements in the collection satisfy the predicate m1.
 * @param collection Collection of elements to check.
 * @return True if all elements satisfy m1, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CleanerUtil.java,unmapHackImpl,org.apache.hadoop.util.CleanerUtil:unmapHackImpl(),89,160,"/**
* Attempts to unmap a DirectByteBuffer, handling potential exceptions.
* Returns an unmapper or an error message if unmapping fails.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,setIncludesFile,org.apache.hadoop.util.HostsFileReader:setIncludesFile(java.lang.String),313,319,"/**
 * Sets the includes file for the current host details.
 * @param includesFile Path to the new includes file.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,setExcludesFile,org.apache.hadoop.util.HostsFileReader:setExcludesFile(java.lang.String),321,328,"/**
 * Sets the excludes file for host details.
 * @param excludesFile Path to the excludes file.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,updateFileNames,"org.apache.hadoop.util.HostsFileReader:updateFileNames(java.lang.String,java.lang.String)",330,337,"/**
 * Updates host details with provided includes/excludes files.
 * @param includesFile Path to the includes file.
 * @param excludesFile Path to the excludes file.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getExcludedHosts,org.apache.hadoop.util.HostsFileReader:getExcludedHosts(),266,269,"/**
 * Retrieves a set of strings from the host details.
 * @return A set of strings representing host details.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGroupsForUserCommand,org.apache.hadoop.util.Shell:getGroupsForUserCommand(java.lang.String),229,239,"/**
 * Retrieves group memberships for a user, platform-dependent.
 * @param user The username to query.
 * @return An array of strings representing the command.
 */
","* A command to get a given user's groups list.
   * If the OS is not WINDOWS, the command will get the user's primary group
   * first and finally get the groups list which includes the primary group.
   * i.e. the user's primary group will be included twice.
   *
   * @param user user.
   * @return groups for user command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGroupsIDForUserCommand,org.apache.hadoop.util.Shell:getGroupsIDForUserCommand(java.lang.String),251,261,"/**
 * Returns command to retrieve user group memberships.
 * Uses different commands based on OS (Windows/Linux).
 */","* A command to get a given user's group id list.
   * The command will get the user's primary group
   * first and finally get the groups list which includes the primary group.
   * i.e. the user's primary group will be included twice.
   * This command does not support Windows and will only return group names.
   *
   * @param user user.
   * @return groups id for user command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGetPermissionCommand,org.apache.hadoop.util.Shell:getGetPermissionCommand(),279,282,"/**
 * Returns OS-specific command array (Windows or Unix).
 */","* Return a command to get permission information.
   *
   * @return permission command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetPermissionCommand,"org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean)",291,301,"/**
 * Generates chmod command based on platform and recursion.
 * @param perm permission string.
 * @param recursive flag for recursive chmod.
 * @return String array representing the chmod command.
 */","* Return a command to set permission.
   *
   * @param perm permission.
   * @param recursive recursive.
   * @return set permission command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetOwnerCommand,org.apache.hadoop.util.Shell:getSetOwnerCommand(java.lang.String),326,330,"/**
 * Returns chown command based on OS.
 * @param owner The user to change ownership to.
 * @return String array representing the chown command.
 */
","* Return a command to set owner.
   *
   * @param owner owner.
   * @return set owner command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSymlinkCommand,"org.apache.hadoop.util.Shell:getSymlinkCommand(java.lang.String,java.lang.String)",339,343,"/**
 * Creates command array for creating symbolic links.
 * @param target The target file/directory.
 * @param link The link file/directory.
 */
","* Return a command to create symbolic links.
   *
   * @param target target.
   * @param link link.
   * @return symlink command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getReadlinkCommand,org.apache.hadoop.util.Shell:getReadlinkCommand(java.lang.String),351,355,"/**
 * Reads a symbolic link based on the OS.
 * @param link The symbolic link to read.
 * @return An array containing readlink command.
 */","* Return a command to read the target of the a symbolic link.
   *
   * @param link link.
   * @return read link command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSignalKillCommand,"org.apache.hadoop.util.Shell:getSignalKillCommand(int,java.lang.String)",373,394,"/**
* Constructs a kill command string based on OS and code.
* @param code Signal code for kill.
* @param pid Process ID to kill.
* @return String array representing the kill command.
*/","* Return a command to send a signal to a given pid.
   *
   * @param code code.
   * @param pid pid.
   * @return signal kill command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,appendScriptExtension,"org.apache.hadoop.util.Shell:appendScriptExtension(java.io.File,java.lang.String)",419,421,"/**
* Creates a File object with the given parent directory and basename.
*/
","* Returns a File referencing a script with the given basename, inside the
   * given parent directory.  The file extension is inferred by platform:
   * <code>"".cmd""</code> on Windows, or <code>"".sh""</code> otherwise.
   *
   * @param parent File parent directory
   * @param basename String script file basename
   * @return File referencing the script in the directory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkHadoopHome,org.apache.hadoop.util.Shell:checkHadoopHome(),483,493,"/**
 * Finds the Hadoop home directory, returning a File object.
 */","*  Centralized logic to discover and validate the sanity of the Hadoop
   *  home directory.
   *
   *  This does a lot of work so it should only be called
   *  privately for initialization once per process.
   *
   * @return A directory that exists and via was specified on the command line
   * via <code>-Dhadoop.home.dir</code> or the <code>HADOOP_HOME</code>
   * environment variable.
   * @throws FileNotFoundException if the properties are absent or the specified
   * path is not a reference to a valid directory.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getHadoopHomeDir,org.apache.hadoop.util.Shell:getHadoopHomeDir(),620,627,"/**
 * Returns the Hadoop home file. Throws exception if failure cause exists.
 */","* Get the Hadoop home directory. If it is invalid,
   * throw an exception.
   * @return a path referring to hadoop home.
   * @throws FileNotFoundException if the directory doesn't exist.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBinInner,"org.apache.hadoop.util.Shell:getQualifiedBinInner(java.io.File,java.lang.String)",656,685,"/**
 * Returns a File object representing the executable in Hadoop bin.
 * @param hadoopHomeDir Hadoop home directory
 * @param executable Executable file name
 * @return File object or throws FileNotFoundException
 */
","* Inner logic of {@link #getQualifiedBin(String)}, accessible
   * for tests.
   * @param hadoopHomeDir home directory (assumed to be valid)
   * @param executable executable
   * @return path to the binary
   * @throws FileNotFoundException if the executable was not found/valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getWinUtilsFile,org.apache.hadoop.util.Shell:getWinUtilsFile(),800,808,"/**
 * Returns WINUTILS_FILE if WINUTILS_FAILURE is null, otherwise throws exception.
 */","* Get a file reference to winutils.
   * Always raises an exception if there isn't one
   * @return the file instance referring to the winutils bin.
   * @throws FileNotFoundException on any failure to locate that file.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,destroyAllShellProcesses,org.apache.hadoop.util.Shell:destroyAllShellProcesses(),1428,1437,"/**
 * Iterates through shells, calls m2 on non-null m1, then calls m4.
 */","* Static method to destroy all running <code>Shell</code> processes.
   * Iterates through a map of all currently running <code>Shell</code>
   * processes and destroys them one by one. This method is thread safe",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:run(),1406,1420,"/**
* Executes a shell process and handles potential exceptions.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownThreadsHelper.java,shutdownThread,org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread),43,45,"/**
 * Attempts to shutdown a thread.
 * @param thread The thread to shutdown.
 * @return True if shutdown was successful, false otherwise.
 */","* @param thread {@link Thread to be shutdown}
   * @return <tt>true</tt> if the thread is successfully interrupted,
   * <tt>false</tt> otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownThreadsHelper.java,shutdownExecutorService,org.apache.hadoop.util.ShutdownThreadsHelper:shutdownExecutorService(java.util.concurrent.ExecutorService),78,81,"/**
 * Shuts down the provided ExecutorService.
 * @param service ExecutorService to shutdown.
 * @throws InterruptedException if interrupted while waiting.
 */","* shutdownExecutorService.
   *
   * @param service {@link ExecutorService to be shutdown}
   * @return <tt>true</tt> if the service is terminated,
   * <tt>false</tt> otherwise
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addNewPhase,org.apache.hadoop.util.Progress:addNewPhase(),80,85,"/**
 * Creates and initializes a Progress object.
 * Populates it with data from phases and context.
 * @return Progress object representing the progress.
 */
",Adds a new phase. Caller needs to set progress weightage,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(float),107,123,"/**
 * Creates a new progress phase with associated weightage.
 * @param weightage Weightage for the new progress phase.
 * @return The newly created Progress phase object.
 */","* Adds a node with a specified progress weightage to the tree.
   *
   * @param weightage weightage.
   * @return Progress.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,getInternal,org.apache.hadoop.util.Progress:getInternal(),244,269,"/**
* Calculates progress based on phases. Returns progress or default if no phases.
*/",Computes progress in this node.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,toString,org.apache.hadoop.util.Progress:toString(java.lang.StringBuilder),282,288,"/**
 * Appends status and nested data to the buffer based on phases.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,complete,org.apache.hadoop.util.Progress:complete(),170,184,"/**
 * Updates progress and notifies parent, if available.
 */","Completes this node, moving the parent node to its next child.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getStringData,org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String),260,266,"/**
 * Reads file content from path as string, UTF-8 encoded.
 * @param path file path
 * @return file content or null if file is not found
 */
","* Get the data in a ZNode.
   * @param path Path of the ZNode.
   * @return The data in the ZNode.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getStringData,"org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String,org.apache.zookeeper.data.Stat)",275,281,"/**
 * Reads file content from path as string, using UTF-8 encoding.
 * @param path file path
 * @param stat file stat object
 * @return File content as string, or null if reading fails.
 */
","* Get the data in a ZNode.
   * @param path Path of the ZNode.
   * @param stat Output statistics of the ZNode.
   * @return The data in the ZNode.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,setData,"org.apache.hadoop.util.curator.ZKCuratorManager:setData(java.lang.String,java.lang.String,int)",301,304,"/**
* Writes data to a file.
* @param path file path, @param data string data, @param version version number
*/","* Set data into a ZNode.
   * @param path Path of the ZNode.
   * @param data Data to set as String.
   * @param version Version of the data to store.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,create,"org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String,java.util.List)",343,353,"/**
 * Creates a Zookeeper node if it doesn't exist, sets ACLs.
 * @param path Zookeeper path to create.
 * @param zkAcl ACLs to apply to the node.
 * @return True if node was created, false otherwise.
 */","* Create a ZNode.
   * @param path Path of the ZNode.
   * @param zkAcl ACL for the node.
   * @return If the ZNode was created.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,delete,org.apache.hadoop.util.curator.ZKCuratorManager:delete(java.lang.String),392,398,"/**
 * Executes a sequence of operations on the Curator based on path.
 * @param path The path to operate on. Returns true on success.
 */","* Delete a ZNode.
   * @param path Path of the ZNode.
   * @return If the znode was deleted.
   * @throws Exception If it cannot contact ZooKeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeCreate,"org.apache.hadoop.util.curator.ZKCuratorManager:safeCreate(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode,java.util.List,java.lang.String)",410,419,"/**
* Creates a node with data, ACLs, and fencing ACL if path exists.
* @param path Node path, data, ACLs, and mode
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeDelete,"org.apache.hadoop.util.curator.ZKCuratorManager:safeDelete(java.lang.String,java.util.List,java.lang.String)",429,437,"/**
 * Applies fencing ACLs to a path if condition is met.
 * @param path Path to apply ACLs to.
 * @param fencingACL List of ACLs to apply.
 * @param fencingNodePath Node path for fencing.
 */
","* Deletes the path. Checks for existence of path as well.
   *
   * @param path Path to be deleted.
   * @param fencingNodePath fencingNodePath.
   * @param fencingACL fencingACL.
   * @throws Exception if any problem occurs while performing deletion.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeSetData,"org.apache.hadoop.util.curator.ZKCuratorManager:safeSetData(java.lang.String,byte[],int,java.util.List,java.lang.String)",439,446,"/**
 * Updates data at a path using a safe transaction.
 * @param path Path to update.
 * @param data Data to write.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,addClass,"org.apache.hadoop.util.ProgramDriver:addClass(java.lang.String,java.lang.Class,java.lang.String)",101,104,"/**
* Calls m1 on 'programs' with name and a ProgramDescription.
* @param name Program name.
* @param mainClass Main class of the program.
* @param description Program description.
*/
","* This is the method that adds the classed to the repository.
   * @param name The name of the string you want the class instance to be called with
   * @param mainClass The class that you want to add to the repository
   * @param description The description of the class
   * @throws NoSuchMethodException when a particular method cannot be found.
   * @throws SecurityException security manager to indicate a security violation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,impl,"org.apache.hadoop.util.dynamic.DynConstructors$Builder:impl(java.lang.String,java.lang.Class[])",134,149,"/**
 * Sets constructor with given class name and types.
 * @param className Class name of the constructor.
 * @param types Class types of the constructor parameters.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,hiddenImpl,org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.Class[]),166,169,"/**
 * Calls m1 with the base class and provided types.
 * @param types Class types to pass to m1.
 * @return This builder instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.String,java.lang.Class[])",171,186,"/**
 * Sets constructor with given class name and types.
 * @param className Class name of the constructor.
 * @param types Class types of the constructor parameters.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,ctorImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.Class,java.lang.Class[])",348,362,"/**
 * Finds and caches a constructor for targetClass with given arg types.
 * @param targetClass Target class.
 * @param argClasses Constructor argument types.
 * @return This builder.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,newInstanceChecked,org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstanceChecked(java.lang.Object[]),56,66,"/**
 * Calls constructor 'm3' with given args, handling exceptions.
 * @param args Arguments to pass to the constructor.
 * @throws Exception if instantiation or access fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeChecked(java.lang.Object,java.lang.Object[])",73,89,"/**
* Calls a method with adjusted arguments, handling exceptions.
* @param target The target object.
* @param args Arguments to pass to the method.
* @return The result of the method call.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.String,java.lang.Class[])",320,333,"/**
 * Loads a method from a class.
 * @param targetClass Class containing the method.
 * @param methodName Method name to load.
 * @param argClasses Method argument classes.
 * @return Builder object.
 */
","* Checks for a method implementation.
     * @param targetClass the class to check for an implementation
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,<init>,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:<init>(java.lang.reflect.Method,java.lang.String)",66,71,"/**
 * Initializes an UnboundMethod with a method and name.
 * @param method The method to be unbound.
 * @param name The name of the method.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,<init>,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:<init>(java.lang.reflect.Constructor,java.lang.Class)",46,50,"/**
 * Initializes a new instance with a constructor and class.
 * @param constructor Constructor to use for instantiation.
 * @param constructed Class being constructed.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.String,java.lang.Class[])",423,438,"/**
 * Loads a hidden method and makes it accessible.
 * @param targetClass Class containing the method
 * @param methodName Method name
 * @param argClasses Method argument classes
 * @return Builder object
 */
","* Checks for a method implementation.
     * @param targetClass the class to check for an implementation
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,noop,org.apache.hadoop.util.dynamic.BindingUtils:noop(java.lang.String),158,160,"/**
 * Creates an UnboundMethod instance with specified name, applying m1 and m2.
 * @param name Method name.
 * @return An UnboundMethod object.
 */
","* Create a no-op method.
   *
   * @param name method name
   *
   * @return a no-op method.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,implemented,org.apache.hadoop.util.dynamic.BindingUtils:implemented(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod[]),169,176,"/**
 * Checks if all methods' m1() return false.
 * @param methods Array of UnboundMethod instances.
 * @return True if all m1() calls return false.
 */
","* Given a sequence of methods, verify that they are all available.
   *
   * @param methods methods
   *
   * @return true if they are all implemented",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,available,org.apache.hadoop.util.dynamic.BindingUtils:available(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod),195,197,"/**
* Returns the inverse boolean value of the result of method.m1().
*/","* Is a method available?
   * @param method method to probe
   * @return true iff the method is found and loaded.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,bind,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:bind(java.lang.Object),118,125,"/**
 * Binds a method to a receiver object.
 * @param receiver The object to bind the method to.
 * @return A BoundMethod instance.
 */
","* Returns this method as a BoundMethod for the given receiver.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} for this method and the receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,asStatic,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:asStatic(),146,149,"/**
* Calls m2 with result of m1 and a message, returns StaticMethod.
*/","* Returns this method as a StaticMethod.
     * @return a {@link StaticMethod} for this method
     * @throws IllegalStateException if the method is not static",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ClassUtil.java,findContainingJar,org.apache.hadoop.util.ClassUtil:findContainingJar(java.lang.Class),38,40,"/**
* Calls m3 with results of clazz.m1(), clazz.m2(), and ""jar"".
*/","* Find a jar that contains a class of the same name, if any.
   * It will return a jar file, even if that is not the first thing
   * on the class path that has a class with the same name.
   * 
   * @param clazz the class to find.
   * @return a jar file that contains the class, or null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ClassUtil.java,findClassLocation,org.apache.hadoop.util.ClassUtil:findClassLocation(java.lang.Class),48,50,"/**
 * Masks a class using m1, m2 methods and ""file"".
 */","* Find the absolute location of the class.
   *
   * @param clazz the class to find.
   * @return the class file with absolute location, or null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeRpcRequestHeader,"org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[],org.apache.hadoop.ipc.AlignmentContext)",178,212,"/**
* Creates an RpcRequestHeaderProto with given parameters.
* @param rpcKind RpcKind enum, operation, callId, retryCount, uuid, alignmentContext
* @return RpcRequestHeaderProto object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,getHeader,org.apache.hadoop.util.DataChecksum:getHeader(),226,235,"/**
 * Creates a byte array header based on type ID and bytesPerChecksum.
 * @return Byte array representing the header.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,mapByteToChecksumType,org.apache.hadoop.util.DataChecksum:mapByteToChecksumType(int),204,212,"/**
 * Converts an integer type to a Type object, or throws exception.
 * @param type Integer representing the type.
 * @throws InvalidChecksumSizeException if type is invalid.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,writeValue,"org.apache.hadoop.util.DataChecksum:writeValue(java.io.DataOutputStream,boolean)",246,263,"/**
 * Writes checksum data to output and resets state.
 * @param out Output stream to write to.
 * @param reset Reset internal state after writing.
 * @return Size of the type.
 */","* Writes the current checksum to the stream.
   * If <i>reset</i> is true, then resets the checksum.
   *
   * @param out out.
   * @param reset reset.
   * @return number of bytes written. Will be equal to getChecksumSize();
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,writeValue,"org.apache.hadoop.util.DataChecksum:writeValue(byte[],int,boolean)",275,296,"/**
* Writes checksum to buffer, resets summer if needed.
* @param buf buffer to write to, @param offset offset, @param reset reset flag
* @return size of checksum type
*/","* Writes the current checksum to a buffer.
    * If <i>reset</i> is true, then resets the checksum.
    *
    * @param buf buf.
    * @param offset offset.
    * @param reset reset.
    * @return number of bytes written. Will be equal to getChecksumSize();
    * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RateLimitingFactory.java,create,org.apache.hadoop.util.RateLimitingFactory:create(int),95,100,"/**
 * Creates a RateLimiting object. Returns m1() if capacity is 0.
 */","* Create an instance.
   * If the rate is 0; return the unlimited rate.
   * @param capacity capacity in permits/second.
   * @return limiter restricted to the given capacity.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SignalLogger.java,register,org.apache.hadoop.util.SignalLogger:register(org.slf4j.Logger),71,92,"/**
 * Registers UNIX signal handlers for TERM, HUP, INT.
 * Throws IllegalStateException if already registered.
 */","* Register some signal handlers.
   *
   * @param log The log4j logfile to use in the signal handlers.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,parseItem,"org.apache.hadoop.util.ComparableVersion:parseItem(boolean,java.lang.String)",455,458,"/**
* Creates an Item based on isDigit. IntegerItem if true, StringItem otherwise.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,<init>,org.apache.hadoop.util.LightWeightGSet:<init>(int),90,97,"/**
 * Creates a LightWeightGSet with a recommended initial length.
 * @param recommended_length initial capacity suggestion
 */
",* @param recommended_length Recommended size of the internal array.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,get,org.apache.hadoop.util.LightWeightGSet:get(java.lang.Object),126,142,"/**
 * Finds an element by key.
 * @param key The key to search for.
 * @return The element or null if not found.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,clear,org.apache.hadoop.util.LightWeightGSet$Values:clear(),256,259,"/**
* Delegates method execution to the superclass's m1().
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,toString,org.apache.hadoop.util.SemaphoredDelegatingExecutor:toString(),200,209,"/**
* Creates a string representation of the executor.
* Uses helper methods to format the details.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToSet,"org.apache.hadoop.util.HostsFileReader:readFileToSet(java.lang.String,java.lang.String,java.util.Set)",77,82,"/**
 * Processes a file based on type, filename, and a set of strings.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readXmlFileToMapWithFileInputStream,"org.apache.hadoop.util.HostsFileReader:readXmlFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)",146,183,"/**
 * Parses XML file, extracts host entries, and populates the map.
 * @param type host type
 * @param filename XML filename
 * @param fileInputStream input stream for the file
 * @param map map to store host entries
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHosts,org.apache.hadoop.util.HostsFileReader:getHosts(),261,264,"/**
 * Retrieves a set of strings from the HostDetails object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHostDetails,"org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Set)",278,283,"/**
 * Populates include/exclude sets with details from current host.
 */","* Retrieve an atomic view of the included and excluded hosts.
   *
   * @param includes set to populate with included hosts
   * @param excludes set to populate with excluded hosts
   * @deprecated use {@link #getHostDetails() instead}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHostDetails,"org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Map)",292,298,"/**
* Populates include/exclude host sets from current host details.
*/","* Retrieve an atomic view of the included and excluded hosts.
   *
   * @param includeHosts set to populate with included hosts
   * @param excludeHosts map to populate with excluded hosts
   * @deprecated use {@link #getHostDetails() instead}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/JenkinsHash.java,hash,"org.apache.hadoop.util.hash.JenkinsHash:hash(byte[],int,int)",86,245,"/**
 * Processes data using a key, updating internal state.
 * @param key Key for processing.
 * @param nbytes Number of bytes to process.
 * @param initval Initial value for the processing.
 */","* taken from  hashlittle() -- hash a variable-length key into a 32-bit value
   * 
   * @param key the key (the unaligned variable-length array of bytes)
   * @param nbytes number of bytes to include in hash
   * @param initval can be any integer value
   * @return a 32-bit value.  Every bit of the key affects every bit of the
   * return value.  Two keys differing by one or two bits will have totally
   * different hash values.
   * 
   * <p>The best hash table sizes are powers of 2.  There is no need to do mod
   * a prime (mod is sooo slow!).  If you need less than 32 bits, use a bitmask.
   * For example, if you need only 10 bits, do
   * <code>h = (h &amp; hashmask(10));</code>
   * In which case, the hash table should have hashsize(10) elements.
   * 
   * <p>If you are hashing n strings byte[][] k, do it like this:
   * for (int i = 0, h = 0; i &lt; n; ++i) h = hash( k[i], h);
   * 
   * <p>By Bob Jenkins, 2006.  bob_jenkins@burtleburtle.net.  You may use this
   * code any way you wish, private, educational, or commercial.  It's free.
   * 
   * <p>Use for hash table lookup, or anything where one collision in 2^^32 is
   * acceptable.  Do NOT use for cryptographic purposes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getInstance,org.apache.hadoop.util.hash.Hash:getInstance(int),75,84,"/**
 * Selects a hash function based on the given type.
 * @param type hash function type (JENKINS_HASH, MURMUR_HASH)
 * @return Hash object or null if type is invalid.
 */
","* Get a singleton instance of hash function of a given type.
   * @param type predefined hash type
   * @return hash function instance, or null if type is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/MurmurHash.java,hash,"org.apache.hadoop.util.hash.MurmurHash:hash(byte[],int,int)",40,43,"/**
* Overload m1 with a default start index of 0.
* @param data byte array to process
* @param length length of data to process
* @param seed seed value for the operation
* @return int result of the operation
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,<init>,org.apache.hadoop.util.bloom.CountingBloomFilter:<init>(),84,84,"/**
 * Constructs a new CountingBloomFilter with default parameters.
 */
",Default constructor - use with readFields,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,<init>,org.apache.hadoop.util.bloom.BloomFilter:<init>(),99,101,"/**
 * Default constructor for the Bloom filter.
 */",Default constructor - use with readFields,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,<init>,org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>(),113,113,"/**
 * Constructs a new, empty DynamicBloomFilter.
 */",* Zero-args constructor for the serialization.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,and,org.apache.hadoop.util.bloom.CountingBloomFilter:and(org.apache.hadoop.util.bloom.Filter),162,176,"/**
 * ANDs this filter with another CountingBloomFilter.
 * @param filter The filter to AND with. Must be compatible.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,or,org.apache.hadoop.util.bloom.CountingBloomFilter:or(org.apache.hadoop.util.bloom.Filter),246,261,"/**
 * Merges the given CountingBloomFilter into the current filter.
 * @param filter The filter to merge; must be compatible.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,write,org.apache.hadoop.util.bloom.CountingBloomFilter:write(java.io.DataOutput),292,299,"/**
 * Writes bucket data to the output stream.
 * @param out Output stream to write to.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,and,org.apache.hadoop.util.bloom.DynamicBloomFilter:and(org.apache.hadoop.util.bloom.Filter),155,173,"/**
 * ANDs this filter with the given DynamicBloomFilter.
 * @param filter The filter to AND with. Must be compatible.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,not,org.apache.hadoop.util.bloom.DynamicBloomFilter:not(),190,195,"/**
 * Calls m1() on each row of the matrix.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,or,org.apache.hadoop.util.bloom.DynamicBloomFilter:or(org.apache.hadoop.util.bloom.Filter),197,214,"/**
 * ORs the given filter with the current filter.
 * @param filter The filter to OR with. Must be compatible.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,xor,org.apache.hadoop.util.bloom.DynamicBloomFilter:xor(org.apache.hadoop.util.bloom.Filter),216,233,"/**
* XORs this filter with another DynamicBloomFilter.
* @param filter The filter to XOR with. Must be compatible.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,write,org.apache.hadoop.util.bloom.BloomFilter:write(java.io.DataOutput),199,216,"/**
* Writes data to output, processing bits and converting to bytes.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/HashFunction.java,hash,org.apache.hadoop.util.bloom.HashFunction:hash(org.apache.hadoop.util.bloom.Key),108,122,"/**
 * Calculates hash values from key data.
 * @param k Key object containing data to hash.
 * @return Array of hash values.
 */
","* Hashes a specified key into several integers.
   * @param k The specified key.
   * @return The array of hashed values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,compareTo,org.apache.hadoop.util.bloom.Key:compareTo(org.apache.hadoop.util.bloom.Key),172,183,"/**
 * Compares this Key with another Key, using bytes and weight.
 * @param other the Key object to compare to
 * @return the comparison result as an integer
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,getWeight,org.apache.hadoop.util.bloom.RetouchedBloomFilter:getWeight(java.util.List),381,387,"/**
 * Calculates the total weight from a list of Key objects.
 * @param keyList List of Key objects, each with a weight.
 * @return The sum of the weights of all keys.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,formatMessage,"org.apache.hadoop.util.JvmPauseMonitor:formatMessage(long,java.util.Map,java.util.Map)",118,143,"/**
 * Generates a report on GC activity after a sleep period.
 * @param extraSleepTime Sleep time in milliseconds.
 * @param gcTimesAfterSleep GC times after sleep.
 * @param gcTimesBeforeSleep GC times before sleep.
 * @return Report string detailing GC activity.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AutoCloseableLock.java,<init>,org.apache.hadoop.util.AutoCloseableLock:<init>(),38,40,"/**
 * Creates an AutoCloseableLock with a ReentrantLock.
 */","* Creates an instance of {@code AutoCloseableLock}, initializes
   * the underlying lock instance with a new {@code ReentrantLock}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AutoCloseableLock.java,close,org.apache.hadoop.util.AutoCloseableLock:close(),94,97,"/**
* Calls the m1 method.
*/","* Attempts to release the lock by making a call to {@code release()}.
   *
   * This is to implement {@code close()} method from {@code AutoCloseable}
   * interface. This allows users to user a try-with-resource syntax, where
   * the lock can be automatically released.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,isNull,org.apache.hadoop.util.ComparableVersion$StringItem:isNull(),208,211,"/**
* Checks if a condition is met based on m1 and m2 results.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,compareTo,org.apache.hadoop.util.ComparableVersion$StringItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item),232,253,"/**
 * Processes an Item, returning an integer based on its type.
 * @param item The Item to process.
 * @return An integer value based on the item's type.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,printStack,"org.apache.hadoop.util.FindClass:printStack(java.lang.Throwable,java.lang.String,java.lang.Object[])",234,237,"/**
 * Logs an error message with details and exception to stderr.
 * @param e The exception to log.
 * @param text Error message format.
 * @param args Arguments for the message format.
 */
","* print a stack trace with text
   * @param e the exception to print
   * @param text text to print",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,explainResult,"org.apache.hadoop.util.FindClass:explainResult(int,java.lang.String)",368,370,"/**
 * Logs an error message with an error code and text.
 * @param errorcode Error code to log.
 * @param text Error message text.
 */
","* Explain an error code as part of the usage
   * @param errorcode error code returned
   * @param text error text",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadedClass,"org.apache.hadoop.util.FindClass:loadedClass(java.lang.String,java.lang.Class)",266,271,"/**
 * Logs class loading information, including name and URL.
 * @param name Class name.
 * @param clazz The class object.
 */
","* Log that a class has been loaded, and where from.
   * @param name classname
   * @param clazz class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,calculateGCTimePercentageWithinObservedInterval,org.apache.hadoop.util.GcTimeMonitor:calculateGCTimePercentageWithinObservedInterval(),186,224,"/**
 * Updates GC data buffer with metrics from GarbageCollectorMXBeans.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,getLatestGcData,org.apache.hadoop.util.GcTimeMonitor:getLatestGcData(),182,184,"/**
 * Retrieves data using the m1() method of the curData object.
 */","* Returns a copy of the most recent data measured by this monitor.
   * @return a copy of the most recent data measured by this monitor",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PureJavaCrc32.java,<init>,org.apache.hadoop.util.PureJavaCrc32:<init>(),45,47,"/**
 * Constructs a PureJavaCrc32 object and initializes it.
 */",Create a new PureJavaCrc32 object.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,executeShutdown,org.apache.hadoop.util.ShutdownHookManager:executeShutdown(),117,136,"/**
* Executes shutdown hooks and returns the number of timeouts.
*/","* Execute the shutdown.
   * This is exposed purely for testing: do not invoke it.
   * @return the number of shutdown hooks which timed out.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,put,org.apache.hadoop.util.PriorityQueue:put(java.lang.Object),61,65,"/**
* Adds an element to the heap and maintains heap properties.
* @param element The element to add to the heap.
*/
","* Adds an Object to a PriorityQueue in log(size) time.
   * If one tries to add more objects than maxSize from initialize
   * a RuntimeException (ArrayIndexOutOfBound) is thrown.
   * @param element element.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,pop,org.apache.hadoop.util.PriorityQueue:pop(),104,114,"/**
 * Removes and returns the root element (first) from the heap.
 * Returns null if the heap is empty.
 */
","* Removes and returns the least element of the PriorityQueue in log(size)
      time.
   * @return T Generics Type T.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,adjustTop,org.apache.hadoop.util.PriorityQueue:adjustTop(),123,125,"/**
* Calls the m1 method.
*/","Should be called when the Object at top changes values.  Still log(n)
   * worst case, but it's at least twice as fast to <pre>
   *  { pq.top().change(); pq.adjustTop(); }
   * </pre> instead of <pre>
   *  { o = pq.pop(); o.change(); pq.push(o); }
   * </pre>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,addAll,"org.apache.hadoop.util.Sets:addAll(java.util.TreeSet,java.lang.Iterable)",161,171,"/**
 * Adds elements from an iterable to a TreeSet.
 * @param addTo TreeSet to add elements to
 * @param elementsToAdd Iterable of elements to add
 * @return True if all elements were added, otherwise false.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.util.Iterator),191,195,"/**
 * Converts an iterator to a HashSet.
 * @param elements Iterator of elements to add to the set.
 * @return A HashSet containing the elements from the iterator.
 */
","* Creates a <i>mutable</i> {@code HashSet} instance containing the given
   * elements. A very thin convenience for creating an empty set and then
   * calling Iterators#addAll.
   *
   * <p><b>Note:</b> if mutability is not required and the elements are
   * non-null, use ImmutableSet#copyOf(Iterator) instead.</p>
   *
   * <p><b>Note:</b> if {@code E} is an {@link Enum} type, you should create
   * an {@link EnumSet} instead.</p>
   *
   * <p>Overall, this method is not very useful and will likely be deprecated
   * in the future.</p>
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return a new, empty thread-safe {@code Set}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSetWithExpectedSize,org.apache.hadoop.util.Sets:newHashSetWithExpectedSize(int),213,215,"/**
* Creates a HashSet with elements from m1, sized for expectedSize.
*/","* Returns a new hash set using the smallest initial table size that can hold
   * {@code expectedSize} elements without resizing. Note that this is not what
   * {@link HashSet#HashSet(int)} does, but it is what most users want and
   * expect it to do.
   *
   * <p>This behavior can't be broadly guaranteed, but has been tested with
   * OpenJDK 1.7 and 1.8.</p>
   *
   * @param expectedSize the number of elements you expect to add to the
   *     returned set
   * @param <E> Generics Type E.
   * @return a new, empty hash set with enough capacity to hold
   *     {@code expectedSize} elements without resizing
   * @throws IllegalArgumentException if {@code expectedSize} is negative",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SequentialNumber.java,skipTo,org.apache.hadoop.util.SequentialNumber:skipTo(long),78,91,"/**
 * Advances to a new value, throwing exception if newValue is too small.
 * @param newValue The target value to advance to.
 * @throws IllegalStateException if newValue is less than current value.
 */","* Skip to the new value.
   * @param newValue newValue.
   * @throws IllegalStateException
   *         Cannot skip to less than the current value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,printUsage,org.apache.hadoop.util.ProgramDriver:printUsage(java.util.Map),85,91,"/**
 * Prints program names and descriptions to the console.
 * @param programs A map of program names to descriptions.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/CommandShell.java,run,org.apache.hadoop.tools.CommandShell:run(java.lang.String[]),63,84,"/**
 * Executes a command sequence, handling errors and exit codes.
 * @param args Command-line arguments passed to the function.
 * @return Exit code of the command sequence.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,<init>,"org.apache.hadoop.tools.TableListing$Column:<init>(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)",52,58,"/**
 * Initializes a new column with a title, justification, and wrap setting.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,build,org.apache.hadoop.tools.TableListing$Builder:build(),194,197,"/**
* Creates a TableListing with specified columns, header, and wrap width.
*/","* Create a new TableListing.
     *
     * @return TableListing.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$7:getDefault(double),522,522,"/**
* Abstract method to apply a mask to a double value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$3:getDefault(double),522,522,"/**
* Abstract method to mask a double value.
* @param value The double value to be masked.
* @return The masked double value.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,isDeprecated,org.apache.hadoop.conf.Configuration:isDeprecated(java.lang.String),670,672,"/**
* Checks if a key is deprecated.
* @param key The key to check.
* @return True if deprecated, false otherwise.
*/
","* checks whether the given <code>key</code> is deprecated.
   * 
   * @param key the parameter which is to be checked for deprecation
   * @return <code>true</code> if the key is deprecated and 
   *         <code>false</code> otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDeprecatedKeyInfo,org.apache.hadoop.conf.Configuration:getDeprecatedKeyInfo(java.lang.String),678,680,"/**
 * Retrieves deprecated key info using a chain of method calls.
 * @param key The key to look up.
 * @return DeprecatedKeyInfo object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpDeprecatedKeys,org.apache.hadoop.conf.Configuration:dumpDeprecatedKeys(),4009,4019,"/**
 * Prints deprecation information, including new keys, to console.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,hasWarnedDeprecation,org.apache.hadoop.conf.Configuration:hasWarnedDeprecation(java.lang.String),4027,4035,"/**
 * Checks if a name is deprecated and accessed.
 * @param name The name to check.
 * @return True if deprecated and accessed, false otherwise.
 */
","* Returns whether or not a deprecated name has been warned. If the name is not
   * deprecated then always return false
   * @param name proprties.
   * @return true if name is a warned deprecation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDeprecatedKey,org.apache.hadoop.conf.Configuration:getDeprecatedKey(java.lang.String),674,676,"/**
* Masks a key using a chain of context methods.
* @param key The key to be masked.
* @return The masked key.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,reloadExistingConfigurations,org.apache.hadoop.conf.Configuration:reloadExistingConfigurations(),880,888,"/**
 * Reloads existing configurations from the registry. Logs action.
 */",* Reload existing configuration instances.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDefaultResource,org.apache.hadoop.conf.Configuration:addDefaultResource(java.lang.String),895,904,"/**
 * Processes a name, loading defaults if not already present.
 */","* Add a default resource. Resources are loaded in the order of the resources 
   * added.
   * @param name file name. File should be present in the classpath.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,boolean)",257,259,"/**
 * Constructs a Resource with resource and restricted parser flag.
 * @param resource The resource object.
 * @param useRestrictedParser Flag to use restricted parser.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurationHelper,"org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1951,1969,"/**
* Parses a time duration string, converts it, and returns a long.
*/","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param vStr The string value with time unit suffix to be converted.
   * @param defaultUnit Unit to convert the stored property, if it exists.
   * @param returnUnit Unit for the returned value.
   * @return time duration in given time unit.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,parse,"org.apache.hadoop.conf.Configuration:parse(java.net.URL,boolean)",3045,3063,"/**
 * Parses an XML stream from a URL, handling JarURLConnection.
 * @param url URL to parse; restricted flag. Returns XMLStreamReader.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleInclude,org.apache.hadoop.conf.Configuration$Parser:handleInclude(),3296,3372,"/**
 * Parses XInclude resources, handling URLs and fallbacks.
 * Reads and parses included resources, adding them to results.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadProperty,"org.apache.hadoop.conf.Configuration:loadProperty(java.util.Properties,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String[])",3546,3568,"/**
* Sets property value, handling nulls and final parameters.
* @param properties Properties object to update
* @param name Property name
* @param attr Attribute name
* @param value Property value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,toString,org.apache.hadoop.conf.Configuration:toString(),3901,3913,"/**
 * Appends configuration details to a StringBuilder.
 * Builds a string representing the configuration.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getAllPropertiesByTags,org.apache.hadoop.conf.Configuration:getAllPropertiesByTags(java.util.List),4056,4062,"/**
 * Creates a Properties object from a list of tags.
 * @param tagList List of tags to process.
 * @return Properties object containing processed tag data.
 */
","* Get all properties belonging to list of input tags. Calls
   * getAllPropertiesByTag internally.
   * @param tagList list of input tags
   * @return Properties with matching tags",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$6:getDefault(double),522,522,"/**
* Abstract method to apply a mask to a double value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$2:getDefault(double),522,522,"/**
* Abstract method to apply a masked transformation to a double value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,redact,"org.apache.hadoop.conf.ConfigRedactor:redact(java.lang.String,java.lang.String)",65,70,"/**
 * Returns REDACTED_TEXT if key matches a condition, otherwise returns value.
 */","* Given a key / value pair, decides whether or not to redact and returns
   * either the original value or text indicating it has been redacted.
   *
   * @param key param key.
   * @param value param value, will return if conditions permit.
   * @return Original value, or text indicating it has been redacted",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,redactXml,"org.apache.hadoop.conf.ConfigRedactor:redactXml(java.lang.String,java.lang.String)",97,102,"/**
 * Returns a redacted value if key matches a condition, otherwise returns the original value.
 */","* Given a key / value pair, decides whether or not to redact and returns
   * either the original value or text indicating it has been redacted.
   *
   * @param key param key.
   * @param value param value, will return if conditions permit.
   * @return Original value, or text indicating it has been redacted",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,getReconfigurationTaskStatus,org.apache.hadoop.conf.ReconfigurableBase:getReconfigurationTaskStatus(),189,196,"/**
 * Returns the ReconfigurationTaskStatus based on thread state.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$4:getDefault(double),522,522,"/**
* Abstract method to apply a mask to a double value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,startReconfigurationTask,org.apache.hadoop.conf.ReconfigurableBase:startReconfigurationTask(),169,187,"/**
* Starts a reconfiguration task, throwing IOException if blocked.
*/","* Start a reconfiguration task to reload configuration in background.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationContext:<init>(org.apache.hadoop.conf.Configuration$DeprecationContext,org.apache.hadoop.conf.Configuration$DeprecationDelta[])",487,517,"/**
 * Constructs a DeprecationContext from another context and deltas.
 * @param other The context to copy from.
 * @param deltas Array of deprecation deltas to apply.
 */
","* Create a new DeprecationContext by copying a previous DeprecationContext
     * and adding some deltas.
     *
     * @param other   The previous deprecation context to copy, or null to start
     *                from nothing.
     * @param deltas  The deltas to apply.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationException.java,<init>,"org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.Throwable)",67,74,"/**
 * Constructs a ReconfigurationException with property details and cause.
 */
","* Create a new instance of {@link ReconfigurationException}.
   * @param property property name.
   * @param newVal new value.
   * @param oldVal old value.
   * @param cause original exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationException.java,<init>,"org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String)",82,88,"/**
 * Constructs a ReconfigurationException with property details.
 * @param property Property name.
 * @param newVal New value.
 * @param oldVal Old value.
 */
","* Create a new instance of {@link ReconfigurationException}.
   * @param property property name.
   * @param newVal new value.
   * @param oldVal old value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleEndProperty,org.apache.hadoop.conf.Configuration$Parser:handleEndProperty(),3415,3446,"/**
 * Parses configuration based on source, handles deprecations, and adds results.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getWarningMessage,org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getWarningMessage(java.lang.String),382,384,"/**
 * Calls overloaded method with null value for the second parameter.
 * @param key The key to pass to the overloaded method.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,iterator,org.apache.hadoop.conf.Configuration$IntegerRanges:iterator(),2283,2286,"/**
 * Returns an iterator for the specified ranges of numbers.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration$IntegerRanges:<init>(java.lang.String),2195,2217,"/**
 * Parses a comma-separated string of integer ranges and adds them.
 * @param newValue String containing comma-separated integer ranges.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageSize.java,parse,org.apache.hadoop.conf.StorageSize:parse(java.lang.String),50,96,"/**
 * Parses a storage size string (e.g., ""1000MB"") into a StorageSize object.
 * @param value The storage size string to parse.
 * @return StorageSize object representing the parsed size.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$5:getDefault(double),522,522,"/**
* Abstract method to apply a mask to a double value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$1:getDefault(double),522,522,"/**
* Abstract method to apply a mask to a double value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reset,org.apache.hadoop.ha.ActiveStandbyElector:reset(),931,934,"/**
 * Initializes the state to INIT and calls method m1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,convert,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:convert(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),144,162,"/**
* Creates HAStateChangeRequestInfoProto based on request source.
* @param reqInfo StateChangeRequestInfo object containing source
* @return HAStateChangeRequestInfoProto object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,convert,org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:convert(org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto),87,105,"/**
 * Creates StateChangeRequestInfo from HAStateChangeRequestInfoProto.
 * @param proto Proto containing request source information.
 * @return StateChangeRequestInfo object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,createReqInfo,org.apache.hadoop.ha.HAAdmin:createReqInfo(),264,266,"/**
 * Creates a StateChangeRequestInfo with the request source.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,createReqInfo,org.apache.hadoop.ha.ZKFailoverController:createReqInfo(),510,512,"/**
 * Creates a StateChangeRequestInfo with the REQUEST_BY_ZKFC source.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,createReqInfo,org.apache.hadoop.ha.FailoverController:createReqInfo(),158,160,"/**
 * Creates a StateChangeRequestInfo with the request source.
 * @return StateChangeRequestInfo object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getServiceStatus,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getServiceStatus(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto)",143,178,"/**
 * Gets service status. Converts HAServiceStatus to response proto.
 * @param controller RPC controller
 * @param request Request object
 * @return GetServiceStatusResponseProto object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,startRPC,org.apache.hadoop.ha.ZKFailoverController:startRPC(),336,338,"/**
* Calls the m1 method on the rpcServer.
* Throws IOException if m1() does.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,parseConfiggedPort,org.apache.hadoop.ha.SshFenceByTcpPort$Args:parseConfiggedPort(java.lang.String),258,266,"/**
 * Parses port string to an integer.
 * @param portStr The port string to parse.
 * @throws BadFencingConfigurationException if parsing fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,checkArgs,org.apache.hadoop.ha.ShellCommandFencer:checkArgs(java.lang.String),72,79,"/**
 * Executes fencing, throws exception if no argument is provided.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String,java.util.Map)",138,149,"/**
 * Prints command usage information to the PrintStream.
 * @param pStr PrintStream to output to
 * @param cmd Command to display usage for
 * @param helpEntries Map of command to UsageInfo
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkManualStateManagementOK,org.apache.hadoop.ha.HAAdmin:checkManualStateManagementOK(org.apache.hadoop.ha.HAServiceTarget),245,262,"/**
* Checks if manual HA management is allowed based on target and flags.
* @param target The HAServiceTarget to check.
* @return True if manual management is allowed, false otherwise.
*/","* Ensure that we are allowed to manually manage the HA state of the target
   * service. If automatic failover is configured, then the automatic
   * failover controllers should be doing state management, and it is generally
   * an error to use the HAAdmin command line to do so.
   * 
   * @param target the target to check
   * @return true if manual state management is allowed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,execCommand,"org.apache.hadoop.ha.SshFenceByTcpPort:execCommand(com.jcraft.jsch.Session,java.lang.String)",177,203,"/**
 * Executes a command on a remote host via SSH and returns exit code.
 * @param session SSH session object
 * @param cmd Command to execute
 * @return Exit code of the command
 */
","* Execute a command through the ssh session, pumping its
   * stderr and stdout to our own logs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,enteredState,org.apache.hadoop.ha.ZKFailoverController$HealthCallbacks:enteredState(org.apache.hadoop.ha.HealthMonitor$State),988,992,"/**
* Updates health state.
* Calls m1 with newState and m2.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,badArg,org.apache.hadoop.ha.ZKFailoverController:badArg(java.lang.String),272,276,"/**
 * Throws an exception with a bad argument message.
 * @param arg The argument that is considered invalid.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,checkEligibleForFailover,org.apache.hadoop.ha.ZKFailoverController:checkEligibleForFailover(),763,776,"/**
 * Checks if failover is possible; throws exception if not healthy.
 */","* If the local node is an observer or is unhealthy it
   * is not eligible for graceful failover.
   * @throws ServiceFailedException if the node is an observer or unhealthy",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,fence,org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget),92,94,"/**
 * Calls m1 with the provided service target and a null context.
 * @param fromSvc The HAServiceTarget to use.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getFencingParameters,org.apache.hadoop.ha.HAServiceTarget:getFencingParameters(),175,179,"/**
* Populates a map with data using m2, then returns it.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,monitorActiveStatus,org.apache.hadoop.ha.ActiveStandbyElector:monitorActiveStatus(),774,781,"/**
 * Checks election eligibility, logs activity, resets retry count, and calls m3.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,zkDoWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction,org.apache.zookeeper.KeeperException$Code)",1145,1159,"/**
 * Executes an action with retries based on retry codes.
 * @param action Action to execute, returns a value of type T.
 * @param retryCode Retry code to check for.
 * @return Result of the action, or throws exception on failure.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readInDirectBuffer,"org.apache.hadoop.fs.VectoredReadUtils:readInDirectBuffer(org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer,org.apache.hadoop.util.functional.Function4RaisingIOE)",189,216,"/**
 * Reads data from a file range into a buffer using an operation.
 * @param range FileRange object defining the range to read.
 */","* Read bytes from stream into a byte buffer using an
   * intermediate byte array.
   *   <pre>
   *     (position, buffer, buffer-offset, length): Void
   *     position:= the position within the file to read data.
   *     buffer := a buffer to read fully `length` bytes into.
   *     buffer-offset := the offset within the buffer to write data
   *     length := the number of bytes to read.
   *   </pre>
   * The passed in function MUST block until the required length of
   * data is read, or an exception is thrown.
   * @param range range to read
   * @param buffer buffer to fill.
   * @param operation operation to use for reading data.
   * @throws IOException any IOE.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateAndSortRanges,"org.apache.hadoop.fs.VectoredReadUtils:validateAndSortRanges(java.util.List,java.util.Optional)",292,337,"/**
* Sorts file ranges and validates against optional file length.
* @param input List of FileRange objects to sort.
* @param fileLength Optional file length for validation.
* @return Sorted list of FileRange objects.
*/","* Validate a list of ranges (including overlapping checks) and
   * return the sorted list.
   * <p>
   * Two ranges overlap when the start offset
   * of second is less than the end offset of first.
   * End offset is calculated as start offset + length.
   * @param input input list
   * @param fileLength file length if known
   * @return a new sorted list.
   * @throws IllegalArgumentException if there are overlapping ranges or
   * a range element is invalid (other than with negative offset)
   * @throws EOFException if the last range extends beyond the end of the file supplied
   *                          or a range offset is negative",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,readVectored,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:readVectored(java.util.List,java.util.function.IntFunction)",319,345,"/**
 * Performs vectored read operations on file ranges asynchronously.
 * @param ranges List of file ranges to read.
 * @param allocate Allocates ByteBuffer for each range.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,<init>,org.apache.hadoop.fs.impl.prefetch.BlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockData),49,53,"/**
 * Constructs a BlockManager with the provided BlockData.
 * @param blockData The BlockData to use for block management.
 */
","* Constructs an instance of {@code BlockManager}.
   *
   * @param blockData information about each block of the underlying file.
   *
   * @throws IllegalArgumentException if blockData is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,release,org.apache.hadoop.fs.impl.prefetch.BlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData),107,111,"/**
* Calls m1 with the provided data and a string key.
* @param data BufferData object to be processed
*/
","* Releases resources allocated to the given block.
   *
   * @param data the {@code BufferData} to release.
   *
   * @throws IllegalArgumentException if data is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,release,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:release(java.lang.Object),88,111,"/**
 * Adds an item to the pool, validating its origin.
 * @param item The item to add to the pool.
 */","* Releases a previously acquired resource.
   *
   * @throws IllegalArgumentException if item is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,throwIfStateIncorrect,org.apache.hadoop.fs.impl.prefetch.BufferData:throwIfStateIncorrect(org.apache.hadoop.fs.impl.prefetch.BufferData$State[]),259,275,"/**
* Throws IllegalStateException if state validation fails.
* Validates states, throws exception if they don't match.
*/","* Helper that asserts the current state is one of the expected values.
   *
   * @param states the collection of allowed states.
   *
   * @throws IllegalArgumentException if states is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.String,java.lang.String)",103,109,"/**
* Calls m1 and m3. m1 takes arg and argName. m3 validates arg.
*/","* Validates that the given string is not null and has non-zero length.
   * @param arg the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNumberOfElements,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNumberOfElements(java.util.Collection,int,java.lang.String)",182,192,"/**
* Validates collection size.
* @param collection Collection to validate.
* @param numElements Expected number of elements.
* @param argName Name of the collection argument.
*/
","* Validates that the given set is not null and has an exact number of items.
   * @param <T> the type of collection's elements.
   * @param collection the argument reference to validate.
   * @param numElements the expected number of elements in the collection.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExists,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExists(java.nio.file.Path,java.lang.String)",346,350,"/**
 * Calls m1 and m3, checking path existence and providing an error message.
 */","* Validates that the given path exists.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,<init>,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:<init>(int),57,65,"/**
 * Creates a bounded resource pool with the specified size.
 * @param size The maximum number of resources in the pool.
 */
","* Constructs a resource pool of the given size.
   *
   * @param size the size of this pool. Cannot be changed post creation.
   *
   * @throws IllegalArgumentException if size is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BufferPool:<init>(int,int,org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics)",90,108,"/**
 * Creates a buffer pool with specified size and buffer size.
 * @param size Pool size
 * @param bufferSize Buffer size
 * @param prefetchingStatistics Prefetching statistics object
 */
","* Initializes a new instance of the {@code BufferPool} class.
   * @param size number of buffer in this pool.
   * @param bufferSize size in bytes of each buffer.
   * @param prefetchingStatistics statistics for this stream.
   * @throws IllegalArgumentException if size is zero or negative.
   * @throws IllegalArgumentException if bufferSize is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.BlockManager:requestPrefetch(int),120,124,"/**
* Calls m1 with the provided block number and a string literal.
* @param blockNumber The block number to pass to m1.
*/
","* Requests optional prefetching of the given block.
   *
   * @param blockNumber the id of the block to prefetch.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BufferData:<init>(int,java.nio.ByteBuffer)",111,118,"/**
 * Constructs a BufferData object.
 * @param blockNumber Block number.
 * @param buffer ByteBuffer containing the data.
 */
","* Constructs an instances of this class.
   *
   * @param blockNumber Number of the block associated with this buffer.
   * @param buffer The buffer associated with this block.
   *
   * @throws IllegalArgumentException if blockNumber is negative.
   * @throws IllegalArgumentException if buffer is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Retryer.java,<init>,"org.apache.hadoop.fs.impl.prefetch.Retryer:<init>(int,int,int)",55,63,"/**
 * Constructs a Retryer with specified delay and update interval.
 * @param perRetryDelay Delay between retries.
 * @param maxDelay Maximum delay value.
 * @param statusUpdateInterval Interval for status updates.
 */
","* Initializes a new instance of the {@code Retryer} class.
   *
   * @param perRetryDelay per retry delay (in ms).
   * @param maxDelay maximum amount of delay (in ms) before retry fails.
   * @param statusUpdateInterval time interval (in ms) at which status update would be made.
   *
   * @throws IllegalArgumentException if perRetryDelay is zero or negative.
   * @throws IllegalArgumentException if maxDelay is less than or equal to perRetryDelay.
   * @throws IllegalArgumentException if statusUpdateInterval is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,throwIfInvalidBlockNumber,org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidBlockNumber(int),243,245,"/**
* Calls m1 with blockNumber, ""blockNumber"", 0, and numBlocks-1.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,throwIfInvalidOffset,org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidOffset(long),247,249,"/**
* Calls m1 with provided offset and file boundaries.
* @param offset Starting offset for the operation.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Object[],java.lang.String)",117,120,"/**
 * Processes an array and a string argument using m1 and m2.
 * @param array The input array of type T.
 * @param argName A string argument used in processing.
 */
","* Validates that the given array is not null and has at least one element.
   * @param <T> the type of array's elements.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(byte[],java.lang.String)",127,130,"/**
* Calls m1 and m2 with the input array and argName.
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(short[],java.lang.String)",137,140,"/**
* Calls m1 and m2 with the array and argName.
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(int[],java.lang.String)",147,150,"/**
 * Calls m1 and m2 with the provided array and argument name.
 */","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(long[],java.lang.String)",157,160,"/**
* Calls m1 and m2 with the array and argName.
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Iterable,java.lang.String)",168,173,"/**
 * Processes an iterable and a string argument using helper methods.
 */","* Validates that the given buffer is not null and has non-zero capacity.
   * @param <T> the type of iterable's elements.
   * @param iter the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/DefaultBulkDeleteOperation.java,bulkDelete,org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:bulkDelete(java.util.Collection),74,91,"/**
* Processes paths, deletes if invalid, and returns a list of results.
*/","* {@inheritDoc}.
     * The default impl just calls {@code FileSystem.delete(path, false)}
     * on the single path in the list.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,applyToIOStatisticsSnapshot,"org.apache.hadoop.io.wrappedio.WrappedStatistics:applyToIOStatisticsSnapshot(java.io.Serializable,org.apache.hadoop.util.functional.FunctionRaisingIOE)",339,344,"/**
 * Applies a function to the result of processing a source.
 * @param source The source object.
 * @param fun The function to apply.
 * @return The result of the function.
 */
","* Apply a function to an object which may be an IOStatisticsSnapshot.
   * @param <T> return type
   * @param source statistics snapshot
   * @param fun function to invoke if {@code source} is valid.
   * @return the applied value
   * @throws UncheckedIOException Any IO exception.
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,<init>,"org.apache.hadoop.fs.impl.FlagSet:<init>(java.lang.Class,java.lang.String,java.util.EnumSet)",83,92,"/**
 * Constructs a FlagSet with an enum class, prefix, and optional flags.
 */","* Create a FlagSet.
   * @param enumClass class of enum
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param flags flags. A copy of these are made.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,seek,org.apache.hadoop.fs.sftp.SFTPInputStream:seek(long),60,67,"/**
 * Sets the next read position. Throws EOFException if position < 0.
 * @param position The next position to read from.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,available,org.apache.hadoop.fs.sftp.SFTPInputStream:available(),69,77,"/**
 * Calculates the remaining content length, capped at Integer.MAX_VALUE.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,close,org.apache.hadoop.fs.FileSystem:close(),2701,2712,"/**
 * Closes the resource and caches it, logging details.
 */","* Close this FileSystem instance.
   * Will release any held locks, delete all files queued for deletion
   * through calls to {@link #deleteOnExit(Path)}, and remove this FS instance
   * from the cache, if cached.
   *
   * After this operation, the outcome of any method call on this FileSystem
   * instance, or any input/output stream created by it is <i>undefined</i>.
   * @throws IOException IO failure",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:equals(java.lang.Object),153,160,"/**
 * Checks if the given object is an MRNflyNode and compares it.
 * @param o Object to check; must be an MRNflyNode.
 * @return True if the object is an MRNflyNode and m1(other) == 0.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,listStatus,org.apache.hadoop.fs.ChecksumFs:listStatus(org.apache.hadoop.fs.Path),567,580,"/**
* Filters FileStatus objects from a path listing.
* @param f Path to list. Returns filtered FileStatus array.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,compareTo,org.apache.hadoop.fs.FileStatus:compareTo(java.lang.Object),425,429,"/**
* Delegates to another m1 method with a FileStatus object.
*/","* Compare this FileStatus to another FileStatus based on lexicographical
   * order of path.
   * This method was added back by HADOOP-14683 to keep binary compatibility.
   *
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.
   * @throws ClassCastException if the specified object is not FileStatus",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,compareTo,org.apache.hadoop.fs.LocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus),181,184,"/**
 * Delegates the method call to the parent class.
 * @param o The FileStatus object to pass to the parent.
 * @return The result of the parent class's m1 method.
 */
","* Compare this FileStatus to another FileStatus
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,stat2Paths,"org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.Path)",133,138,"/**
 * Returns a Path array. If stats is null, returns a single Path.
 */","* convert an array of FileStatus to an array of Path.
   * If stats if null, return path
   * @param stats
   *          an array of FileStatus objects
   * @param path
   *          default path to return in stats is null
   * @return an array of paths corresponding to the input",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.HarFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),1300,1303,"/**
 * Delegates m1 operation to the fs object.
 * @param f Path object to be processed.
 * @return short value returned by fs.m1(f)
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),447,450,"/**
* Delegates the Path processing to the fs object.
* @param f the Path to process
* @return short value returned by fs.m1(f)
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,run,org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner:run(),4166,4181,"/**
 * Cleans statistics data references from a queue until stopped.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandler.java,openConnection,org.apache.hadoop.fs.FsUrlStreamHandler:openConnection(java.net.URL),46,49,"/**
* Creates a new FsUrlConnection for the given URL.
* @param url The URL to create the connection for.
* @return A new FsUrlConnection object.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,read,"org.apache.hadoop.fs.FSInputStream:read(long,byte[],int,int)",66,89,"/**
* Reads data from a stream at a specific position.
* @param position Stream position to read from.
* @param buffer Data buffer.
* @param offset Offset in the buffer.
* @param length Number of bytes to read.
* @return Number of bytes read.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.HarFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",896,905,"/**
 * Checks if a path has a specific capability (read-only).
 * @param path The path to check.
 * @param capability Capability string.
 * @return True if capability is FS_READ_ONLY_CONNECTOR.
 */
","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,serializer,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:serializer(),262,264,"/**
 * Creates a JsonSerialization for IOStatisticsSnapshot.
 * Configures for non-field serialization and pretty printing.
 */","* Get a JSON serializer for this class.
   * @return a serializer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,publishAsStorageStatistics,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:publishAsStorageStatistics(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)",701,704,"/**
 * Creates a StorageStatistics object from IOStatistics.
 * @param name Statistics name.
 * @param scheme Statistics scheme.
 * @param source IOStatistics source.
 */
","* Publish the IOStatistics as a set of storage statistics.
   * This is dynamic.
   * @param name storage statistics name.
   * @param scheme FS scheme; may be null.
   * @param source IOStatistics source.
   * @return a dynamic storage statistics object.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStorageStatistics,org.apache.hadoop.fs.FileSystem:getStorageStatistics(),4651,4653,"/**
* Returns storage statistics based on m1() and m2().
*/","* Get the StorageStatistics for this FileSystem object.  These statistics are
   * per-instance.  They are not shared with any other FileSystem object.
   *
   * <p>This is a default method which is intended to be overridden by
   * subclasses. The default implementation returns an empty storage statistics
   * object.</p>
   *
   * @return    The StorageStatistics for this FileSystem instance.
   *            Will never be null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsDirectoryException:<init>(java.lang.String),24,26,"/**
* Constructs a PathIsDirectoryException with the given path.
* @param path The path that is a directory.
*/
",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsNotDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsNotDirectoryException:<init>(java.lang.String),24,26,"/**
* Constructs a PathIsNotDirectoryException with the given path.
* @param path The path that is not a directory.
*/
",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathOperationException.java,<init>,org.apache.hadoop.fs.PathOperationException:<init>(java.lang.String),24,26,"/**
 * Constructs a PathOperationException with the given path.
 * @param path The path associated with the exception.
 */
",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsNotEmptyDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsNotEmptyDirectoryException:<init>(java.lang.String),23,25,"/**
 * Constructs a DirectoryIsNotEmptyException with the given path.
 * @param path The path of the directory.
 */
",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,bufferSize,org.apache.hadoop.fs.FSDataOutputStreamBuilder:bufferSize(int),175,178,"/**
* Initializes buffer size and calls m1().
* @param bufSize The size of the buffer.
* @return The result of calling m1().
*/
","* Set the size of the buffer to be used.
   *
   * @param bufSize buffer size.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,replication,org.apache.hadoop.fs.FSDataOutputStreamBuilder:replication(short),190,193,"/**
* Sets the replication value and returns the result of m1().
* @param replica The replication value to set.
* @return The result of calling the m1() method.
*/
","* Set replication factor.
   *
   * @param replica replica.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,blockSize,org.apache.hadoop.fs.FSDataOutputStreamBuilder:blockSize(long),205,208,"/**
* Sets the block size and calls m1().
* @param blkSize The desired block size.
* @return The result of calling m1().
*/
","* Set block size.
   *
   * @param blkSize block size.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,recursive,org.apache.hadoop.fs.FSDataOutputStreamBuilder:recursive(),224,227,"/**
 * Sets recursive to true and returns the result of m1().
 */","* Create the parent directory if they do not exist.
   *
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,create,org.apache.hadoop.fs.FSDataOutputStreamBuilder:create(),254,257,"/**
* Sets the CREATE flag and returns the result of m2().
*/","* Create an FSDataOutputStream at the specified path.
   *
   * @return return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,overwrite,org.apache.hadoop.fs.FSDataOutputStreamBuilder:overwrite(boolean),267,274,"/**
 * Sets the overwrite flag based on the input and returns a B object.
 */","* Set to true to overwrite the existing file.
   * Set it to false, an exception will be thrown when calling {@link #build()}
   * if the file exists.
   *
   * @param overwrite overrite.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,append,org.apache.hadoop.fs.FSDataOutputStreamBuilder:append(),281,284,"/**
* Sets the APPEND flag and returns the result of m2().
*/","* Append to an existing file (optional operation).
   *
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,hashCode,org.apache.hadoop.fs.permission.FsCreateModes:hashCode(),103,108,"/**
 * Calculates a result based on superclass's m1() and m2().
 * @return An integer value computed from inherited and related methods.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],org.apache.hadoop.fs.StorageType[],long,long,boolean)",161,197,"/**
 * Constructs a BlockLocation with provided data, defaulting null arrays.
 * @param offset starting offset of the block
 * @param length block length
 * @param corrupt indicates if the block is corrupt
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setHosts,org.apache.hadoop.fs.BlockLocation:setHosts(java.lang.String[]),312,318,"/**
 * Sets the hosts array, using StringInterner if provided.
 * @param hosts Array of host strings, can be null.
 */","* Set the hosts hosting this block.
   * @param hosts hosts array.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setCachedHosts,org.apache.hadoop.fs.BlockLocation:setCachedHosts(java.lang.String[]),324,330,"/**
 * Sets cachedHosts. Uses StringInterner if not null.
 */","* Set the hosts hosting a cached replica of this block.
   * @param cachedHosts cached hosts.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setNames,org.apache.hadoop.fs.BlockLocation:setNames(java.lang.String[]),337,343,"/**
* Sets the names array, using StringInterner if names is provided.
*/","* Set the names (host:port) hosting this block.
   * @param names names.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setTopologyPaths,org.apache.hadoop.fs.BlockLocation:setTopologyPaths(java.lang.String[]),351,357,"/**
* Sets topology paths, using StringInterner if provided.
* @param topologyPaths Array of topology path strings.
*/","* Set the network topology paths of the hosts.
   *
   * @param topologyPaths topology paths.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setStorageIds,org.apache.hadoop.fs.BlockLocation:setStorageIds(java.lang.String[]),359,365,"/**
* Sets storageIds. Uses StringInterner if not null.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPathInternal,org.apache.hadoop.fs.shell.Command:processPathInternal(org.apache.hadoop.fs.shell.PathData),382,388,"/**
 * Processes a PathData item, potentially recursively.
 * @param item The PathData item to process.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processPath,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData),250,272,"/**
* Processes PathData based on configuration, potentially updating file system ACLs.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,processPath,org.apache.hadoop.fs.shell.Stat:processPath(org.apache.hadoop.fs.shell.PathData),91,155,"/**
 * Formats a PathData item into a string based on the format string.
 * @param item PathData object to format.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,isFile,org.apache.hadoop.fs.FileSystem:isFile(org.apache.hadoop.fs.Path),1895,1902,"/**
 * Delegates to m1(f).m2(), returning false on FileNotFoundException.
 */","True iff the named path is a regular file.
   * Note: Avoid using this method. Instead reuse the FileStatus
   * returned by {@link #getFileStatus(Path)} or listStatus() methods.
   *
   * @param f path to check
   * @throws IOException IO failure
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @return if f is file true, not false.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isFile,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isFile(),479,482,"/**
* Delegates method execution to the wrapped realStatus object.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isFile,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isFile(),55,58,"/**
* Delegates the call to the underlying FileSystem object's m1() method.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,toString,org.apache.hadoop.fs.FileStatus:toString(),458,488,"/**
 * Generates a formatted string representation of file metadata.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getSymlink(),539,542,"/**
 * Delegates to the realStatus's m1() method and returns the Path.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getSymlink,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getSymlink(),115,118,"/**
 * Delegates to the underlying FileSystem's m1() method.
 * @return Path object returned by the FileSystem's m1()
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type)",64,71,"/**
 * Constructs a FsServerDefaults with default settings.
 * @param blockSize Block size for file system.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String)",73,80,"/**
 * Constructs a FsServerDefaults with default encryption flag.
 * @param blockSize Block size for file system operations.
 * @param checksumType Data checksum type.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setXAttr,"org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",359,363,"/**
* Delegates m1 call to the underlying file system.
* @param path Path to the file.
* @param name File name.
* @param value File content as bytes.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,"org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.Throwable)",28,30,"/**
 * Constructs a PathAccessDeniedException with path and cause.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.Throwable)",30,32,"/**
 * Constructs a PathPermissionException with a path and cause.
 * @param path The path associated with the permission error.
 * @param cause The underlying Throwable that caused the exception.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.Throwable)",30,32,"/**
 * Constructs a PathNotFoundException with a path and cause.
 * @param path The path not found.
 * @param cause The underlying cause of the exception.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Concat.java,processArguments,org.apache.hadoop.fs.shell.Concat:processArguments(java.util.LinkedList),49,85,"/**
* Copies files from source paths to target path, concatenating them.
* @param args LinkedList of PathData objects representing source paths.
* @throws IOException if target or source paths are invalid.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,wrapException,"org.apache.hadoop.io.IOUtils:wrapException(java.lang.String,java.lang.String,java.io.IOException)",460,480,"/**
 * Handles exceptions, returning interrupted/path exceptions or wrapping others.
 * @param path file/directory path
 * @param methodName method name
 * @param exception the exception to handle
 * @return handled IOException
 */
","* Takes an IOException, file/directory path, and method name and returns an
   * IOException with the input exception as the cause and also include the
   * file,method details. The new exception provides the stack trace of the
   * place where the exception is thrown and some extra diagnostics
   * information.
   *
   * Return instance of same exception if exception class has a public string
   * constructor; Otherwise return an PathIOException.
   * InterruptedIOException and PathIOException are returned unwrapped.
   *
   * @param path file/directory path
   * @param methodName method name
   * @param exception the caught exception.
   * @return an exception to throw",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$Reader:sync(long),2831,2864,"/**
 * Synchronizes input based on position, handling edge cases & checksum.
 */","* Seek to the next sync mark past a given position.
     * @param position position.
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,reset,org.apache.hadoop.io.MapFile$Reader:reset(),638,640,"/**
* Calls data.m1 with the provided firstPosition.
*/","* Re-positions the reader before its first key.
     *
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[]),82,85,"/**
* Calls overloaded method with full byte array.
* @param b byte array to process
* @return int result of the overloaded method
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,registerExpressions,org.apache.hadoop.fs.shell.find.Find:registerExpressions(org.apache.hadoop.fs.shell.find.ExpressionFactory),102,106,"/**
 * Registers expression classes with the given ExpressionFactory.
 * @param factory Factory to register expression classes with.
 */
",Register the expressions with the expression factory.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,registerCommands,org.apache.hadoop.fs.shell.FsCommand:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),52,74,"/**
 * Registers various command classes with the given CommandFactory.
 */","* Register the command classes used by the fs subcommand
   * @param factory where to register the class",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,registerCommands,org.apache.hadoop.fs.FsShell:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),111,118,"/**
 * Executes FsCommand if m1().m2(FsShell.class) is true.
 * @param factory CommandFactory to create the command.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobExpander.java,expand,org.apache.hadoop.fs.GlobExpander:expand(java.lang.String),63,77,"/**
* Expands a file pattern into a list of fully qualified paths.
* @param filePattern The file pattern to expand.
* @return List of expanded file paths.
*/
","* Expand globs in the given <code>filePattern</code> into a collection of
   * file patterns so that in the expanded set no file pattern has a slash
   * character (""/"") in a curly bracket pair.
   * <p>
   * Some examples of how the filePattern is expanded:<br>
   * <pre>
   * <b>
   * filePattern         - Expanded file pattern </b>
   * {a/b}               - a/b
   * /}{a/b}             - /}a/b
   * p{a/b,c/d}s         - pa/bs, pc/ds
   * {a/b,c/d,{e,f}}     - a/b, c/d, {e,f}
   * {a/b,c/d}{e,f}      - a/b{e,f}, c/d{e,f}
   * {a,b}/{b,{c/d,e/f}} - {a,b}/b, {a,b}/c/d, {a,b}/e/f
   * {a,b}/{c/\d}        - {a,b}/c/d
   * </pre>
   * 
   * @param filePattern file pattern.
   * @return expanded file patterns
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,fetchMore,org.apache.hadoop.fs.FileSystem$DirListingIterator:fetchMore(),2326,2330,"/**
* Initializes entries by fetching a token and refreshing entries.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,printXAttr,"org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:printXAttr(java.lang.String,byte[])",120,128,"/**
 * Writes a key-value pair to the output.
 * @param name key name
 * @param value value as byte array
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2119,2124,"/**
 * Lists files under a path, filtered by the given filter.
 * @param f The path to list.
 * @param filter Filter to apply.
 * @return FileStatus array of matching files.
 */","* Filter files/directories in the given path using the user-supplied path
   * filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param f
   *          a path name
   * @param filter
   *          the user-supplied path filter
   * @return an array of FileStatus objects for the files under the given path
   *         after applying the filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)",2161,2168,"/**
 * Retrieves FileStatus objects for paths matching the filter.
 * @param files array of paths to check
 * @param filter filter to apply
 * @return array of FileStatus objects
 */","* Filter files/directories in the given list of paths using user-supplied
   * path filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param files
   *          a list of paths
   * @param filter
   *          the user-supplied path filter
   * @return a list of statuses for the files under the given paths after
   *         applying the filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,<init>,"org.apache.hadoop.metrics2.lib.MutableCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",36,39,"/**
 * Initializes a MutableCounter with given metrics info and initial value.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,<init>,"org.apache.hadoop.metrics2.lib.MutableCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",37,40,"/**
 * Initializes a MutableCounterLong with given info and initial value.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",37,40,"/**
 * Constructs a MutableGaugeLong with initial value.
 * @param info MetricsInfo object
 * @param initValue Initial gauge value
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)",33,36,"/**
 * Constructs a MutableGaugeFloat with initial value.
 * @param info MetricsInfo object
 * @param initValue Initial float value for the gauge
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",37,40,"/**
* Constructs a MutableGaugeInt with initial value.
* @param info MetricsInfo object
* @param initValue Initial integer value for the gauge
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricCounterLong.java,<init>,"org.apache.hadoop.metrics2.impl.MetricCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",29,32,"/**
 * Constructs a MetricCounterLong with provided info and initial value.
 * @param info MetricsInfo object
 * @param value Initial long value for the counter
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeLong.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",29,32,"/**
 * Constructs a MetricGaugeLong with provided info and initial value.
 * @param info MetricsInfo object
 * @param value The initial long value for the gauge.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricCounterInt.java,<init>,"org.apache.hadoop.metrics2.impl.MetricCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",29,32,"/**
 * Constructs a MetricCounterInt with given info and initial value.
 * @param info MetricsInfo object
 * @param value Initial integer value for the counter
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeFloat.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)",29,32,"/**
 * Creates a MetricGaugeFloat with given info and initial value.
 * @param info MetricsInfo object
 * @param value The float value for the gauge
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeDouble.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeDouble:<init>(org.apache.hadoop.metrics2.MetricsInfo,double)",29,32,"/**
 * Creates a MetricGaugeDouble with given info and value.
 * @param info MetricsInfo object
 * @param value The double value for the gauge
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeInt.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",29,32,"/**
 * Constructs a MetricGauge with provided info and initial value.
 * @param info MetricsInfo object
 * @param value Initial gauge value
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",369,373,"/**
 * Renews a delegation token.
 * @param url Token URL. @param token The token to renew.
 * @param renewer Renewer string.
 */
","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @return a delegation token.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",416,419,"/**
 * Calls m1 with a null request context.
 * @param url The URL to fetch.
 * @param token Authentication token.
 * @return Long value returned by m1.
 */","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",457,460,"/**
* Calls overloaded method with null for the third parameter.
* @param url The URL to process.
* @param token Authentication token.
*/","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @throws IOException if an IO error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,<init>,"org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)",262,266,"/**
 * Constructs a ValueQueue with default generation policy.
 * @param numValues Initial queue size, lowWaterMark, expiry,
 *                   numFillerThreads, and fetcher are provided.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileEncryptionInfo.java,<init>,"org.apache.hadoop.fs.FileEncryptionInfo:<init>(org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,byte[],byte[],java.lang.String,java.lang.String)",57,74,"/**
 * Constructs a FileEncryptionInfo object with provided encryption details.
 */
","* Create a FileEncryptionInfo.
   *
   * @param suite CipherSuite used to encrypt the file
   * @param edek encrypted data encryption key (EDEK) of the file
   * @param iv initialization vector (IV) used to encrypt the file
   * @param keyName name of the key used for the encryption zone
   * @param ezKeyVersionName name of the KeyVersion used to encrypt the
   *                         encrypted data encryption key.
   * @param version version.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,getFS,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getFS(),106,109,"/**
 * Masks the filesystem using m1 and returns it.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,permission,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:permission(org.apache.hadoop.fs.permission.FsPermission),121,126,"/**
 * Sets the permission and returns a B object.
 * @param perm The permission to set.
 * @return A B object.
 */
",* Set permission for the file.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,checksumOpt,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt),211,216,"/**
* Processes ChecksumOpt and returns a B object.
* @param chksumOpt ChecksumOpt object to process
* @return A B object.
*/
",* Set checksum opt.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WrappedIOException.java,<init>,org.apache.hadoop.fs.impl.WrappedIOException:<init>(java.io.IOException),47,49,"/**
 * Constructs a WrappedIOException with the specified cause.
 * @param cause The underlying IOException being wrapped.
 */
","* Construct from a non-null IOException.
   * @param cause inner cause
   * @throws NullPointerException if the cause is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FsLinkResolution.java,<init>,org.apache.hadoop.fs.impl.FsLinkResolution:<init>(org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction),50,52,"/**
 * Constructs a FsLinkResolution with the given resolution function.
 * @param fn The function to resolve file system links.
 */
","* Construct an instance with the given function.
   * @param fn function to invoke.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileContext),437,440,"/**
 * Constructs a GlobBuilder with the provided FileContext.
 * @param fc The FileContext to use; must not be null.
 */
","* Construct bonded to a file context.
     * @param fc file context.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileSystem),446,449,"/**
 * Constructs a GlobBuilder with the given file system.
 * @param fs The file system to use for globbing.
 */
","* Construct bonded to a filesystem.
     * @param fs file system.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getFS,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getFS(),141,144,"/**
* Masks the filesystem using m1 and returns it.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,permission,org.apache.hadoop.fs.FSDataOutputStreamBuilder:permission(org.apache.hadoop.fs.permission.FsPermission),159,163,"/**
* Sets the permission and returns a B object.
* @param perm The permission to set.
* @return A B object with the updated permission.
*/
","* Set permission for the file.
   *
   * @param perm permission.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,progress,org.apache.hadoop.fs.FSDataOutputStreamBuilder:progress(org.apache.hadoop.util.Progressable),239,243,"/**
 * Processes a Progressable object, sets progress, and returns a B object.
 */","* Set the facility of reporting progress.
   *
   * @param prog progress.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,checksumOpt,org.apache.hadoop.fs.FSDataOutputStreamBuilder:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt),296,300,"/**
* Processes checksum options and returns a B object.
* @param chksumOpt ChecksumOpt object to process
* @return A B object after processing.
*/","* Set checksum opt.
   *
   * @param chksumOpt check sum opt.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,validateWriteArgs,"org.apache.hadoop.fs.store.DataBlocks:validateWriteArgs(byte[],int,int)",110,118,"/**
 * Validates byte array bounds for writing.
 * @param b byte array
 * @param off starting index
 * @param len number of bytes to write
 */","* Validate args to a write command. These are the same validation checks
   * expected for any implementation of {@code OutputStream.write()}.
   *
   * @param b   byte array containing data.
   * @param off offset in array where to start.
   * @param len number of bytes to be written.
   * @throws NullPointerException      for a null buffer
   * @throws IndexOutOfBoundsException if indices are out of range
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,set,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:set(java.lang.Object),219,224,"/**
 * Sets the value, validates input, and triggers a related action.
 * @param v The value to set.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,getLowerLayerAsyncReturn,org.apache.hadoop.io.retry.AsyncCallHandler:getLowerLayerAsyncReturn(),78,83,"/**
 * Retrieves and validates an AsyncGet object.
 * Returns the AsyncGet or throws an exception if invalid.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,setGcTimeMonitor,org.apache.hadoop.metrics2.source.JvmMetrics:setGcTimeMonitor(org.apache.hadoop.util.GcTimeMonitor),107,110,"/**
 * Sets the GcTimeMonitor instance for tracking GC timings.
 * @param gcTimeMonitor The GcTimeMonitor instance to set.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,init,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:init(byte[],byte[])",136,142,"/**
 * Initializes the cipher with a key and IV.
 * @param key encryption key
 * @param iv initialization vector
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,init,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:init(byte[],byte[])",128,138,"/**
 * Resets the cipher with the given key and IV.
 * @param key encryption key
 * @param iv initialization vector
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,equalsIgnoreCase,"org.apache.hadoop.util.StringUtils:equalsIgnoreCase(java.lang.String,java.lang.String)",1259,1264,"/**
 * Checks if s1 contains s2.
 * @param s1 The string to search within.
 * @param s2 The string to search for.
 * @return True if s1 contains s2, false otherwise.
 */
","* Compare strings locale-freely by using String#equalsIgnoreCase.
   *
   * @param s1  Non-null string to be converted
   * @param s2  string to be converted
   * @return     the str, converted to uppercase.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LimitInputStream.java,<init>,"org.apache.hadoop.util.LimitInputStream:<init>(java.io.InputStream,long)",43,48,"/**
 * Creates a LimitInputStream with the given input stream and limit.
 * @param in the input stream
 * @param limit the maximum number of bytes to read
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String[],java.lang.String)",432,439,"/**
 * Constructs a DeprecationDelta with key, new keys, and optional message.
 * @param key The key being deprecated.
 * @param newKeys New keys replacing the old one.
 * @param customMessage Optional custom deprecation message.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,setReconfigurationUtil,org.apache.hadoop.conf.ReconfigurableBase:setReconfigurationUtil(org.apache.hadoop.conf.ReconfigurationUtil),88,91,"/**
 * Sets the reconfigurationUtil, ensuring it's not null.
 * @param ru The ReconfigurationUtil to set.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,isStaleClient,org.apache.hadoop.ha.ActiveStandbyElector:isStaleClient(java.lang.Object),1173,1181,"/**
 * Checks if the context is a valid ZooKeeper client.
 * @param ctx The context object to validate.
 * @return True if stale, false otherwise.
 */
","* The callbacks and watchers pass a reference to the ZK client
   * which made the original call. We don't want to take action
   * based on any callbacks from prior clients after we quit
   * the election.
   * @param ctx the ZK client passed into the watcher
   * @return true if it matches the current client",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatistics,"org.apache.hadoop.fs.FileSystem:getStatistics(java.lang.String,java.lang.Class)",4586,4605,"/**
* Retrieves statistics for a filesystem.
* @param scheme Filesystem scheme; must not be null.
* @param cls Filesystem class
* @return Statistics object
*/","* Get the statistics for a particular file system.
   * @param scheme scheme.
   * @param cls the class to lookup
   * @return a statistics object
   * @deprecated use {@link #getGlobalStorageStatistics()}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedByteArray,"org.apache.hadoop.io.WritableUtils:writeCompressedByteArray(java.io.DataOutput,byte[])",61,83,"/**
* Compresses bytes using gzip, writes to output, and returns compression ratio.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,int,boolean)",64,81,"/**
 * Copies data from InputStream to OutputStream, optionally closing streams.
 * @param in Input stream.
 * @param out Output stream.
 * @param buffSize Buffer size.
 * @param close Whether to close streams.
 */
","* Copies from one stream to another.
   *
   * @param in InputStrem to read from
   * @param out OutputStream to write to
   * @param buffSize the size of the buffer 
   * @param close whether or not close the InputStream and 
   * OutputStream at the end. The streams are closed in the finally clause.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,long,boolean)",145,175,"/**
* Copies data from an InputStream to an OutputStream.
* @param in Input stream to read from.
* @param out Output stream to write to.
* @param count Number of bytes to copy.
* @param close Whether to close streams after copying.
*/","* Copies count bytes from one stream to another.
   *
   * @param in InputStream to read from
   * @param out OutputStream to write to
   * @param count number of bytes to copy
   * @param close whether to close the streams
   * @throws IOException if bytes can not be read or written",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client$IpcStreams:close(),1955,1959,"/**
* Calls IOUtils.m1 for both 'out' and 'in' streams.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,<init>,org.apache.hadoop.util.VersionInfo:<init>(java.lang.String),41,55,"/**
 * Loads version information from a properties file.
 * @param component Component name for version info file.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopSinks,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSinks(),471,479,"/**
 * Stops all metrics sinks and clears the sink registry.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,finalize,org.apache.hadoop.crypto.random.OsSecureRandom:finalize(),128,131,"/**
* Calls the m1() method.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,doDiskIo,org.apache.hadoop.util.DiskChecker:doDiskIo(java.io.File),255,274,"/**
 * Checks a directory, attempting to process files until success.
 * @param dir Directory to check; throws DiskErrorException on failure.
 */","* Performs some disk IO by writing to a new file in the given directory
   * and sync'ing file contents to disk.
   *
   * This increases the likelihood of catching catastrophic disk/controller
   * failures sooner.
   *
   * @param dir directory to be checked.
   * @throws DiskErrorException if we hit an error while trying to perform
   *         disk IO against the file.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List)",44,46,"/**
 * Constructs a PartialListing with a path and partial listing.
 * @param listedPath Path of the listing.
 * @param partialListing List of items in the listing.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.ipc.RemoteException)",48,50,"/**
 * Constructs a PartialListing with a Path and optional RemoteException.
 * @param listedPath Path of the listing.
 * @param exception Optional RemoteException.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object),50,52,"/**
 * Constructs a CallReturn with a result object.
 * @param r The result object associated with the call.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Throwable),53,56,"/**
 * Constructs a CallReturn with an exception.
 * @param t The exception that occurred. Must not be null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(org.apache.hadoop.io.retry.CallReturn$State),57,59,"/**
 * Constructs a CallReturn object with a State, other fields null.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])",66,70,"/**
* Calls super.m3, passing additional argument from m1().m2().
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:calculateIV(byte[],long,byte[])",52,56,"/**
* Calls super.m3 with additional argument from m1().m2().
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])",48,52,"/**
* Calls super.m3 with additional argument from m1().m2().
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceAesCtrCryptoCodec:calculateIV(byte[],long,byte[])",48,52,"/**
* Calls super.m3 with additional argument from m1().m2().
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,build,org.apache.hadoop.util.GcTimeMonitor$Builder:build(),95,98,"/**
 * Creates a new GcTimeMonitor instance with provided parameters.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,isTypeQuotaSet,org.apache.hadoop.fs.QuotaUsage:isTypeQuotaSet(),193,202,"/**
 * Checks if any storage type has a positive quota.
 * @return True if any type quota is positive, false otherwise.
 */","* Return true if any storage type quota has been set.
   *
   * @return if any storage type quota has been set true, not false.
   *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,isTypeConsumedAvailable,org.apache.hadoop.fs.QuotaUsage:isTypeConsumedAvailable(),210,219,"/**
 * Checks if any storage type has consumed resources.
 * @return True if any type has consumed resources, false otherwise.
 */
","* Return true if any storage type consumption information is available.
   *
   * @return if any storage type consumption information
   * is available, not false.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,getAndCheckStorageTypes,org.apache.hadoop.fs.shell.Count:getAndCheckStorageTypes(java.lang.String),180,193,"/**
 * Parses storage types from a string, returns a list.
 * @param types Comma-separated storage type string.
 * @return List of StorageType objects.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,equals,org.apache.hadoop.fs.LocatedFileStatus:equals(java.lang.Object),190,193,"/**
* Delegates the call to the superclass's m1 method.
* @param o The object passed to the super method.
* @return The result of the superclass's m1 method.
*/
","Compare if this object is equal to another object
   * @param   o the object to be compared.
   * @return  true if two file status has the same path name; false if not.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:equals(java.lang.Object),549,552,"/**
 * Delegates the call to the realStatus's m1 method.
 * @param o The object to pass to the delegated method.
 * @return The result of the delegated method.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,equals,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:equals(java.lang.Object),40,43,"/**
 * Delegates the call to the superclass's m1 method.
 * @param o The object passed to the super method.
 * @return The result of the superclass's m1 method.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,hashCode,org.apache.hadoop.fs.LocatedFileStatus:hashCode(),201,204,"/**
 * Calls the m1 method of the superclass.
 * Returns the integer value returned by the superclass's m1.
 */","* Returns a hash code value for the object, which is defined as
   * the hash code of the path name.
   *
   * @return  a hash code value for the path name.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:hashCode(),554,557,"/**
* Delegates m1() call to the realStatus object.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,hashCode,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:hashCode(),45,48,"/**
* Calls the m1 method of the superclass.
* Returns the integer value returned by the superclass's m1.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,getFolderUsage,org.apache.hadoop.fs.DUHelper:getFolderUsage(java.lang.String),34,36,"/**
* Delegates disk usage calculation to DUHelper.
* @param folder The folder to calculate disk usage for.
* @return Disk usage in bytes.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,clear,org.apache.hadoop.fs.statistics.MeanStatistic:clear(),147,149,"/**
* Calls m1 with initial values of 0 and 0.
*/",* Set the values to 0.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,set,org.apache.hadoop.fs.statistics.MeanStatistic:set(org.apache.hadoop.fs.statistics.MeanStatistic),168,170,"/**
* Calls m3 with values obtained from the given MeanStatistic.
*/","* Set the statistic to the values of another.
   * Synchronized.
   * @param other the source.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsToString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToString(org.apache.hadoop.fs.statistics.IOStatistics),77,91,"/**
 * Creates a statistics string from IOStatistics.
 * @param statistics IOStatistics object, or null for empty string.
 * @return Statistics string or empty string if null.
 */","* Convert IOStatistics to a string form.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,mapToSortedString,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToSortedString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.util.function.Predicate)",159,164,"/**
 * Appends processed map entries to StringBuilder, filtering with predicate.
 * @param sb StringBuilder to append to
 * @param type Type string
 * @param map Map to process
 * @param isEmpty Predicate to check for empty values
 */
","* Given a map, produce a string with all the values, sorted.
   * Needs to create a treemap and insert all the entries.
   * @param sb string buffer to append to
   * @param type type (for output)
   * @param map map to evaluate
   * @param <E> type of values of the map",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationTrackerFactory.java,trackDuration,org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String),60,62,"/**
* Returns a DurationTracker with the given key and default value 1.
* @param key the key for the DurationTracker
* @return a DurationTracker object
*/
","* Initiate a duration tracking operation by creating/returning
   * an object whose {@code close()} call will
   * update the statistics.
   * The expected use is within a try-with-resources clause.
   * @param key statistic key
   * @return an object to close after an operation completes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/PairedDurationTrackerFactory.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory:trackDuration(java.lang.String,long)",50,55,"/**
 * Creates a PairedDurationTracker with durations from global and local trackers.
 * @param key Identifier for the duration.
 * @param count The count associated with the duration.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,getLongStatistics,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLongStatistics(),66,78,"/**
 * Creates an iterator for LongStatistic objects from counters.
 */","* Take a snapshot of the current counter values
   * and return an iterator over them.
   * @return all the counter statistics.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,long)",440,445,"/**
 * Calls m1, m2, and m3 with the given prefix and duration.
 * @param prefix Prefix string to be used in the calls.
 * @param durationMillis Duration in milliseconds.
 */
","* Add a duration to the min/mean/max statistics, using the
   * given prefix and adding a suffix for each specific value.
   * <p>
   * The update is non -atomic, even though each individual statistic
   * is updated thread-safely. If two threads update the values
   * simultaneously, at the end of each operation the state will
   * be correct. It is only during the sequence that the statistics
   * may be observably inconsistent.
   * </p>
   * @param prefix statistic prefix
   * @param durationMillis duration in milliseconds.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,build,org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:build(),51,56,"/**
 * Generates and returns IO statistics.
 * Uses m1() to create stats, resets instance.
 */","* Build the IOStatistics instance.
   * @return an instance.
   * @throws IllegalStateException if the builder has already been built.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionCounter(java.lang.String,java.util.function.ToLongFunction)",74,78,"/**
 * Updates statistics for a key using the provided evaluation function.
 * @param key The key to update statistics for.
 * @param eval Function to evaluate the key and return a long value.
 */
","* Add a new evaluator to the counter statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionGauge(java.lang.String,java.util.function.ToLongFunction)",125,129,"/**
* Sets the key and evaluation function for IO statistics.
* @param key The key for the statistics.
* @param eval Function to evaluate the value as a long.
*/
","* Add a new evaluator to the gauge statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMinimum(java.lang.String,java.util.function.ToLongFunction)",163,167,"/**
* Updates statistics for a key using the provided function.
* @param key The key for the statistics.
* @param eval Function to evaluate the value as a long.
*/
","* Add a new evaluator to the minimum statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMaximum(java.lang.String,java.util.function.ToLongFunction)",202,206,"/**
 * Sets the key and evaluation function for IO statistics.
 * @param key The key for the statistics.
 * @param eval Function to evaluate the value as a long.
 */","* Add a new evaluator to the maximum statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withMeanStatisticFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMeanStatisticFunction(java.lang.String,java.util.function.Function)",242,246,"/**
 * Adds a statistic evaluation to the builder.
 * @param key Statistic key.
 * @param eval Function to calculate the statistic.
 */
","* Add a new evaluator to the mean statistics.
   *
   * This is a function which must return the mean and the sample count.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,register,org.apache.hadoop.service.launcher.InterruptEscalator:register(java.lang.String),142,146,"/**
 * Creates and dispatches an interrupt handler for the given signal.
 * @param signalName The name of the interrupt signal.
 */
","* Register an interrupt handler.
   * @param signalName signal name
   * @throws IllegalArgumentException if the registration failed",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,unreference,org.apache.hadoop.net.unix.DomainSocket:unreference(boolean),176,182,"/**
 * Calls refCount.m1() if checkClosed is false, else refCount.m2().
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EvaluatingStatisticsMap.java,snapshot,org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:snapshot(),146,148,"/**
* Delegates to IOStatisticsBinding.m1, returning a map.
*/","* Take a snapshot.
   * @return a map snapshot.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,snapshotMap,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map),200,204,"/**
* Delegates to overloaded method with a passthrough function.
* @param source Input map; returned unchanged.
*/","* Take a snapshot of a supplied map, where the copy option simply
   * uses the existing value.
   *
   * For this to be safe, the map must refer to immutable objects.
   * @param source source map
   * @param <E> type of values.
   * @return a new map referencing the same values.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,snapshot,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:snapshot(),44,47,"/**
 * Creates and returns a new IOStatisticsSnapshot object.
 */","* Create a new empty snapshot.
   * A new one is always created for isolation.
   *
   * @return a statistics snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,<init>,"org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:<init>(long,long)",64,67,"/**
 * Constructs a new IOStatisticsContextImpl with thread and ID.
 * @param threadId The ID of the thread.
 * @param id The unique ID for this context.
 */
","* Constructor.
   * @param threadId thread ID
   * @param id instance ID.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,snapshotIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics(),59,63,"/**
 * Creates and returns a new IOStatisticsSnapshot object.
 */","* Create a snapshot statistics instance ready to aggregate data.
   *
   * The instance can be serialized, and its
   * {@code toString()} method lists all the values.
   * @return an empty snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_enabled,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_enabled(),279,281,"/**
 * Delegates to IOStatisticsContext.m1() and returns its result.
 */","* Static probe to check if the thread-level IO statistics enabled.
   * @return true if the thread-level IO statistics are enabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/BufferedIOStatisticsOutputStream.java,getIOStatistics,org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:getIOStatistics(),86,89,"/**
 * Returns IO statistics by calling m1 with the output stream.
 */","* Ask the inner stream for their IOStatistics.
   * @return any IOStatistics offered by the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/BufferedIOStatisticsInputStream.java,getIOStatistics,org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:getIOStatistics(),64,67,"/**
 * Delegates IO statistics calculation to m1, passing 'in'.
 */","* Return any IOStatistics offered by the inner stream.
   * @return inner IOStatistics or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,getIOStatistics,org.apache.hadoop.fs.BufferedFSInputStream:getIOStatistics(),156,159,"/**
 * Delegates IO statistics calculation to m1, passing 'in'.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getIOStatistics,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getIOStatistics(),315,318,"/**
* Returns IO statistics using the provided data.
*/","* Get the IO Statistics of the nested stream, falling back to
     * null if the stream does not implement the interface
     * {@link IOStatisticsSource}.
     * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getIOStatistics,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:getIOStatistics(),676,679,"/**
* Returns IO statistics using provided data.
* @return IOStatistics object containing statistics.
*/
","* Get the IO Statistics of the nested stream, falling back to
     * null if the stream does not implement the interface
     * {@link IOStatisticsSource}.
     * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,getIOStatistics,org.apache.hadoop.fs.FSDataInputStream:getIOStatistics(),289,292,"/**
* Returns IO statistics using the provided input stream.
*/","* Get the IO Statistics of the nested stream, falling back to
   * null if the stream does not implement the interface
   * {@link IOStatisticsSource}.
   * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,getIOStatistics,org.apache.hadoop.fs.FSDataOutputStream:getIOStatistics(),167,170,"/**
* Returns IO statistics for the wrapped stream.
*/","* Get the IO Statistics of the nested stream, falling back to
   * empty statistics if the stream does not implement the interface
   * {@link IOStatisticsSource}.
   * @return an IOStatistics instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionOutputStream.java,getIOStatistics,org.apache.hadoop.io.compress.CompressionOutputStream:getIOStatistics(),107,110,"/**
* Returns IO statistics using the provided output stream.
*/","* Return any IOStatistics provided by the underlying stream.
   * @return IO stats from the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionInputStream.java,getIOStatistics,org.apache.hadoop.io.compress.CompressionInputStream:getIOStatistics(),81,84,"/**
* Returns IO statistics using the provided input stream.
*/","* Return any IOStatistics provided by the underlying stream.
   * @return IO stats from the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getIOStatistics,org.apache.hadoop.crypto.CryptoInputStream:getIOStatistics(),882,885,"/**
* Delegates IO statistics calculation to m1, using 'in'.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,getIOStatistics,org.apache.hadoop.crypto.CryptoOutputStream:getIOStatistics(),321,324,"/**
 * Returns IO statistics by calling m1 with the 'out' object.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,getIOStatistics,org.apache.hadoop.util.LineReader:getIOStatistics(),159,162,"/**
* Returns IO statistics using the provided input stream.
*/","* Return any IOStatistics provided by the source.
   * @return IO stats from the input stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:getIOStatistics(),351,354,"/**
* Returns IO statistics using the singleton instance.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:getIOStatistics(),405,408,"/**
 * Returns IO statistics using the provided source.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:getIOStatistics(),449,452,"/**
 * Returns IO statistics for the source.
 * @return IOStatistics object containing the statistics.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunked,"org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.nio.ByteBuffer,int,java.nio.ByteBuffer,java.lang.String,long)",429,472,"/**
 * Verifies CRC checksums in data against provided CRC values.
 * @param type Type of data, algorithm, data buffer, CRC buffer, filename, basePos
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunked,"org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,byte[],int,int,int,byte[],int,java.lang.String,long)",478,512,"/**
 * Verifies CRC checksums against provided data and expected values.
 */","* Implementation of chunked verification specifically on byte arrays. This
   * is to avoid the copy when dealing with ByteBuffers that have array backing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,updateDecryptor,"org.apache.hadoop.crypto.CryptoInputStream:updateDecryptor(org.apache.hadoop.crypto.Decryptor,long,byte[])",297,302,"/**
 * Decrypts data at a given position using provided parameters.
 * @param decryptor Decryptor instance
 * @param position Data position
 * @param iv Initialization vector
 */","Calculate the counter and iv, update the decryptor.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,encrypt,org.apache.hadoop.crypto.CryptoOutputStream:encrypt(),178,217,"/**
* Encrypts data from inBuffer to outBuffer, handling padding.
*/","* Do the encryption, input is {@link #inBuffer} and output is 
   * {@link #outBuffer}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,hasNext,org.apache.hadoop.fs.BatchedRemoteIterator:hasNext(),98,102,"/**
 * Executes m1() and returns true if entries is not null.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,next,org.apache.hadoop.fs.BatchedRemoteIterator:next(),111,120,"/**
* Retrieves the next element. Throws exception if no elements.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>(),27,29,"/**
 * Default constructor for MD5MD5CRC32GzipFileChecksum.
 */","Same as this(0, 0, null)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>(),27,29,"/**
 * Default constructor for MD5MD5CRC32CastagnoliFileChecksum.
 */","Same as this(0, 0, null)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,<init>,org.apache.hadoop.fs.GlobPattern:<init>(java.lang.String),41,43,"/**
 * Constructs a GlobPattern with the given glob pattern string.
 */","* Construct the glob pattern object with a glob pattern string
   * @param globPattern the glob pattern string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeShellPath,org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File),668,670,"/**
 * Reads file content.
 * @param file The file to read.
 * @throws IOException If an I/O error occurs.
 */
","* Convert a os-native filename to a path that works for the shell.
   * @param file The filename to convert
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeSecureShellPath,org.apache.hadoop.fs.FileUtil:makeSecureShellPath(java.io.File),679,686,"/**
 * Masks file content on non-Windows systems.
 * @param file The file to mask.
 * @throws IOException If an I/O error occurs.
 */
","* Convert a os-native filename to a path that works for the shell
   * and avoids script injection attacks.
   * @param file The filename to convert
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,linkCount,org.apache.hadoop.fs.HardLink$HardLinkCGUnix:linkCount(java.io.File),108,116,"/**
 * Extracts link counts into a string array.
 * @param file The file to process.
 * @return String array containing link counts.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unZip,"org.apache.hadoop.fs.FileUtil:unZip(java.io.InputStream,java.io.File)",739,775,"/**
 * Extracts a zip file to the specified directory, ensuring security.
 * @param inputStream Zip archive input stream
 * @param toDir Directory to extract files to
 */","* Given a stream input it will unzip the it in the unzip directory.
   * passed as the second parameter
   * @param inputStream The zip file as input
   * @param toDir The unzip directory where to unzip the zip file.
   * @throws IOException an exception occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unZip,"org.apache.hadoop.fs.FileUtil:unZip(java.io.File,java.io.File)",827,871,"/**
 * Unzips a file to a directory, handling file creation and permissions.
 * @param inFile Input zip file
 * @param unzipDir Directory to unzip to
 */","* Given a File input it will unzip it in the unzip directory.
   * passed as the second parameter
   * @param inFile The zip file as input
   * @param unzipDir The unzip directory where to unzip the zip file.
   * @throws IOException An I/O exception has occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingJava,"org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.File,java.io.File,boolean)",1086,1109,"/**
 * Extracts tar archive entries to a directory.
 * @param inFile Input tar file.
 * @param untarDir Target directory for extraction.
 * @param gzipped True if the tar is gzipped.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingJava,"org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.InputStream,java.io.File,boolean)",1111,1128,"/**
 * Extracts tar archive entries to a directory.
 * @param inputStream Input stream for tar archive.
 * @param untarDir Directory to extract files to.
 * @param gzipped Whether the archive is gzipped.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/PowerShellFencer.java,tryFence,"org.apache.hadoop.ha.PowerShellFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",57,105,"/**
* Executes a PowerShell script for fencing, returns true on success.
* @param target HAServiceTarget object
* @param argsStr PowerShell script arguments
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,toString,org.apache.hadoop.fs.permission.FsCreateModes:toString(),82,86,"/**
* Masks the result of super.m1() with m2().
* @return Formatted string combining masked and unmasked values.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,disconnect,org.apache.hadoop.fs.sftp.SFTPFileSystem:disconnect(com.jcraft.jsch.ChannelSftp),165,167,"/**
 * Releases the SFTP channel to the connection pool.
 * @param channel The SFTP channel to release.
 */","* Logout and disconnect the given channel.
   *
   * @param client
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,shutdown,org.apache.hadoop.fs.sftp.SFTPConnectionPool:shutdown(),87,113,"/**
* Shuts down the connection map, closing active SFTP connections.
*/",Shutdown the connection pool and close all open connections.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,<init>,"org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics)",82,84,"/**
 * Constructs a FSDataOutputStream with statistics.
 * @param out The underlying OutputStream.
 * @param stats Statistics object for tracking operations.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,<init>,org.apache.hadoop.fs.FSOutputSummer:<init>(org.apache.hadoop.util.DataChecksum),53,58,"/**
 * Initializes the FSOutputSummer with a DataChecksum object.
 * @param sum The DataChecksum object used for calculations.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,setChecksumBufSize,org.apache.hadoop.fs.FSOutputSummer:setChecksumBufSize(int),257,261,"/**
 * Initializes the buffer with the given size.
 * @param size The size of the buffer to create.
 */
","* Resets existing buffer with a new one of the specified size.
   *
   * @param size size.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,read,org.apache.hadoop.fs.sftp.SFTPInputStream:read(),108,124,"/**
 * Reads a byte from the stream. Returns -1 if EOF or end of content.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,read,org.apache.hadoop.fs.ftp.FTPInputStream:read(),70,84,"/**
 * Reads a single byte from the wrapped stream.
 * @return The byte read, or -1 if end of stream.
 * @throws IOException if the stream is closed.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,read,"org.apache.hadoop.fs.ftp.FTPInputStream:read(byte[],int,int)",86,101,"/**
 * Reads bytes from the stream.
 * @param buf buffer to read into, @param off offset, @param len count
 * @return number of bytes read, or -1 if end of stream.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(),217,231,"/**
 * Reads a single value from the input stream.
 * Returns the value or throws FSError on IO error.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(byte[],int,int)",233,249,"/**
 * Reads bytes from the input stream.
 * @param b buffer to hold read bytes
 * @param off offset into buffer
 * @param len number of bytes to read
 * @return number of bytes read, or -1 if EOF
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(long,byte[],int,int)",251,272,"/**
* Reads data from the stream at the given position.
* @param position file position, b buffer, off offset, len length
* @return number of bytes read or -1 on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,write,org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(int),51,58,"/**
 * Delegates to the wrapped stream, increments position, updates stats.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,write,"org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(byte[],int,int)",60,67,"/**
* Writes bytes to the output stream, updating position and stats.
* @param b byte array to write
* @param off offset in the array
* @param len number of bytes to write
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem$Statistics:<init>(org.apache.hadoop.fs.FileSystem$Statistics),4110,4125,"/**
 * Constructs a Statistics object as a copy of another Statistics.
 */","* Copy constructor.
     *
     * @param other    The input Statistics object which is cloned.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesRead,org.apache.hadoop.fs.FileSystem$Statistics:getBytesRead(),4308,4321,"/**
 * Calculates total bytes read using a custom StatisticsAggregator.
 * @return Total bytes read as a long.
 */
","* Get the total number of bytes read.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesWritten,org.apache.hadoop.fs.FileSystem$Statistics:getBytesWritten(),4327,4340,"/**
 * Calculates total bytes written using a custom statistics aggregator.
 * @return Total bytes written as a long.
 */
","* Get the total number of bytes written.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getReadOps,org.apache.hadoop.fs.FileSystem$Statistics:getReadOps(),4346,4360,"/**
 * Calculates total read operations using a StatisticsAggregator.
 * @return The total read operation count.
 */
","* Get the number of file system read operations such as list files.
     * @return number of read operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLargeReadOps,org.apache.hadoop.fs.FileSystem$Statistics:getLargeReadOps(),4367,4380,"/**
 * Calculates the total large read operations using StatisticsAggregator.
 * @return int representing the total large read operations.
 */","* Get the number of large file system read operations such as list files
     * under a large directory.
     * @return number of large read operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getWriteOps,org.apache.hadoop.fs.FileSystem$Statistics:getWriteOps(),4387,4400,"/**
 * Calculates the total write operations using a StatisticsAggregator.
 * @return Total write operations count.
 */
","* Get the number of file system write operations such as create, append
     * rename etc.
     * @return number of write operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getRemoteReadTime,org.apache.hadoop.fs.FileSystem$Statistics:getRemoteReadTime(),4436,4449,"/**
 * Calculates the total remote read time MS using a StatisticsAggregator.
 * @return Total remote read time in milliseconds.
 */","* Get total time taken in ms for bytes read from remote.
     * @return time taken in ms for remote bytes read.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getData,org.apache.hadoop.fs.FileSystem$Statistics:getData(),4456,4469,"/**
 * Calculates and returns statistics data using an aggregator.
 * @return StatisticsData object containing aggregated results.
 */
","* Get all statistics data.
     * MR or other frameworks can use the method to get all statistics at once.
     * @return the StatisticsData",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesReadErasureCoded,org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadErasureCoded(),4475,4488,"/**
 * Calculates total bytes read during erasure coding.
 * Returns the sum of bytesReadErasureCoded.
 */","* Get the total number of bytes read on erasure-coded files.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,toString,org.apache.hadoop.fs.FileSystem$Statistics:toString(),4490,4504,"/**
 * Calls m3 with an aggregator to compute and return a statistic.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,reset,org.apache.hadoop.fs.FileSystem$Statistics:reset(),4524,4539,"/**
 * Aggregates statistics data using a StatisticsAggregator.
 */","* Resets all statistics to 0.
     *
     * In order to reset, we add up all the thread-local statistics data, and
     * set rootData to the negative of that.
     *
     * This may seem like a counterintuitive way to reset the statistics.  Why
     * can't we just zero out all the thread-local data?  Well, thread-local
     * data can only be modified by the thread that owns it.  If we tried to
     * modify the thread-local data from this thread, our modification might get
     * interleaved with a read-modify-write operation done by the thread that
     * owns the data.  That would result in our update getting lost.
     *
     * The approach used here avoids this problem because it only ever reads
     * (not writes) the thread-local data.  Both reads and writes to rootData
     * are done under the lock, so we're free to modify rootData from any thread
     * that holds the lock.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,toString,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:toString(),113,116,"/**
 * Concatenates the result of m1() with md5, separated by a colon.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validate,"org.apache.hadoop.fs.CreateFlag:validate(java.lang.Object,boolean,java.util.EnumSet)",172,187,"/**
 * Validates file creation flags and throws exceptions if needed.
 * @param path The file path.
 * @param pathExists Whether the path exists.
 * @param flag Creation flags.
 */","* Validate the CreateFlag for create operation
   * @param path Object representing the path; usually String or {@link Path}
   * @param pathExists pass true if the path exists in the file system
   * @param flag set of CreateFlag
   * @throws IOException on error
   * @throws HadoopIllegalArgumentException if the CreateFlag is invalid",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validateForAppend,org.apache.hadoop.fs.CreateFlag:validateForAppend(java.util.EnumSet),195,201,"/**
 * Processes flags and throws exception if APPEND flag is missing.
 * @param flag EnumSet of CreateFlag values.
 */
","* Validate the CreateFlag for the append operation. The flag must contain
   * APPEND, and cannot contain OVERWRITE.
   *
   * @param flag enum set flag.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getUri,"org.apache.hadoop.fs.AbstractFileSystem:getUri(java.net.URI,java.lang.String,boolean,int)",316,341,"/**
* Constructs a URI based on input, scheme, authority, and port.
* @param uri The base URI.
* @throws URISyntaxException if URI syntax is invalid.
*/","* Get the URI for the file system based on the given URI. The path, query
   * part of the given URI is stripped out and default file system port is used
   * to form the URI.
   * 
   * @param uri FileSystem URI.
   * @param authorityNeeded if true authority cannot be null in the URI. If
   *          false authority must be null.
   * @param defaultPort default port to use if port is not specified in the URI.
   * 
   * @return URI of the file system
   * 
   * @throws URISyntaxException <code>uri</code> has syntax error",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecodeImpl,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",85,96,"/**
* Processes input data, calculates erasure signatures, and outputs results.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,byte[][],byte[][])",36,50,"/**
 * Initializes the ByteArrayEncodingState with encoder, inputs, and outputs.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,java.nio.ByteBuffer[],java.nio.ByteBuffer[])",35,47,"/**
 * Initializes the encoding state with encoder, inputs, and outputs.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Class),113,116,"/**
* Creates a new ArrayPrimitiveWritable for the given primitive type.
* @param componentType The Class object representing the primitive type.
*/
","* Construct an instance of known type but no value yet
   * for use with type-specific wrapper classes.
   *
   * @param componentType componentType.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,set,org.apache.hadoop.io.ArrayPrimitiveWritable:set(java.lang.Object),142,150,"/**
* Processes an object, stores its type and length.
* @param value The object to process.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/DefaultFailoverProxyProvider.java,close,org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:close(),55,58,"/**
* Calls RPC.m1 with the provided proxy.
* @param proxy The proxy object to use.
* @throws IOException if an I/O error occurs.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:close(),49,52,"/**
* Calls RPC.m1 with the rpcProxy. Throws IOException if RPC fails.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:close(),54,57,"/**
 * Calls RPC.m1 with the rpcProxy. Throws IOException if RPC fails.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:close(),52,55,"/**
 * Calls RPC.m1 with the rpcProxy. Throws IOException if RPC fails.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:close(),49,52,"/**
* Calls RPC.m1 with the rpcProxy. Throws IOException if RPC fails.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:close(),47,50,"/**
* Calls RPC.m1 with the rpcProxy. Throws IOException if RPC fails.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:close(),72,75,"/**
 * Calls RPC.m1 with the rpcProxy.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:close(),165,168,"/**
* Calls RPC.m1 with the rpcProxy.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,getPermFromString,org.apache.hadoop.util.ZKUtil:getPermFromString(java.lang.String),44,71,"/**
 * Parses permission string and returns the corresponding integer permission.
 */","* Parse ACL permission string, partially borrowed from
   * ZooKeeperMain private method",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,processChecksumOpt,"org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt)",339,342,"/**
* Calls m1 with a default value for the third parameter.
* @param defaultOpt Default ChecksumOpt value.
* @param userOpt User-provided ChecksumOpt value.
*/
","* A helper method for processing user input and default value to 
     * create a combined checksum option. 
     *
     * @param defaultOpt Default checksum option
     * @param userOpt User-specified checksum option
     *
     * @return ChecksumOpt.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getUriDefaultPort,org.apache.hadoop.fs.DelegateToFileSystem:getUriDefaultPort(),174,177,"/**
* Delegates to m1 with fsImpl as an argument.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,canonicalizeUri,org.apache.hadoop.fs.HarFileSystem:canonicalizeUri(java.net.URI),323,326,"/**
 * Delegates URI processing to the file system.
 * @param uri The URI to process.
 * @return The processed URI.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getCanonicalUri,org.apache.hadoop.fs.FileSystem:getCanonicalUri(),383,385,"/**
 * Returns a URI by applying m2 to the result of m1.
 */","* Return a canonicalized form of this FileSystem's URI.
   *
   * The default implementation simply calls {@link #canonicalizeUri(URI)}
   * on the filesystem's own URI, so subclasses typically only need to
   * implement that method.
   *
   * @see #canonicalizeUri(URI)
   * @return the URI of this filesystem.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,canonicalizeUri,org.apache.hadoop.fs.FilterFileSystem:canonicalizeUri(java.net.URI),118,121,"/**
 * Delegates URI processing to the file system.
 * @param uri The URI to process.
 * @return The processed URI.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,"org.apache.hadoop.fs.ContentSummary:<init>(long,long,long)",162,165,"/**
 * Creates a ContentSummary with default values for some fields.
 * @param length Total length of content.
 * @param fileCount Number of files.
 * @param directoryCount Number of directories.
 */
","*  Constructor, deprecated by ContentSummary.Builder
   *  This constructor implicitly set spaceConsumed the same as length.
   *  spaceConsumed and length must be set explicitly with
   *  ContentSummary.Builder.
   *
   * @param length length.
   * @param fileCount file count.
   * @param directoryCount directory count.
   *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,toString,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:toString(),160,166,"/**
 * Checks token renewal status. Returns a renewal message.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,<init>,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:<init>(org.apache.hadoop.fs.FileSystem),71,75,"/**
 * Constructs a RenewAction with a WeakReference to the feeder.
 * @param fs The feeder object; may be garbage collected.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getStatus,org.apache.hadoop.fs.HarFileSystem:getStatus(org.apache.hadoop.fs.Path),283,286,"/**
* Delegates m1 operation to the underlying file system.
* @param p The path to operate on.
* @return FsStatus returned by the underlying file system.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFsStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus(org.apache.hadoop.fs.Path),153,156,"/**
* Delegates file status retrieval to the underlying file system implementation.
* @param f Path to the file for which to retrieve status.
* @return FsStatus object representing the file status.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatus,org.apache.hadoop.fs.FileSystem:getStatus(),3026,3028,"/**
 * Calls m1 with a null argument.
 * @return FsStatus object representing the result.
 */
","* Returns a status object describing the use and capacity of the
   * filesystem. If the filesystem has multiple partitions, the
   * use and capacity of the root partition is reflected.
   *
   * @return a FsStatus object
   * @throws IOException
   *           see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getStatus,org.apache.hadoop.fs.FilterFileSystem:getStatus(org.apache.hadoop.fs.Path),329,332,"/**
* Delegates m1 call to the underlying file system.
* @param p The path to operate on.
* @return FsStatus returned by the underlying file system.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,toString,org.apache.hadoop.fs.impl.CombinedFileRange:toString(),91,96,"/**
* Appends range count and data size to the superclass's m1 result.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,<init>,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:<init>(org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder),151,180,"/**
 * Constructs an HttpReferrerAuditHeader from a builder, initializing its attributes.
 */","* Instantiate.
   * <p>
   * All maps/enums passed down are copied into thread safe equivalents.
   * as their origin is unknown and cannot be guaranteed to
   * not be shared.
   * <p>
   * Context and operationId are expected to be well formed
   * numeric/hex strings, at least adequate to be
   * used as individual path elements in a URL.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,<init>,"org.apache.hadoop.fs.impl.WeakReferenceThreadMap:<init>(java.util.function.Function,java.util.function.Consumer)",36,39,"/**
 * Constructs a WeakReferenceThreadMap with a factory and lost reference handler.
 * @param factory Creates values for keys.
 * @param referenceLost Handles lost references.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,hasCapability,org.apache.hadoop.fs.FSDataOutputStream:hasCapability(java.lang.String),128,131,"/**
* Delegates capability check to StoreImplementationUtils.
* @param capability The capability to check.
* @return True if capability exists, false otherwise.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hasCapability,org.apache.hadoop.crypto.CryptoOutputStream:hasCapability(java.lang.String),316,319,"/**
* Delegates capability check to StoreImplementationUtils.
* @param capability The capability to check.
* @return True if capability exists, false otherwise.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,hasCapability,org.apache.hadoop.fs.FSDataInputStream:hasCapability(java.lang.String),242,245,"/**
* Delegates capability check to StoreImplementationUtils.
* @param capability The capability to check.
* @return True if capability exists, false otherwise.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getPrefetched,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getPrefetched(int),168,172,"/**
 * Retrieves a prefetch operation for the given block number.
 * @param blockNumber The block number for the prefetch operation.
 * @return An Operation object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getCached,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getCached(int),174,178,"/**
 * Retrieves an operation for a given block number.
 * @param blockNumber The block number for the operation.
 * @return An Operation object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getRead,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getRead(int),180,184,"/**
 * Retrieves an operation for a given block number.
 * @param blockNumber The block number for the operation.
 * @return An Operation object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,release,org.apache.hadoop.fs.impl.prefetch.BlockOperations:release(int),186,190,"/**
 * Releases a block.
 * @param blockNumber The block to release.
 * @return An Operation object representing the release.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestPrefetch(int),192,196,"/**
* Prefetches a block.
* @param blockNumber The block to prefetch.
* @return An Operation object representing the prefetch request.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,prefetch,org.apache.hadoop.fs.impl.prefetch.BlockOperations:prefetch(int),198,202,"/**
 * Prefetches a block.
 * @param blockNumber Block to prefetch. Returns an Operation object.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,cancelPrefetches,org.apache.hadoop.fs.impl.prefetch.BlockOperations:cancelPrefetches(),204,206,"/**
* Creates and returns a cancel prefetches operation.
* Returns the result of calling m1 with the created operation.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,close,org.apache.hadoop.fs.impl.prefetch.BlockOperations:close(),208,210,"/**
* Creates and returns an Operation with Kind.CLOSE and value -1.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,requestCaching,org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestCaching(int),212,216,"/**
* Requests caching for a block.
* @param blockNumber The block number to request caching for.
* @return An Operation object representing the caching request.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,addToCache,org.apache.hadoop.fs.impl.prefetch.BlockOperations:addToCache(int),218,222,"/**
 * Executes a cache put operation for a given block number.
 * @param blockNumber The block number to cache.
 * @return An Operation object representing the cache put.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,end,org.apache.hadoop.fs.impl.prefetch.BlockOperations:end(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),224,226,"/**
 * Applies a mask to an operation.
 * @param op The operation to mask.
 * @return The masked operation.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,fromSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations:fromSummary(java.lang.String),377,424,"/**
 * Parses a summary string into BlockOperations.
 * @param summary String containing block operation summary
 * @return BlockOperations object representing parsed operations
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,release,org.apache.hadoop.fs.impl.prefetch.BufferPool:release(org.apache.hadoop.fs.impl.prefetch.BufferData),236,255,"/**
 * Releases a buffer associated with the given data.
 * @param data BufferData object containing release info.
 */","* Releases a previously acquired resource.
   * @param data the {@code BufferData} instance to release.
   * @throws IllegalArgumentException if data is null.
   * @throws IllegalArgumentException if data cannot be released due to its state.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,toString,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:toString(),153,158,"/**
 * Returns a formatted string with resource statistics.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getDurationInfo,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getDurationInfo(java.lang.StringBuilder),252,295,"/**
 * Calculates and reports operation durations for specified kinds.
 * @param sb StringBuilder to append the duration statistics to.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,createCache,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:createCache(int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",554,556,"/**
* Creates a SingleFilePerBlockCache with given parameters.
* @param maxBlocksCount Max blocks count.
* @param trackerFactory Tracker factory.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,<init>,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean)",90,95,"/**
 * Constructs a SemaphoredDelegatingExecutor with a permit count.
 * @param executorDelegatee The underlying ExecutorService.
 * @param permitCount The initial number of permits.
 * @param fair Whether permits are granted fairly.
 */
","* Instantiate without collecting executor aquisition duration information.
   * @param executorDelegatee Executor to delegate to
   * @param permitCount number of permits into the queue permitted
   * @param fair should the semaphore be ""fair""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getEntry,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getEntry(int),297,307,"/**
 * Retrieves an entry from the cache for the given block number.
 * @param blockNumber The block number to retrieve.
 * @return The Entry object or throws exception if not found.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,releaseReadyBlock,org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseReadyBlock(int),205,224,"/**
 * Releases a 'ready' block with highest priority for a block number.
 * @param blockNumber The block number to prioritize for release.
 */","* If no blocks were released after calling releaseDoneBlocks() a few times,
   * we may end up waiting forever. To avoid that situation, we try releasing
   * a 'ready' block farthest away from the given block.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,toString,org.apache.hadoop.fs.impl.prefetch.BufferPool:toString(),278,292,"/**
 * Generates a string from buffer data, sorted by a specific field.
 * @return Formatted string containing sorted buffer data.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,buffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:buffer(),130,133,"/**
 * Initializes and returns a ByteBuffer.
 * Calls m1() to populate the buffer.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,data,org.apache.hadoop.fs.impl.prefetch.FilePosition:data(),135,138,"/**
 * Retrieves buffer data after executing m1().
 * @return BufferData object containing the processed data.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,relative,org.apache.hadoop.fs.impl.prefetch.FilePosition:relative(),172,175,"/**
* Calls m1() and returns the result of buffer.m2().
*/","* Gets the current position within this file relative to the start of the associated buffer.
   *
   * @return the current position within this file relative to the start of the associated buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,isWithinCurrentBuffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:isWithinCurrentBuffer(long),183,187,"/**
 * Checks if the given position is within the buffer's bounds.
 * @param pos The position to check.
 * @return True if within bounds, false otherwise.
 */
","* Determines whether the given absolute position lies within the current buffer.
   *
   * @param pos the position to check.
   * @return true if the given absolute position lies within the current buffer, false otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,bufferStartOffset,org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferStartOffset(),231,234,"/**
* Initializes resources and returns the buffer start offset.
*/","* Gets the start of the current block's absolute offset.
   *
   * @return the start of the current block's absolute offset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,getCurrentIOStatisticsContext,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:getCurrentIOStatisticsContext(),122,126,"/**
 * Returns the IO statistics context based on thread IO stats.
 */","* Get the current thread's IOStatisticsContext instance. If no instance is
   * present for this thread ID, create one using the factory.
   * @return instance of IOStatisticsContext.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,setThreadIOStatisticsContext,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext),133,145,"/**
 * Updates IO statistics context, null safe.
 * @param statisticsContext Context to update, or null.
 */","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,mergeSortedRanges,"org.apache.hadoop.fs.VectoredReadUtils:mergeSortedRanges(java.util.List,int,int,int)",380,398,"/**
* Combines overlapping file ranges into larger, optimized ranges.
* @param sortedRanges List of sorted file ranges to combine.
* @param chunkSize Chunk size for combining ranges.
* @return List of combined file ranges.
*/
","* Merge sorted ranges to optimize the access from the underlying file
   * system.
   * The motivations are that:
   * <ul>
   *   <li>Upper layers want to pass down logical file ranges.</li>
   *   <li>Fewer reads have better performance.</li>
   *   <li>Applications want callbacks as ranges are read.</li>
   *   <li>Some file systems want to round ranges to be at checksum boundaries.</li>
   * </ul>
   *
   * @param sortedRanges already sorted list of ranges based on offset.
   * @param chunkSize round the start and end points to multiples of chunkSize
   * @param minimumSeek the smallest gap that we should seek over in bytes
   * @param maxSize the largest combined file range in bytes
   * @return the list of sorted CombinedFileRanges that cover the input",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,findChecksumRanges,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:findChecksumRanges(java.util.List,int,int,int)",344,362,"/**
* Combines overlapping FileRange objects into CombinedFileRange.
* @param dataRanges List of FileRange objects to combine.
* @return List of combined FileRange objects.
*/","* Find the checksum ranges that correspond to the given data ranges.
     * @param dataRanges the input data ranges, which are assumed to be sorted
     *                   and non-overlapping
     * @return a list of AsyncReaderUtils.CombinedFileRange that correspond to
     *         the checksum ranges",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name:<init>(),48,50,"/**
 * Default constructor. Initializes with the default visibility.
 */
",Creates a case sensitive name expression.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,subset,org.apache.hadoop.metrics2.impl.MetricsConfig:subset(java.lang.String),144,147,"/**
 * Creates a new MetricsConfig with the given prefix.
 * @param prefix The prefix for the metrics config.
 * @return A new MetricsConfig object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/And.java,apply,"org.apache.hadoop.fs.shell.find.And:apply(org.apache.hadoop.fs.shell.PathData,int)",57,68,"/**
 * Processes PathData recursively.
 * @param item PathData to process, depth is ignored.
 * @return Result of processing, fails if any child fails.
 */
","* Applies child expressions to the {@link PathData} item. If all pass then
   * returns {@link Result#PASS} else returns the result of the first
   * non-passing expression.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getOptions,org.apache.hadoop.fs.shell.find.Find:getOptions(),235,241,"/**
 * Returns the configured FindOptions, initializing if null.
 */","Returns the current find options, creating them if necessary.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,parse,org.apache.hadoop.fs.shell.CommandFormat:parse(java.util.List),99,140,"/**
* Processes command-line arguments, handling options and values.
*/","Parse parameters from the given list of args.  The list is
   *  destructively modified to remove the options.
   * 
   * @param args as a list of input arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getDescription,org.apache.hadoop.fs.shell.Command:getDescription(),547,551,"/**
* Returns a string based on m1(): deprecated message or m2() result.
*/","* The long usage suitable for help output
   * @return text of the usage",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayWarning,org.apache.hadoop.fs.shell.Command:displayWarning(java.lang.String),510,512,"/**
 * Logs an error message by prepending a prefix.
 * @param message The error message to log.
 */
","* Display an warning string prefaced with the command name.
   * @param message warning message to display",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getUsage,org.apache.hadoop.fs.shell.Command:getUsage(),537,541,"/**
* Constructs a command string based on internal logic.
* Returns command or command + usage string.
*/","* The short usage suitable for the synopsis
   * @return ""name options""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,long,boolean)",71,88,"/**
 * Constructs a MetricsSourceAdapter with provided configuration.
 * @param prefix Metric name prefix.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,addRecord,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(org.apache.hadoop.metrics2.MetricsInfo),42,50,"/**
 * Creates a MetricsRecordBuilderImpl based on MetricsInfo.
 * @param info MetricsInfo object used for building the record.
 * @return A MetricsRecordBuilderImpl instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,<init>,org.apache.hadoop.util.ChunkedArrayList:<init>(),94,96,"/**
 * Constructs a ChunkedArrayList with default initial chunk capacity.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ScopedAclEntries.java,<init>,org.apache.hadoop.fs.permission.ScopedAclEntries:<init>(java.util.List),47,57,"/**
 * Constructs ScopedAclEntries from a list of AclEntry objects.
 * Separates access and default entries based on a pivot.
 */
","* Creates a new ScopedAclEntries from the given list.  It is assumed that the
   * list is already sorted such that all access entries precede all default
   * entries.
   *
   * @param aclEntries List&lt;AclEntry&gt; to separate",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,printToStream,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:printToStream(java.io.PrintStream),312,334,"/**
* Prints rows to the output stream formatted according to widths.
* @param out PrintStream to write the formatted rows to.
*/","* Render the table to a stream.
     * @param out PrintStream for output",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,moved,org.apache.hadoop.fs.Options$HandleOpt:moved(boolean),432,434,"/**
 * Creates a new Location object.
 * @param allow boolean indicating location permission
 * @return A new Location object.
 */
","* @param allow If true, resolve references to this entity anywhere in
     *              the namespace.
     * @return Handle option encoding parameter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,changed,org.apache.hadoop.fs.Options$HandleOpt:changed(boolean),423,425,"/**
 * Creates a Data object.
 * @param allow boolean value to initialize the Data object.
 * @return A new Data object.
 */
","* @param allow If true, resolve references to this entity even if it has
     *             been modified.
     * @return Handle option encoding parameter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,<init>,"org.apache.hadoop.fs.DF:<init>(java.io.File,long)",54,59,"/**
 * Constructs a DirectoryFormatter with a specified interval.
 * @param path Directory to format.
 * @param dfInterval Interval for directory format checks.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell:<init>(),901,903,"/**
 * Default constructor, initializes with a default timeout.
 */","* Create an instance with no minimum interval between runs; stderr is
   * not merged with stdout.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,<init>,"org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:<init>(org.apache.hadoop.fs.CachingGetSpaceUsed,boolean)",204,207,"/**
 * Initializes a RefreshThread with a CachingGetSpaceUsed and flag.
 * @param spaceUsed The CachingGetSpaceUsed instance.
 * @param runImmediately Whether to run immediately.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,"org.apache.hadoop.security.token.Token$PrivateToken:<init>(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",255,263,"/**
 * Constructs a PrivateToken, cloning from a public Token.
 * @param publicToken The public Token to clone from.
 * @param newService The new service for the private token.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,generateDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateDelegationToken(org.apache.hadoop.security.token.Token),1144,1153,"/**
 * Creates a DelegationTokenAuthenticatedURL.Token from a Token.
 * @param dToken The input Token to create the new Token from.
 * @return A new DelegationTokenAuthenticatedURL.Token object.
 */","* Generate a DelegationTokenAuthenticatedURL.Token from the given generic
   * typed delegation token.
   *
   * @param dToken The delegation token.
   * @return The DelegationTokenAuthenticatedURL.Token, with its delegation
   *         token set to the delegation token passed in.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:listLocatedStatus(org.apache.hadoop.fs.Path),847,852,"/**
* Delegates to the superclass implementation for file status iteration.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),284,288,"/**
* Delegates to the underlying filesystem's m1 method.
* @param f The path to retrieve the iterator for.
* @return RemoteIterator of LocatedFileStatus.
*/",List files and its block locations in a directory.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setVerifyChecksum,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setVerifyChecksum(boolean),1368,1372,"/**
* Sets verify checksum flag. Throws AccessControlException.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileChecksum,org.apache.hadoop.fs.DelegateToFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),124,128,"/**
* Delegates checksum calculation to the file system implementation.
* @param f Path to the file.
* @return FileChecksum object.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileChecksum,org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),502,505,"/**
* Delegates checksum calculation to the file system.
* @param f Path to the file.
* @return FileChecksum object.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processPath,org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData),179,186,"/**
 * Writes data to the filesystem based on name or xname.
 * @param item PathData object containing filesystem and path.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setXAttr,"org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",623,627,"/**
* Delegates the m1 call to the underlying file system.
* @param path The path to the file.
* @param name The name of the file.
* @param value The file content as bytes.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.DelegateToFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",282,285,"/**
* Delegates to the underlying filesystem implementation.
* @param path Path to open.
* @param parameters Open file parameters.
* @return CompletableFuture wrapping FSDataInputStream.
*/
","* Open a file by delegating to
   * {@link FileSystem#openFileWithOptions(Path, org.apache.hadoop.fs.impl.OpenFileParameters)}.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   *
   * @return a future which will evaluate to the opened file.ControlAlpha
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",721,726,"/**
* Delegates to the underlying filesystem's open method.
* @param path Path to open
* @param parameters Open parameters
* @return CompletableFuture wrapping the input stream
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",444,449,"/**
 * Delegates to the underlying FileSystem's open method.
 * @param path Path to open
 * @param parameters Open file parameters
 * @return CompletableFuture of FSDataInputStream
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)",728,733,"/**
 * Delegates file input stream opening to the underlying file system.
 * @param pathHandle Path handle for the file.
 * @param parameters Parameters for opening the file.
 * @return CompletableFuture wrapping the FSDataInputStream.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",249,252,"/**
 * Constructs a directory link node.
 * @param pathToNode Path to the node.
 * @param aUgi UserGroupInformation.
 * @param link The INodeLink object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addDir,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addDir(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",211,220,"/**
 * Creates a new directory.
 * @param pathComponent directory name
 * @param aUgi user group information
 * @return INodeDir object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getChildFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:getChildFileSystems(),1035,1058,"/**
* Collects child FileSystem objects based on mount points.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getFallbackFileSystem,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getFallbackFileSystem(),400,411,"/**
 * Gets the fallback filesystem. Returns null if unavailable or error.
 */","* @return Gets the fallback file system configured. Usually, this will be the
   * default cluster.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,addCall,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:addCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall),118,124,"/**
 * Adds an async call to the queue and triggers processing.
 * @param call The async call to be added.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,update,"org.apache.hadoop.crypto.OpensslCipher:update(java.nio.ByteBuffer,java.nio.ByteBuffer)",231,241,"/**
 * Copies data from input to output buffers.
 * @param input Input buffer.
 * @param output Output buffer.
 * @return Number of bytes copied.
 */","* Continues a multiple-part encryption or decryption operation. The data
   * is encrypted or decrypted, depending on how this cipher was initialized.
   * <p>
   * 
   * All <code>input.remaining()</code> bytes starting at 
   * <code>input.position()</code> are processed. The result is stored in
   * the output buffer.
   * <p>
   * 
   * Upon return, the input buffer's position will be equal to its limit;
   * its limit will not have changed. The output buffer's position will have
   * advanced by n, when n is the value returned by this method; the output
   * buffer's limit will not have changed.
   * <p>
   * 
   * If <code>output.remaining()</code> bytes are insufficient to hold the
   * result, a <code>ShortBufferException</code> is thrown.
   * 
   * @param input the input ByteBuffer
   * @param output the output ByteBuffer
   * @return int number of bytes stored in <code>output</code>
   * @throws ShortBufferException if there is insufficient space in the
   * output buffer",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,doFinal,org.apache.hadoop.crypto.OpensslCipher:doFinal(java.nio.ByteBuffer),270,277,"/**
 * Encrypts data to the output buffer.
 * @param output ByteBuffer to write encrypted data to
 * @return Number of bytes written to the output buffer
 */","* Finishes a multiple-part operation. The data is encrypted or decrypted,
   * depending on how this cipher was initialized.
   * <p>
   * The result is stored in the output buffer. Upon return, the output buffer's
   * position will have advanced by n, where n is the value returned by this
   * method; the output buffer's limit will not have changed.
   * </p>
   * If <code>output.remaining()</code> bytes are insufficient to hold the result,
   * a <code>ShortBufferException</code> is thrown.
   * <p>
   * Upon finishing, this method resets this cipher object to the state it was
   * in when previously initialized. That is, the object is available to encrypt
   * or decrypt more data.
   * </p>
   * If any exception is thrown, this cipher object need to be reset before it
   * can be used again.
   *
   * @param output the output ByteBuffer
   * @return int number of bytes stored in <code>output</code>
   * @throws ShortBufferException      if there is insufficient space in the output buffer.
   * @throws IllegalBlockSizeException This exception is thrown when the length
   *                                   of data provided to a block cipher is incorrect.
   * @throws BadPaddingException       This exception is thrown when a particular
   *                                   padding mechanism is expected for the input
   *                                   data but the data is not padded properly.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointInterceptorFactory.java,create,org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorFactory:create(java.lang.String),41,66,"/**
 * Creates a RegexMountPointInterceptor based on settings.
 * @param interceptorSettingsString settings string to parse
 * @return RegexMountPointInterceptor or null if invalid.
 */
","* interceptorSettingsString string should be like ${type}:${string},
   * e.g. replaceresolveddstpath:word1,word2.
   *
   * @param interceptorSettingsString
   * @return Return interceptor based on setting or null on bad/unknown config.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,toString,org.apache.hadoop.fs.DF:toString(),129,139,"/**
 * Generates a formatted string containing mount information.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,normalizePath,"org.apache.hadoop.fs.Path:normalizePath(java.lang.String,java.lang.String)",297,318,"/**
* Normalizes a path string, replacing backslashes with forward slashes
* if on Windows and adjusting length based on separators.
*/","* Normalize a path string to use non-duplicated forward slashes as
   * the path separator and remove any trailing path separators.
   *
   * @param scheme the URI scheme. Used to deduce whether we
   * should replace backslashes or not
   * @param path the scheme-specific part
   * @return the normalized path string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isWindowsAbsolutePath,"org.apache.hadoop.fs.Path:isWindowsAbsolutePath(java.lang.String,boolean)",341,348,"/**
* Checks if a path string is valid based on given criteria.
* @param pathString The path string to validate.
* @param slashed Whether the path should be slashed.
* @return True if the path is valid, false otherwise.
*/
","* Determine whether a given path string represents an absolute path on
   * Windows. e.g. ""C:/a/b"" is an absolute path. ""C:a/b"" is not.
   *
   * @param pathString the path string to evaluate
   * @param slashed true if the given path is prefixed with ""/""
   * @return true if the supplied path looks like an absolute path with a Windows
   * drive-specifier",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isUriPathAbsolute,org.apache.hadoop.fs.Path:isUriPathAbsolute(),388,391,"/**
* Checks if the URI contains a separator after the initial part.
*/","* Returns true if the path component (i.e. directory) of this URI is
   * absolute.
   *
   * @return whether this URI's path is absolute",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getHarHash,org.apache.hadoop.fs.HarFileSystem:getHarHash(org.apache.hadoop.fs.Path),488,490,"/**
 * Masks the result of Path method calls with a bitmask.
 * @param p The Path object to process.
 * @return An integer result, masked to remove sign extension.
 */","* the hash of the path p inside  the filesystem
   * @param p the path in the harfilesystem
   * @return the hash code of the path.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,build,org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:build(),4694,4714,"/**
* Opens a data output stream based on create/overwrite/append flags.
* @throws IOException if an I/O error occurs or flags are invalid.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FileStatus),108,135,"/**
 * Converts a FileStatus to a FileStatusProto.
 * @param stat The FileStatus to convert.
 * @return FileStatusProto object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,<init>,"org.apache.hadoop.fs.sftp.SFTPInputStream:<init>(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem$Statistics)",46,58,"/**
 * Initializes an SFTPInputStream with a channel, path, and stats.
 * @param channel SFTP channel
 * @param path file path
 * @param stats file statistics
 * @throws IOException if an error occurs during initialization
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPath,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPath(org.apache.hadoop.fs.Path),69,73,"/**
 * Validates that the given path is a child of the base path.
 * @param path The path to validate.
 */
","* Validate a path.
   * @param path path to check.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,getExecString,org.apache.hadoop.fs.Stat:getExecString(),91,108,"/**
* Constructs the 'stat' command based on the operating system.
* Returns a String array representing the command.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)",103,106,"/**
* Creates an AccessControlException with operation and path details.
* @param operation The operation being performed.
* @param p The path being accessed.
* @return An AccessControlException.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)",181,184,"/**
* Creates an AccessControlException using the operation name.
* @param operation The operation being performed.
* @param p The Path object.
* @return An AccessControlException.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,getPathToResolve,"org.apache.hadoop.fs.viewfs.RegexMountPoint:getPathToResolve(java.lang.String,boolean)",217,227,"/**
 * Extracts path up to the last slash, or returns original if resolveLastComponent is true.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,checkDependencies,"org.apache.hadoop.fs.FileContext:checkDependencies(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2284,2298,"/**
* Checks if source and destination paths are valid for copying.
* @param qualSrc Source path.
* @param qualDst Destination path.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compareTo,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:compareTo(java.lang.Object),3794,3805,"/**
 * Compares two SegmentDescriptor objects based on their fields.
 * @param o the SegmentDescriptor to compare to
 * @return -1, 0, or 1 based on comparison result
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:equals(java.lang.Object),3807,3820,"/**
 * Checks if two SegmentDescriptor objects are equal.
 * @param o the object to compare with, must be SegmentDescriptor
 * @return true if objects are equal, false otherwise
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getNextIdToTry,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNextIdToTry(org.apache.hadoop.fs.Path,int)",731,753,"/**
 * Finds the next available ID for new files in a directory.
 * @param initial The base path.
 * @param lastId The last known ID.
 * @return The next available ID.
 */
","* Return the next ID suffix to use when creating the log file. This method
   * will look at the files in the directory, find the one with the highest
   * ID suffix, and 1 to that suffix, and return it. This approach saves a full
   * linear probe, which matters in the case where there are a large number of
   * log files.
   *
   * @param initial the base file path
   * @param lastId the last ID value that was used
   * @return the next ID to try
   * @throws IOException thrown if there's an issue querying the files in the
   * directory",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getPathAsString,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPathAsString(),144,146,"/**
* Returns the result of calling m1() and then m2() on it.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createServiceURL,org.apache.hadoop.crypto.key.kms.KMSClientProvider:createServiceURL(org.apache.hadoop.fs.Path),447,453,"/**
 * Constructs a URL based on the provided Path, adjusting the path.
 * @param path Path object containing URL components.
 * @return URL object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,seek,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seek(long),313,319,"/**
* Seeks to a given position.
* @param pos The position to seek to.
* @throws IOException if seeking past the end of the stream.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,skip,org.apache.hadoop.fs.FSInputChecker:skip(long),405,413,"/**
 * Applies a mask to a value.
 * @param n The value to be masked. Returns n.
 */","* Skips over and discards <code>n</code> bytes of data from the
   * input stream.
   *
   * <p>This method may skip more bytes than are remaining in the backing
   * file. This produces no exception and the number of bytes skipped
   * may include some number of bytes that were beyond the EOF of the
   * backing file. Attempting to read from the stream after skipping past
   * the end will result in -1 indicating the end of the file.
   *
   *<p>If <code>n</code> is negative, no bytes are skipped.
   *
   * @param      n   the number of bytes to be skipped.
   * @return     the actual number of bytes skipped.
   * @exception  IOException  if an I/O error occurs.
   *             ChecksumException if the chunk to skip to is corrupted",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ByteBufferUtil.java,fallbackRead,"org.apache.hadoop.fs.ByteBufferUtil:fallbackRead(java.io.InputStream,org.apache.hadoop.io.ByteBufferPool,int)",57,117,"/**
 * Reads data from an InputStream into a ByteBuffer from a pool.
 * @param stream Input stream to read from
 * @param bufferPool ByteBuffer pool for buffer allocation
 * @param maxLength Maximum number of bytes to read
 * @return ByteBuffer containing the read data
 * @throws IOException if an I/O error occurs
 */","* Perform a fallback read.
   *
   * @param stream input stream.
   * @param bufferPool bufferPool.
   * @param maxLength maxLength.
   * @throws IOException raised on errors performing I/O.
   * @return byte buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,reset,org.apache.hadoop.fs.audit.CommonAuditContext:reset(),185,188,"/**
 * Executes m1 on evaluatedEntries and then calls m2.
 */","* Rest the context; will set the standard options again.
   * Primarily for testing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,createInstance,org.apache.hadoop.fs.audit.CommonAuditContext:createInstance(),212,216,"/**
 * Creates and initializes a CommonAuditContext object.
 * Returns the initialized context.
 */","* Demand invoked to create the instance for this thread.
   * @return an instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,toString,org.apache.hadoop.tools.TableListing:toString(),229,292,"/**
* Generates a formatted table string based on column data.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,"org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction)",82,84,"/**
 * Constructs a FsPermission with user, group, and other actions.
 * @param u User action
 * @param g Group action
 * @param o Other action
 */
","* Construct by the given {@link FsAction}.
   * @param u user action
   * @param g group action
   * @param o other action",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(short),95,95,"/**
 * Constructs a FsPermission object from a short mode value.
 */","* Construct by the given mode.
   * @param mode mode.
   * @see #toShort()",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,readFields,org.apache.hadoop.fs.permission.FsPermission:readFields(java.io.DataInput),185,189,"/**
* Delegates reading from DataInput to m2.
* @param in DataInput object to read from.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,read,org.apache.hadoop.fs.permission.FsPermission:read(java.io.DataInput),214,218,"/**
 * Creates a FsPermission object from input stream.
 * @param in Input stream containing permission bits.
 * @return FsPermission object constructed from the input.
 */
","* Create and initialize a {@link FsPermission} from {@link DataInput}.
   *
   * @param in data input.
   * @throws IOException raised on errors performing I/O.
   * @return FsPermission.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,getEffectivePermission,org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry),230,232,"/**
* Calls m1 with the provided AclEntry and default permission.
* @param entry The AclEntry to process.
* @return An FsAction object.
*/
","* Get the effective permission for the AclEntry
   * @param entry AclEntry to get the effective action
   * @return FsAction.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,createImmutable,"org.apache.hadoop.fs.permission.PermissionStatus:createImmutable(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",49,57,"/**
 * Creates a PermissionStatus object with the given user, group, and permission.
 */","* Create an immutable {@link PermissionStatus} object.
   * @param user user.
   * @param group group.
   * @param permission permission.
   * @return PermissionStatus.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,parseAclSpec,"org.apache.hadoop.fs.permission.AclEntry:parseAclSpec(java.lang.String,boolean)",235,245,"/**
 * Parses aclSpec into AclEntry objects.
 * @param aclSpec comma-separated ACL string
 * @param includePermission flag to include permission
 * @return List of AclEntry objects
 */
","* Parses a string representation of an ACL spec into a list of AclEntry
   * objects. Example: ""user::rwx,user:foo:rw-,group::r--,other::---""
   * The expected format of ACL entries in the string parameter is the same
   * format produced by the {@link #toStringStable()} method.
   * 
   * @param aclSpec
   *          String representation of an ACL spec.
   * @param includePermission
   *          for setAcl operations this will be true. i.e. AclSpec should
   *          include permissions.<br>
   *          But for removeAcl operation it will be false. i.e. AclSpec should
   *          not contain permissions.<br>
   *          Example: ""user:foo,group:bar""
   * @return Returns list of {@link AclEntry} parsed",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,create,"org.apache.hadoop.fs.permission.FsCreateModes:create(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",58,63,"/**
 * Creates an FsCreateModes object with the given masked and unmasked permissions.
 */","* Create from masked and unmasked modes.
   *
   * @param masked masked.
   * @param unmasked unmasked.
   * @return FsCreateModes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,printExtendedAclEntry,"org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printExtendedAclEntry(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.AclEntry)",137,152,"/**
* Outputs ACL entry permissions, comparing effective permission.
* @param aclStatus ACL status object.
* @param fsPerm FsPermission object.
* @param entry AclEntry object.
*/","* Prints a single extended ACL entry.  If the mask restricts the
     * permissions of the entry, then also prints the restricted version as the
     * effective permissions.  The mask applies to all named entries and also
     * the unnamed group entry.
     * @param aclStatus AclStatus for the path
     * @param fsPerm FsPermission for the path
     * @param entry AclEntry extended ACL entry to print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,toString,org.apache.hadoop.fs.permission.AclEntry:toString(),102,108,"/**
* Delegates to m1() and returns its result.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStrings,org.apache.hadoop.util.StringUtils:getStrings(java.lang.String),407,410,"/**
 * Splits a string by comma.
 * @param str The string to split.
 * @return String array of split strings.
 */
","* Returns an arraylist of strings.
   * @param str the comma separated string values
   * @return the arraylist of the comma separated string values",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/UmaskParser.java,<init>,org.apache.hadoop.fs.permission.UmaskParser:<init>(java.lang.String),41,45,"/**
 * Creates a UmaskParser with the given mode string.
 * @param modeStr String representing the umask mode.
 * @throws IllegalArgumentException if modeStr is invalid.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/RawParser.java,<init>,org.apache.hadoop.fs.permission.RawParser:<init>(java.lang.String),35,38,"/**
 * Constructs a RawParser with specified mode string.
 * @param modeStr Mode string for parsing; throws IllegalArgumentException on error.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ChmodParser.java,<init>,org.apache.hadoop.fs.permission.ChmodParser:<init>(java.lang.String),38,40,"/**
 * Creates a ChmodParser with the given mode string.
 * @param modeStr The chmod mode string to parse.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,create,"org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",530,535,"/**
 * Creates a ByteArrayBlock with specified limit and statistics.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,available,org.apache.hadoop.fs.store.ByteBufferInputStream:available(),116,120,"/**
* Calls m1() and returns the result of byteBuffer.m2().
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,position,org.apache.hadoop.fs.store.ByteBufferInputStream:position(),126,129,"/**
* Calls m1() and returns the result of byteBuffer.m2().
*/","* Get the current buffer position.
   * @return the buffer position",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,hasRemaining,org.apache.hadoop.fs.store.ByteBufferInputStream:hasRemaining(),135,138,"/**
 * Calls m1() and returns the result of byteBuffer.m2().
 */","* Check if there is data left.
   * @return true if there is data remaining in the buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,reset,org.apache.hadoop.fs.store.ByteBufferInputStream:reset(),147,152,"/**
* Resets the buffer and calls m2. Logs ""reset"" and calls byteBuffer.m3().
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$DataBlock:startUpload(),454,458,"/**
 * Initiates a data block upload. Returns null.
 */","* Switch to the upload state and return a stream for uploading.
     * Base class calls {@link #enterState(DestState, DestState)} to
     * manage the state machine.
     *
     * @return the stream.
     * @throws IOException trouble",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,enterClosedState,org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterClosedState(),466,473,"/**
 * Attempts to close the state; returns true if successful.
 */","* Enter the closed state.
     *
     * @return true if the class was in any other state, implying that
     * the subclass should do its close operations.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$DiskBlock:write(byte[],int,int)",878,885,"/**
 * Writes a portion of the byte array to the output stream.
 * @param b byte array to write, offset start index, len length
 * @return Number of bytes written
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:write(byte[],int,int)",747,753,"/**
 * Writes data to the buffer, invoking superclass and updating.
 * @param b buffer to write from, offset start, len length
 * @return number of bytes written
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,flush,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:flush(),940,943,"/**
 * Calls super.m1() and then calls out.m1().
 */","* Flush operation will flush to disk.
     *
     * @throws IOException IOE raised on FileOutputStream",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:write(byte[],int,int)",612,618,"/**
 * Writes len bytes from b to the buffer, calling super.m1 first.
 * @param b byte array to write from
 * @param offset offset in the byte array
 * @param len number of bytes to write
 * @return number of bytes written
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toString,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:toString(),767,776,"/**
 * Returns a string representation of the ByteBufferBlock.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getStatistics,org.apache.hadoop.fs.FileContext:getStatistics(java.net.URI),2396,2398,"/**
 * Delegates to AbstractFileSystem.m1 to get statistics for a URI.
 * @param uri The URI to get statistics for.
 * @return Statistics object.
 */
","* Get the statistics for a particular file system
   * 
   * @param uri
   *          the uri to lookup the statistics. Only scheme and authority part
   *          of the uri are used as the key to store and lookup.
   * @return a statistics object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createMultipartUploader,org.apache.hadoop.fs.FilterFs:createMultipartUploader(org.apache.hadoop.fs.Path),457,461,"/**
* Delegates to the underlying FileSystem's m1 method.
* @param basePath The base path to use.
* @return MultipartUploaderBuilder object.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getCurrentDirectoryIndex,org.apache.hadoop.fs.LocalDirAllocator:getCurrentDirectoryIndex(),257,260,"/**
* Delegates to a context's m2() method.
* @return Result of context.m2()
*/","* Get the current directory index for the given configuration item.
   * @return the current directory index for the given configuration item.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,getPos,org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:getPos(),49,52,"/**
 * Delegates m1() call to the fsOut object.
 * @return long value returned by fsOut.m1()
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$Writer:sync(),1369,1375,"/**
* Syncs data if necessary, writing SYNC_ESCAPE and sync data.
*/","* create a sync point.
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getLength,org.apache.hadoop.io.SequenceFile$Writer:getLength(),1530,1532,"/**
 * Returns a value from the output stream.
 * @return Long value obtained from m1() method.
 * @throws IOException if an I/O error occurs.
 */","@return Returns the current length of the output file.
     *
     * <p>This always returns a synchronized position.  In other words,
     * immediately after calling {@link SequenceFile.Reader#seek(long)} with a position
     * returned by this method, {@link SequenceFile.Reader#next(Writable)} may be called.  However
     * the key may be earlier in the file than key last written when this
     * method was called (e.g., with block-compression, it may be the first key
     * in the block that was being written when this method was called).</p>
     *
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCurrentPos,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCurrentPos(),156,158,"/**
* Calculates a value by summing results from fsOut and fsBufferedOutput.
*/","* Get the current position in file.
       * 
       * @return The current byte offset in underlying file.
       * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getContentSummary,org.apache.hadoop.fs.FileSystem:getContentSummary(org.apache.hadoop.fs.Path),1923,1945,"/**
 * Calculates content summary for a file or directory.
 * @param f Path to the file or directory.
 * @return ContentSummary object with aggregated file stats.
 */
","Return the {@link ContentSummary} of a given {@link Path}.
   * @param f path to use
   * @throws FileNotFoundException if the path does not resolve
   * @throws IOException IO failure
   * @return content summary.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,buildACL,org.apache.hadoop.security.authorize.AccessControlList:buildACL(java.lang.String[]),107,126,"/**
* Processes user/group strings to populate users and groups.
* Populates users/groups based on input strings; uses m2/m3.
*/","* Build ACL from the given array of strings.
   * The strings contain comma separated values.
   *
   * @param userGroupStrings build ACL from array of Strings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfigurationHelper.java,parseEnumSet,"org.apache.hadoop.util.ConfigurationHelper:parseEnumSet(java.lang.String,java.lang.String,java.lang.Class,boolean)",68,99,"/**
 * Creates an EnumSet from a string value, using enum mapping.
 * @param key Key identifier, valueString comma-separated values.
 */","* Given a comma separated list of enum values,
   * trim the list, map to enum values in the message (case insensitive)
   * and return the set.
   * Special handling of ""*"" meaning: all values.
   * @param key Configuration object key -used in error messages.
   * @param valueString value from Configuration
   * @param enumClass class of enum
   * @param ignoreUnknown should unknown values be ignored?
   * @param <E> enum type
   * @return a mutable set of enum values parsed from the valueString, with any unknown
   * matches stripped if {@code ignoreUnknown} is true.
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   * or there are two entries in the enum which differ only by case.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,ensureCurrentState,org.apache.hadoop.service.ServiceStateModel:ensureCurrentState(org.apache.hadoop.service.Service$STATE),97,104,"/**
 * Checks if the service state matches the expected state.
 * @param expectedState The expected service state.
 * @throws ServiceStateException if states do not match.
 */
","* Verify that that a service is in a given state.
   * @param expectedState the desired state
   * @throws ServiceStateException if the service state is different from
   * the desired state",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,<init>,org.apache.hadoop.service.AbstractService:<init>(java.lang.String),113,116,"/**
 * Initializes the service with a name and associated state model.
 * @param name Service name.
 */
","* Construct the service.
   * @param name service name",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,checkStateTransition,"org.apache.hadoop.service.ServiceStateModel:checkStateTransition(java.lang.String,org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)",128,135,"/**
 * Checks if a state transition is valid.
 * @param name Service name, state, and proposed state.
 * @throws ServiceStateException if transition is invalid.
 */
","* Check that a state tansition is valid and
   * throw an exception if not
   * @param name name of the service (can be null)
   * @param state current state
   * @param proposed proposed new state",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceCreationFailure,org.apache.hadoop.service.launcher.ServiceLauncher:serviceCreationFailure(java.lang.Exception),745,747,"/**
 * Creates a ServiceLaunchException wrapping the given exception.
 * @param exception The exception to wrap.
 * @return A new ServiceLaunchException.
 */
","* Generate an exception announcing a failure to create the service.
   * @param exception inner exception.
   * @return a new exception, with the exit code
   * {@link LauncherExitCodes#EXIT_SERVICE_CREATION_FAILURE}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,verifyConfigurationFilesExist,org.apache.hadoop.service.launcher.ServiceLauncher:verifyConfigurationFilesExist(java.lang.String[]),989,1003,"/**
 * Validates configuration files exist.
 * @param filenames array of configuration file paths
 */","* Verify that all the specified filenames exist.
   * @param filenames a list of files
   * @throws ServiceLaunchException if a file is not found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String,java.lang.Object[])",1088,1092,"/**
 * Constructs a KerberosDiagsFailure with a formatted message.
 * @param category Failure category.
 * @param message Message format string.
 * @param args Arguments for message formatting.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,convertToExitException,org.apache.hadoop.service.launcher.ServiceLauncher:convertToExitException(java.lang.Throwable),714,737,"/**
 * Creates a ServiceLaunchException from a Throwable.
 * @param thrown The exception to wrap.
 * @return A ServiceLaunchException with details from thrown.
 */","* Convert an exception to an {@code ExitException}.
   *
   * This process may just be a simple pass through, otherwise a new
   * exception is created with an exit code, the text of the supplied
   * exception, and the supplied exception as an inner cause.
   * 
   * <ol>
   *   <li>If is already the right type, pass it through.</li>
   *   <li>If it implements {@link ExitCodeProvider#getExitCode()},
   *   the exit code is extracted and used in the new exception.</li>
   *   <li>Otherwise, the exit code
   *   {@link LauncherExitCodes#EXIT_EXCEPTION_THROWN} is used.</li>
   * </ol>
   *  
   * @param thrown the exception thrown
   * @return an {@code ExitException} with a status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,<init>,org.apache.hadoop.service.launcher.ServiceShutdownHook:<init>(org.apache.hadoop.service.Service),52,54,"/**
 * Initializes the shutdown hook with a weak reference to the service.
 * @param service The service to be shut down.
 */
","* Create an instance.
   * @param service the service",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,toString,org.apache.hadoop.service.launcher.InterruptEscalator:toString(),89,101,"/**
 * Generates a string representation of InterruptEscalator.
 * Includes signal status, owner, shutdown time, and timeout flag.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,noteFailure,org.apache.hadoop.service.AbstractService:noteFailure(java.lang.Exception),257,272,"/**
 * Records a failure, storing the exception and state.
 * @param exception The exception that caused the failure.
 */
","* Failure handling: record the exception
   * that triggered it -if there was not one already.
   * Services are free to call this themselves.
   * @param exception the exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,recordLifecycleEvent,org.apache.hadoop.service.AbstractService:recordLifecycleEvent(),421,426,"/**
 * Records a lifecycle event with timestamp and state.
 */",* Add a state change event to the lifecycle history,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceInit,org.apache.hadoop.service.CompositeService:serviceInit(org.apache.hadoop.conf.Configuration),104,113,"/**
 * Initializes services using the provided configuration.
 * @param conf Configuration object for service initialization.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,stop,"org.apache.hadoop.service.CompositeService:stop(int,boolean)",147,170,"/**
* Stops services, optionally only started ones. Throws exception if any occur.
*/","* Stop the services in reverse order
   *
   * @param numOfServicesStarted index from where the stop should work
   * @param stopOnlyStartedServices flag to say ""only start services that are
   * started, not those that are NOTINITED or INITED.
   * @throws RuntimeException the first exception raised during the
   * stop process -<i>after all services are stopped</i>",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.hadoop.service.Service),65,67,"/**
 * Calls overloaded method with default logger.
 * @param service The service object to process.
 * @return An Exception if processing fails.
 */
","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,progressable,org.apache.hadoop.io.SequenceFile$Writer:progressable(org.apache.hadoop.util.Progressable),1036,1038,"/**
 * Creates a ProgressableOption from a Progressable object.
 * @param value The Progressable to wrap in an Option.
 * @return A ProgressableOption containing the provided value.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,blockSize,org.apache.hadoop.io.SequenceFile$Writer:blockSize(long),1032,1034,"/**
 * Creates a BlockSizeOption with the given value.
 * @param value The block size value.
 * @return A BlockSizeOption object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,syncInterval,org.apache.hadoop.io.SequenceFile$Writer:syncInterval(int),1061,1063,"/**
 * Creates a SyncIntervalOption with the given value.
 * @param value The interval value for the option.
 * @return A SyncIntervalOption object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,replication,org.apache.hadoop.io.SequenceFile$Writer:replication(short),1024,1026,"/**
 * Creates a ReplicationOption with the given short value.
 * @param value The short value for the option.
 * @return A new ReplicationOption object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,bufferSize,org.apache.hadoop.io.SequenceFile$Writer:bufferSize(int),1016,1018,"/**
 * Creates a BufferSizeOption with the given value.
 * @param value The buffer size value.
 * @return A BufferSizeOption object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,write,org.apache.hadoop.io.ObjectWritable$NullInstance:write(java.io.DataOutput),126,129,"/**
* Writes the declared class name to the output stream.
* @param out DataOutput stream to write to.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,write,org.apache.hadoop.io.ArrayPrimitiveWritable:write(java.io.DataOutput),171,200,"/**
 * Writes the component data to the output stream based on type.
 * @param out DataOutput to write to; throws IOException.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,valueClass,org.apache.hadoop.io.SequenceFile$Writer:valueClass(java.lang.Class),1044,1046,"/**
 * Creates a ValueClassOption for the given class.
 * @param value The class to wrap in an Option.
 * @return A ValueClassOption containing the class.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,keyClass,org.apache.hadoop.io.MapFile$Writer:keyClass(java.lang.Class),285,287,"/**
 * Creates a KeyClassOption for the given WritableComparable type.
 * @param value The WritableComparable class.
 * @return A new KeyClassOption instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,keyClass,org.apache.hadoop.io.SequenceFile$Writer:keyClass(java.lang.Class),1040,1042,"/**
 * Creates a KeyClassOption for the given class.
 * @param value The class to be wrapped in the option.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,compareTo,org.apache.hadoop.io.UTF8:compareTo(org.apache.hadoop.io.UTF8),156,160,"/**
 * Compares bytes using WritableComparator.
 * @param o The other byte array to compare.
 * @return Comparison result.
 */
",Compare two UTF8s.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,equals,org.apache.hadoop.io.UTF8:equals(java.lang.Object),193,203,"/**
 * Compares this UTF8 object with another.
 * @param o The object to compare to.
 * @return True if equal, false otherwise.
 */
",Returns true iff <code>o</code> is a UTF8 with the same contents.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,compareTo,org.apache.hadoop.io.MD5Hash:compareTo(org.apache.hadoop.io.MD5Hash),241,245,"/**
 * Compares this MD5 hash with another using WritableComparator.
 * @param that the MD5Hash to compare with
 * @return int result of the comparison
 */
",Compares this object with the specified object for order.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,compareTo,org.apache.hadoop.io.BinaryComparable:compareTo(org.apache.hadoop.io.BinaryComparable),50,56,"/**
 * Compares this object with another BinaryComparable object.
 * @param other The other object to compare with.
 * @return Comparison result using WritableComparator.
 */
","* Compare bytes from {#getBytes()}.
   * @see org.apache.hadoop.io.WritableComparator#compareBytes(byte[],int,int,byte[],int,int)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,compareTo,"org.apache.hadoop.io.BinaryComparable:compareTo(byte[],int,int)",66,69,"/**
* Compares two byte arrays using WritableComparator.
* @param other The second byte array to compare.
* @param off Offset in 'other' to start comparison.
* @param len Length of data to compare in 'other'.
*/","* Compare bytes from {#getBytes()} to those provided.
   *
   * @param other other.
   * @param off off.
   * @param len len.
   * @return compareBytes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/CompareUtils.java,compare,"org.apache.hadoop.io.file.tfile.CompareUtils$MemcmpRawComparator:compare(byte[],int,int,byte[],int,int)",89,92,"/**
* Compares two byte arrays using WritableComparator.
* @param b1, s1, l1 First array, start offset, length.
* @param b2, s2, l2 Second array, start offset, length.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,hashCode,org.apache.hadoop.io.UTF8:hashCode(),205,208,"/**
* Compares bytes using WritableComparator.
* @return Comparison result based on byte array.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,hashCode,org.apache.hadoop.io.BinaryComparable:hashCode(),88,91,"/**
* Compares two objects using WritableComparator.
* @return int result of the comparison.
*/","* Return a hash of the bytes returned from {#getBytes()}.
   * @see org.apache.hadoop.io.WritableComparator#hashBytes(byte[],int)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,hashCode,org.apache.hadoop.security.token.Token:hashCode(),402,405,"/**
* Calculates the mask for the identifier using WritableComparator.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readDouble,"org.apache.hadoop.io.WritableComparator:readDouble(byte[],int)",311,313,"/**
* Calculates a double value based on byte array and start index.
*/","* Parse a double from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return double from a byte array.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,setSize,org.apache.hadoop.io.BytesWritable:setSize(int),131,138,"/**
* Sets the size, adjusting if it exceeds the maximum allowed size.
* @param size The desired size to set.
*/
","* Change the size of the buffer. The values in the old range are preserved
   * and any new values are undefined. The capacity is changed if it is 
   * necessary.
   * @param size The new number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MergeSort.java,<init>,org.apache.hadoop.util.MergeSort:<init>(java.util.Comparator),38,40,"/**
 * Constructs a MergeSort with the given comparator.
 * @param comparator Comparator for comparing IntWritable objects.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canRead,org.apache.hadoop.fs.FileUtil:canRead(java.io.File),1412,1423,"/**
* Checks if a file is readable, platform-specific.
* @param f The file to check.
* @return True if readable, false otherwise.
*/
","* Platform independent implementation for {@link File#canRead()}
   * @param f input file
   * @return On Unix, same as {@link File#canRead()}
   *         On Windows, true if process has read access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canWrite,org.apache.hadoop.fs.FileUtil:canWrite(java.io.File),1431,1442,"/**
 * Checks if a file is writable. Uses NativeIO on Windows, f.m1() otherwise.
 */","* Platform independent implementation for {@link File#canWrite()}
   * @param f input file
   * @return On Unix, same as {@link File#canWrite()}
   *         On Windows, true if process has write access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canExecute,org.apache.hadoop.fs.FileUtil:canExecute(java.io.File),1450,1461,"/**
 * Checks if a file has execute permission.
 * @param f The file to check.
 * @return True if executable, false otherwise.
 */
","* Platform independent implementation for {@link File#canExecute()}
   * @param f input file
   * @return On Unix, same as {@link File#canExecute()}
   *         On Windows, true if process has execute access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,assertCodeLoaded,org.apache.hadoop.io.nativeio.NativeIO$POSIX:assertCodeLoaded(),364,368,"/**
 * Throws IOException if NativeIO wasn't loaded successfully.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,getInstance,org.apache.hadoop.io.ReadaheadPool:getInstance(),55,62,"/**
 * Returns the ReadaheadPool instance, creating it if necessary.
 */",* @return Return the singleton instance for the current process.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,verifyCanMlock,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:verifyCanMlock(),300,302,"/**
* Delegates to NativeIO.m1() and returns its boolean result.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.java,getLoadingFailureReason,org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:getLoadingFailureReason(),53,61,"/**
 * Checks if NativeIO is available and OS is UNIX; returns null if both are true.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getMemlockLimit,org.apache.hadoop.io.nativeio.NativeIO:getMemlockLimit(),884,886,"/**
 * Returns m2 if m1 is true, otherwise returns 0.
 */","* Get the maximum number of bytes that can be locked into memory at any
   * given point.
   *
   * @return 0 if no bytes can be locked into memory;
   *         Long.MAX_VALUE if there is no limit;
   *         The number of bytes that can be locked into memory otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,calculateChunkedSums,"org.apache.hadoop.util.DataChecksum:calculateChunkedSums(byte[],int,int,byte[],int)",576,600,"/**
 * Calculates checksums for data, using native or custom methods.
 * @param data Data to checksum.
 * @param dataOffset Offset into data.
 * @param dataLength Length of data.
 * @param sums Checksum storage.
 * @param sumsOffset Offset into sums.
 */","* Implementation of chunked calculation specifically on byte arrays. This
   * is to avoid the copy when dealing with ByteBuffers that have array backing.
   *
   * @param data data.
   * @param dataOffset dataOffset.
   * @param dataLength dataLength.
   * @param sums sums.
   * @param sumsOffset sumsOffset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getCreateForWriteFileOutputStream,"org.apache.hadoop.io.nativeio.NativeIO:getCreateForWriteFileOutputStream(java.io.File,int)",1001,1037,"/**
 * Opens a file for writing, creating it if it doesn't exist.
 * @param f The file to open.
 * @param permissions File permissions.
 * @return A FileOutputStream object.
 */","* @return Create the specified File for write access, ensuring that it does not exist.
   * @param f the file that we want to create
   * @param permissions we want to have on the file (if security is enabled)
   *
   * @throws AlreadyExistsException if the file already exists
   * @throws IOException if any other error occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,run,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:run(),210,232,"/**
 * Performs a readahead operation, potentially using native IO.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,cleanBufferPool,org.apache.hadoop.crypto.CryptoInputStream:cleanBufferPool(),816,821,"/**
* Drains the buffer pool, processing each buffer with CryptoStreamUtils.
*/",Clean direct buffer pool,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,freeBuffers,org.apache.hadoop.crypto.CryptoOutputStream:freeBuffers(),311,314,"/**
* Applies a cryptographic operation to both inBuffer and outBuffer.
*/",Forcibly free the direct buffers.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getFstat,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getFstat(java.io.FileDescriptor),575,596,"/**
* Gets file stat information for a file descriptor.
* @param fd The file descriptor.
* @return Stat object containing file statistics.
*/","* Returns the file stat for a file descriptor.
     *
     * @param fd file descriptor.
     * @return the file descriptor file stat.
     * @throws IOException thrown if there was an IO error while obtaining the file stat.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getStat,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getStat(java.lang.String),606,627,"/**
* Retrieves Stat object for the given path.
* @param path file path to get stat for
* @return Stat object or null if an error occurs
*/","* Return the file stat for a file path.
     *
     * @param path  file path
     * @return  the file stat
     * @throws IOException  thrown if there is an IO error while obtaining the
     * file stat",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,"org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int,int)",55,57,"/**
* Constructs a BoundedByteArrayOutputStream with given capacity and limit.
*/
","* Create a BoundedByteArrayOutputStream with the specified
   * capacity and limit.
   * @param capacity The capacity of the underlying byte array
   * @param limit The maximum limit upto which data can be written",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,<init>,org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet),80,82,"/**
 * Constructs an EnumSetWritable with the given EnumSet.
 * @param value The EnumSet to wrap.
 */
","* Construct a new EnumSetWritable. Argument <tt>value</tt> should not be null
   * or empty.
   * 
   * @param value enumSet value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,write,"org.apache.hadoop.io.OutputBuffer:write(java.io.InputStream,int)",107,109,"/**
* Delegates input stream processing to the buffer.
* @param in Input stream to process.
* @param length Number of bytes to read.
*/","* Writes bytes from a InputStream directly into the buffer.
   * @param in input in.
   * @param length input length.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,nextBytes,org.apache.hadoop.crypto.random.OsSecureRandom:nextBytes(byte[]),97,108,"/**
 * Copies data from byte array to reservoir, filling it up.
 * @param bytes The input byte array to copy from.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,next,org.apache.hadoop.crypto.random.OsSecureRandom:next(int),110,118,"/**
 * Extracts `nbits` from the reservoir, masked with a bitmask.
 * @param nbits Number of bits to extract.
 * @return Extracted bits, masked to `nbits`.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,fromString,org.apache.hadoop.io.DefaultStringifier:fromString(java.lang.String),75,81,"/**
 * Restores an object from a Base64 encoded string.
 * @param str Base64 encoded string to restore from.
 * @return Restored object of type T.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKeyStream,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyStream(),1806,1809,"/**
* Reads key data using keyDataInputStream and returns it.
*/","* Streaming access to the key. Useful for desrializing the key into
         * user objects.
         * 
         * @return The input stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeWritable,"org.apache.hadoop.security.token.Token:decodeWritable(org.apache.hadoop.io.Writable,java.lang.String)",355,366,"/**
 * Decodes a Base64 string and writes it to a Writable object.
 * @param obj Writable object to write to
 * @param newValue Base64 encoded string
 */","* Modify the writable to the value from the newValue.
   * @param obj the object to read into
   * @param newValue the string with the url-safe base64 encoded bytes
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeUncompressedBytes,org.apache.hadoop.io.SequenceFile$CompressedBytes:writeUncompressedBytes(java.io.DataOutputStream),699,715,"/**
* Decompresses data and writes it to the output stream.
* Uses codec and DataInputBuffer for decompression.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compare,"org.apache.hadoop.io.WritableComparator:compare(byte[],int,int,byte[],int,int)",177,192,"/**
 * Processes byte arrays using keys, then calls m3 with them.
 * @param b1, b2 byte arrays to process, s1, s2 their offsets, l1, l2 their lengths
 * @return Result of calling m3 with derived keys.
 */
","Optimization hook.  Override this to make SequenceFile.Sorter's scream.
   *
   * <p>The default implementation reads the data into two {@link
   * WritableComparable}s (using {@link
   * Writable#readFields(DataInput)}, then calls {@link
   * #compare(WritableComparable,WritableComparable)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,36,"/**
 * Constructs a new RSErasureCodec with configuration and options.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,36,"/**
 * Constructs an HHXORErasureCodec with configuration and options.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",32,34,"/**
 * Constructs a DummyErasureCodec with configuration and options.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,37,"/**
 * Initializes a XORErasureCodec with given config and options.
 * Asserts that the schema has only one parity unit.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawCoderFactory,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawCoderFactory(java.lang.String,java.lang.String)",154,161,"/**
 * Retrieves a RawErasureCoderFactory by codec and coder names.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,anyRecoverable,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:anyRecoverable(org.apache.hadoop.io.erasurecode.ECBlockGroup),86,90,"/**
 * Checks if the erased count in the block group is within range.
 * @param blockGroup ECBlockGroup object to check.
 * @return True if erased count is within range, false otherwise.
 */
","* Given a BlockGroup, tell if any of the missing blocks can be recovered,
   * to be called by ECManager
   * @param blockGroup a blockGroup that may contain erased blocks but not sure
   *                   recoverable or not
   * @return true if any erased block recoverable, false otherwise",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getNumErasedBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),132,136,"/**
* Recursively calculates a sum based on block group data.
*/","* Get the number of erased blocks in the block group.
   * @param blockGroup blockGroup.
   * @return number of erased blocks",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getErasedIndexes,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getErasedIndexes(org.apache.hadoop.io.erasurecode.ECBlock[]),159,174,"/**
 * Extracts indices of erased blocks from input.
 * @param inputBlocks ECBlock array to process.
 * @return Array of erased block indices.
 */","* Get indexes of erased blocks from inputBlocks
   * @param inputBlocks inputBlocks.
   * @return indexes of erased blocks from inputBlocks",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,checkInputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkInputBuffers(java.nio.ByteBuffer[]),98,122,"/**
* Validates input buffers, throws exception if invalid or insufficient.
*/","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,checkInputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkInputBuffers(byte[][]),95,115,"/**
 * Validates input buffers and throws exception if invalid or insufficient.
 */","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,<init>,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],int[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",52,65,"/**
 * Initializes erasure decoding step with input blocks, erased indexes, decoders, encoders.
 */","* The constructor with all the necessary info.
   * @param inputBlocks inputBlocks.
   * @param erasedIndexes the indexes of erased blocks in inputBlocks array
   * @param outputBlocks outputBlocks.
   * @param rawDecoder underlying RS decoder for hitchhiker decoding
   * @param rawEncoder underlying XOR encoder for hitchhiker decoding",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingState.java,checkParameters,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingState:checkParameters(java.lang.Object[],int[],java.lang.Object[])",38,54,"/**
 * Validates input arrays and lengths for decoding.
 * @param inputs Input array.
 * @param erasedIndexes Indexes to be erased.
 * @param outputs Output array.
 */
","* Check and validate decoding parameters, throw exception accordingly. The
   * checking assumes it's a MDS code. Other code  can override this.
   * @param inputs input buffers to check
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,<init>,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",48,57,"/**
 * Initializes an HHXORErasureEncodingStep with encoders and block arrays.
 */","* The constructor with all the necessary info.
   *
   * @param inputBlocks inputBlocks.
   * @param outputBlocks outputBlocks.
   * @param rsRawEncoder  underlying RS encoder for hitchhiker encoding
   * @param xorRawEncoder underlying XOR encoder for hitchhiker encoding",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/EncodingState.java,checkParameters,"org.apache.hadoop.io.erasurecode.rawcoder.EncodingState:checkParameters(java.lang.Object[],java.lang.Object[])",36,43,"/**
 * Validates input and output array lengths against encoder sizes.
 */","* Check and validate decoding parameters, throw exception accordingly.
   * @param inputs input buffers to check
   * @param outputs output buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
 * Creates a RawErasureDecoder using the provided options.
 * @param coderOptions ErasureCoderOptions for decoder creation
 * @return A new XORRawDecoder instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),36,45,"/**
 * Creates an ErasureDecodingStep with provided block group.
 * @param blockGroup ECBlockGroup for erasure decoding.
 * @return ErasureDecodingStep object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,39,"/**
 * Creates a RawErasureDecoder using the provided options.
 * @param coderOptions ErasureCoderOptions for decoder configuration
 * @return A RawErasureDecoder instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
 * Initializes the NativeXORRawDecoder with the given options.
 * @param coderOptions ErasureCoderOptions object for configuration.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
 * Initializes the NativeRSRawDecoder with erasure coding options.
 * @param coderOptions ErasureCoderOptions object for configuration.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,getPiggyBackForDecode,"org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBackForDecode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,int,int)",150,201,"/**
 * Calculates piggyback data based on parity index and units.
 * @param inputs Input ByteBuffer array
 * @param outputs Output ByteBuffer array
 * @return ByteBuffer containing piggyback data
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,add,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int[],int[])",371,384,"/**
 * Merges two arrays, prioritizing elements from both up to the shorter length.
 */","* Compute the sum of two polynomials. The index in the array corresponds to
   * the power of the entry. For example p[0] is the constant term of the
   * polynomial p.
   *
   * @param p input polynomial
   * @param q input polynomial
   * @return polynomial represents p+q",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,multiply,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int[],int[])",327,340,"/**
* Combines two int arrays using m2 and recursive m1 calls.
* @param p First int array.
* @param q Second int array.
* @return Combined int array.
*/","* Compute the multiplication of two polynomials. The index in the array
   * corresponds to the power of the entry. For example p[0] is the constant
   * term of the polynomial p.
   *
   * @param p input polynomial
   * @param q input polynomial
   * @return polynomial represents p*q",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,gaussianElimination,org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:gaussianElimination(int[][]),549,588,"/**
 * Performs row operations on a matrix to transform it.
 * Asserts matrix validity; uses helper methods m1, m2, m3.
 */","* Perform Gaussian elimination on the given matrix. This matrix has to be a
   * fat matrix (number of rows &gt; number of columns).
   *
   * @param matrix matrix.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,getPrimitivePower,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:getPrimitivePower(int,int)",38,45,"/**
 * Calculates and returns an array of primitive powers.
 * @param numDataUnits Number of data units.
 * @param numParityUnits Number of parity units.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/DumpUtil.java,dumpChunks,"org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunks(java.lang.String,org.apache.hadoop.io.erasurecode.ECChunk[])",80,87,"/**
 * Processes chunks after printing a header and footer.
 * @param header Header string to print.
 * @param chunks Array of ECChunk objects to process.
 */
","* Print data in hex format in an array of chunks.
   * @param header header.
   * @param chunks chunks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
 * Initializes the XOR raw encoder with specified options.
 * @param coderOptions ErasureCoderOptions object for configuration.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
 * Initializes the NativeRSRawEncoder with given erasure options.
 * @param coderOptions ErasureCoderOptions object for encoder config
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
 * Creates and returns an XORRawEncoder with the given options.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),36,44,"/**
 * Creates an ErasureEncodingStep using the provided block group.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),31,34,"/**
* Creates a RawErasureEncoder using provided options.
* @param coderOptions ErasureCoderOptions object
* @return A RawErasureEncoder instance.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetOutputBuffers,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(java.nio.ByteBuffer[],int)",87,91,"/**
* Processes data in each buffer with m1.
* @param buffers Array of ByteBuffers to process.
* @param dataLen Length of data to be processed.
*/
",* Initialize the output buffers with ZERO bytes.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,toBuffers,org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[]),108,125,"/**
 * Converts ECChunks to ByteBuffer array, applying optional encoding.
 * @param chunks Array of ECChunk objects to convert.
 * @return ByteBuffer array containing the converted data.
 */","* Convert an array of this chunks to an array of ByteBuffers
   * @param chunks chunks to convertToByteArrayState into buffers
   * @return an array of ByteBuffers",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetOutputBuffers,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(byte[][],int[],int)",96,101,"/**
 * Processes data in buffers using m1, offset by offset values.
 */",* Initialize the output buffers with ZERO bytes.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),83,96,"/**
 * Processes encoding state, logs performance advisory, and updates outputs.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),86,99,"/**
 * Processes decoding state, logs performance advisory, and updates outputs.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),57,71,"/**
 * Constructs a RSRawDecoder with provided options.
 * @param coderOptions ErasureCoderOptions object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),42,61,"/**
 * Initializes the RSRawEncoder with erasure coding options.
 * @param coderOptions ErasureCoderOptions object for configuration.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,generateDecodeMatrix,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:generateDecodeMatrix(int[]),143,176,"/**
 * Decodes data units based on erased indexes and matrices.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,getInstance,org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance(),121,123,"/**
* Creates a GaloisField with default size and polynomial.
* @return A GaloisField object.
*/","* Get the object performs Galois field arithmetic with default setting.
   * @return GaloisField.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsR,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsR(long),616,638,"/**
* Reads n bits from the input stream, updating bsLive and bsBuff.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetBit,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetBit(),640,658,"/**
* Reads a bit from the input stream.
* Updates bsLive and bsBuff accordingly.
* @return True if the bit is 1, false otherwise.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,init,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:init(),756,772,"/**
* Initializes data, writes headers, resets CRC, and performs m2.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,endCompression,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endCompression(),833,849,"/**
 * Performs a sequence of operations, likely related to initialization.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues(),951,992,"/**
* Initializes MTF values and calls helper methods for processing.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,mainSort,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainSort(),1738,1901,"/**
 * Performs a complex data processing sequence.
 * Uses various data structures for sorting/mapping.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,"org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",49,51,"/**
 * Creates a CompressorStream with a default buffer size.
 * @param out The output stream to write compressed data.
 * @param compressor The compressor to use for compression.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int,int)",54,58,"/**
 * Creates a BlockCompressorStream.
 * @param out Output stream.
 * @param compressor Compressor.
 * @param bufferSize Buffer size.
 * @param compressionOverhead Overhead for compression.
 */
","* Create a {@link BlockCompressorStream}.
   * 
   * @param out stream
   * @param compressor compressor to be used
   * @param bufferSize size of buffer
   * @param compressionOverhead maximum 'overhead' of the compression 
   *                            algorithm with given bufferSize",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,write,org.apache.hadoop.io.compress.CompressorStream:write(int),115,119,"/**
* Writes a byte to a buffer and calls m1 to process it.
* @param b The byte to write.
* @throws IOException if an I/O error occurs.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",68,72,"/**
 * Constructs a DecompressorStream with default skip buffer size.
 * @param in Input stream to decompress.
 * @param decompressor Decompressor instance.
 * @param bufferSize Buffer size for decompression.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,<init>,org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:<init>(java.io.InputStream),162,166,"/**
 * Constructs a PassthroughDecompressorStream with the given input stream.
 * @param input The input stream to be decompressed.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream),64,66,"/**
 * Creates a BlockDecompressorStream with the given input stream.
 * @param in The input stream to decompress.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,decompress,"org.apache.hadoop.io.compress.DecompressorStream:decompress(byte[],int,int)",108,173,"/**
 * Decompresses data from the input stream.
 * @param b buffer to read from, @param off offset, @param len length.
 * @return Number of bytes decompressed, or -1 if EOF.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,finish,org.apache.hadoop.io.compress.BlockCompressorStream:finish(),136,145,"/**
 * Handles compression: if fails, reports size, compresses, then waits.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>(int),78,85,"/**
 * Creates a ZStandardDecompressor with a specified buffer size.
 * @param bufferSize The size of the direct buffers to allocate.
 */
","* Creates a new decompressor.
   * @param bufferSize bufferSize.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,finalize,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:finalize(),248,251,"/**
* Calls the m1 method.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:reset(),307,311,"/**
* Calls superclass's m1() and sets endOfInput to true.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int,int)",94,103,"/**
 * Initializes a ZStandardCompressor with specified parameters.
 * @param level Compression level
 * @param inputBufferSize Input buffer size
 * @param outputBufferSize Output buffer size
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,decompress,"org.apache.hadoop.io.compress.BlockDecompressorStream:decompress(byte[],int,int)",68,112,"/**
 * Decompresses data from the input byte array.
 * @param b byte array to decompress, off offset, len length
 * @return Number of decompressed bytes or -1 if EOF.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(),102,104,"/**
 * Creates a new Lz4Compressor with the default direct buffer size.
 */
",* Creates a new compressor with the default buffer size.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecClassByName,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClassByName(java.lang.String),274,281,"/**
 * Resolves a CompressionCodec by name.
 * @param codecName Name of the codec to resolve.
 * @return CompressionCodec instance or null if not found.
 */
","* Find the relevant compression codec for the codec's canonical class name
   * or by codec alias and returns its implemetation class.
   * <p>
   * Codec aliases are case insensitive.
   * <p>
   * The code alias is the short class name (without the package name).
   * If the short class name ends with 'Codec', then there are two aliases for
   * the codec, the complete short class name and the short class name without
   * the 'Codec' ending. For example for the 'GzipCodec' codec class name the
   * alias are 'gzip' and 'gzipcodec'.
   *
   * @param codecName the canonical class name of the codec
   * @return the codec class",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getCompressor,"org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration)",149,165,"/**
* Obtains a compressor, either from a pool or creates a new one.
* @param codec Compression codec to use.
* @param conf Hadoop configuration.
* @return A Compressor instance.
*/","* Get a {@link Compressor} for the given {@link CompressionCodec} from the 
   * pool or a new one.
   *
   * @param codec the <code>CompressionCodec</code> for which to get the 
   *              <code>Compressor</code>
   * @param conf the <code>Configuration</code> object which contains confs for creating or reinit the compressor
   * @return <code>Compressor</code> for the given 
   *         <code>CompressionCodec</code> from the pool or a new one",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getDecompressor,org.apache.hadoop.io.compress.CodecPool:getDecompressor(org.apache.hadoop.io.compress.CompressionCodec),180,195,"/**
* Retrieves a Decompressor from a pool or creates a new one.
* @param codec CompressionCodec used to obtain the decompressor
* @return A Decompressor instance
*/","* Get a {@link Decompressor} for the given {@link CompressionCodec} from the
   * pool or a new one.
   *  
   * @param codec the <code>CompressionCodec</code> for which to get the 
   *              <code>Decompressor</code>
   * @return <code>Decompressor</code> for the given 
   *         <code>CompressionCodec</code> the pool or a new one",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,returnCompressor,org.apache.hadoop.io.compress.CodecPool:returnCompressor(org.apache.hadoop.io.compress.Compressor),202,215,"/**
 * Processes a compressor, potentially using a pool and updating counts.
 * @param compressor The compressor object to process.
 */","* Return the {@link Compressor} to the pool.
   * 
   * @param compressor the <code>Compressor</code> to be returned to the pool",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,returnDecompressor,org.apache.hadoop.io.compress.CodecPool:returnDecompressor(org.apache.hadoop.io.compress.Decompressor),223,236,"/**
 * Processes a Decompressor, potentially triggering decompression.
 * @param decompressor The Decompressor object to process.
 */","* Return the {@link Decompressor} to the pool.
   * 
   * @param decompressor the <code>Decompressor</code> to be returned to the 
   *                     pool",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressorType,org.apache.hadoop.io.compress.ZStandardCodec:getCompressorType(),151,155,"/**
* Returns the ZStandardCompressor class. Calls m1() first.
*/","* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of compressor needed by this codec.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getDecompressorType,org.apache.hadoop.io.compress.ZStandardCodec:getDecompressorType(),209,213,"/**
 * Returns the ZStandardDecompressor class. Calls m1() first.
 */","* Get the type of {@link Decompressor} needed by
   * this {@link CompressionCodec}.
   *
   * @return the type of decompressor needed by this codec.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,getCompressorType,org.apache.hadoop.io.compress.DefaultCodec:getCompressorType(),69,72,"/**
* Returns the compressor class using the configuration.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,getDecompressorType,org.apache.hadoop.io.compress.DefaultCodec:getDecompressorType(),95,98,"/**
* Returns the Decompressor class using the configuration.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:<init>(org.apache.hadoop.conf.Configuration),66,68,"/**
 * Initializes a BuiltInGzipCompressor with the given configuration.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,reinit,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:reinit(org.apache.hadoop.conf.Configuration),167,175,"/**
* Initializes state for processing a configuration.
* @param conf Configuration object to use.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>(),122,126,"/**
 * Constructs a GzipZlibCompressor with default compression settings.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>(org.apache.hadoop.conf.Configuration),128,133,"/**
 * Constructs a GzipZlibCompressor with configuration parameters.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(),234,239,"/**
 * Constructs a ZlibCompressor with default compression settings.
 */
","* Creates a new compressor with the default compression level.
   * Compressed data will be generated in ZLIB format.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.conf.Configuration),245,250,"/**
 * Constructs a ZlibCompressor with default header and buffer size.
 * @param conf Configuration object for compression parameters.
 */
","* Creates a new compressor, taking settings from the configuration.
   * @param conf configuration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,reinit,org.apache.hadoop.io.compress.zlib.ZlibCompressor:reinit(org.apache.hadoop.conf.Configuration),283,298,"/**
 * Reinitializes the compressor with a new configuration.
 * @param conf Configuration object for compressor settings.
 */","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's compression level
   * and compression strategy.
   * 
   * @param conf Configuration storing new settings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>(),352,354,"/**
 * Constructs a ZlibDirectDecompressor with default header and window size.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)",356,358,"/**
 * Initializes a ZlibDirectDecompressor with a header and buffer size.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>(),117,119,"/**
 * Default constructor. Uses default header and buffer size.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibDecompressor:<init>(),137,139,"/**
 * Constructs a GzipZlibDecompressor with auto-detect header.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:reset(),365,369,"/**
* Calls superclass's m1() and sets endOfInput to true.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,executeHeaderState,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeHeaderState(),257,358,"/**
* Processes Gzip header fields based on current state and input buffer.
*/","* Parse the gzip header (assuming we're in the appropriate state).
   * In order to deal with degenerate cases (e.g., user buffer is one byte
   * long), we copy (some) header bytes to another buffer.  (Filename,
   * comment, and extra-field bytes are simply skipped.)</p>
   *
   * See http://www.ietf.org/rfc/rfc1952.txt for the gzip spec.  Note that
   * no version of gzip to date (at least through 1.4.0, 2010-01-20) supports
   * the FHCRC header-CRC16 flagbit; instead, the implementation treats it
   * as a multi-file continuation flag (which it also doesn't support). :-(
   * Sun's JDK v6 (1.6) supports the header CRC, however, and so do we.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readCompressedStringArray,org.apache.hadoop.io.WritableUtils:readCompressedStringArray(java.io.DataInput),181,189,"/**
 * Reads an array of strings from the input.
 * @param in DataInput to read from. Returns null if empty.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,write,org.apache.hadoop.io.VIntWritable:write(java.io.DataOutput),54,57,"/**
* Writes the value to the output stream.
* @param out DataOutput stream to write to.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,write,org.apache.hadoop.io.Text:write(java.io.DataOutput),395,399,"/**
 * Writes data to the output stream.
 * @param out DataOutput to write to.
 * @throws IOException if an I/O error occurs.
 */
","* Serialize. Write this object to out length uses zero-compressed encoding.
   *
   * @see Writable#write(DataOutput)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,write,"org.apache.hadoop.io.Text:write(java.io.DataOutput,int)",401,409,"/**
 * Writes data to the output stream, ensuring it's within maxLength.
 * @param out Output stream to write to.
 * @param maxLength Maximum allowed length of data.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,write,org.apache.hadoop.security.token.delegation.DelegationKey:write(java.io.DataOutput),94,104,"/**
 * Writes data to the output stream: keyId, expiryDate, and keyBytes.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,write,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:write(java.io.DataOutput),737,747,"/**
 * Writes data to the output stream, including renewDate, password length/value, and trackingId.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVInt,org.apache.hadoop.io.WritableUtils:readVInt(java.io.DataInput),334,340,"/**
 * Reads a long from the stream and casts to int, throws IOException if out of range.
 */","* Reads a zero-compressed encoded integer from input stream and returns it.
   * @param stream Binary input stream
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer from stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVIntInRange,"org.apache.hadoop.io.WritableUtils:readVIntInRange(java.io.DataInput,int,int)",354,370,"/**
 * Reads an integer from stream, validates range [lower, upper].
 * @param stream Input stream.
 * @param lower Lower bound (inclusive).
 * @param upper Upper bound (inclusive).
 * @return Integer value from stream.
 */","* Reads an integer from the input stream and returns it.
   *
   * This function validates that the integer is between [lower, upper],
   * inclusive.
   *
   * @param stream Binary input stream
   * @param lower input lower.
   * @param upper input upper.
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer from stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,readFields,org.apache.hadoop.io.VLongWritable:readFields(java.io.DataInput),49,52,"/**
* Reads a value from the input stream and assigns it to 'value'.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(java.lang.String),95,97,"/**
 * Constructs a Text object with the given string.
 * @param string The string to be set as the text content.
 */
","* Construct from a string.
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,find,"org.apache.hadoop.io.Text:find(java.lang.String,int)",187,221,"/**
* Finds the index of a sequence in bytes, returns -1 if not found.
*/","* Finds any occurrence of <code>what</code> in the backing
   * buffer, starting as position <code>start</code>. The starting
   * position is measured in bytes and the return value is in
   * terms of byte position in the buffer. The backing buffer is
   * not converted to a string for this operation.
   *
   * @param what input what.
   * @param start input start.
   * @return byte position of the first occurrence of the search
   *         string in the UTF-8 buffer or -1 if not found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,writeString,"org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)",582,588,"/**
 * Writes string length to output and then the string itself.
 * @param out DataOutput to write to
 * @param s String to write
 * @return Length of the written string
 */
","* Write a UTF8 encoded string to out.
   *
   * @param out input out.
   * @param s input s.
   * @throws IOException raised on errors performing I/O.
   * @return a UTF8 encoded string to out.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,writeString,"org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String,int)",598,610,"/**
 * Writes a string to the output, ensuring it's within maxLength.
 * @param out DataOutput to write to
 * @param s String to write
 * @param maxLength Max length of string in bytes
 * @return Length of the written string
 */","* @return Write a UTF8 encoded string with a maximum size to out.
   *
   * @param out input out.
   * @param s input s.
   * @param maxLength input maxLength.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(byte[]),246,254,"/**
 * Processes a byte array. Handles empty arrays separately.
 */","* Set to a utf8 byte array. If the length of <code>utf8</code> is
   * <em>zero</em>, actually clear {@link #bytes} and any existing
   * data is lost.
   *
   * @param utf8 input utf8.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(org.apache.hadoop.io.Text),260,263,"/**
* Calls m3 with values from 'other' object.
* Updates textLength with value from 'other'.
*/","* Copy a text.
   * @param other other.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readDefaultLine,"org.apache.hadoop.util.LineReader:readDefaultLine(org.apache.hadoop.io.Text,int,int)",197,263,"/**
 * Reads text until newline or maxBytes, appending to a buffer.
 * @param str Text buffer to append to
 * @param maxLineLength Max chars per line
 * @param maxBytesToConsume Max bytes to consume
 * @return Number of bytes consumed
 */","* Read a line terminated by one of CR, LF, or CRLF.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readCustomLine,"org.apache.hadoop.util.LineReader:readCustomLine(org.apache.hadoop.io.Text,int,int)",268,371,"/**
* Extracts data from a Text stream until a delimiter is found.
* @param str Text object to read from, maxLineLength, maxBytesToConsume
* @return Number of bytes consumed from the stream.
*/",* Read a line terminated by a custom delimiter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,toString,org.apache.hadoop.io.Text:toString(),337,344,"/**
 * Calls m1 to process bytes, handling CharacterCodingException.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,<init>,org.apache.hadoop.io.SortedMapWritable:<init>(),45,48,"/**
 * Constructs a new SortedMapWritable.
 * Initializes the internal TreeMap.
 */",default constructor.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,<init>,org.apache.hadoop.io.MapWritable:<init>(),43,46,"/**
 * Constructs a new MapWritable with an internal HashMap.
 */",Default constructor.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,write,org.apache.hadoop.io.SortedMapWritable:write(java.io.DataOutput),182,198,"/**
 * Writes data to the output stream, including instance data and entries.
 * @param out Output stream to write data to.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,write,org.apache.hadoop.io.MapWritable:write(java.io.DataOutput),147,163,"/**
 * Writes data to the output stream, including instance data and entries.
 * @param out DataOutput stream to write to.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getDeserializer,"org.apache.hadoop.io.SequenceFile$Reader:getDeserializer(org.apache.hadoop.io.serializer.SerializationFactory,java.lang.Class)",2163,2166,"/**
 * Creates a Deserializer using the given SerializationFactory and class.
 * @param sf SerializationFactory to use
 * @param c Class to deserialize
 * @return Deserializer instance
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,<init>,org.apache.hadoop.util.bloom.Key:<init>(byte[]),87,89,"/**
 * Constructs a Key from a byte array.
 * Uses a default precision of 1.0.
 */
","* Constructor.
   * <p>
   * Builds a key with a default weight.
   * @param value The byte value of <i>this</i> key.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compression,org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType),1052,1054,"/**
 * Creates a CompressionOption for the given CompressionType.
 * @param value The CompressionType to create an option for.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,compression,"org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",302,306,"/**
* Returns a SequenceFile.Writer.Option with given compression type/codec.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/DeserializerComparator.java,<init>,org.apache.hadoop.io.serializer.DeserializerComparator:<init>(org.apache.hadoop.io.serializer.Deserializer),52,57,"/**
 * Initializes the comparator with a deserializer and opens it.
 * @param deserializer The deserializer to use.
 * @throws IOException If an I/O error occurs during deserialization.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/DeserializerComparator.java,compare,"org.apache.hadoop.io.serializer.DeserializerComparator:compare(byte[],int,int,byte[],int,int)",59,73,"/**
 * Processes byte arrays, deserializes keys, and calls m3.
 * @param b1, b2 byte arrays to process; s1, s2 their offsets; l1, l2 their lengths.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,readFields,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:readFields(java.io.DataInput),99,104,"/**
* Reads CRC bytes, CRC per block, and MD5 hash from input.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(byte[]),118,120,"/**
 * Calculates MD5 hash of the input byte array.
 * @param data The byte array to hash.
 * @return MD5Hash object representing the hash.
 */
","* Construct a hash value for a byte array.
   * @param data data.
   * @return MD5Hash.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(org.apache.hadoop.io.UTF8),195,197,"/**
* Calculates MD5 hash of UTF8 string.
* @param utf8 UTF8 object to hash.
* @return MD5Hash object representing the hash.
*/","* Construct a hash value for a String.
   * @param utf8 utf8.
   * @return MD5Hash.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,<init>,org.apache.hadoop.io.MD5Hash:<init>(java.lang.String),61,63,"/**
 * Constructs an MD5Hash object from a hexadecimal string.
 * @param hex The hexadecimal representation of the hash.
 */
","* Constructs an MD5Hash from a hex string.
   * @param hex input hex.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,stream,org.apache.hadoop.io.SequenceFile$Writer:stream(org.apache.hadoop.fs.FSDataOutputStream),1020,1022,"/**
 * Creates a StreamOption from an FSDataOutputStream.
 * @param value The FSDataOutputStream to wrap.
 * @return A StreamOption object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendIfExists,org.apache.hadoop.io.SequenceFile$Writer:appendIfExists(boolean),1028,1030,"/**
 * Creates an AppendIfExistsOption based on the given boolean value.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,file,org.apache.hadoop.io.SequenceFile$Writer:file(org.apache.hadoop.fs.Path),994,996,"/**
 * Creates a FileOption from the given Path.
 * @param value The Path to wrap in a FileOption.
 * @return A FileOption representing the Path.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:shouldRetry(java.lang.Exception,int,int,boolean)",421,436,"/**
 * Determines retry action based on exception and retry count.
 * @param e Exception occurred, curRetry retry count, etc.
 * @return RetryAction object with retry decision and sleep time.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryForeverWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryForeverWithFixedSleep(long,java.util.concurrent.TimeUnit)",82,86,"/**
 * Creates a RetryPolicy with max attempts and fixed sleep.
 * @param sleepTime Sleep time between retries.
 * @param timeUnit Time unit of the sleep time.
 */
","* <p>
   * Keep trying forever with a fixed time between attempts.
   * </p>
   *
   * @param sleepTime sleepTime.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumCountWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithFixedSleep(int,long,java.util.concurrent.TimeUnit)",99,101,"/**
 * Creates a RetryPolicy with max retries and fixed sleep time.
 * @param maxRetries Maximum number of retries.
 * @param sleepTime Sleep time between retries.
 * @param timeUnit Time unit for sleepTime.
 */
","* <p>
   * Keep trying a limited number of times, waiting a fixed time between attempts,
   * and then fail by re-throwing the exception.
   * </p>
   *
   * @param maxRetries maxRetries.
   * @param sleepTime sleepTime.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:<init>(long,long,java.util.concurrent.TimeUnit)",346,351,"/**
 * Initializes a retry strategy with max time & sleep interval.
 * @param maxTime Maximum retry time.
 * @param sleepTime Sleep time between retries.
 * @param timeUnit Time unit for maxTime and sleepTime.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,exponentialBackoffRetry,"org.apache.hadoop.io.retry.RetryPolicies:exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)",148,151,"/**
 * Creates an ExponentialBackoffRetry policy with given params.
 * @param maxRetries Max retry attempts.
 * @param sleepTime Initial sleep time.
 * @param timeUnit Time unit for sleepTime.
 */
","* <p>
   * Keep trying a limited number of times, waiting a growing amount of time between attempts,
   * and then fail by re-throwing the exception.
   * The time between attempts is <code>sleepTime</code> mutliplied by a random
   * number in the range of [0, 2 to the number of retries)
   * </p>
   *
   *
   * @param timeUnit timeUnit.
   * @param maxRetries maxRetries.
   * @param sleepTime sleepTime.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumCountWithProportionalSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)",130,132,"/**
 * Creates a RetryPolicy with proportional sleep.
 * @param maxRetries Max retries allowed.
 * @param sleepTime Initial sleep time.
 * @param timeUnit Time unit for sleepTime.
 */
","* <p>
   * Keep trying a limited number of times, waiting a growing amount of time between attempts,
   * and then fail by re-throwing the exception.
   * The time between attempts is <code>sleepTime</code> mutliplied by the number of tries so far.
   * </p>
   *
   * @param sleepTime sleepTime.
   * @param maxRetries maxRetries.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)",210,215,"/**
 * Creates a FailoverOnNetworkExceptionRetry policy.
 * @param fallbackPolicy Fallback policy to use on failure.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,newAsyncCall,"org.apache.hadoop.io.retry.AsyncCallHandler:newAsyncCall(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler)",322,327,"/**
 * Creates a new AsyncCall instance.
 * @param method The method to be invoked asynchronously.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,newRetryInfo,"org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:newRetryInfo(org.apache.hadoop.io.retry.RetryPolicy,java.lang.Exception,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,boolean,long)",273,302,"/**
 * Determines retry info based on policy, exception, and counters.
 * Returns RetryInfo object with delay, action, failover count, and exception.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,tryStop,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStop(org.apache.hadoop.util.Daemon),186,190,"/**
* Processes a Daemon if queue has waited long enough.
* @param d The Daemon object to process.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,read,org.apache.hadoop.security.Groups$TimerToTickerAdapter:read(),292,296,"/**
 * Calculates a value based on timer.m1() and converts to milliseconds.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,unlock,org.apache.hadoop.util.InstrumentedWriteLock:unlock(),59,69,"/**
 * Executes a process, potentially reporting write lock timings.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedWriteLock:startLockTiming(),74,79,"/**
 * Acquires a write lock and records the timestamp if successful.
 */",* Starts timing for the instrumented write lock.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long,org.apache.hadoop.util.Timer)",87,99,"/**
 * Constructs an InstrumentedLock with provided dependencies.
 * @param name Lock name, logger, lock, timing params, clock.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedLock:startLockTiming(),177,179,"/**
 * Records the timestamp when the lock is acquired.
 */",* Starts timing for the instrumented lock.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,unlock,org.apache.hadoop.util.InstrumentedReadLock:unlock(),65,75,"/**
 * Executes a process, potentially reporting lock metrics.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedReadLock:startLockTiming(),81,86,"/**
 * Updates timestamp if read lock is held.
 * @return void
 */","* Starts timing for the instrumented read lock.
   * It records the time to ThreadLocal.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",327,330,"/**
 * Constructs a RetryInvocationHandler with default retry policies.
 * @param proxyProvider FailoverProxyProvider for retrying operations.
 * @param retryPolicy RetryPolicy to use for retries.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,java.util.Map,org.apache.hadoop.io.retry.RetryPolicy)",100,110,"/**
 * Creates a dynamic proxy implementing the given interface.
 * @param iface Interface to implement.
 * @return Proxy object.
 */
","* Create a proxy for an interface of implementations of that interface using
   * the given {@link FailoverProxyProvider} and the a set of retry policies
   * specified by method name. If no retry policy is defined for a method then a
   * default of {@link RetryPolicies#TRY_ONCE_THEN_FAIL} is used.
   * 
   * @param iface the interface that the retry will implement
   * @param proxyProvider provides implementation instances whose methods should be retried
   * @param methodNameToPolicyMap map of method names to retry policies
   * @param defaultPolicy defaultPolicy.
   * @param <T> T.
   * @return the retry proxy",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,failover,"org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:failover(long,java.lang.reflect.Method,int)",217,229,"/**
 * Handles failover logic, logs errors, and updates proxy info.
 * @param expectedFailoverCount Expected failover count.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,log,"org.apache.hadoop.io.retry.RetryInvocationHandler:log(java.lang.reflect.Method,boolean,int,int,long,java.lang.Exception)",398,430,"/**
 * Logs invocation failure details, including failover/retry info.
 * @param method Method being invoked.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,invokeMethod,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])",48,65,"/**
* Processes method call, retries if retry count is low, otherwise returns result.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,hashCode,org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:hashCode(),150,153,"/**
* Delegates the call to m1() to multipleLinearRandomRetry.
* Returns the value returned by the delegate method.
*/
","* Similarly, remoteExceptionToRetry is ignored as part of hashCode since it
     * does not affect connection failure handling.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,equals,org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:equals(java.lang.Object),135,144,"/**
 * Checks if this policy is equivalent to another.
 * @param obj The object to compare with.
 * @return True if equivalent, false otherwise.
 */
","* remoteExceptionToRetry is ignored as part of equals since it does not
     * affect connection failure handling.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:shouldRetry(java.lang.Exception,int,int,boolean)",283,291,"/**
 * Determines retry action based on exception, retries, and idempotency.
 * @param e Exception occurred. @param retries Number of retries.
 * @return RetryAction object with retry decision and delay.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,mayThrow,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:mayThrow(java.util.List),314,324,"/**
 * Throws an IOException if replication is below the minimum.
 * @param ioExceptions List of IOExceptions to check.
 * @throws IOException if replication is insufficient.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:getFileStatus(org.apache.hadoop.fs.Path),875,915,"/**
* Retrieves FileStatus, finds most recent if flag is set, throws IOExceptions.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MultipleIOException.java,build,org.apache.hadoop.io.MultipleIOException$Builder:build(),83,85,"/**
* Delegates exception handling to m1.
* @return IOException - exception thrown by m1
*/","* @return null if nothing is added to this builder;
     *         otherwise, return an {@link IOException}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.lang.String),878,883,"/**
 * Constructs a DataIndex with a default compression algorithm.
 * @param defaultCompressionAlgorithmName Name of compression algorithm.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getSupportedCompressionAlgorithms,org.apache.hadoop.io.file.tfile.TFile:getSupportedCompressionAlgorithms(),198,200,"/**
* Delegates to Compression.m1() and returns its result.
*/","* Get names of supported compression algorithms. The names are acceptable by
   * TFile.Writer.
   * 
   * @return Array of strings, each represents a supported compression
   *         algorithm. Currently, the following compression algorithms are
   *         supported.
   *         <ul>
   *         <li>""none"" - No compression.
   *         <li>""lzo"" - LZO compression.
   *         <li>""gz"" - GZIP compression.
   *         </ul>",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressionName(),576,578,"/**
* Delegates to rBlkState's m1() method and returns the result.
*/","* Get the name of the compression algorithm used to compress the block.
       * 
       * @return name of the compression algorithm.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,register,"org.apache.hadoop.io.file.tfile.BCFile$Writer$MetaBlockRegister:register(long,long,long)",451,455,"/**
 * Adds a meta index entry with specified region and compression.
 * @param raw raw data size, begin/end define the region.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,readString,org.apache.hadoop.io.file.tfile.Utils:readString(java.io.DataInput),276,282,"/**
 * Reads a string from the input stream.
 * @param in DataInput stream to read from
 * @return String read from the stream, or null if error.
 */
","* Read a String as a VInt n, followed by n Bytes in Text format.
   * 
   * @param in
   *          The input stream.
   * @return The string
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:<init>(java.io.DataInput),2302,2307,"/**
 * Constructs a TFileIndexEntry from a DataInput stream.
 * @param in Input stream containing the entry data.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,readLength,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:readLength(),102,109,"/**
* Reads data, adjusts 'remain', and sets 'lastChunk' flag.
*/","* Reading the length of next chunk.
     * 
     * @throws java.io.IOException
     *           when no more data is available.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.lang.String),2051,2057,"/**
 * Constructs a TFileMeta object with the given comparator.
 * @param comparator Comparator string, defaults to empty if null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,makeComparator,org.apache.hadoop.io.file.tfile.TFile:makeComparator(java.lang.String),176,178,"/**
* Returns a Comparator<RawComparable> using TFileMeta.m1.
* @param name Used by TFileMeta.m1 to create the comparator.
*/
","* Make a raw comparator from a string name.
   * 
   * @param name
   *          Comparator name
   * @return A RawComparable comparator.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,writeChunk,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeChunk(byte[],int,int,boolean)",253,266,"/**
 * Writes a chunk of data to the output stream, handling last chunk.
 * @param chunk Data to write.
 * @param offset Starting offset in the chunk.
 * @param len Length of data to write.
 * @param last True if this is the last chunk.
 */","* Write out a chunk.
     * 
     * @param chunk
     *          The chunk buffer.
     * @param offset
     *          Offset to chunk buffer for the beginning of chunk.
     * @param len
     * @param last
     *          Is this the last call to flushBuffer?",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,writeBufData,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeBufData(byte[],int,int)",279,287,"/**
* Writes data to the output stream, adjusting for buffer size.
* @param data Data to write.
* @param offset Starting offset in data.
* @param len Number of bytes to write.
*/","* Write out a chunk that is a concatenation of the internal buffer plus
     * user supplied data. This will never be the last block.
     * 
     * @param data
     *          User supplied data buffer.
     * @param offset
     *          Offset to user data buffer.
     * @param len
     *          User data buffer size.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:write(java.io.DataOutput),2335,2339,"/**
 * Writes key and kvEntries to the output stream.
 * @param out DataOutput to write to.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,<init>,"org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:<init>(java.io.DataOutputStream,int)",377,382,"/**
 * Initializes the encoder with an output stream and chunk size.
 * @param out Output stream for writing encoded data.
 * @param size Chunk size to be encoded.
 */
","* Constructor.
     * 
     * @param out
     *          the underlying output stream.
     * @param size
     *          The total # of bytes to be written as a single chunk.
     * @throws java.io.IOException
     *           if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getEntryComparator,org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryComparator(),925,944,"/**
 * Creates a comparator for Scanner.Entry objects based on m4.
 * Throws RuntimeException if entries are not comparable.
 */","* Get a Comparator object to compare Entries. It is useful when you want
     * stores the entries in a collection (such as PriorityQueue) and perform
     * sorting or comparison among entries based on the keys without copying out
     * the key.
     * 
     * @return An Entry Comparator..",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareKeys,"org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(byte[],int,int,byte[],int,int)",1007,1012,"/**
 * Compares two byte arrays using a comparator.
 * @param a, b arrays to compare; o1, o2 offsets; l1, l2 lengths.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareKeys,"org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1014,1019,"/**
* Compares two RawComparable objects using a comparator.
* Throws exception if comparison is not supported.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,setFirstKey,"org.apache.hadoop.io.file.tfile.TFile$TFileIndex:setFirstKey(byte[],int,int)",2250,2253,"/**
* Copies a byte array segment into a new ByteArray.
* @param key Source byte array.
* @param offset Start offset in the source array.
* @param length Number of bytes to copy.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLastKey,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLastKey(),2255,2260,"/**
* Returns a RawComparable object based on the index value.
* Returns null if index is 0, otherwise creates a ByteArray.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,atEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:atEnd(),1588,1590,"/**
* Checks if currentLocation is before endLocation.
* @return True if currentLocation is before endLocation.
*/","* Is cursor at the end location?
       * 
       * @return true if the cursor is at the end location.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationNear(long),1031,1035,"/**
 * Finds the location of a block given an offset.
 * @param offset The offset into the data.
 * @return Location object or end if block not found.
 */
","* Get the location pointing to the beginning of the first key-value pair in
     * a compressed block whose byte offset in the TFile is greater than or
     * equal to the specified offset.
     * 
     * @param offset
     *          the user supplied offset.
     * @return the location to the corresponding entry; or end() if no such
     *         entry exists.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,clone,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:clone(),759,762,"/**
 * Creates a new Location object using blockIndex and recordIndex.
 */",* @see java.lang.Object#clone(),,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationByRecordNum,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLocationByRecordNum(long),2238,2242,"/**
 * Calculates record location based on recNum.
 * @param recNum The record number.
 * @return Reader.Location object representing the location.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),713,715,"/**
* Constructor to create a new Location object from another.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[]),1840,1842,"/**
 * Calls overloaded method with offset 0.
 * @param buf input byte array
 * @throws IOException if I/O error occurs
 */
","* Copy value into user-supplied buffer. User supplied buffer must be
         * large enough to hold the whole value. The value part of the key-value
         * pair pointed by the current cursor is not cached and can only be
         * examined once. Calling any of the following functions more than once
         * without moving the cursor will result in exception:
         * {@link #getValue(byte[])}, {@link #getValue(byte[], int)},
         * {@link #getValueStream}.
         *
         * @param buf buf.
         * @return the length of the value. Does not require
         *         isValueLengthKnown() to be true.
         * @throws IOException raised on errors performing I/O.
         *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,set,org.apache.hadoop.io.UTF8:set(java.lang.String),96,118,"/**
* Truncates string if too long, converts to bytes, and writes to buffer.
*/","* Set to contain the contents of a string.
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,toByteArray,org.apache.hadoop.io.WritableUtils:toByteArray(org.apache.hadoop.io.Writable[]),461,472,"/**
 * Converts Writable objects to a byte array.
 * @param writables Array of Writable objects to convert.
 * @return Byte array representation of the writables.
 */","* Convert writables to a byte array.
   * @param writables input writables.
   * @return ByteArray.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,getBytes,org.apache.hadoop.io.UTF8:getBytes(java.lang.String),238,249,"/**
 * Converts a string to a byte array using a specific encoding.
 * @param string The input string to convert.
 * @return Byte array representing the encoded string.
 */","* @return Convert a string to a UTF-8 encoded byte array.
   * @see String#getBytes(String)
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeBuffer,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:writeBuffer(org.apache.hadoop.io.DataOutputBuffer),1623,1635,"/**
 * Deflates data from buffer to output stream.
 * Uses deflateFilter, buffer, and deflateOut for processing.
 */",Workhorse to check and write out compressed data/lengths,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,byteArrayForBloomKey,org.apache.hadoop.io.BloomMapFile:byteArrayForBloomKey(org.apache.hadoop.io.DataOutputBuffer),71,79,"/**
 * Extracts data from a buffer, resizing if necessary.
 * @param buf DataOutputBuffer to extract data from.
 * @return Byte array containing extracted data.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,lessThan,"org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:lessThan(java.lang.Object,java.lang.Object)",3537,3548,"/**
 * Compares two segment descriptors based on a comparator.
 * @param a, b SegmentDescriptor objects to compare.
 * @return True if 'a' is less than 'b' according to comparator.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,copy,org.apache.hadoop.io.AbstractMapWritable:copy(org.apache.hadoop.io.Writable),125,142,"/**
* Copies data from the provided Writable object.
* @param other The Writable object to copy from.
*/","* Used by child copy constructors.
   * @param other other.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$RecordCompressWriter:append(java.lang.Object,java.lang.Object)",1545,1575,"/**
 * Writes a key-value pair to the output stream with type validation.
 * @param key The key object.
 * @param val The value object.
 */",Append a key/value pair.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,toString,org.apache.hadoop.io.DefaultStringifier:toString(java.lang.Object),83,90,"/**
 * Serializes object to Base64 string.
 * @param obj Object to serialize.
 * @return Base64 encoded string representation.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,checkKey,org.apache.hadoop.io.MapFile$Writer:checkKey(org.apache.hadoop.io.WritableComparable),418,429,"/**
 * Writes a key to the output buffer, ensuring sorted order.
 * @param key The key to write; must be in sorted order.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/TokenIdentifier.java,getBytes,org.apache.hadoop.security.token.TokenIdentifier:getBytes(),60,68,"/**
 * Generates a byte array by writing data to a buffer.
 * Uses m1 to write data; returns the resulting byte array.
 */","* Get the bytes for the token identifier
   * @return the bytes of the identifier",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,encodeWritable,org.apache.hadoop.security.token.Token:encodeWritable(org.apache.hadoop.io.Writable),340,347,"/**
 * Masks object data to a Base64 string.
 * @param obj Writable object to be masked.
 * @return Base64 encoded string representation of the object.
 */","* Generate a string with the url-quoted base64 encoded serialized form
   * of the Writable.
   * @param obj the object to serialize
   * @return the encoded string
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,moveData,org.apache.hadoop.util.ReflectionUtils$CopyInCopyOutBuffer:moveData(),314,316,"/**
* Calls m3 on inBuffer with results from outBuffer's m1 and m2.
*/",* Move the data from the output buffer to the input buffer.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNSDomainNameResolver.java,getAllResolvedHostnameByDomainName,"org.apache.hadoop.net.DNSDomainNameResolver:getAllResolvedHostnameByDomainName(java.lang.String,boolean)",64,80,"/**
 * Resolves host addresses for a domain.
 * @param domainName Domain name to resolve.
 * @param useFQDN Whether to use fully qualified domain name.
 * @return Array of host addresses.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDistanceByPath,"org.apache.hadoop.net.NetworkTopology:getDistanceByPath(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",376,400,"/**
 * Calculates a distance between two nodes based on their paths.
 * @param node1 The first node.
 * @param node2 The second node.
 * @return Distance value.
 */
","Return the distance between two nodes by comparing their network paths
   * without checking if they belong to the same ancestor node by reference.
   * It is assumed that the distance from one node to its parent is 1
   * The distance between two nodes is calculated by summing up their distances
   * to their closest common ancestor.
   * @param node1 one node
   * @param node2 another node
   * @return the distance between node1 and node2",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:equals(java.lang.Object),108,112,"/**
* Delegates the call to the superclass's m1 method.
* @param o The object passed to the super method.
* @return The result of the superclass's m1 method.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,equals,org.apache.hadoop.net.InnerNodeImpl:equals(java.lang.Object),316,319,"/**
 * Delegates the call to the superclass's m1 method.
 * @param to The parameter passed to the superclass's m1.
 * @return The result of the superclass's m1 method.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:hashCode(),114,118,"/**
* Calls the m1 method of the superclass and returns its result.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,hashCode,org.apache.hadoop.net.InnerNodeImpl:hashCode(),311,314,"/**
* Calls the parent class's m1() method and returns its result.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getNodeForNetworkLocation,org.apache.hadoop.net.NetworkTopology:getNodeForNetworkLocation(org.apache.hadoop.net.Node),193,195,"/**
* Delegates to m2 with the result of node.m1().
*/","* Return a reference to the node given its string representation.
   * Default implementation delegates to {@link #getNode(String)}.
   * 
   * <p>To be overridden in subclasses for specific NetworkTopology 
   * implementations, as alternative to overriding the full {@link #add(Node)}
   *  method.
   * 
   * @param node The string representation of this node's network location is
   * used to retrieve a Node object. 
   * @return a reference to the node; null if the node is not in the tree
   * 
   * @see #add(Node)
   * @see #getNode(String)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getLeaves,org.apache.hadoop.net.NetworkTopology:getLeaves(java.lang.String),643,655,"/**
 * Collects leaf nodes within a given scope.
 * @param scope defines the scope to search within
 * @return List of leaf Node objects found in scope
 */
","return leaves in <i>scope</i>
   * @param scope a path string
   * @return leaves nodes under specific scope",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,countNumOfAvailableNodes,"org.apache.hadoop.net.NetworkTopology:countNumOfAvailableNodes(java.lang.String,java.util.Collection)",664,711,"/**
 * Calculates a count based on scope, excluded nodes, and cluster size.
 */","return the number of leaves in <i>scope</i> but not in <i>excludedNodes</i>
   * if scope starts with ~, return the number of nodes that are not
   * in <i>scope</i> and <i>excludedNodes</i>; 
   * @param scope a path string that may start with ~
   * @param excludedNodes a list of nodes
   * @return number of available nodes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,interRemoveNodeWithEmptyRack,org.apache.hadoop.net.NetworkTopology:interRemoveNodeWithEmptyRack(org.apache.hadoop.net.Node),1104,1122,"/**
* Processes a node, updating rack membership and triggering updates.
*/","* Internal function for update empty rack number
   * for remove or decommission a node.
   * @param node node to be removed; can be null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer,boolean)",970,1000,"/**
* Sorts nodes based on weight, then applies secondary sort.
* @param nodes Array to store sorted nodes.
* @param activeLen Number of active nodes to sort.
*/","* Sort nodes array by network distance to <i>reader</i>.
   * <p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   * And it helps choose node with more fast storage type.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param nonDataNodeReader True if the reader is not a datanode",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,recommissionNode,org.apache.hadoop.net.NetworkTopology:recommissionNode(org.apache.hadoop.net.Node),1040,1055,"/**
 * Decommissions a node, ensuring it's not an inner node.
 * @param node The node to decommission.
 */","* Update empty rack number when add a node like recommission.
   * @param node node to be added; can be null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream$Reader:<init>(java.nio.channels.ReadableByteChannel,long)",50,53,"/**
 * Constructs a Reader from a ReadableByteChannel with a timeout.
 * @param channel Channel to read from.
 * @param timeout Timeout value in seconds.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream$Writer:<init>(java.nio.channels.WritableByteChannel,long)",55,58,"/**
 * Initializes a Writer with a channel and timeout.
 * @param channel WritableByteChannel to write to.
 * @param timeout Timeout in milliseconds.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,write,org.apache.hadoop.net.SocketOutputStream:write(int),101,109,"/**
 * Writes a single byte to the output stream.
 * @param b The byte to write.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,transferToFully,"org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int)",264,267,"/**
* Overloads m1 with default buffer values.
* @param fileCh FileChannel to read from.
* @param position Start position.
* @param count Number of bytes to read.
*/","* Call
   * {@link #transferToFully(FileChannel, long, int, LongWritable, LongWritable)
   * }
   * with null <code>waitForWritableTime</code> and <code>transferToTime</code>.
   *
   * @param fileCh input fileCh.
   * @param position input position.
   * @param count input count.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/CachedDNSToSwitchMapping.java,resolve,org.apache.hadoop.net.CachedDNSToSwitchMapping:resolve(java.util.List),106,125,"/**
 * Processes a list of hostnames, resolving and caching them.
 * @param names List of hostnames to process.
 * @return Processed list of hostnames.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,wrapException,"org.apache.hadoop.net.NetUtils:wrapException(java.lang.String,int,java.lang.String,int,java.io.IOException)",850,949,"/**
* Wraps an IOException with a more descriptive error message.
* @param exception The original IOException to wrap.
*/","* Take an IOException , the local host port and remote host port details and
   * return an IOException with the input exception as the cause and also
   * include the host details. The new exception provides the stack trace of the
   * place where the exception is thrown and some extra diagnostics information.
   * If the exception is of type BindException, ConnectException,
   * UnknownHostException, SocketTimeoutException or has a String constructor,
   * return a new one of the same type; Otherwise return an IOException.
   *
   * @param destHost target host (nullable)
   * @param destPort target port
   * @param localHost local host (nullable)
   * @param localPort local port
   * @param exception the caught exception.
   * @return an exception to throw",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,<init>,org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:<init>(),144,144,"/**
 * Default constructor for RawScriptBasedMappingWithDependency.
 */","* Constructor. The mapping is not ready to use until
     * {@link #setConf(Configuration)} has been called",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,<init>,org.apache.hadoop.net.TableMapping:<init>(),63,65,"/**
 * Constructs a TableMapping with a RawTableMapping as its delegate.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping),95,97,"/**
 * Initializes a ScriptBasedMapping with a DNSToSwitchMapping.
 * @param rawMap The raw mapping data.
 */
","* Create an instance from the given raw mapping
   * @param rawMap raw DNSTOSwithMapping",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,<init>,org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String),48,50,"/**
 * Constructs an InnerNodeImpl with the given path.
 * @param path The path for this node.
 */
","* Construct an InnerNode from a path-like string.
   * @param path input path.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.ChRootedFileSystem)",99,102,"/**
 * Constructs a NflyNode with hostname, rackname, and file system.
 * @param hostName Hostname of the node.
 * @param rackName Rack name of the node.
 * @param fs The ChRootedFileSystem for the node.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,<init>,"org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.InnerNode,int)",60,63,"/**
 * Constructs an InnerNodeImpl with name, location, parent, and level.
 */","* Construct an InnerNode
   * from its name, its network location, its parent, and its level.
   * @param name input name.
   * @param location input location.
   * @param parent input parent.
   * @param level input level.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,kick,org.apache.hadoop.net.unix.DomainSocketWatcher:kick(),359,374,"/**
 * Sends a notification via socket, handling potential IO errors.
 */",* Wake up the DomainSocketWatcher thread.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,handle,org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler:handle(org.apache.hadoop.net.unix.DomainSocket),103,130,"/**
 * Attempts to read from a domain socket. Returns true on error.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,bindAndListen,org.apache.hadoop.net.unix.DomainSocket:bindAndListen(java.lang.String),191,200,"/**
 * Creates a DomainSocket connected to the specified path.
 * @param path Socket path.
 * @throws IOException if an I/O error occurs.
 */
","* Create a new DomainSocket listening on the given path.
   *
   * @param path         The path to bind and listen on.
   * @return             The new DomainSocket.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,socketpair,org.apache.hadoop.net.unix.DomainSocket:socketpair(),210,216,"/**
 * Creates and returns a DomainSocket array from file descriptors.
 */","* Create a pair of UNIX domain sockets which are connected to each other
   * by calling socketpair(2).
   *
   * @return                An array of two UNIX domain sockets connected to
   *                        each other.
   * @throws IOException    on error.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,connect,org.apache.hadoop.net.unix.DomainSocket:connect(java.lang.String),255,261,"/**
 * Creates a DomainSocket using the given path.
 * @param path socket path
 * @return DomainSocket object
 */
","* Create a new DomainSocket connected to the given path.
   *
   * @param path              The path to connect to.
   * @throws IOException      If there was an I/O error performing the connect.
   *
   * @return                  The new DomainSocket.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,sendCallbackAndRemove,"org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallbackAndRemove(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)",435,440,"/**
 * Processes entries and calls m2 if m1 returns true.
 * @param caller Caller identifier, entries, fdSet, fd
 */
","* Send callback, and if the domain socket was closed as a result of
   * processing, then also remove the entry for the file descriptor.
   *
   * @param caller reason for call
   * @param entries mapping of file descriptor to entry
   * @param fdSet set of file descriptors
   * @param fd file descriptor",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,isOpen,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:isOpen(),592,595,"/**
* Delegates m1() call to the enclosing DomainSocket instance.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:close(),597,600,"/**
 * Delegates the m1 call to the parent DomainSocket.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:close(),558,561,"/**
* Delegates m1() call to the parent DomainSocket class.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,close,org.apache.hadoop.net.unix.DomainSocketWatcher:close(),266,284,"/**
 * Closes the resource, notifies the watcher thread, and releases the lock.
 */","* Close the DomainSocketWatcher and wait for its thread to terminate.
   *
   * If there is more than one close, all but the first will be ignored.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:close(),547,550,"/**
* Delegates m1() call to the parent DomainSocket instance.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,get,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:get(java.nio.channels.SelectableChannel),387,403,"/**
 * Retrieves a SelectorInfo for the given channel.
 * Creates one if it doesn't exist.
 */","* Takes one selector from end of LRU list of free selectors.
     * If there are no selectors awailable, it creates a new selector.
     * Also invokes trimIdleSelectors(). 
     * 
     * @param channel
     * @return 
     * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,release,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:release(org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo),411,417,"/**
 * Updates activity time and registers provider info.
 * @param info SelectorInfo object containing provider details.
 */","* puts selector back at the end of LRU list of free selectos.
     * Also invokes trimIdleSelectors().
     * 
     * @param info",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,getLeaf,"org.apache.hadoop.net.InnerNodeImpl:getLeaf(int,org.apache.hadoop.net.Node)",253,300,"/**
 * Finds a child node at a given leaf index, excluding a node.
 * @param leafIndex index of the desired leaf node
 * @param excludedNode node to exclude from the search
 * @return Child Node or null if not found
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,remove,org.apache.hadoop.net.InnerNodeImpl:remove(org.apache.hadoop.net.Node),189,234,"/**
* Removes a node from the tree.
* @param n Node to remove. Returns true if removal succeeded.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPs,org.apache.hadoop.net.DNS:getIPs(java.lang.String),152,155,"/**
 * Calls m1 with the interface string and default flag true.
 * @param strInterface Interface string to use.
 * @throws UnknownHostException if host resolution fails.
 */","* @return Like {@link DNS#getIPs(String, boolean)}, but returns all
   * IPs associated with the given interface and its subinterfaces.
   *
   * @param strInterface input strInterface.
   * @throws UnknownHostException
   * If no IP address for the local host could be found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getHosts,"org.apache.hadoop.net.DNS:getHosts(java.lang.String,java.lang.String,boolean)",248,277,"/**
 * Resolves hostnames for a given interface, using fallback resolution if needed.
 * @param strInterface Interface to resolve
 * @return Array of resolved hostnames.
 */","* Returns all the host names associated by the provided nameserver with the
   * address bound to the specified network interface
   *
   * @param strInterface
   *            The name of the network interface or subinterface to query
   *            (e.g. eth0 or eth0:0)
   * @param nameserver
   *            The DNS host name
   * @param tryfallbackResolution
   *            if true and if reverse DNS resolution fails then attempt to
   *            resolve the hostname with
   *            {@link InetAddress#getCanonicalHostName()} which includes
   *            hosts file resolution.
   * @return A string vector of all host names associated with the IPs tied to
   *         the specified interface
   * @throws UnknownHostException if the given interface is invalid",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,read,org.apache.hadoop.net.SocketInputStream:read(),112,127,"/**
 * Reads a single byte from the input stream.
 * Returns the byte as an int, or throws IOException on error.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,listBeans,"org.apache.hadoop.jmx.JMXJsonServlet:listBeans(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)",235,328,"/**
 * Lists MBeans for a query, serializing to JSON.
 * @param jg JSON generator, qry query, attribute attribute name
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseArguments,org.apache.hadoop.log.LogLevel$CLI:parseArguments(java.lang.String[]),141,170,"/**
 * Processes command-line arguments for level and protocol settings.
 * Throws IllegalArgumentException for invalid arguments.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.util.Map)",117,132,"/**
* Prints help entries to a PrintStream, including command usage.
* @param pStr PrintStream to write help information to.
* @param helpEntries Map of command names to UsageInfo objects.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,<init>,org.apache.hadoop.log.LogThrottlingHelper:<init>(long),145,147,"/**
 * Constructs a LogThrottlingHelper with a minimum log period.
 * @param minLogPeriodMs Minimum time (ms) between log entries.
 */
","* Create a log helper without any primary recorder.
   *
   * @see #LogThrottlingHelper(long, String)
   * @param minLogPeriodMs input minLogPeriodMs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,record,org.apache.hadoop.log.LogThrottlingHelper:record(double[]),195,197,"/**
* Creates a LogAction with default recorder name and timestamp.
* @param values double values to be logged
* @return LogAction object
*/
","* Record some set of values at the current time into this helper. Note that
   * this does <i>not</i> actually write information to any log. Instead, this
   * will return a LogAction indicating whether or not the caller should write
   * to its own log. The LogAction will additionally contain summary information
   * about the values specified since the last time the caller was expected to
   * write to its log.
   *
   * <p>Specifying multiple values will maintain separate summary statistics
   * about each value. For example:
   * <pre>{@code
   *   helper.record(1, 0);
   *   LogAction action = helper.record(3, 100);
   *   action.getStats(0); // == 2
   *   action.getStats(1); // == 50
   * }</pre>
   *
   * @param values The values about which to maintain summary information. Every
   *               time this method is called, the same number of values must
   *               be specified.
   * @return A LogAction indicating whether or not the caller should write to
   *         its log.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,getEvent,org.apache.hadoop.http.ProfileServlet:getEvent(javax.servlet.http.HttpServletRequest),366,373,"/**
 * Retrieves an event from the request, or defaults to Event.CPU.
 * @param req The HTTP request containing the event argument.
 * @return The Event object or Event.CPU if no event is provided.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,main,org.apache.hadoop.http.HtmlQuoting:main(java.lang.String[]),215,224,"/**
 * Processes command-line arguments, quoting and unquoting them.
 * @param args Command-line arguments to process.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameter,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameter(java.lang.String),1804,1808,"/**
* Processes a name string using HtmlQuoting and rawRequest.
* @param name The name to be processed.
* @return The processed name string.
*/
",* Unquote the name and quote the value.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameterValues,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterValues(java.lang.String),1810,1822,"/**
 * Retrieves and re-quotes values for a given name.
 * @param name The name to look up.
 * @return String array of re-quoted values, or null.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameterMap,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterMap(),1824,1838,"/**
* Transforms raw request data, quoting HTML values.
* @return Map with transformed data.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getRequestURL,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getRequestURL(),1844,1848,"/**
 * Retrieves a URL from rawRequest, quotes it, and returns as StringBuffer.
 */","* Quote the url so that users specifying the HOST HTTP header
       * can't inject attacks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getServerName,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getServerName(),1854,1857,"/**
* Quotes the raw request string using HtmlQuoting.m2().
* Returns the quoted string.
*/","* Quote the server name so that users specifying the HOST HTTP header
       * can't inject attacks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addAsyncProfilerServlet,"org.apache.hadoop.http.HttpServer2:addAsyncProfilerServlet(org.eclipse.jetty.server.handler.ContextHandlerCollection,org.apache.hadoop.conf.Configuration)",797,816,"/**
 * Configures the /prof endpoint based on Async Profiler setup.
 * Creates servlet context if Async Profiler is enabled.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addNoCacheFilter,org.apache.hadoop.http.HttpServer2:addNoCacheFilter(org.eclipse.jetty.servlet.ServletContextHandler),888,891,"/**
 * Calls m3 with specified parameters.
 * ctxt: ServletContextHandler, other params are constants.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,makeConfigurationChangeMonitor,"org.apache.hadoop.http.HttpServer2$Builder:makeConfigurationChangeMonitor(long,org.eclipse.jetty.util.ssl.SslContextFactory$Server)",634,663,"/**
 * Creates and starts a timer to monitor keystore/truststore changes.
 * @param reloadInterval Reload interval in milliseconds.
 * @param sslContextFactory SSL context factory to reload.
 * @return Timer object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileMonitoringTimerTask.java,<init>,"org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.nio.file.Path,java.util.function.Consumer,java.util.function.Consumer)",61,64,"/**
 * Creates a FileMonitoringTimerTask for a single file.
 * @param filePath Path to monitor.
 * @param onFileChange Callback on file change.
 * @param onChangeFailure Callback on failure.
 */
","* See {@link #FileMonitoringTimerTask(List, Consumer, Consumer)}.
   *
   * @param filePath The file to monitor.
   * @param onFileChange What to do when the file changes.
   * @param onChangeFailure What to do when <code>onFileChange</code>
   *                        throws an exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,close,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:close(),877,892,"/**
 * Closes the current output stream and handles potential errors.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRatesWithAggregation,org.apache.hadoop.metrics2.lib.MetricsRegistry:newRatesWithAggregation(java.lang.String),331,337,"/**
 * Retrieves rates with aggregation for a given name.
 * @param name Identifier for the rates to retrieve.
 * @return MutableRatesWithAggregation object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,add,"org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,org.apache.hadoop.metrics2.lib.MutableMetric)",348,351,"/**
* Records a metric with a given name.
* @param name Metric name.
* @param metric The metric to record.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsFactory.java,getAnnotatedMetricsFactory,org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getAnnotatedMetricsFactory(),33,35,"/**
 * Returns a MutableMetricsFactory instance.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,flush,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:flush(),863,875,"/**
 * Flushes the current file system output stream, handling potential IO errors.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,currentConfig,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:currentConfig(),348,358,"/**
 * Stringifies the configuration to a string representation.
 * Uses PropertiesConfiguration to convert config to string.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getPluginLoader,org.apache.hadoop.metrics2.impl.MetricsConfig:getPluginLoader(),228,264,"/**
 * Retrieves a ClassLoader, using plugin URLs or parent's.
 * Returns a ClassLoader or the default loader if none found.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,toString,org.apache.hadoop.metrics2.impl.MetricsConfig:toString(org.apache.commons.configuration2.Configuration),287,298,"/**
 * Generates a masked configuration string based on input.
 * @param c Configuration object to mask.
 * @return Masked configuration string.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,consume,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:consume(org.apache.hadoop.metrics2.impl.MetricsBuffer),172,200,"/**
* Processes metrics from a buffer, filters, and pushes records to sink.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,tag,"org.apache.hadoop.metrics2.MetricStringBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",81,84,"/**
* Delegates to m1 to build a MetricsRecordBuilder.
* @param info MetricsInfo object
* @param value Value to be associated with the metric
* @return MetricsRecordBuilder object
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.AbstractMetric),91,95,"/**
* Sets metric values using the provided metric's m1 and m2.
* @param metric The metric object to extract values from.
* @return This builder object for chaining.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",102,105,"/**
* Delegates to m1 to build a MetricsRecordBuilder.
* @param info MetricsInfo object
* @param value Integer value
* @return MetricsRecordBuilder object
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",107,110,"/**
* Delegates to m1 to build a MetricsRecordBuilder.
* @param info MetricsInfo object
* @param value The value to be recorded
* @return MetricsRecordBuilder object
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",112,115,"/**
* Delegates to m1 to build a MetricsRecordBuilder.
* @param info MetricsInfo object
* @param value Integer value
* @return MetricsRecordBuilder object
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",117,120,"/**
* Delegates to m1 to build a MetricsRecordBuilder.
* @param info MetricsInfo object
* @param value Value to be used in the record
* @return MetricsRecordBuilder object
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",122,125,"/**
* Delegates to m1 to build a MetricsRecordBuilder.
* @param info MetricsInfo object
* @param value Float value for the metric
* @return MetricsRecordBuilder object
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",127,130,"/**
* Delegates to m1 to build a MetricsRecordBuilder.
* @param info MetricsInfo object
* @param value The value to be recorded
* @return A MetricsRecordBuilder instance
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,update,org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord),184,186,"/**
 * Creates a record from a MetricsRecord.
 * @param mr The MetricsRecord to use.
 * @return A Record object.
 */
","* Update the cache and return the current cache record
   * @param mr the update record
   * @return the updated cache record",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,flush,org.apache.hadoop.metrics2.sink.GraphiteSink:flush(),110,122,"/**
 * Flushes metrics to Graphite, handling exceptions and closing connection.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,write,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:write(java.lang.String),167,174,"/**
 * Writes a message based on m1() result.
 * Writes msg via writer.m3 if m1() returns true.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,init,org.apache.hadoop.metrics2.sink.GraphiteSink:init(org.apache.commons.configuration2.SubsetConfiguration),54,68,"/**
 * Initializes Graphite reporter with configuration.
 * @param conf Configuration object containing reporter settings.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,writeMetrics,org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:writeMetrics(java.io.Writer),111,163,"/**
 * Writes Prometheus metrics to the given writer.
 * @param writer Writer to write the metrics to.
 * @throws IOException if an I/O error occurs.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,consume,org.apache.hadoop.metrics2.impl.SinkQueue:consume(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer),65,75,"/**
 * Executes a consumer with a value from m1(), then calls m4.
 * @param consumer Consumer to be executed with the value from m1()
 */
","* Consume one element, will block if queue is empty
   * Only one consumer at a time is allowed
   * @param consumer  the consumer callback object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,consumeAll,org.apache.hadoop.metrics2.impl.SinkQueue:consumeAll(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer),82,94,"/**
 * Executes a consumer multiple times, performing setup/cleanup.
 * @param consumer The consumer to execute repeatedly.
 */
","* Consume all the elements, will block if queue is empty
   * @param consumer  the consumer callback object
   * @throws InterruptedException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheHit,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheHit(),64,66,"/**
* Calls the m1() method on the cacheHit object.
*/",* One cache hit event,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheCleared,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheCleared(),71,73,"/**
 * Clears the cache using the provided cacheClearer object.
 */",* One cache cleared,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheUpdated,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheUpdated(),78,80,"/**
* Calls the m1() method of the cacheUpdated object.
*/",* One cache updated,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthenticationFailures,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationFailures(),215,217,"/**
* Calls the m1() method on the rpcAuthenticationFailures object.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthenticationSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationSuccesses(),223,225,"/**
 * Calls the m1 method on the rpcAuthenticationSuccesses object.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthorizationSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationSuccesses(),231,233,"/**
* Calls the m1 method on the rpcAuthorizationSuccesses object.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthorizationFailures,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationFailures(),239,241,"/**
* Calls the m1() method on the rpcAuthorizationFailures object.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrClientBackoff,org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoff(),343,345,"/**
* Calls the m1 method on the rpcClientBackoff object.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrClientBackoffDisconnected,org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoffDisconnected(),350,352,"/**
 * Calls the m1 method on the rpcClientBackoffDisconnected object.
 */",* Client was disconnected due to backoff,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrSlowRpc,org.apache.hadoop.ipc.metrics.RpcMetrics:incrSlowRpc(),366,368,"/**
 * Calls the m1 method of the rpcSlowCalls object.
 */",* Increments the Slow RPC counter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrRequeueCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:incrRequeueCalls(),373,375,"/**
 * Requeues RPC calls using the rpcRequeueCalls object.
 */",* Increments the Requeue Calls counter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrRpcCallSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrRpcCallSuccesses(),380,382,"/**
* Calls the m1() method on the rpcCallSuccesses object.
*/",* One RPC call success event.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,channelWrite,"org.apache.hadoop.ipc.Server:channelWrite(java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)",3923,3932,"/**
 * Writes data from buffer to channel, handling buffer size limits.
 * @param channel WritableByteChannel to write to
 * @param buffer ByteBuffer to read from
 * @return Number of bytes written
 */
","* This is a wrapper around {@link WritableByteChannel#write(ByteBuffer)}.
   * If the amount of data is large, it writes to channel in smaller chunks. 
   * This is to avoid jdk from creating many direct buffers as the size of 
   * buffer increases. This also minimizes extra copies in NIO layer
   * as a result of multiple write operations required to write a large 
   * buffer.  
   *
   * @see WritableByteChannel#write(ByteBuffer)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,channelRead,"org.apache.hadoop.ipc.Server:channelRead(java.nio.channels.ReadableByteChannel,java.nio.ByteBuffer)",3943,3952,"/**
* Reads data from a channel into a buffer.
* @param channel ReadableByteChannel to read from.
* @param buffer ByteBuffer to write to.
* @return Number of bytes read.
*/","* This is a wrapper around {@link ReadableByteChannel#read(ByteBuffer)}.
   * If the amount of data is large, it writes to channel in smaller chunks. 
   * This is to avoid jdk from creating many direct buffers as the size of 
   * ByteBuffer increases. There should not be any performance degredation.
   * 
   * @see ReadableByteChannel#read(ByteBuffer)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,getRecord,org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:getRecord(),152,157,"/**
 * Creates a MetricsRecordImpl if conditions are met.
 * Returns null otherwise.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,newAttrInfo,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",58,60,"/**
* Delegates to another m3 method with extracted info.
* @param info MetricsInfo object containing data.
* @param type Type string for further processing.
* @return MBeanAttributeInfo object.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,get,org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:get(),96,112,"/**
* Creates an MBeanInfo object from collected metrics records.
* Processes records to populate attributes and logs results.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateAttrCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateAttrCache(java.lang.Iterable),250,268,"/**
* Processes metrics records, updating caches and counting metrics.
* @param lastRecs Iterable of MetricsRecordImpl objects.
* @return Total number of metrics processed.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newObjectName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newObjectName(java.lang.String),128,137,"/**
 * Creates an ObjectName for a given name.
 * @param name The name to create an ObjectName from.
 * @return An ObjectName, or throws MetricsException if name exists.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newSourceName,"org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newSourceName(java.lang.String,boolean)",147,156,"/**
 * Checks/adds a metrics source name. Returns name or adds it.
 * @param name Source name. @param dupOK Allows duplicate names.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,putMetrics,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,long)",96,107,"/**
 * Enqueues a buffer if logicalTimeMs is a multiple of periodMs.
 * @param buffer The MetricsBuffer to enqueue.
 * @param logicalTimeMs Logical timestamp in milliseconds.
 * @return True if enqueued, false if dropped.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,putMetricsImmediate,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer),109,126,"/**
* Attempts to put metrics into the queue.
* @param buffer MetricsBuffer to be put.
* @return True if successful, false otherwise.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,diskCheckFailed,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:diskCheckFailed(),146,149,"/**
* Increments failure count and updates the last failure timestamp.
*/",* Increase the failure count and update the last failure timestamp.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,fetchGroupSet,org.apache.hadoop.security.Groups$GroupCacheLoader:fetchGroupSet(java.lang.String),413,424,"/**
 * Retrieves user groups, logs duration, and warns on slow execution.
 * @param user User identifier.
 * @return Set of user group names.
 */
","* Queries impl for groups belonging to the user.
     * This could involve I/O and take awhile.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,shutdown,org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:shutdown(),66,69,"/**
* Records JVM metrics and resets the 'impl' reference to null.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,registerIfNeeded,org.apache.hadoop.metrics2.source.JvmMetrics:registerIfNeeded(),72,79,"/**
 * Initializes JvmMetrics if not already present in the MetricsSystem.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,create,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:create(),147,149,"/**
* Creates and initializes UgiMetrics object.
* Returns a UgiMetrics instance configured by DefaultMetricsSystem.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:shutdown(),105,107,"/**
 * Logs the 'name' to the default metrics system.
 */",* Shutdown the instrumentation process.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:shutdown(),108,110,"/**
* Logs the name to the default metrics system.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.RpcMetrics:shutdown(),247,249,"/**
 * Records the 'name' metric using the DefaultMetricsSystem.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,registerMetrics2Source,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:registerMetrics2Source(java.lang.String),891,894,"/**
 * Records a metric with the given namespace.
 * @param namespace Metric identifier prefix.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,unregister,org.apache.hadoop.metrics2.util.MBeans:unregister(javax.management.ObjectName),136,149,"/**
 * Unregisters an MBean and updates metrics.
 * @param mbeanName The name of the MBean to unregister.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2Metrics.java,remove,org.apache.hadoop.http.HttpServer2Metrics:remove(),161,163,"/**
 * Records a metric for the HttpServer2 with the given port.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,snapshot,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",228,230,"/**
* Delegates metric recording to the registry.
* @param rb MetricsRecordBuilder to use.
* @param all If true, record all metrics.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRates:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",80,83,"/**
* Delegates method execution to the registry.
* @param rb MetricsRecordBuilder for building metrics.
* @param all Flag to indicate all metrics should be included.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,setQuantiles,"org.apache.hadoop.metrics2.lib.MutableQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)",117,125,"/**
 * Calculates and applies quantile-based formatting.
 * @param ucName, uvName, desc, lvName input strings.
 * @param pDecimalFormat DecimalFormat for percentile formatting.
 */","* Sets quantileInfo.
   *
   * @param ucName capitalized name of the metric
   * @param uvName capitalized type of the values
   * @param desc uncapitalized long-form textual description of the metric
   * @param lvName uncapitalized type of the values
   * @param pDecimalFormat Number formatter for percentile value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,setQuantiles,"org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)",75,83,"/**
* Processes inverse quantiles, generating names and descriptions.
* @param ucName, uvName, desc, lvName Input strings for formatting.
* @param df DecimalFormat object for number formatting.
*/","* Sets quantileInfo.
   *
   * @param ucName capitalized name of the metric
   * @param uvName capitalized type of the values
   * @param desc uncapitalized long-form textual description of the metric
   * @param lvName uncapitalized type of the values
   * @param df Number formatter for inverse percentile value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,<init>,org.apache.hadoop.metrics2.lib.MetricsRegistry:<init>(java.lang.String),49,51,"/**
 * Creates a MetricsRegistry with the given name.
 * @param name The name of the metrics registry.
 */
","* Construct the registry with a record name
   * @param name  of the record of the metrics",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",169,197,"/**
 * Records metrics averages, conditionally, and resets internal state.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,tag,"org.apache.hadoop.metrics2.lib.Interns:tag(java.lang.String,java.lang.String,java.lang.String)",162,164,"/**
* Creates a MetricsTag with given name, description, and value.
*/","* Get a metrics tag.
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @return an interned metrics tag",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(java.lang.Class,org.apache.hadoop.metrics2.annotation.Metrics)",141,146,"/**
 * Creates a MetricsInfo object from a class and annotation.
 * @param cls The class to create metrics for.
 * @param annotation The metrics annotation.
 * @return A MetricsInfo object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.String)",162,174,"/**
 * Creates a MetricsInfo object from annotation data or defaults.
 * @param annotation Metric annotation to process.
 * @param defaultName Default name if annotation data is incomplete.
 * @return MetricsInfo object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)",64,85,"/**
 * Creates a MutableStat with capitalized names and descriptions.
 * @param name Stat name.
 * @param description Stat description.
 */
","* Construct a sample statistics metric
   * @param name        of the metric
   * @param description of the metric
   * @param sampleName  of the metric (e.g. ""Ops"")
   * @param valueName   of the metric (e.g. ""Time"", ""Latency"")
   * @param extended    create extended stats (stdev, min/max etc.) by default.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,metricInfo,org.apache.hadoop.metrics2.lib.MethodMetric:metricInfo(java.lang.reflect.Method),145,147,"/**
* Creates a metrics info string for a given method.
* Uses internal helpers to format the method name.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getGcInfo,org.apache.hadoop.metrics2.source.JvmMetrics:getGcInfo(java.lang.String),209,222,"/**
* Retrieves or creates MetricsInfo for a given garbage collector name.
* @param gcName Garbage collector name.
* @return MetricsInfo array.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addUniqueIdentityCount,org.apache.hadoop.ipc.DecayRpcScheduler:addUniqueIdentityCount(org.apache.hadoop.metrics2.MetricsRecordBuilder),1030,1033,"/**
 * Records the total number of unique callers using the provided builder.
 * @param rb MetricsRecordBuilder to add the metric to.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addDecayedCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1036,1039,"/**
* Adds decayed call volume metric to the record builder.
* @param rb MetricsRecordBuilder to add the metric to.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addRawCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1041,1044,"/**
* Adds call volume metric to the record builder.
* @param rb MetricsRecordBuilder to add the metric to.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addServiceUserDecayedCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1047,1051,"/**
 * Records the decayed service user call volume metric.
 * @param rb MetricsRecordBuilder to add the metric to.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addServiceUserRawCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1054,1058,"/**
* Adds ServiceUserCallVolume metric to the record.
* @param rb MetricsRecordBuilder to add the metric to.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addCallVolumePerPriority,org.apache.hadoop.ipc.DecayRpcScheduler:addCallVolumePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder),1061,1067,"/**
 * Adds completed call volume metrics for each priority level.
 * @param rb MetricsRecordBuilder to populate with data.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addAvgResponseTimePerPriority,org.apache.hadoop.ipc.DecayRpcScheduler:addAvgResponseTimePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder),1070,1076,"/**
 * Adds average response times for each priority level to the metrics record.
 * @param rb MetricsRecordBuilder to populate with the data.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSystem,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSystem(),488,490,"/**
* Logs hostname information using injected tags.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,tag,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",73,79,"/**
 * Adds a tag to the metrics record.
 * @param info MetricsInfo object
 * @param value Tag value string
 * @return MetricsRecordBuilderImpl instance
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String,boolean)",415,420,"/**
 * Registers a metric with given info and value, optionally overriding.
 * @param info Metric info object
 * @param value Metric value
 * @param override Whether to override existing metrics
 * @return MetricsRegistry instance
 */
","* Add a tag to the metrics
   * @param info  metadata of the tag
   * @param value of the tag
   * @param override existing tag if true
   * @return the registry (for keep adding tags etc.)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr(),42,45,"/**
 * Calls m1 with a default value of 1.0f.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:decr(),47,50,"/**
* Calls m1 with a default value of -1.0f.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,add,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:add(double),178,180,"/**
 * Calls the m1 method of the stat object with the given double.
 * @param x The double value passed to stat.m1()
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,add,org.apache.hadoop.metrics2.lib.MutableStat:add(long),132,136,"/**
 * Updates statistics and minimum/maximum values, then calls m2.
 * @param value The value to use for updating statistics.
 */","* Add a snapshot to the metric.
   * @param value of the metric",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,"org.apache.hadoop.metrics2.util.SampleStat:reset(long,double,double,org.apache.hadoop.metrics2.util.SampleStat$MinMax)",48,53,"/**
* Initializes data and calls minmax.m1.
* @param numSamples1 Number of samples.
* @param mean1 The mean value.
* @param s1 Standard deviation.
* @param minmax1 MinMax object.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,snapshotInto,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:snapshotInto(org.apache.hadoop.metrics2.lib.MutableRate),182,187,"/**
 * Updates metric with values from stat, if stat.m1() is positive.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newImpl,org.apache.hadoop.metrics2.lib.MethodMetric:newImpl(org.apache.hadoop.metrics2.annotation.Metric$Type),55,70,"/**
 * Creates a MutableMetric based on the given metric type.
 * @param metricType The type of metric to create.
 * @return MutableMetric object or null if type is unsupported.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,toString,org.apache.hadoop.metrics2.util.SampleStat:toString(),145,156,"/**
 * Generates a string with sample statistics.
 * Returns super's result on error.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingStdDev,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingStdDev(),412,414,"/**
* Returns the combined processing time from rpcProcessingTime.
*/","* Return Standard Deviation of the Processing Time.
   * @return  double",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingStdDev,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingStdDev(),445,447,"/**
* Returns the deferred RPC processing time.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,insert,org.apache.hadoop.metrics2.util.SampleQuantiles:insert(long),113,123,"/**
 * Adds a value to the buffer and increments counters.
 * @param v The value to add to the buffer.
 */
","* Add a new value from the stream.
   * 
   * @param v v.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,snapshot,org.apache.hadoop.metrics2.util.SampleQuantiles:snapshot(),236,250,"/**
 * Calculates quantile counts. Returns map of quantile to count.
 */","* Get a snapshot of the current values of all the tracked quantiles.
   * 
   * @return snapshot of the tracked quantiles. If no items are added
   * to the estimator, returns null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTopTokenRealOwners,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTopTokenRealOwners(int),880,898,"/**
 * Returns the top N token owners.
 * @param n number of top owners to retrieve
 * @return List of NameValuePair objects representing top owners
 */","* Return top token real owners list as well as the tokens count.
   *
   * @param n top number of users
   * @return map of owners to counts",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getTopCallers,org.apache.hadoop.ipc.DecayRpcScheduler:getTopCallers(int),1099,1112,"/**
 * Returns the top N callers based on their total call cost.
 * @param n The number of top callers to retrieve.
 * @return TopN object containing the top N callers.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,cacheGroupsAdd,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List),90,103,"/**
 * Processes a list of groups, caching those meeting criteria.
 */","* Add a group to cache, only netgroups are cached
   *
   * @param groups list of group names to add to cache",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getTokens,org.apache.hadoop.security.UserGroupInformation:getTokens(),1724,1729,"/**
 * Returns a collection of tokens, synchronized on 'subject'.
 */","* Obtain the collection of tokens associated with this user.
   * 
   * @return an unmodifiable collection of tokens associated with user",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/User.java,<init>,org.apache.hadoop.security.User:<init>(java.lang.String),42,44,"/**
 * Constructs a User object with a name, other fields default to null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroups,org.apache.hadoop.security.Groups:getGroups(java.lang.String),213,217,"/**
 * Transforms user input and returns a list of strings.
 * @param user The user input string.
 * @return A list of transformed strings.
 */","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * Note this method can be expensive as it involves Set {@literal ->} List
   * conversion. For user with large group membership
   * (i.e., {@literal >} 1000 groups), we recommend using getGroupSet
   * to avoid the conversion and fast membership look up via contains().
   * @param user User's name
   * @return the group memberships of the user as list
   * @throws IOException if user does not exist
   * @deprecated Use {@link #getGroupsSet(String user)} instead.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroupsSet,org.apache.hadoop.security.Groups:getGroupsSet(java.lang.String),231,233,"/**
* Transforms user data to a set of strings.
* @param user User identifier; returns a Set<String>.
*/","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * This provide better performance when user has large group membership via
   * <br>
   * 1) avoid {@literal set->list->set} conversion for the caller
   * UGI/PermissionCheck <br>
   * 2) fast lookup using contains() via Set instead of List
   * @param user User's name
   * @return the group memberships of the user as set
   * @throws IOException if user does not exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,getGroups,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String),67,73,"/**
 * Retrieves user groups, caches them, and returns the list.
 */","* Gets unix groups and netgroups for the user.
   *
   * It gets all unix groups as returned by id -Gn but it
   * only returns netgroups that are used in ACLs (there is
   * no way to get all netgroups for a given user, see
   * documentation for getent netgroup)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,println,org.apache.hadoop.security.KDiag:println(),868,870,"/**
 * Calls m1 with an empty string as an argument.
 */",* Print a new line,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printSysprop,org.apache.hadoop.security.KDiag:printSysprop(java.lang.String),897,900,"/**
* Sets the value of a property to UNSET.
* @param property The name of the property to set.
*/","* Print a system property, or {@link #UNSET} if unset.
   * @param property property to print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printEnv,org.apache.hadoop.security.KDiag:printEnv(java.lang.String),916,919,"/**
* Logs the value of an environment variable.
* @param variable Name of the environment variable.
*/","* Print an environment variable's name and value; printing
   * {@link #UNSET} if it is not set.
   * @param variable environment variable",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dump,org.apache.hadoop.security.KDiag:dump(java.io.File),926,932,"/**
 * Reads a file line by line and prints each line to the console.
 * @param file The file to read.
 * @throws IOException if an I/O error occurs.
 */
","* Dump any file to standard out.
   * @param file file to dump
   * @throws IOException IO problems",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,error,"org.apache.hadoop.security.KDiag:error(java.lang.String,java.lang.String,java.lang.Object[])",1011,1013,"/**
* Logs an error message with a category and formatted arguments.
* @param category Error category.
* @param message Error message format.
* @param args Arguments for message formatting.
*/
","* Print a message as an error
   * @param category error category
   * @param message format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,warn,"org.apache.hadoop.security.KDiag:warn(java.lang.String,java.lang.String,java.lang.Object[])",1020,1022,"/**
 * Logs a warning message with a specified category and formatted arguments.
 */","* Print a message as an warning
   * @param category error category
   * @param message format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullUserMap,org.apache.hadoop.security.ShellBasedIdMapping:loadFullUserMap(),359,370,"/**
 * Populates uidNameMap with user data, OS-dependent command used.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullGroupMap,org.apache.hadoop.security.ShellBasedIdMapping:loadFullGroupMap(),372,384,"/**
 * Populates gidNameMap with group names based on OS.
 * Updates lastUpdateTime with the current timestamp.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod),1844,1846,"/**
* Delegates authentication to the user object.
* @param authMethod Authentication method to use.
*/","* Sets the authentication method in the subject
   * 
   * @param authMethod authMethod.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,org.apache.hadoop.security.SaslOutputStream:write(int),123,131,"/**
 * Writes a byte to the output stream, optionally wrapped.
 * @param b The byte to write.
 * @throws IOException if an I/O error occurs.
 */","* Writes the specified byte to this output stream.
   * 
   * @param b
   *          the <code>byte</code>.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,org.apache.hadoop.security.SaslOutputStream:write(byte[]),148,151,"/**
* Calls overloaded m1 with full byte array.
* @param b byte array to process
* @throws IOException if an I/O error occurs
*/","* Writes <code>b.length</code> bytes from the specified byte array to this
   * output stream.
   * <p>
   * The <code>write</code> method of <code>SASLOutputStream</code> calls the
   * <code>write</code> method of three arguments with the three arguments
   * <code>b</code>, <code>0</code>, and <code>b.length</code>.
   * 
   * @param b
   *          the data.
   * @exception NullPointerException
   *              if <code>b</code> is null.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createKeyManagers,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyManagers(),1078,1088,"/**
 * Creates KeyManagers from keystore. Returns null if keystore is invalid.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createTrustManagers,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createTrustManagers(),1090,1100,"/**
 * Creates TrustManagers from the trust store.
 * Returns null if trust store location is invalid.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,doFilter,"org.apache.hadoop.security.http.RestCsrfPreventionFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",205,212,"/**
 * Delegates request/response handling to an HttpInteraction object.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,doFilter,"org.apache.hadoop.security.http.CrossOriginFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",92,98,"/**
 * Executes m1 and passes request/response to the next filter.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,init,org.apache.hadoop.security.http.CrossOriginFilter:init(javax.servlet.FilterConfig),84,90,"/**
 * Initializes filter resources using the provided FilterConfig.
 * @param filterConfig Configuration parameters for the filter.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,getKey,org.apache.hadoop.security.token.delegation.DelegationKey:getKey(),75,82,"/**
* Retrieves a SecretKey from keyBytes, or null if empty.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,checkToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),528,546,"/**
* Retrieves delegation token information, validating its existence and expiry.
* @param identifier Token identifier
* @return DelegationTokenInformation or throws InvalidToken
*/","* Find the DelegationTokenInformation for the given token id, and verify that
   * if the token is expired. Note that this method should be called with 
   * acquiring the secret manager's monitor.
   *
   * @param identifier identifier.
   * @throws InvalidToken invalid token exception.
   * @return DelegationTokenInformation.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(),703,705,"/**
 * Default constructor for DelegationTokenInformation.
 * Initializes with a token count of 0 and null token.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,logExpireTokens,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logExpireTokens(java.util.Collection),786,793,"/**
 * Processes a collection of expired tokens.
 * Iterates and calls m1, logs removal, then calls m4.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,setExternalDelegationTokenSecretManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager),143,146,"/**
* Delegates the secret manager to the token manager for processing.
*/","* Sets an external <code>DelegationTokenSecretManager</code> instance to
   * manage creation and verification of Delegation Tokens.
   * <p>
   * This is useful for use cases where secrets must be shared across multiple
   * services.
   *
   * @param secretManager a <code>DelegationTokenSecretManager</code> instance",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,destroy,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:destroy(),189,193,"/**
* Calls m1() on tokenManager and authHandler.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,updateCurrentKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateCurrentKey(),440,456,"/**
 * Updates the master key for generating delegation tokens.
 * Uses m2() to get new ID, creates DelegationKey, and updates.
 */","* Update the current master key 
   * This is called once by startThreads before tokenRemoverThread is created, 
   * and only by tokenRemoverThread afterwards.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,<init>,org.apache.hadoop.security.token.delegation.DelegationKey:<init>(),47,49,"/**
 * Default constructor for DelegationKey, initializes with default values.
 */
",Default constructore required for Writable,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",183,188,"/**
 * Constructs a DelegationTokenAuthenticatedURL.
 * @param authenticator DelegationTokenAuthenticator
 * @param connConfigurator ConnectionConfigurator
 */
","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   *
   * @param authenticator the {@link DelegationTokenAuthenticator} instance to
   * use, if <code>null</code> the default one will be used.
   * @param connConfigurator a connection configurator.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,renew,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:renew(),126,151,"/**
* Attempts to renew a delegation token, or obtains a new one.
* Returns true if a filesystem was obtained, false otherwise.
*/","* Renew or replace the delegation token for this file system.
     * It can only be called when the action is not in the queue.
     * @return
     * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,cancel,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:cancel(),153,158,"/**
 * Retrieves a file system and updates the token with its details.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(),568,573,"/**
 * Reads the first byte from the input stream.
 * @return The first byte as an int, or -1 on error.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[]),575,578,"/**
* Reads bytes from a byte array.
* @param b byte array to read from
* @return Number of bytes read
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setSaslClient,org.apache.hadoop.ipc.Client$IpcStreams:setSaslClient(org.apache.hadoop.security.SaslRpcClient),1905,1910,"/**
* Processes data using client methods m1, m3 and internal methods m2, m4.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,noPasswordWarning,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordWarning(),348,352,"/**
* Retrieves a password using provided environment variable and key.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,noPasswordWarning,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordWarning(),315,319,"/**
* Retrieves a value using ProviderUtils.m1 with provided keys.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,noPasswordError,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordError(),354,358,"/**
* Retrieves a credential password using utility methods.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,noPasswordError,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordError(),321,325,"/**
* Delegates keystore password retrieval to ProviderUtils.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(),193,207,"/**
 * Reads a byte from the input stream, wrapping if enabled.
 * @return The byte value, or -1 if end of stream.
 * @throws IOException if an I/O error occurs.
 */","* Reads the next byte of data from this input stream. The value byte is
   * returned as an <code>int</code> in the range <code>0</code> to
   * <code>255</code>. If no byte is available because the end of the stream has
   * been reached, the value <code>-1</code> is returned. This method blocks
   * until input data is available, the end of the stream is detected, or an
   * exception is thrown.
   * <p>
   * 
   * @return the next byte of data, or <code>-1</code> if the end of the stream
   *         is reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,"org.apache.hadoop.security.SaslInputStream:read(byte[],int,int)",248,275,"/**
* Reads bytes from the input stream into the provided byte array.
*/","* Reads up to <code>len</code> bytes of data from this input stream into an
   * array of bytes. This method blocks until some input is available. If the
   * first argument is <code>null,</code> up to <code>len</code> bytes are read
   * and discarded.
   * 
   * @param b
   *          the buffer into which the data is read.
   * @param off
   *          the start offset of the data.
   * @param len
   *          the maximum number of bytes read.
   * @return the total number of bytes read into the buffer, or <code>-1</code>
   *         if there is no more data because the end of the stream has been
   *         reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ImpersonationProvider.java,authorize,"org.apache.hadoop.security.authorize.ImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",51,58,"/**
 * Calls m2 with an InetAddress derived from remoteAddress.
 * @param user UserGroupInformation for authorization.
 * @param remoteAddress Remote address string.
 * @throws AuthorizationException if host resolution fails.
 */
","* Authorize the superuser which is doing doAs.
   * {@link #authorize(UserGroupInformation, InetAddress)} should
   *             be preferred to avoid possibly re-resolving the ip address.
   * @param user ugi of the effective or proxy user which contains a real user.
   * @param remoteAddress the ip address of client.
   * @throws AuthorizationException Authorization Exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKeytab,org.apache.hadoop.security.UserGroupInformation:getKeytab(),814,819,"/**
* Retrieves a value using HadoopLoginContext, or null if context is null.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isHadoopLogin,org.apache.hadoop.security.UserGroupInformation:isHadoopLogin(),825,828,"/**
* Checks if m1() returns a non-null value.
*/","* Is the ugi managed by the UGI or an external subject?
   * @return true if managed by UGI.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createProxyUser,"org.apache.hadoop.security.UserGroupInformation:createProxyUser(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",1522,1537,"/**
 * Creates a UserGroupInformation with provided user and real user.
 * @param user User identifier.
 * @param realUser The real UserGroupInformation.
 * @return A UserGroupInformation object.
 */","* Create a proxy user using username of the effective user and the ugi of the
   * real user.
   * @param user user.
   * @param realUser realUser.
   * @return proxyUser ugi",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getName,org.apache.hadoop.security.UserGroupInformation$RealUser:getName(),472,475,"/**
 * Delegates to the realUser's m1() method.
 * @return String returned by realUser.m1()
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,shouldBackOff,org.apache.hadoop.ipc.DecayRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable),720,745,"/**
* Checks if a Schedulable object requires backoff based on response times.
* @param obj The Schedulable object to check.
* @return True if backoff is required, false otherwise.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealUserOrSelf,org.apache.hadoop.security.UserGroupInformation:getRealUserOrSelf(org.apache.hadoop.security.UserGroupInformation),1558,1564,"/**
 * Returns the underlying UserGroupInformation or the input if null.
 */","* If this is a proxy user, get the real user. Otherwise, return
   * this user.
   * @param user the user to check
   * @return the real user or self",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,toString,org.apache.hadoop.security.UserGroupInformation:toString(),1819,1827,"/**
 * Constructs a string with authentication and source information.
 */",* Return the username.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod(),1863,1869,"/**
 * Gets the authentication method. Uses m1() to get UGI,
 * otherwise uses the current object. Returns the authentication method.
 */","* Get the authentication method from the real user's subject.  If there
   * is no real user, return the given user's authentication method.
   * 
   * @return AuthenticationMethod in the subject, null if not present.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation),1878,1885,"/**
 * Determines the authentication method for a UserGroupInformation.
 * @param ugi UserGroupInformation object.
 * @return AuthenticationMethod for the user.
 */
","* Returns the authentication method of a ugi. If the authentication method is
   * PROXY, returns the authentication method of the real user.
   * 
   * @param ugi ugi.
   * @return AuthenticationMethod",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeIpcConnectionContext,"org.apache.hadoop.util.ProtoUtil:makeIpcConnectionContext(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",91,122,"/**
* Creates an IpcConnectionContextProto based on protocol, ugi, auth.
* @param protocol Protocol string.
* @param ugi UserGroupInformation object.
* @param authMethod Authentication method.
* @return IpcConnectionContextProto object.
*/
","* This method creates the connection context  using exactly the same logic
   * as the old connection context as was done for writable where
   * the effective and real users are set based on the auth method.
   *
   * @param protocol protocol.
   * @param ugi ugi.
   * @param authMethod authMethod.
   * @return IpcConnectionContextProto.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,close,org.apache.hadoop.ipc.Server$ConnectionManager:close(org.apache.hadoop.ipc.Server$Connection),4110,4126,"/**
 * Disconnects a client connection.
 * @param connection The connection to disconnect.
 * @return True if the connection existed, false otherwise.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UserIdentityProvider.java,makeIdentity,org.apache.hadoop.ipc.UserIdentityProvider:makeIdentity(org.apache.hadoop.ipc.Schedulable),28,35,"/**
 * Extracts a string from a Schedulable object.
 * @param obj The Schedulable object to process.
 * @return A string from the object, or null if it's null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,verify,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:verify(java.lang.String,javax.net.ssl.SSLSession)",262,273,"/**
 * Validates SSL session certificate against the host.
 * @param host The hostname to validate against.
 * @param session The SSL session to validate.
 * @return True if validation succeeds, false otherwise.
 */
","* The javax.net.ssl.HostnameVerifier contract.
         *
         * @param host    'hostname' we used to create our socket
         * @param session SSLSession with the remote server
         * @return true if the host matched the one in the certificate.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,java.security.cert.X509Certificate)",280,284,"/**
* Calls m1 with the given host and certificate as an array.
* @param host The hostname.
* @param cert The X.509 certificate.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],javax.net.ssl.SSLSocket)",292,347,"/**
 * Verifies SSL session certificates and calls m6 with the first cert.
 * @param host host array, SSLSocket ssl socket
 * @throws IOException if an I/O error occurs
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadResource,org.apache.hadoop.util.FindClass:loadResource(java.lang.String),172,180,"/**
 * Retrieves a resource URL by name. Returns SUCCESS or E_NOT_FOUND.
 */","* Load a resource
   * @param name resource name
   * @return the status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,<init>,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:<init>(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),132,151,"/**
 * Initializes the DelegatingSSLSocketFactory with a preferred channel mode.
 * @param preferredChannelMode Preferred SSL channel mode.
 * @throws IOException if initialization fails.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,<init>,org.apache.hadoop.fs.shell.Command:<init>(org.apache.hadoop.conf.Configuration),86,88,"/**
 * Constructs a Command object with the given configuration.
 */","* Constructor.
   *
   * @param conf configuration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,<init>,org.apache.hadoop.fs.shell.CommandFactory:<init>(org.apache.hadoop.conf.Configuration),53,55,"/**
 * Constructs a CommandFactory with the given configuration.
 * @param conf Configuration object for factory setup.
 */
","* Factory constructor for commands
   * @param conf the hadoop configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem:<init>(),777,779,"/**
 * Default constructor. Calls superclass constructor with null.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell:<init>(org.apache.hadoop.conf.Configuration),75,77,"/**
 * Constructs a FsShell object with the given configuration.
 * @param conf Hadoop configuration object.
 */
","* Construct a FsShell with the given configuration.  Commands can be
   * executed via {@link #run(String[])}
   * @param conf the hadoop configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,org.apache.hadoop.io.ObjectWritable$NullInstance:<init>(),109,109,"/**
 * Constructs a NullInstance with a null superclass constructor.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,"org.apache.hadoop.io.ObjectWritable$NullInstance:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)",110,113,"/**
 * Constructs a NullInstance with the given class and configuration.
 * @param declaredClass Class of the null instance.
 * @param conf Configuration object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintWriter,java.io.File,java.lang.String,long,boolean)",171,184,"/**
 * Constructs a KDiag object with provided configuration and details.
 * @param conf Configuration object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,<init>,org.apache.hadoop.util.FindClass:<init>(org.apache.hadoop.conf.Configuration),131,133,"/**
 * Constructs a FindClass object using the provided configuration.
 * @param conf Configuration object for class finding.
 */
","* Create a class with a specified configuration
   * @param conf configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,<init>,"org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintStream)",53,56,"/**
 * Constructs a GetGroupsBase with a configuration and print stream.
 * @param conf Configuration object
 * @param out PrintStream for output
 */
","* Used exclusively for testing.
   * 
   * @param conf The configuration to use.
   * @param out The PrintStream to write to, instead of System.out",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configured.java,<init>,org.apache.hadoop.conf.Configured:<init>(),32,34,"/**
 * Default constructor. Calls the parameterized constructor with null.
 */",Construct a Configured.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,<init>,org.apache.hadoop.ha.HAAdmin:<init>(org.apache.hadoop.conf.Configuration),103,105,"/**
 * Constructs a HAAdmin object with the given configuration.
 * @param conf The configuration object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByNameWithSearch,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByNameWithSearch(java.lang.String),706,718,"/**
 * Resolves a host address. Tries host directly, then with suffixes.
 * @param host The hostname to resolve.
 * @return InetAddress object or null if resolution fails.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getAppConfigurationEntry,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getAppConfigurationEntry(java.lang.String),2211,2230,"/**
* Retrieves AppConfigurationEntry array based on app name.
* Returns entries based on configuration parameters.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,parseStaticMap,org.apache.hadoop.security.ShellBasedIdMapping:parseStaticMap(java.io.File),588,630,"/**
 * Parses a static mapping file and returns a StaticMapping object.
 * @param staticMapFile File containing uid/gid mappings.
 * @return StaticMapping object containing parsed mappings.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getAclString,org.apache.hadoop.security.authorize.AccessControlList:getAclString(),301,312,"/**
* Constructs a masked string based on the allAllowed flag.
* Returns the masked string.
*/","* Returns the access control list as a String that can be used for building a
   * new instance by sending it to the constructor of {@link AccessControlList}.
   * @return acl string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,createCredentialEntry,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:createCredentialEntry(java.lang.String,char[])",228,244,"/**
* Adds a credential entry with the given alias.
* @param alias Credential alias.
* @param credential Credential characters.
* @throws IOException If credential already exists or error occurs.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,execute,org.apache.hadoop.security.alias.CredentialShell$CreateCommand:execute(),438,465,"/**
* Creates a credential, displaying help if ""-help"" is provided.
* Handles exceptions and logs success/failure messages.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getTGT,org.apache.hadoop.security.UserGroupInformation:getTGT(),852,861,"/**
 * Finds a valid Kerberos ticket from the subject's tickets.
 * @return A valid KerberosTicket or null if none found.
 */
","* Get the Kerberos TGT
   * @return the user's TGT or null if none was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setSslConfiguration,"org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",819,823,"/**
 * Calls m1 with a default ClientX509Util.
 * @param zkClientConfig ZK client configuration.
 * @param truststoreKeystore Truststore/keystore details.
 */
","* Configure ZooKeeper Client with SSL/TLS connection.
   * @param zkClientConfig ZooKeeper Client configuration
   * @param truststoreKeystore truststore keystore, that we use to set the SSL configurations
   * @throws ConfigurationException if the SSL configs are empty",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,unprotectedRelogin,"org.apache.hadoop.security.UserGroupInformation:unprotectedRelogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)",1347,1377,"/**
 * Performs user logout and re-login, respecting last login time.
 * @param login HadoopLoginContext object for login management
 * @param ignoreLastLoginTime Whether to ignore last login time
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,getServerProperties,org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.lang.String),124,129,"/**
* Delegates to another m2 method with an InetAddress object.
* @param clientAddress String representation of client address
* @return Map of String key-value pairs.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,handle,org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:handle(javax.security.auth.callback.Callback[]),294,348,"/**
 * Processes SASL DIGEST-MD5 callbacks to configure authentication.
 * @param callbacks Array of callbacks to process.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long,boolean)",97,118,"/**
 * Creates a CryptoOutputStream for encrypting data.
 * @param out Output stream, codec, buffer size, key, IV, offset, close flag.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createEncryptor(),54,58,"/**
 * Creates and returns a JceCtrCipher encryptor with specified parameters.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createDecryptor(),60,64,"/**
 * Creates and returns a JceCtrCipher instance for decryption.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createEncryptor(),54,58,"/**
 * Creates and returns a JceCtrCipher encryptor using derived keys.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createDecryptor(),60,64,"/**
 * Creates and returns a JceCtrCipher instance for decryption.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,getInstance,"org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String,java.lang.String)",131,140,"/**
 * Creates an OpensslCipher with given transformation and engine.
 * @param transformation Cipher transformation string.
 * @param engineId Engine identifier, or null for default.
 * @return An OpensslCipher object.
 */","* Return an <code>OpensslCipher</code> object that implements the specified
   * transformation.
   * 
   * @param transformation the name of the transformation, e.g., 
   * AES/CTR/NoPadding.
   * @param engineId the openssl engine to use.if not set,
   * defalut engine will be used.
   * @return OpensslCipher an <code>OpensslCipher</code> object
   * @throws NoSuchAlgorithmException if <code>transformation</code> is null, 
   * empty, in an invalid format, or if Openssl doesn't implement the 
   * specified algorithm.
   * @throws NoSuchPaddingException if <code>transformation</code> contains 
   * a padding scheme that is not available.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,isSupported,org.apache.hadoop.crypto.OpensslCipher:isSupported(org.apache.hadoop.crypto.CipherSuite),181,193,"/**
 * Checks CipherSuite transforms, returns true if valid.
 * @param suite CipherSuite to validate.
 * @return True if transforms are valid, false otherwise.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersions(java.lang.String),376,398,"/**
* Retrieves KeyVersion objects for a given name.
* @param name The name of the key version to retrieve.
* @return List of KeyVersion objects.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,createKey,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",433,460,"/**
* Adds a key version with given name and material to the store.
* @param name Key name
* @param material Key material
* @param options Options for the new key version
* @return KeyVersion object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:rollNewVersion(java.lang.String,byte[])",508,527,"/**
* Creates a new key version with the given material.
* @param name Key name
* @param material Key material
* @throws IOException if key not found or material length is incorrect
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSEncryptedKeyVersion:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,byte[])",246,250,"/**
 * Creates a KMSEncryptedKeyVersion with provided parameters.
 * @param keyName Key name
 * @param keyVersionName Version name
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONKeyVersion,org.apache.hadoop.util.KMSUtil:parseJSONKeyVersion(java.util.Map),185,202,"/**
* Creates a KeyVersion object from the provided map.
* Returns null if the map doesn't contain necessary data.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONMetadata,org.apache.hadoop.util.KMSUtil:parseJSONMetadata(java.util.Map),204,218,"/**
* Creates Metadata object from valueMap.
* @param valueMap Map containing metadata values.
* @return KeyProvider.Metadata object or null if map is invalid.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,getCurrentKey,org.apache.hadoop.crypto.key.KeyProviderExtension:getCurrentKey(java.lang.String),66,69,"/**
* Delegates KeyVersion retrieval to the keyProvider.
* @param name Key name.
* @return KeyVersion object.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,createKey,"org.apache.hadoop.crypto.key.KeyProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",567,571,"/**
 * Creates a KeyVersion using provided name and material.
 * @param name KeyVersion name.
 * @param options Options object for key creation.
 * @return KeyVersion object.
 */
","* Create a new key generating the material for it.
   * The given key must not already exist.
   * <p>
   * This implementation generates the key material and calls the
   * {@link #createKey(String, byte[], Options)} method.
   *
   * @param name the base name of the key
   * @param options the options for the new key.
   * @return the version name of the first version of the key.
   * @throws IOException raised on errors performing I/O.
   * @throws NoSuchAlgorithmException no such algorithm exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.KeyProvider:rollNewVersion(java.lang.String),612,621,"/**
 * Retrieves a KeyVersion for the given name.
 * @param name Key name; throws IOException if metadata not found.
 * @return KeyVersion object.
 */
","* Roll a new version of the given key generating the material for it.
   * <p>
   * This implementation generates the key material and calls the
   * {@link #rollNewVersion(String, byte[])} method.
   *
   * @param name the basename of the key
   * @return the name of the new version of the key
   * @throws IOException              raised on errors performing I/O.
   * @throws NoSuchAlgorithmException This exception is thrown when a particular
   *                                  cryptographic algorithm is requested
   *                                  but is not available in the environment.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String,byte[])",140,146,"/**
 * Creates a KeyVersion.
 * @param name Key name.
 * @param material Key material.
 * @return The created KeyVersion.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,toJSON,org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),110,122,"/**
 * Creates a JSON map from an EncryptedKeyVersion object.
 * @param encryptedKeyVersion The version to convert; null safe.
 * @return A JSON map representing the version.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,warmUpEncryptedKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:warmUpEncryptedKeys(java.lang.String[]),288,308,"/**
 * Attempts to warm up keys for each configured provider.
 * Throws IOException if all providers fail.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,close,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:close(),544,554,"/**
 * Iterates through providers, calling m3() on each, handling IO errors.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,readLock,org.apache.hadoop.crypto.key.kms.ValueQueue:readLock(java.lang.String),115,117,"/**
* Calls m1, then m2 on the result, and finally m3.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,readUnlock,org.apache.hadoop.crypto.key.kms.ValueQueue:readUnlock(java.lang.String),119,121,"/**
 * Executes a sequence of operations on an object identified by keyName.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,writeUnlock,org.apache.hadoop.crypto.key.kms.ValueQueue:writeUnlock(java.lang.String),123,125,"/**
* Executes a sequence of operations on an object identified by keyName.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,writeLock,org.apache.hadoop.crypto.key.kms.ValueQueue:writeLock(java.lang.String),127,129,"/**
 * Calls m1, then m2 on the result, and finally m3.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String),139,141,"/**
 * Constructs a Builder with a context and default separator.
 * @param context The context string.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,unpack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto),70,80,"/**
 * Converts GenericRefreshResponseProto to RefreshResponse objects.
 * @param collection Collection of GenericRefreshResponseProto
 * @return List of RefreshResponse objects.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolServerSideTranslatorPB.java,refresh,"org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:refresh(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto)",44,62,"/**
 * Processes refresh request, returns GenericRefreshResponseCollectionProto.
 * @param controller RPC controller
 * @param request Refresh request proto
 * @return Refresh response collection
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,newEntry,"org.apache.hadoop.ipc.RetryCache:newEntry(long,byte[],int)",335,339,"/**
 * Creates a CacheEntry with client ID, call ID, and expiration time.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long)",155,159,"/**
 * Constructs a CacheEntryWithPayload.
 * @param clientId Client identifier.
 * @param callId Call identifier.
 * @param payload The payload to store.
 * @param expirationTime Expiration timestamp.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long,boolean)",84,88,"/**
 * Constructs a CacheEntry with success/failure state.
 * @param clientId Client identifier.
 * @param callId Call identifier.
 * @param expirationTime Expiration timestamp.
 * @param success True for success, false for failure.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isServerFailOverEnabledByQueue,org.apache.hadoop.ipc.Server:isServerFailOverEnabledByQueue(),3885,3888,"/**
 * Delegates to the callQueue's m1 method and returns the result.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,put,org.apache.hadoop.ipc.CallQueueManager:put(org.apache.hadoop.ipc.Schedulable),289,299,"/**
 * Processes element 'e'. Delegates if m1() fails, or acts based on m2().
 */","* Insert e into the backing queue or block until we can.  If client
   * backoff is enabled this method behaves like add which throws if
   * the queue overflows.
   * If we block and the queue changes on us, we will insert while the
   * queue is drained.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,add,org.apache.hadoop.ipc.CallQueueManager:add(org.apache.hadoop.ipc.Schedulable),301,304,"/**
 * Delegates to m1 with the 'enable' flag set to true.
 * @param e The element to process.
 * @return Result of the m1 method.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,callQueueLength,org.apache.hadoop.ipc.metrics.RpcMetrics:callQueueLength(),167,169,"/**
* Returns the length of the call queue from the server.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,initialize,org.apache.hadoop.ipc.WritableRpcEngine:initialize(),80,84,"/**
 * Initializes the RPC invoker and sets the initialization flag.
 */",* Register the rpcRequest deserializer for WritableRpcEngine,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,registerProtocolEngine,org.apache.hadoop.ipc.ProtobufRpcEngine2:registerProtocolEngine(),70,77,"/**
 * Registers RPC protocol buffer invoker if not already registered.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,setExpirationTime,"org.apache.hadoop.util.LightWeightCache:setExpirationTime(org.apache.hadoop.util.LightWeightCache$Entry,long)",147,149,"/**
* Sets the expiration time on the given entry.
* @param e Entry to update.
* @param expirationPeriod Expiration time in milliseconds.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,start,org.apache.hadoop.util.StopWatch:start(),58,65,"/**
 * Starts the stopwatch. Throws exception if already running.
 * Returns the StopWatch instance for chaining.
 */","* Start to measure times and make the state of stopwatch running.
   * @return this instance of StopWatch.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,stop,org.apache.hadoop.util.StopWatch:stop(),71,79,"/**
 * Stops the stopwatch and returns the elapsed time.
 * Returns this StopWatch instance.
 */
","* Stop elapsed time and make the state of stopwatch stop.
   * @return this instance of StopWatch.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,now,org.apache.hadoop.util.StopWatch:now(),105,109,"/**
* Returns elapsed time, considering if the timer is started.
*/",* @return current elapsed time in nanosecond.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,sendResponse,org.apache.hadoop.ipc.Server$Call:sendResponse(),1086,1094,"/**
 * Processes a response, potentially sending a null response.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,abortResponse,org.apache.hadoop.ipc.Server$Call:abortResponse(java.lang.Throwable),1096,1103,"/**
 * Handles throwable if response wait count is positive.
 * @param t The throwable to handle.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getFingerprint,org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(java.lang.reflect.Method[]),140,142,"/**
* Delegates to a helper method after processing the input array.
*/","* Get the hash code of an array of methods
   * Methods are sorted before hashcode is calculated.
   * So the returned value is irrelevant of the method order in the array.
   * 
   * @param methods an array of methods
   * @return the hash code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getSigFingerprint,"org.apache.hadoop.ipc.ProtocolSignature:getSigFingerprint(java.lang.Class,long)",186,200,"/**
 * Retrieves or creates a ProtocolSigFingerprint for the given protocol and server version.
 */","* Return a protocol's signature and finger print from cache
   * 
   * @param protocol a protocol class
   * @param serverVersion protocol version
   * @return its signature and finger print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,valueOf,org.apache.hadoop.ipc.RemoteException:valueOf(org.xml.sax.Attributes),131,134,"/**
 * Creates a RemoteException with class and message from attributes.
 */","* Create RemoteException from attributes.
   * @param attrs may not be null.
   * @return RemoteException.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,readResponse,org.apache.hadoop.ipc.Client$IpcStreams:readResponse(),1922,1944,"/**
* Reads a ByteBuffer from the input stream, validating its length.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldFailoverOnException,org.apache.hadoop.io.retry.RetryPolicies:shouldFailoverOnException(java.lang.Exception),773,781,"/**
 * Checks if an exception is a StandbyException after unwrapping.
 * @param e The exception to check.
 * @return True if unwrapped exception is a StandbyException.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getWrappedRetriableException,org.apache.hadoop.io.retry.RetryPolicies:getWrappedRetriableException(java.lang.Exception),795,803,"/**
 * Extracts a RetriableException from a RemoteException, if present.
 * @param e The exception to check.
 * @return RetriableException or null if not found.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,monitorHealth,"org.apache.hadoop.ha.HAServiceProtocolHelper:monitorHealth(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",34,42,"/**
 * Calls svc.m2() and handles RemoteException, wrapping it.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToActive,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",44,52,"/**
 * Calls svc.m2 and handles RemoteException, wrapping it.
 * @param svc HAServiceProtocol instance
 * @param reqInfo StateChangeRequestInfo object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToStandby,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",54,62,"/**
 * Calls svc.m2 and wraps RemoteException as ServiceFailedException.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToObserver,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",64,71,"/**
 * Delegates m2 call to service, handling RemoteException.
 * @param svc Service object.
 * @param reqInfo Request information.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,isHealthCheckFailedException,org.apache.hadoop.ha.HealthMonitor:isHealthCheckFailedException(java.lang.Throwable),229,235,"/**
 * Checks if throwable is a HealthCheckFailedException or related.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,get,org.apache.hadoop.fs.PartialListing:get(),67,72,"/**
* Returns a partial listing, throwing an exception if present.
*/","* Partial listing of the path being listed. In the case where the path is
   * a file. The list will be a singleton with the file itself.
   *
   * @return Partial listing of the path being listed.
   * @throws IOException if there was an exception getting the listing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,org.apache.hadoop.ipc.Server$Call:<init>(org.apache.hadoop.ipc.Server$Call),985,988,"/**
 * Constructs a Call object using values from the provided Call.
 * @param call The Call object to copy values from.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[])",990,992,"/**
 * Calls RPC with given ID, retry count, kind, and client ID.
 * Delegates to overloaded constructor with null args.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,java.lang.Void,java.lang.Void,org.apache.hadoop.ipc.RPC$RpcKind,byte[])",994,998,"/**
 * Constructs a Call with provided parameters, uses default ignores.
 * @param id Call identifier, retryCount retry attempts, kind RPC kind, clientId client ID.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcScheduler.java,addResponseTime,"org.apache.hadoop.ipc.RpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",65,78,"/**
 * Records processing details for a schedulable task.
 * @param callName The name of the call.
 * @param schedulable The schedulable object.
 * @param details Processing details object.
 */
","* Store a processing time value for an RPC call into this scheduler.
   *
   * @param callName The name of the call.
   * @param schedulable The schedulable representing the incoming call.
   * @param details The details of processing time.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numDroppedConnections,org.apache.hadoop.ipc.metrics.RpcMetrics:numDroppedConnections(),171,173,"/**
* Returns the number of dropped connections from the server.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,register,"org.apache.hadoop.ipc.Server$ConnectionManager:register(java.nio.channels.SocketChannel,int,boolean)",4097,4108,"/**
 * Establishes a new connection.
 * @param channel SocketChannel to use.
 * @param ingressPort Port for incoming connections.
 * @param isOnAuxiliaryPort Flag indicating auxiliary port usage.
 * @return New Connection object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numOpenConnections,org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnections(),153,155,"/**
* Returns the number of open connections from the server.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offerQueues,"org.apache.hadoop.ipc.FairCallQueue:offerQueues(int,org.apache.hadoop.ipc.Schedulable,boolean)",257,267,"/**
 * Checks if an element 'e' can be processed at a given priority.
 * @param priority Priority level to check.
 * @param e Element to process.
 * @param includeLast Whether to include the last priority.
 */
","* Offer the element to queue of the given or lower priority.
   * @param priority - starting queue priority
   * @param e - element to add
   * @param includeLast - whether to attempt last queue
   * @return boolean if added to a queue",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,"org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)",269,279,"/**
 * Offers an element to a queue with a timeout.
 * @param e element to offer, priority from e.m1()
 * @param timeout timeout value
 * @param unit timeout unit
 * @return True if offer successful, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object),281,290,"/**
 * Adds element to the queue with matching priority.
 * @param e element to add; priority obtained via e.m1()
 * @return True if added successfully, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,populateResponseParamsOnError,"org.apache.hadoop.ipc.Server$RpcCall:populateResponseParamsOnError(java.lang.Throwable,org.apache.hadoop.ipc.Server$RpcCall$ResponseParams)",1279,1302,"/**
* Handles exceptions, populates response params with error details.
*/","* @param t              the {@link java.lang.Throwable} to use to set
     *                       errorInfo
     * @param responseParams the {@link ResponseParams} instance to populate",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolVersionForRpcKind,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersionForRpcKind(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",106,120,"/**
 * Extracts protocol versions based on RPC kind and protocol.
 * @param rpcKind RPC kind.
 * @param protocol Protocol name.
 * @return Array of protocol versions.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcNoSuchProtocolException.java,<init>,org.apache.hadoop.ipc.RpcNoSuchProtocolException:<init>(java.lang.String),29,31,"/**
 * Constructs a NoSuchProtocolException with the given error message.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcNoSuchMethodException.java,<init>,org.apache.hadoop.ipc.RpcNoSuchMethodException:<init>(java.lang.String),30,32,"/**
 * Constructs a RpcNoSuchMethodException with the given error message.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,<init>,"org.apache.hadoop.ipc.RPC$VersionMismatch:<init>(java.lang.String,long,long)",248,255,"/**
 * Constructs a VersionMismatch exception with interface name & versions.
 * @param interfaceName Interface name.
 * @param clientVersion Client version.
 * @param serverVersion Server version.
 */
","* Create a version mismatch exception
     * @param interfaceName the name of the protocol mismatch
     * @param clientVersion the client's version of the protocol
     * @param serverVersion the server's version of the protocol",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.io.IOException)",1986,1989,"/**
 * Constructs a FatalRpcServerException with error code and IO exception.
 * @param errCode The RPC error code.
 * @param ioe The underlying IO exception.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:<init>(int),78,81,"/**
 * Creates a FramedBuffer with specified capacity.
 * @param capacity initial buffer capacity
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,reset,org.apache.hadoop.ipc.ResponseBuffer:reset(),70,74,"/**
* Resets the buffer and calls the FramedBuffer's m1().
* Returns this ResponseBuffer object.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,writeTo,org.apache.hadoop.ipc.ResponseBuffer:writeTo(java.io.OutputStream),48,50,"/**
* Delegates writing to an OutputStream.
* @param out The OutputStream to write to.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,toByteArray,org.apache.hadoop.ipc.ResponseBuffer:toByteArray(),52,54,"/**
* Delegates to m1().m2() and returns the resulting byte array.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,recomputeScheduleCache,org.apache.hadoop.ipc.DecayRpcScheduler:recomputeScheduleCache(),545,560,"/**
 * Updates the cache schedule with computed levels for each ID.
 */",* Update the scheduleCache to match current conditions in callCosts.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,cachedOrComputedPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:cachedOrComputedPriorityLevel(java.lang.Object),642,661,"/**
* Retrieves priority based on cache or cost.
* @param identity Object used as key for priority lookup.
* @return Priority integer value.
*/","* Returns the priority level for a given identity by first trying the cache,
   * then computing it.
   * @param identity an object responding to toString and hashCode
   * @return integer scheduling decision from 0 to numLevels - 1",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,setPriorityLevel,"org.apache.hadoop.ipc.CallQueueManager:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",272,276,"/**
 * Delegates m1 to DecayRpcScheduler if scheduler is an instance.
 * @param user UserGroupInformation object
 * @param priority Priority value to pass to the scheduler.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getCallVolumeSummary,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getCallVolumeSummary(),914,922,"/**
 * Delegates to the delegate's m2() method.
 * Returns ""No Active Scheduler"" if delegate's m1() returns null.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,constructRpcRequest,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:constructRpcRequest(java.lang.reflect.Method,com.google.protobuf.Message)",296,299,"/**
 * Creates a RpcProtobufRequest with header and request.
 * @param method Method to use for request header.
 * @param theRequest The request message.
 * @return RpcProtobufRequest object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,constructRpcRequest,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:constructRpcRequest(java.lang.reflect.Method,org.apache.hadoop.thirdparty.protobuf.Message)",306,309,"/**
 * Creates a RpcProtobufRequest with header and request.
 * @param method The method to use.
 * @param theRequest The request message.
 * @return A RpcProtobufRequest object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,refreshServiceAcl,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:refreshServiceAcl(),54,58,"/**
* Calls m2 with a refresh service ACL request via rpcProxy.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,refreshUserToGroupsMappings,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshUserToGroupsMappings(),59,63,"/**
 * Calls m2 with a request to refresh user-to-groups mapping.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshSuperUserGroupsConfiguration(),65,69,"/**
 * Calls m2 with a request to refresh superuser group configurations.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,refreshCallQueue,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:refreshCallQueue(),54,58,"/**
 * Calls m2 with a refresh queue request via the RPC proxy.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,getGroupsForUser,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:getGroupsForUser(java.lang.String),52,59,"/**
 * Retrieves group names for a user.
 * @param user User identifier.
 * @return Array of group names.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,cedeActive,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:cedeActive(int),56,63,"/**
 * Sends a cede active request via RPC.
 * @param millisToCede Milliseconds for the cede request.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,gracefulFailover,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:gracefulFailover(),65,69,"/**
 * Calls m3 with a request to rpcProxy.m2, handling exceptions.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,monitorHealth,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:monitorHealth(),84,87,"/**
* Calls m2 with a supplier that invokes rpcProxy.m1.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,getServiceStatus,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:getServiceStatus(),114,128,"/**
 * Retrieves HA service status using RPC call.
 * Returns HAServiceStatus object with status details.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeMethod,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod(),165,171,"/**
* Executes a method call, potentially with RPC retry logic.
* Returns the result of the method execution.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,close,org.apache.hadoop.ipc.WritableRpcEngine$Invoker:close(),264,270,"/**
 * Closes the connection if not already closed, then calls m1.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,close,org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:close(),325,331,"/**
* Closes the client connection if not already closed.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,close,org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:close(),335,341,"/**
 * Closes the client connection if not already closed.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getMetrics,"org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",462,477,"/**
 * Records FairCallQueue metrics (sizes and overflowed calls).
 * @param collector MetricsCollector to record to.
 * @param all Whether to record all metrics.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,wrap,org.apache.hadoop.ipc.RpcWritable:wrap(java.lang.Object),40,53,"/**
 * Wraps an object into an RpcWritable. Handles Message, Writable.
 * @param o Object to wrap; must be RpcWritable, Message, or Writable.
 * @return RpcWritable object or throws IllegalArgumentException.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,hashCode,org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:hashCode(),174,177,"/**
* Calls the m1 method of the superclass and returns its result.
*/",Override hashcode to avoid findbugs warnings,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,getAndAdvanceCurrentIndex,org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getAndAdvanceCurrentIndex(),145,149,"/**
* Returns a value based on m1() and m2() calls.
* Uses m1() to get an index, then calls m2().
*/",* Use the mux by getting and advancing index.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,registerForDeferredResponse,org.apache.hadoop.ipc.ProtobufRpcEngine$Server:registerForDeferredResponse(),421,426,"/**
 * Creates and initializes a ProtobufRpcEngineCallback instance.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,registerForDeferredResponse2,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:registerForDeferredResponse2(),453,458,"/**
 * Creates and initializes a ProtobufRpcEngineCallback2 instance.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,writeTo,org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:writeTo(org.apache.hadoop.ipc.ResponseBuffer),110,116,"/**
 * Writes the message to the buffer, including size prefix.
 * @param out ResponseBuffer to write the message to.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,writeTo,org.apache.hadoop.ipc.RpcWritable$Buffer:writeTo(org.apache.hadoop.ipc.ResponseBuffer),159,163,"/**
* Writes data to the response buffer.
* Uses data from the buffer 'bb' for writing.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufWrapperLegacy.java,writeTo,org.apache.hadoop.ipc.ProtobufWrapperLegacy:writeTo(org.apache.hadoop.ipc.ResponseBuffer),63,70,"/**
 * Serializes the message to the ResponseBuffer, calculating length.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,cleanupCalls,org.apache.hadoop.ipc.Client$Connection:cleanupCalls(),1307,1314,"/**
* Iterates through calls, notifies, and handles exceptions.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteAddress,org.apache.hadoop.ipc.Server:getRemoteAddress(),436,439,"/**
* Gets address string. Returns null if address is null.
*/","@return Returns remote address as a string when invoked inside an RPC.
   *  Returns null in case of an error.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numOpenConnectionsPerUser,org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnectionsPerUser(),162,165,"/**
* Delegates to server.m1() and returns the result.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doAsyncWrite,org.apache.hadoop.ipc.Server$Responder:doAsyncWrite(java.nio.channels.SelectionKey),1798,1821,"/**
* Processes an RPC call, ensuring channel consistency and handling ops.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRespond,org.apache.hadoop.ipc.Server$Responder:doRespond(org.apache.hadoop.ipc.Server$RpcCall),1927,1939,"/**
 * Processes an RPC call, potentially wrapping and queuing responses.
 * @param call The RPC call object to process.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsTracer.java,get,org.apache.hadoop.fs.FsTracer:get(org.apache.hadoop.conf.Configuration),39,47,"/**
 * Returns the Tracer instance, creating it if it doesn't exist.
 * @param conf Configuration object used for Tracer initialization.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,"org.apache.hadoop.util.MachineList:<init>(java.lang.String,org.apache.hadoop.util.MachineList$InetAddressFactory)",77,79,"/**
 * Creates a MachineList with trimmed host entries.
 * @param hostEntries Comma-separated host entries.
 * @param addressFactory Factory for creating InetAddresses.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,org.apache.hadoop.util.MachineList:<init>(java.util.Collection),85,87,"/**
 * Constructs a MachineList with the given host entries and default InetAddressFactory.
 * @param hostEntries Collection of host entries to initialize the list.
 */
","*
   * @param hostEntries collection of separated ip/cidr/host addresses",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,isIn,org.apache.hadoop.util.FileBasedIPList:isIn(java.lang.String),71,77,"/**
 * Checks if an IP address is in the address list.
 * @param ipAddress The IP address to check.
 * @return True if the IP is in the list, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,<init>,org.apache.hadoop.util.SysInfoLinux:<init>(),180,183,"/**
 * Default constructor, uses default file paths and jiffy length.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcMemInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(),215,217,"/**
 * Calls m1 with the default value of false.
 */","* Read /proc/meminfo, parse and compute memory information only once.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getAvailablePhysicalMemorySize,org.apache.hadoop.util.SysInfoLinux:getAvailablePhysicalMemorySize(),609,616,"/**
* Calculates available RAM size in KB. Uses inactiveFileSize if available.
*/",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCumulativeCpuTime,org.apache.hadoop.util.SysInfoLinux:getCumulativeCpuTime(),646,650,"/**
* Calls m1() and returns the result of cpuTimeTracker.m2().
*/",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCpuUsagePercentage,org.apache.hadoop.util.SysInfoLinux:getCpuUsagePercentage(),653,661,"/**
 * Calculates overall CPU usage, dividing by a factor.
 * Returns usage or UNAVAILABLE if not available.
 */",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumVCoresUsed,org.apache.hadoop.util.SysInfoLinux:getNumVCoresUsed(),664,672,"/**
 * Calculates and returns overall vCore usage, dividing by 100 if available.
 */",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getStorageBytesRead,org.apache.hadoop.util.SysInfoLinux:getStorageBytesRead(),688,692,"/**
* Returns the number of bytes read.
* Reads data and returns the byte count.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getStorageBytesWritten,org.apache.hadoop.util.SysInfoLinux:getStorageBytesWritten(),694,698,"/**
* Returns the number of bytes written.
* Increments m1() before returning the value.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,<init>,org.apache.hadoop.util.IdentityHashStore:<init>(int),62,72,"/**
 * Initializes the IdentityHashStore with a given capacity.
 * @param capacity Initial capacity of the store (must be >= 0)
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,put,"org.apache.hadoop.util.IdentityHashStore:put(java.lang.Object,java.lang.Object)",118,126,"/**
 * Inserts a key-value pair into the buffer, resizing if needed.
 * @param k The key to insert.
 * @param v The value associated with the key.
 */
","* Add a new (key, value) mapping.
   *
   * Inserting a new (key, value) never overwrites a previous one.
   * In other words, you can insert the same key multiple times and it will
   * lead to multiple entries.
   *
   * @param k Generics Type k.
   * @param v Generics Type v.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,releaseBuffer,org.apache.hadoop.fs.FSDataInputStream:releaseBuffer(java.nio.ByteBuffer),222,235,"/**
 * Delegates buffer handling, releases if not stream-created.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,hasNext,org.apache.hadoop.util.LightWeightGSet$SetIterator:hasNext(),329,333,"/**
* Checks if the next element exists. Calls m1() first.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,next,org.apache.hadoop.util.LightWeightGSet$SetIterator:next(),335,344,"/**
 * Retrieves the next element and advances the iterator.
 * Throws exception if no more elements are available.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,put,org.apache.hadoop.util.LightWeightGSet:put(java.lang.Object),149,177,"/**
* Replaces an element in the list.
* @param element The element to replace, must be a LinkedElement.
* @return The previous element at the replaced index or null.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,org.apache.hadoop.util.LightWeightGSet:remove(java.lang.Object),221,228,"/**
 * Retrieves a value associated with the given key.
 * @param key The key to look up. Throws NullPointerException if null.
 * @return The value associated with the key.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:sort(int),3253,3256,"/**
 * Calls m1 and m2 to process data.
 * @param count number of elements to process
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,newSecureTransformerFactory,org.apache.hadoop.util.XMLUtils:newSecureTransformerFactory(),148,154,"/**
 * Creates a TransformerFactory with secure processing enabled.
 * @return TransformerFactory instance with secure processing on.
 */
","* This method should be used if you need a {@link TransformerFactory}. Use this method
   * instead of {@link TransformerFactory#newInstance()}. The factory that is returned has
   * secure configuration enabled.
   *
   * @return a {@link TransformerFactory} with secure configuration enabled
   * @throws TransformerConfigurationException if the {@code JAXP} transformer does not
   * support the secure configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,newSecureSAXTransformerFactory,org.apache.hadoop.util.XMLUtils:newSecureSAXTransformerFactory(),165,171,"/**
 * Creates and configures a SAXTransformerFactory.
 * Enables secure processing and applies custom settings.
 */
","* This method should be used if you need a {@link SAXTransformerFactory}. Use this method
   * instead of {@link SAXTransformerFactory#newInstance()}. The factory that is returned has
   * secure configuration enabled.
   *
   * @return a {@link SAXTransformerFactory} with secure configuration enabled
   * @throws TransformerConfigurationException if the {@code JAXP} transformer does not
   * support the secure configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,formatSize,"org.apache.hadoop.fs.ContentSummary:formatSize(long,boolean)",477,481,"/**
 * Formats size as a string. Human-readable if flag is true.
 * @param size The size in bytes.
 * @param humanReadable Flag to format as human-readable.
 */
","* Formats a size to be human readable or in bytes.
   * @param size value to be formatted
   * @param humanReadable flag indicating human readable or not
   * @return String representation of the size",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,formatSize,org.apache.hadoop.fs.shell.Ls:formatSize(long),126,130,"/**
* Formats size into human-readable string.
* @param size The size to format.
* @return Human-readable size string.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,formatSize,org.apache.hadoop.fs.shell.FsUsage:formatSize(long),55,59,"/**
 * Formats size to human-readable string.
 * @param size size in bytes, returns formatted string.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,formatSize,"org.apache.hadoop.fs.QuotaUsage:formatSize(long,boolean)",394,398,"/**
 * Formats size as string, human-readable if specified.
 * @param size Size in bytes.
 * @param humanReadable True for human-readable format.
 */
","* Formats a size to be human readable or in bytes.
   * @param size value to be formatted
   * @param humanReadable flag indicating human readable or not
   * @return String representation of the size",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,humanReadableInt,org.apache.hadoop.util.StringUtils:humanReadableInt(long),132,135,"/**
 * Converts a number to a string with a traditional binary prefix.
 * @param number The number to convert.
 * @return String representation with binary prefix.
 */","* Given an integer, return a string that is in an approximate, but human 
   * readable format. 
   * @param number the number to format
   * @return a human readable form of the integer
   *
   * @deprecated use {@link TraditionalBinaryPrefix#long2String(long, String, int)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteDesc,org.apache.hadoop.util.StringUtils:byteDesc(long),1022,1024,"/**
 * Converts a length to a binary prefix string (e.g., 1024 -> 1KiB).
 * @param len The length value to convert.
 * @return Binary prefix string representation.
 */","* a byte description of the given long interger value.
   *
   * @param len len.
   * @return a byte description of the given long interger value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,computeCapacity,"org.apache.hadoop.util.LightWeightGSet:computeCapacity(long,double,java.lang.String)",379,417,"/**
 * Calculates memory capacity based on maxMemory, percentage, and VM bit.
 * @param maxMemory Maximum memory in bytes.
 * @param percentage Percentage of maxMemory to use.
 * @param mapName Name of the map.
 * @return Calculated memory capacity.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,addToUsagesTable,"org.apache.hadoop.fs.shell.FsUsage$Df:addToUsagesTable(java.net.URI,org.apache.hadoop.fs.FsStatus,java.lang.String)",114,127,"/**
 * Reports filesystem status.
 * @param uri URI of the filesystem, fsStatus, mountedOnPath
 */","* Add a new row to the usages table for the given FileSystem URI.
     *
     * @param uri - FileSystem URI
     * @param fsStatus - FileSystem status
     * @param mountedOnPath - FileSystem mounted on path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,readChecksumChunk,"org.apache.hadoop.fs.FSInputChecker:readChecksumChunk(byte[],int,int)",293,334,"/**
 * Reads data into the byte array, handling checksum errors and retries.
 * @param b byte array to fill, off offset, len length, checksum checksum value
 * @return Number of bytes read or -1 if an error occurs.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,readChars,"org.apache.hadoop.io.UTF8:readChars(java.io.DataInput,java.lang.StringBuilder,int)",279,332,"/**
 * Decodes UTF8 data from input stream into a StringBuilder.
 * @param in Input stream containing UTF8 data.
 * @param buffer StringBuilder to append decoded characters.
 * @param nBytes Number of bytes to decode.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,org.apache.hadoop.util.StringUtils:byteToHexString(byte[]),199,201,"/**
* Calls overloaded method with full byte array.
* @param bytes The byte array to process.
*/","* Same as byteToHexString(bytes, 0, bytes.length).
   * @param bytes bytes.
   * @return byteToHexString.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HeapSort.java,sort,"org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)",51,54,"/**
 * Calls m1 with an additional null parameter.
 * @param s IndexedSortable object
 * @param p starting index
 * @param r ending index
 */
","* Sort the given range of items using heap sort.
   * {@inheritDoc}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exit,org.apache.hadoop.service.launcher.ServiceLauncher:exit(org.apache.hadoop.util.ExitUtil$ExitException),874,876,"/**
* Logs an ExitException using ExitUtil.
* @param ee The exception to log.
*/","* Exit the JVM using an exception for the exit code and message,
   * invoking {@link ExitUtil#terminate(ExitUtil.ExitException)}.
   *
   * This is the standard way a launched service exits.
   * An error code of 0 means success -nothing is printed.
   *
   * If {@link ExitUtil#disableSystemExit()} has been called, this
   * method will throw the exception.
   *
   * The method <i>may</i> be subclassed for testing
   * @param ee exit exception
   * @throws ExitUtil.ExitException if ExitUtil exceptions are disabled",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exitWithMessage,"org.apache.hadoop.service.launcher.ServiceLauncher:exitWithMessage(int,java.lang.String)",1024,1026,"/**
* Logs a ServiceLaunchException with the given status and message.
*/","* Exit with a printed message. 
   * @param status status code
   * @param message message message to print before exiting
   * @throws ExitUtil.ExitException if exceptions are disabled",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,"org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.Throwable)",338,344,"/**
 * Handles an exception, potentially wrapping it as an ExitException.
 * @param status Exit status code.
 * @param t The exception to handle.
 */
","* Like {@link #terminate(int, String)} but uses the given throwable to
   * build the message to display or throw as an
   * {@link ExitException}.
   * <p>
   * @param status exit code to use if the exception is not an ExitException.
   * @param t throwable which triggered the termination. If this exception
   * is an {@link ExitException} its status overrides that passed in.
   * @throws ExitException if {@link System#exit(int)}  is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,"org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.String)",380,382,"/**
* Throws an ExitException with the given status and message.
* @param status Exit status code.
* @param msg Error message.
*/
","* Terminate the current process. Note that terminate is the *only* method
   * that should be used to terminate the daemon processes.
   *
   * @param status exit code
   * @param msg message used to create the {@code ExitException}
   * @throws ExitException if {@link System#exit(int)} is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,"org.apache.hadoop.util.ExitUtil:halt(int,java.lang.Throwable)",354,360,"/**
 * Handles HaltException, recursively escalating if needed.
 * @param status Status code for the exception.
 * @param t The exception to handle.
 */","* Forcibly terminates the currently running Java virtual machine.
   *
   * @param status exit code to use if the exception is not a HaltException.
   * @param t throwable which triggered the termination. If this exception
   * is a {@link HaltException} its status overrides that passed in.
   * @throws HaltException if {@link System#exit(int)}  is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,"org.apache.hadoop.util.ExitUtil:halt(int,java.lang.String)",399,401,"/**
* Throws a HaltException with the given status and message.
*/","* Forcibly terminates the currently running Java virtual machine.
   * @param status status code
   * @param message message
   * @throws HaltException if {@link Runtime#halt(int)} is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,unregister,org.apache.hadoop.service.launcher.ServiceShutdownHook:unregister(),69,75,"/**
 * Unregisters the shutdown hook. Handles potential IllegalStateExceptions.
 */",* Unregister the hook.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sort,"org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)",63,67,"/**
 * Sorts a portion of the IndexedSortable using a recursive approach.
 * @param s sortable object, p start index, r end index, rep progress reporter
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,org.apache.hadoop.util.LightWeightResizableGSet:<init>(),83,85,"/**
 * Constructs a LightWeightResizableGSet with default capacity and load factor.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,org.apache.hadoop.util.LightWeightResizableGSet:<init>(int),87,89,"/**
 * Constructs a LightWeightResizableGSet with initial capacity.
 * @param initCapacity initial size of the backing store.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.lang.Iterable),91,98,"/**
 * Converts an iterable to an ArrayList. Handles null and iterables.
 */","* Creates a <i>mutable</i> {@code ArrayList} instance containing the
   * given elements; a very thin shortcut for creating an empty list then
   * calling Iterables#addAll.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newLinkedList,org.apache.hadoop.util.Lists:newLinkedList(java.lang.Iterable),185,190,"/**
* Creates a LinkedList from an Iterable of elements.
* @param elements Iterable source for LinkedList elements
* @return LinkedList containing elements from the Iterable
*/
","* Creates a <i>mutable</i> {@code LinkedList} instance containing the given
   * elements; a very thin shortcut for creating an empty list then calling
   * Iterables#addAll.
   *
   * <p><b>Performance note:</b> {@link ArrayList} and
   * {@link java.util.ArrayDeque} consistently
   * outperform {@code LinkedList} except in certain rare and specific
   * situations. Unless you have spent a lot of time benchmarking your
   * specific needs, use one of those instead.</p>
   *
   * @param elements elements.
   * @param <E> Generics Type E.
   * @return Generics Type E List.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclUtil.java,getAclFromPermAndEntries,"org.apache.hadoop.fs.permission.AclUtil:getAclFromPermAndEntries(org.apache.hadoop.fs.permission.FsPermission,java.util.List)",42,90,"/**
* Masks ACL entries based on permission and existing entries.
* @param perm FsPermission to apply.
* @param entries Existing ACL entries to mask.
* @return Masked list of ACL entries.
*/","* Given permissions and extended ACL entries, returns the full logical ACL.
   *
   * @param perm FsPermission containing permissions
   * @param entries List&lt;AclEntry&gt; containing extended ACL entries
   * @return List&lt;AclEntry&gt; containing full logical ACL",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,addChunk,org.apache.hadoop.util.ChunkedArrayList:addChunk(int),156,160,"/**
 * Initializes a new chunk with the given capacity.
 * @param capacity The size of the new chunk.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.lang.Object[]),70,80,"/**
 * Creates an ArrayList from a variable number of elements.
 * @param elements elements to add to the list
 * @return ArrayList containing the provided elements
 */
","* Creates a <i>mutable</i> {@code ArrayList} instance containing the given
   * elements.
   *
   * <p>Note that even when you do need the ability to add or remove,
   * this method provides only a tiny bit of syntactic sugar for
   * {@code newArrayList(}
   * {@link Arrays#asList asList}
   * {@code (...))}, or for creating an empty list then calling
   * {@link Collections#addAll}.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayListWithExpectedSize,org.apache.hadoop.util.Lists:newArrayListWithExpectedSize(int),148,151,"/**
* Creates an ArrayList with the estimated size.
* @param estimatedSize Initial capacity of the ArrayList.
*/","* Creates an {@code ArrayList} instance to hold {@code estimatedSize}
   * elements, <i>plus</i> an unspecified amount of padding;
   * you almost certainly mean to call {@link
   * #newArrayListWithCapacity} (see that method for further advice on usage).
   *
   * @param estimatedSize an estimate of the eventual {@link List#size()}
   *     of the new list.
   * @return a new, empty {@code ArrayList}, sized appropriately to hold the
   *     estimated number of elements.
   * @throws IllegalArgumentException if {@code estimatedSize} is negative.
   *
   * @param <E> Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,loadClass,org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String),155,158,"/**
* Loads a class by name. Overloads m1(String, boolean).
* @param name The name of the class to load.
* @return The Class object or null if not found.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,save,"org.apache.hadoop.util.JsonSerialization:save(java.io.File,java.lang.Object)",201,204,"/**
* Processes a file using an instance.
* @param file Input file to process.
* @param instance Instance used for processing.
*/
","* Save to a local file. Any existing file is overwritten unless
   * the OS blocks that.
   * @param file file
   * @param instance instance
   * @throws IOException IO exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,<init>,"org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String,long)",72,81,"/**
 * Initializes a duration tracker with a key and initial count.
 * @param iostats Store for IO statistics.
 * @param key Identifier for the tracker.
 * @param count Initial counter value.
 */
","* Constructor.
   * If the supplied count is greater than zero, the counter
   * of the key name is updated.
   * @param iostats statistics to update
   * @param key Key to use as prefix of values.
   * @param count #of times to increment the matching counter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,<init>,"org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])",69,83,"/**
 * Creates a DurationInfo object, logs start message based on config.
 * @param log Logger instance
 * @param logAtInfo Whether to log at info level
 * @param format Format string for the duration text
 * @param args Arguments for the format string
 */
","* Create the duration text from a {@code String.format()} code call
   * and log either at info or debug.
   * @param log log to write to
   * @param logAtInfo should the log be at info, rather than debug
   * @param format format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,toString,org.apache.hadoop.util.OperationDuration:toString(),91,94,"/**
* Delegates to the m1() method and returns its result.
*/","* Return the duration as {@link #humanTime(long)}.
   * @return a printable duration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,newStripedCrcComposer,"org.apache.hadoop.util.CrcComposer:newStripedCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long,long)",83,92,"/**
 * Creates a CrcComposer with specified type, hint size, and stripe length.
 */","* Returns a CrcComposer which will collapse CRCs for every combined
   * underlying data size which aligns with the specified stripe boundary. For
   * example, if ""update"" is called with 20 CRCs and bytesPerCrc == 5, and
   * stripeLength == 10, then every two (10 / 5) consecutive CRCs will be
   * combined with each other, yielding a list of 10 CRC ""stripes"" in the
   * final digest, each corresponding to 10 underlying data bytes. Using
   * a stripeLength greater than the total underlying data size is equivalent
   * to using a non-striped CrcComposer.
   *
   * @param type type.
   * @param bytesPerCrcHint bytesPerCrcHint.
   * @param stripeLength stripeLength.
   * @return a CrcComposer which will collapse CRCs for every combined.
   * underlying data size which aligns with the specified stripe boundary.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,compose,"org.apache.hadoop.util.CrcUtil:compose(int,int,long,int)",102,105,"/**
 * Calculates a masked CRC value.
 * @param crcA, crcB CRC values, lengthB length, mod modulus.
 */","* compose.
   *
   * @param crcA crcA.
   * @param crcB crcB.
   * @param lengthB length of content corresponding to {@code crcB}, in bytes.
   * @param mod mod.
   * @return compose result.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,getBytes,org.apache.hadoop.fs.CompositeCrcFileChecksum:getBytes(),64,67,"/**
* Calculates the CRC checksum using CrcUtil.
* @return CRC checksum as a byte array.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,digest,org.apache.hadoop.util.CrcComposer:digest(),204,213,"/**
 * Calculates and returns a digest based on stripe data.
 * Resets composite CRC and position after calculation.
 */","* Returns byte representation of composed CRCs; if no stripeLength was
   * specified, the digest should be of length equal to exactly one CRC.
   * Otherwise, the number of CRCs in the returned array is equal to the
   * total sum bytesPerCrc divided by stripeLength. If the sum of bytesPerCrc
   * is not a multiple of stripeLength, then the last CRC in the array
   * corresponds to totalLength % stripeLength underlying data bytes.
   *
   * @return byte representation of composed CRCs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJarAndSave,"org.apache.hadoop.util.RunJar:unJarAndSave(java.io.InputStream,java.io.File,java.lang.String,java.util.regex.Pattern)",168,178,"/**
 * Extracts a file from an input stream to a directory.
 * @param inputStream Source input stream
 * @param toDir Destination directory
 * @param name File name
 * @param unpackRegex Regex for unpacking (optional)
 */","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped. Keep also a copy
   * of the entire jar in the same directory for backward compatibility.
   * TODO remove this feature in a new release and do only unJar
   *
   * @param inputStream the jar stream to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   * @param name name.
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File)",104,106,"/**
* Delegates to overloaded method with default match filter.
* @param jarFile Input JAR file.
* @param toDir Output directory.
*/","* Unpack a jar file into a directory.
   *
   * This version unpacks all files inside the jar regardless of filename.
   *
   * @param jarFile the .jar file to unpack
   * @param toDir the destination directory into which to unpack the jar
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/UTF8ByteArrayUtils.java,findNthByte,"org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],byte,int)",98,100,"/**
* Overloaded method to call the recursive implementation.
* @param utf byte array, b byte value, n int value
*/
","* Find the nth occurrence of the given byte b in a UTF-8 encoded string
   * @param utf a byte array containing a UTF-8 encoded string
   * @param b the byte to find
   * @param n the desired occurrence of the given byte
   * @return position that nth occurrence of the given byte if exists; otherwise -1",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,get,org.apache.hadoop.util.WeakReferenceMap:get(java.lang.Object),147,171,"/**
* Retrieves a value by key, potentially creating/reclaiming it.
*/","* Get the value, creating if needed.
   * @param key key.
   * @return an instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,check,"org.apache.hadoop.util.InstrumentedLock:check(long,long,boolean)",190,226,"/**
 * Logs lock held/wait time if exceeding threshold, avoids spamming.
 * @param acquireTime Lock acquisition timestamp.
 * @param releaseTime Lock release timestamp.
 * @param checkLockHeld Flag to check if lock was held.
 */","* Log a warning if the lock was held for too long.
   *
   * Should be invoked by the caller immediately AFTER releasing the lock.
   *
   * @param acquireTime  - timestamp just after acquiring the lock.
   * @param releaseTime - timestamp just before releasing the lock.
   * @param checkLockHeld checkLockHeld.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getFormattedTimeWithDiff,"org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(java.lang.String,long,long)",390,400,"/**
 * Constructs a formatted string including finish time and duration.
 * @param formattedFinishTime Finish time string.
 * @param finishTime Finish time in milliseconds.
 * @param startTime Start time in milliseconds.
 */","* Formats time in ms and appends difference (finishTime - startTime)
   * as returned by formatTimeDiff().
   * If finish time is 0, empty string is returned, if start time is 0
   * then difference is not appended to return value.
   * @param formattedFinishTime formattedFinishTime to use
   * @param finishTime finish time
   * @param startTime start time
   * @return formatted value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,split,org.apache.hadoop.util.StringUtils:split(java.lang.String),570,572,"/**
* Splits a string by a delimiter, escaping a character.
* @param str The string to split.
*/
","* Split a string using the default separator
   * @param str a string that may have escaped separator
   * @return an array of strings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,camelize,org.apache.hadoop.util.StringUtils:camelize(java.lang.String),1094,1102,"/**
* Masks a string by replacing characters with underscores.
* @param s The string to mask.
* @return The masked string.
*/
","* Convert SOME_STUFF to SomeStuff
   *
   * @param s input string
   * @return camelized string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,"org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char)",678,681,"/**
* Escapes characters in a string using a custom escape character.
* @param str String to escape
* @param escapeChar Escape character
* @param charToEscape Character to escape
*/","* Escape <code>charToEscape</code> in the string 
   * with the escape char <code>escapeChar</code>
   * 
   * @param str string
   * @param escapeChar escape char
   * @param charToEscape the char to be escaped
   * @return an escaped string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,"org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char)",736,739,"/**
* Escapes characters in a string using a custom escape character.
* @param str String to escape.
* @param escapeChar Escape character.
* @param charToEscape Character to escape.
*/
","* Unescape <code>charToEscape</code> in the string 
   * with the escape char <code>escapeChar</code>
   * 
   * @param str string
   * @param escapeChar escape char
   * @param charToEscape the escaped char
   * @return an unescaped string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,createStartupShutdownMessage,"org.apache.hadoop.util.StringUtils:createStartupShutdownMessage(java.lang.String,java.lang.String,java.lang.String[])",837,851,"/**
* Constructs a startup message string with class, host, args, version info.
*/","* Generate the text for the startup/shutdown message of processes.
   * @param classname short classname of the class
   * @param hostname hostname
   * @param args Command arguments
   * @return a string to log.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getBuildVersion,org.apache.hadoop.util.VersionInfo:getBuildVersion(),162,164,"/**
* Returns a masked value from the COMMON_VERSION_INFO object.
*/","* Returns the buildVersion which includes version,
   * revision, user and date.
   * @return the buildVersion",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:next(),650,658,"/**
* Returns the next element if available, otherwise throws exception.
*/","* Return the next value.
     * Will retrieve the next elements if needed.
     * This is where the mapper takes place.
     * @return true if there is another data element.
     * @throws IOException failure in fetch operation or the transformation.
     * @throws NoSuchElementException no more data",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:close(),695,707,"/**
 * Closes the resource, logs the action, and closes associated resources.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceHasNext,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceHasNext(),466,479,"/**
 * Checks for next element, handles IO errors, and calls m4 if absent.
 */","* Check for the source having a next element.
     * If it does not, this object's close() method
     * is called and false returned
     * @return true if there is a new value
     * @throws IOException failure to retrieve next value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,lazyAutoCloseablefromSupplier,org.apache.hadoop.util.functional.LazyAutoCloseableReference:lazyAutoCloseablefromSupplier(java.util.function.Supplier),99,101,"/**
 * Creates a LazyAutoCloseableReference from a supplier.
 * @param supplier Supplier of AutoCloseable resource.
 * @return LazyAutoCloseableReference instance.
 */","* Create from a supplier.
   * This is not a constructor to avoid ambiguity when a lambda-expression is
   * passed in.
   * @param supplier supplier implementation.
   * @return a lazy reference.
   * @param <T> type of reference",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,bulkDelete_pageSize,"org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",72,79,"/**
 * Deletes files/directories recursively using BulkDelete.
 * @param fs FileSystem object
 * @param path Path to delete
 * @return Number of deleted items
 */
","* Get the maximum number of objects/files to delete in a single request.
   * @param fs filesystem
   * @param path path to delete under.
   * @return a number greater than or equal to zero.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException path not valid.
   * @throws UncheckedIOException if an IOE was raised.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,bulkDelete_delete,"org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)",104,113,"/**
 * Deletes paths using a bulk delete operation.
 * @param fs FileSystem object
 * @param base Base path for deletion
 * @param paths Paths to delete
 * @return List of deleted paths
 */
","* Delete a list of files/objects.
   * <ul>
   *   <li>Files must be under the path provided in {@code base}.</li>
   *   <li>The size of the list must be equal to or less than the page size.</li>
   *   <li>Directories are not supported; the outcome of attempting to delete
   *       directories is undefined (ignored; undetected, listed as failures...).</li>
   *   <li>The operation is not atomic.</li>
   *   <li>The operation is treated as idempotent: network failures may
   *        trigger resubmission of the request -any new objects created under a
   *        path in the list may then be deleted.</li>
   *    <li>There is no guarantee that any parent directories exist after this call.
   *    </li>
   * </ul>
   * @param fs filesystem
   * @param base path to delete under.
   * @param paths list of paths which must be absolute and under the base path.
   * @return a list of all the paths which couldn't be deleted for a reason other
   *          than ""not found"" and any associated error message.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws UncheckedIOException if an IOE was raised.
   * @throws IllegalArgumentException if a path argument is invalid.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,fileSystem_openFile,"org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)",165,191,"/**
 * Opens a data input stream with specified policy, status, length, options.
 * @param fs Filesystem object
 * @param path Path to open
 * @return FSDataInputStream object
 */","* OpenFile assistant, easy reflection-based access to
   * {@link FileSystem#openFile(Path)} and blocks
   * awaiting the operation completion.
   * @param fs filesystem
   * @param path path
   * @param policy read policy
   * @param status optional file status
   * @param length optional file length
   * @param options nullable map of other options
   * @return stream of the opened file
   * @throws UncheckedIOException if an IOE was raised.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,byteBufferPositionedReadable_readFully,"org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)",213,224,"/**
* Reads data into a buffer at a specified position from an input stream.
*/","* Delegate to {@link ByteBufferPositionedReadable#read(long, ByteBuffer)}.
   * @param in input stream
   * @param position position within file
   * @param buf the ByteBuffer to receive the results of the read operation.
   * Note: that is the default behaviour of {@link FSDataInputStream#readFully(long, ByteBuffer)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_load,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",134,139,"/**
 * Retrieves IO statistics snapshot for a given file system path.
 * @param fs file system
 * @param path path to snapshot
 * @return IOStatisticsSnapshot object
 */
","* Load IOStatisticsSnapshot from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return the loaded snapshot
   * @throws UncheckedIOException Any IO exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_fromJsonString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String),192,196,"/**
* Creates an IOStatisticsSnapshot from JSON.
* @param json JSON string containing snapshot data.
* @return IOStatisticsSnapshot object.
*/
","* Load IOStatisticsSnapshot from a JSON string.
   * @param json JSON string value.
   * @return deserialized snapshot.
   * @throws UncheckedIOException Any IO/jackson exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,get,org.apache.hadoop.util.functional.LazyAtomicReference:get(),120,123,"/**
* Delegates evaluation to m1, providing this object as the evaluator.
*/","* Implementation of {@code Supplier.get()}.
   * <p>
   * Invoke {@link #eval()} and convert IOEs to
   * UncheckedIOException.
   * <p>
   * This is the {@code Supplier.get()} implementation, which allows
   * this class to passed into anything taking a supplier.
   * @return the value
   * @throws UncheckedIOException if the constructor raised an IOException.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Iterable),581,583,"/**
 * Creates a Builder for masking items from an iterable.
 * @param items Iterable of items to be masked.
 * @return Builder object for masking.
 */
","* Create a task builder for the iterable.
   * @param items item source.
   * @param <I> type of result.
   * @return builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Object[]),595,597,"/**
 * Creates a Builder with a masked array of items.
 * @param items The input array to be masked.
 * @return A Builder object initialized with the masked array.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,raiseInnerCause,org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.ExecutionException),106,110,"/**
 * Delegates to FutureIO.m1, throwing IOException.
 * @param e The ExecutionException to delegate.
 * @return The result of FutureIO.m1.
 */
","* From the inner cause of an execution exception, extract the inner cause
   * if it is an IOE or RTE.
   * See {@link FutureIO#raiseInnerCause(ExecutionException)}.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitFuture,org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future),97,110,"/**
 * Retrieves the result from a Future, handling exceptions.
 * @param future Future object to retrieve the result from.
 * @return Result of the Future or handles exceptions.
 */
","* Given a future, evaluate it.
   * <p>
   * Any exception generated in the future is
   * extracted and rethrown.
   * </p>
   * If this thread is interrupted while waiting for the future to complete,
   * an {@code InterruptedIOException} is raised.
   * However, if the future is cancelled, a {@code CancellationException}
   * is raised in the {code Future.get()} call. This is
   * passed up as is -so allowing the caller to distinguish between
   * thread interruption (such as when speculative task execution is aborted)
   * and future cancellation.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitFuture,"org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)",129,145,"/**
* Gets the result of a Future with a timeout.
* @param future Future to check.
* @param timeout Timeout duration.
* @param unit Timeout unit (e.g., SECONDS).
* @return Result of the Future.
* @throws TimeoutException if timeout expires.
*/
","* Given a future, evaluate it.
   * <p>
   * Any exception generated in the future is
   * extracted and rethrown.
   * </p>
   * @param future future to evaluate
   * @param timeout timeout to wait.
   * @param unit time unit.
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,raiseInnerCause,org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.CompletionException),123,127,"/**
 * Delegates to FutureIO.m1(e).
 * @param e CompletionException to delegate.
 * @return Result from FutureIO.m1(e).
 */
","* Extract the cause of a completion failure and rethrow it if an IOE
   * or RTE.
   * See {@link FutureIO#raiseInnerCause(CompletionException)}.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,setConf,"org.apache.hadoop.util.ReflectionUtils:setConf(java.lang.Object,org.apache.hadoop.conf.Configuration)",74,81,"/**
 * Delegates m1 and m2 to theObject if it's Configurable.
 * @param theObject Object to potentially delegate to.
 * @param conf Configuration object to pass.
 */
","* Check and set 'configuration' if necessary.
   * 
   * @param theObject object for which to set configuration
   * @param conf Configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableName.java,getClass,"org.apache.hadoop.io.WritableName:getClass(java.lang.String,org.apache.hadoop.conf.Configuration)",91,103,"/**
 * Retrieves a Class object by name, using NAME_TO_CLASS or config.
 * @param name Class name to retrieve.
 * @param conf Configuration object.
 * @return Class object or throws IOException if not found.
 */","* Return the class for a name.
   * Default is {@link Class#forName(String)}.
   *
   * @param name input name.
   * @param conf input configuration.
   * @return class for a name.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createCodec,"org.apache.hadoop.io.erasurecode.CodecUtil:createCodec(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",232,254,"/**
 * Creates and returns an ErasureCodec instance.
 * @param conf Configuration object.
 * @param codecClassName Class name of the codec.
 * @param options ErasureCodecOptions object.
 * @return An ErasureCodec instance.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,loadClass,"org.apache.hadoop.io.ObjectWritable:loadClass(org.apache.hadoop.conf.Configuration,java.lang.String)",412,424,"/**
 * Loads a class by name, using configuration if provided.
 * @param conf Configuration object, or null.
 * @param className Class name to load.
 * @return The loaded Class object.
 */","* Find and load the class with given name <tt>className</tt> by first finding
   * it in the specified <tt>conf</tt>. If the specified <tt>conf</tt> is null,
   * try load it directly.
   *
   * @param conf configuration.
   * @param className classname.
   * @return Class.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getProtocolClass,"org.apache.hadoop.ipc.Server:getProtocolClass(java.lang.String,org.apache.hadoop.conf.Configuration)",331,339,"/**
 * Retrieves protocol class by name, caching for efficiency.
 * @param protocolName Name of the protocol class.
 * @param conf Configuration object.
 * @return Class object representing the protocol.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,getClass,org.apache.hadoop.util.FindClass:getClass(java.lang.String),154,156,"/**
* Loads a class by name.
* @param name The name of the class to load.
* @return The loaded Class object.
*/
","* Get a class fromt the configuration
   * @param name the class name
   * @return the class
   * @throws ClassNotFoundException if the class was not found
   * @throws Error on other classloading problems",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,logThreadInfo,"org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.apache.commons.logging.Log,java.lang.String,long)",232,254,"/**
 * Logs a stack trace if the minimum interval has passed.
 * @param log Logger instance.
 * @param title Title for the log message.
 * @param minInterval Minimum time interval in seconds.
 */","* Log the current thread stacks at INFO level.
   * @param log the logger that logs the stack trace
   * @param title a descriptive title for the call stacks
   * @param minInterval the minimum time from the last
   * @deprecated to be removed with 3.4.0. Use {@link #logThreadInfo(Logger, String, long)} instead.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,logThreadInfo,"org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.slf4j.Logger,java.lang.String,long)",262,283,"/**
 * Logs a stack trace if the minimum interval has passed.
 * @param log Logger instance
 * @param title Stack trace title
 * @param minInterval Minimum time interval in seconds
 */","* Log the current thread stacks at INFO level.
   * @param log the logger that logs the stack trace
   * @param title a descriptive title for the call stacks
   * @param minInterval the minimum time from the last",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(java.util.Optional,java.util.Optional)",105,113,"/**
 * Creates a builder, ensuring only path or pathHandle is provided.
 * @param optionalPath Optional path to use.
 * @param optionalPathHandle Optional path handle to use.
 */
","* Constructor with both optional path and path handle.
   * Either or both argument may be empty, but it is an error for
   * both to be defined.
   * @param optionalPath a path or empty
   * @param optionalPathHandle a path handle/empty
   * @throws IllegalArgumentException if both parameters are set.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(),819,821,"/**
 * Default constructor. Initializes with default configuration settings.
 */
",A new configuration.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,validateResponse,"org.apache.hadoop.util.HttpExceptionUtils:validateResponse(java.net.HttpURLConnection,int)",143,191,"/**
 * Throws an exception based on HTTP response status and error details.
 */","* Validates the status of an <code>HttpURLConnection</code> against an
   * expected HTTP status code. If the current status code is not the expected
   * one it throws an exception with a detail message using Server side error
   * messages if available.
   * <p>
   * <b>NOTE:</b> this method will throw the deserialized exception even if not
   * declared in the <code>throws</code> of the method signature.
   *
   * @param conn the <code>HttpURLConnection</code>.
   * @param expectedStatus the expected HTTP status code.
   * @throws IOException thrown if the current status code does not match the
   * expected one.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newCrc32C,org.apache.hadoop.util.DataChecksum:newCrc32C(),102,112,"/**
 * Creates a Checksum instance, preferring Java9Crc32C if available.
 * Falls back to PureJavaCrc32C on failure.
 */
","* The flag is volatile to avoid synchronization here.
   * Re-entrancy is unlikely except in failure mode (and inexpensive).",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,removeAll,org.apache.hadoop.util.IntrusiveCollection:removeAll(java.util.Collection),361,370,"/**
 * Checks if any element in the collection satisfies the m1 predicate.
 * @param collection Collection of elements to check.
 * @return True if any element satisfies m1, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,toArray,org.apache.hadoop.util.IntrusiveCollection:toArray(java.lang.Object[]),266,278,"/**
 * Fills the input array with elements from m3().
 * @param array The array to be filled.
 * @return The filled array.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsForUserCommand,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsForUserCommand(java.lang.String),143,145,"/**
 * Delegates to Shell.m1 to retrieve string array based on username.
 */","* Returns just the shell command to be used to fetch a user's groups list.
   * This is mainly separate to make some tests easier.
   * @param userName The username that needs to be passed into the command built
   * @return An appropriate shell command with arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsIDForUserCommand,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsIDForUserCommand(java.lang.String),164,166,"/**
 * Delegates to Shell.m1 to retrieve string array by username.
 * @param userName The username to use for the request.
 * @return String array returned by Shell.m1.
 */
","* Returns just the shell command to be used to fetch a user's group IDs list.
   * This is mainly separate to make some tests easier.
   * @param userName The username that needs to be passed into the command built
   * @return An appropriate shell command with arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetPermissionCommand,"org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean,java.lang.String)",311,318,"/**
 * Appends a file path to a command array.
 * @param perm permission string, recursive flag, file path
 * @return Command array with the added file path.
 */
","* Return a command to set permission for specific file.
   *
   * @param perm String permission to set
   * @param recursive boolean true to apply to all sub-directories recursively
   * @param file String file to set
   * @return String[] containing command and arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getCheckProcessIsAliveCommand,org.apache.hadoop.util.Shell:getCheckProcessIsAliveCommand(java.lang.String),362,364,"/**
* Delegates processing to m1 with initial value 0 and given PID.
*/","* Return a command for determining if process with specified pid is alive.
   * @param pid process ID
   * @return a <code>kill -0</code> command or equivalent",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getHadoopHome,org.apache.hadoop.util.Shell:getHadoopHome(),610,612,"/**
* Delegates to m1().m2() and returns the result.
*/","* Get the Hadoop home directory. Raises an exception if not found
   * @return the home dir
   * @throws IOException if the home directory cannot be located.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBin,org.apache.hadoop.util.Shell:getQualifiedBin(java.lang.String),642,646,"/**
* Returns a File object for the executable path.
* Uses m1() to get base dir, then combines with executable.
*/","*  Fully qualify the path to a binary that should be in a known hadoop
   *  bin location. This is primarily useful for disambiguating call-outs
   *  to executable sub-components of Hadoop to avoid clashes with other
   *  executables that may be in the path.  Caveat:  this call doesn't
   *  just format the path to the bin directory.  It also checks for file
   *  existence of the composed path. The output of this call should be
   *  cached by callers.
   *
   * @param executable executable
   * @return executable file reference
   * @throws FileNotFoundException if the path does not exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,linkCount,org.apache.hadoop.fs.HardLink$HardLinkCGWin:linkCount(java.io.File),137,146,"/**
 * Extracts link counts and file name into a String array.
 * @param file The file to get the name from.
 * @return String array containing link counts and file name.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:<init>(org.apache.hadoop.util.Shell),1402,1404,"/**
 * Constructs a ShellTimeoutTimerTask, associating it with a Shell.
 * @param shell The Shell instance this task belongs to.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(),71,77,"/**
 * Returns a Progress object, calculating phase weightage.
 */","* Adds a node to the tree. Gives equal weightage to all phases.
   * @return Progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhases,org.apache.hadoop.util.Progress:addPhases(int),130,137,"/**
 * Executes m1() n times, then calculates phase progress.
 * @param n Number of times to execute m1(); affects progress.
 */","* Adds n nodes to the tree. Gives equal weightage to all phases.
   *
   * @param n n.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,"org.apache.hadoop.util.Progress:addPhase(java.lang.String,float)",94,99,"/**
 * Creates a Progress object, sets status, and returns it.
 * @param status Progress status string
 * @param weightage Progress weightage float
 * @return Progress object with updated status
 */
","* Adds a named node with a specified progress weightage to the tree.
   *
   * @param status status.
   * @param weightage weightage.
   * @return Progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,get,org.apache.hadoop.util.Progress:get(),225,231,"/**
* Traverses up the parent chain to find the root and returns its m2 value.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,getProgress,org.apache.hadoop.util.Progress:getProgress(),239,241,"/**
 * Synchronized method that returns the result of m1().
 */","* Returns progress in this node. get() would give overall progress of the
   * root node(not just given current node).
   *
   * @return progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,toString,org.apache.hadoop.util.Progress:toString(),275,280,"/**
 * Appends data to StringBuilder and returns the string representation.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,create,org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String),332,334,"/**
 * Calls m1 with null arguments.
 * @param path The path to be processed.
 * @return True if successful, false otherwise.
 */
","* Create a ZNode.
   * @param path Path of the ZNode.
   * @return If the ZNode was created.
   * @throws Exception If it cannot contact Zookeeper.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,createRootDirRecursively,"org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String,java.util.List)",372,384,"/**
 * Processes a path, applying ACLs to each segment.
 * @param path The path to process.
 * @param zkAcl ACLs to apply to each path segment.
 */","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   * @param path Path of the znode to create.
   * @param zkAcl ACLs for ZooKeeper.
   * @throws Exception If it cannot create the file.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,ctorImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.String,java.lang.Class[])",364,378,"/**
 * Sets the method to be constructed using DynConstructors.
 * @param className Class name.
 * @param argClasses Constructor argument classes.
 * @return Builder object.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,newInstance,org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstance(java.lang.Object[]),68,75,"/**
 * Calls m2 with provided args, handles exceptions, and re-throws.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,invokeChecked,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invokeChecked(java.lang.Object,java.lang.Object[])",84,89,"/**
 * Delegates to m2 with arguments, throws exception if target is null.
 * @param target The target object.
 * @param args Arguments to pass to m2.
 * @return Result of m2, cast to type R.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invokeChecked(java.lang.Object[]),215,217,"/**
* Delegates to the underlying method with provided arguments.
* @param args Arguments to pass to the underlying method.
* @return Result of the underlying method.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invoke(java.lang.Object,java.lang.Object[])",91,98,"/**
 * Delegates to m2, handling exceptions and re-throwing as RuntimeException.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invokeChecked(java.lang.Object[]),198,200,"/**
* Delegates to the method 'm1' on the 'method' object.
* @param args Arguments to pass to the underlying method.
* @return The result of the underlying method.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.String,java.lang.Class[])",284,298,"/**
 * Sets method details, loading class if needed.
 * @param className Class name to load.
 * @param methodName Method name.
 * @param argClasses Argument classes.
 */","* Checks for an implementation, first finding the given class by name.
     * @param className name of a class
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.Class[])",343,346,"/**
 * Calls m1 with the targetClass and provided argument classes.
 * @param targetClass The class to be targeted.
 * @param argClasses Argument classes for the method.
 * @return This builder instance.
 */
","* Checks for a method implementation.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param targetClass the class to check for an implementation
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.String,java.lang.Class[])",387,401,"/**
 * Sets method details, loading class if not found.
 * @param className Class name.
 * @param methodName Method name.
 * @param argClasses Argument class types.
 */","* Checks for an implementation, first finding the given class by name.
     * @param className name of a class
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.Class[])",448,451,"/**
 * Calls m1 with the provided target class and argument classes.
 * @param targetClass The class to be targeted.
 * @param argClasses Argument classes to be passed.
 * @return This builder instance.
 */
","* Checks for a method implementation.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param targetClass the class to check for an implementation
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,loadInvocation,"org.apache.hadoop.util.dynamic.BindingUtils:loadInvocation(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])",101,121,"/**
 * Creates an unbound method. Returns it or a default if source is null.
 */","* Get an invocation from the source class, which will be unavailable() if
   * the class is null or the method isn't found.
   *
   * @param <T> return type
   * @param source source. If null, the method is a no-op.
   * @param returnType return type class (unused)
   * @param name method name
   * @param parameterTypes parameters
   *
   * @return the method or ""unavailable""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,requireAllMethodsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:requireAllMethodsAvailable(),225,242,"/**
 * Checks if all unbound methods are supported; throws exception if not.
 */","* For testing: verify that all methods were found.
   * @throws UnsupportedOperationException if the method was not found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_available(),249,251,"/**
* Delegates deletion to m1, using bulkDeleteDeleteMethod.
*/","* Are the bulk delete methods available?
   * @return true if the methods were found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,fileSystem_openFile_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile_available(),306,308,"/**
* Delegates to m1 with fileSystemOpenFileMethod.
* Returns the result of the delegation.
*/
","* Is the {@link #fileSystem_openFile(FileSystem, Path, String, FileStatus, Long, Map)}
   * method available.
   * @return true if the optimized open file method can be invoked.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_available(),380,382,"/**
* Delegates to m1 with the provided buffer state.
* @return True if m1 succeeds, false otherwise.
*/
","* Are the ByteBufferPositionedReadable methods loaded?
   * This does not check that a specific stream implements the API;
   * use {@link #byteBufferPositionedReadable_readFullyAvailable(InputStream)}.
   * @return true if the hadoop libraries have the method.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_readFullyAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream),392,400,"/**
 * Processes input stream based on a condition.
 * @param in Input stream to process. Returns boolean result.
 */","* Probe to see if the input stream is an instance of ByteBufferPositionedReadable.
   * If the stream is an FSDataInputStream, the wrapped stream is checked.
   * @param in input stream
   * @return true if the API is available, the stream implements the interface
   * (including the innermost wrapped stream) and that it declares the stream capability.
   * @throws IOException if the operation was attempted and failed.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,ioStatisticsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsAvailable(),360,362,"/**
* Delegates to m1 with the istatisticsSnapshotCreateMethod.
*/","* Are the core IOStatistics methods and classes available.
   * @return true if the relevant methods are loaded.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,ioStatisticsContextAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsContextAvailable(),368,370,"/**
* Delegates to m1 with the istatisticsContextEnabledMethod flag.
*/","* Are the IOStatisticsContext methods and classes available?
   * @return true if the relevant methods are loaded.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,checkAvailable,org.apache.hadoop.util.dynamic.BindingUtils:checkAvailable(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod),183,188,"/**
 * Executes unbound method. Throws UnsupportedOperationException if fails.
 */","* Require a method to be available.
   * @param method method to probe
   * @throws UnsupportedOperationException if the method was not found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildChecked,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildChecked(java.lang.Object),490,492,"/**
* Delegates to m1() and then calls m2() with the receiver.
*/","* Returns the first valid implementation as a BoundMethod or throws a
     * NoSuchMethodException if there is none.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} with a valid implementation and receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible
     * @throws NoSuchMethodException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,build,org.apache.hadoop.util.dynamic.DynMethods$Builder:build(java.lang.Object),503,505,"/**
* Delegates to m1 and passes the receiver to m2.
*/","* Returns the first valid implementation as a BoundMethod or throws a
     * RuntimeError if there is none.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} with a valid implementation and receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible
     * @throws RuntimeException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildStaticChecked,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStaticChecked(),514,516,"/**
* Chains method calls: returns the result of m1().m2().
*/","* Returns the first valid implementation as a StaticMethod or throws a
     * NoSuchMethodException if there is none.
     * @return a {@link StaticMethod} with a valid implementation
     * @throws IllegalStateException if the method is not static
     * @throws NoSuchMethodException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildStatic,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStatic(),525,527,"/**
* Chains method calls: returns the result of m1().m2().
*/","* Returns the first valid implementation as a StaticMethod or throws a
     * RuntimeException if there is none.
     * @return a {@link StaticMethod} with a valid implementation
     * @throws IllegalStateException if the method is not static
     * @throws RuntimeException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,loadFileSystems,org.apache.hadoop.fs.FileSystem:loadFileSystems(),3516,3545,"/**
 * Loads file systems using ServiceLoader and registers them.
 */","* Load the filesystem declarations from service resources.
   * This is a synchronized operation.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,main,org.apache.hadoop.util.VersionInfo:main(java.lang.String[]),182,193,"/**
 * Prints version and build information to the console.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeRpcRequestHeader,"org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[])",171,176,"/**
 * Creates an RpcRequestHeaderProto with optional fields null.
 * @param rpcKind RpcKind enum, operation OperationProto,
 * callId call identifier, retryCount retry attempts, uuid unique ID
 * @return RpcRequestHeaderProto object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,parseVersion,org.apache.hadoop.util.ComparableVersion:parseVersion(java.lang.String),360,453,"/**
 * Parses a version string and creates a canonical representation.
 * @param version The version string to parse.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,<init>,"org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long,org.apache.hadoop.util.Timer)",120,145,"/**
 * Constructs a LightWeightCache with specified parameters.
 * @param recommendedLength, sizeLimit, expiration periods, timer
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,get,org.apache.hadoop.util.LightWeightResizableGSet:get(java.lang.Object),98,101,"/**
* Calls the parent class's m1 method, maintaining synchronization.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,contains,org.apache.hadoop.util.LightWeightGSet:contains(java.lang.Object),144,147,"/**
 * Checks if a key exists.
 * @param key The key to check.
 * @return True if the key exists, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,toString,org.apache.hadoop.util.BlockingThreadPoolExecutorService:toString(),158,166,"/**
 * Creates a string representation of the BlockingThreadPoolExecutorService.
 * @return String describing the service's state.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToMapWithFileInputStream,"org.apache.hadoop.util.HostsFileReader:readFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)",130,144,"/**
 * Processes input based on file type. XML uses m5, else uses m3.
 * @param type Processing type, filename, input stream, map.
 * @throws IOException if an I/O error occurs.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/JenkinsHash.java,main,org.apache.hadoop.util.hash.JenkinsHash:main(java.lang.String[]),252,266,"/**
* Calculates and prints the Jenkins hash of a file.
* @param args Command-line arguments; expects filename.
* @throws IOException if file reading fails.
*/","* Compute the hash of the specified file
   * @param args name of file to compute hash of.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/HashFunction.java,<init>,"org.apache.hadoop.util.bloom.HashFunction:<init>(int,int,int)",83,97,"/**
 * Constructs a HashFunction with specified max value, hash count, and type.
 * @param maxValue Max value for hashing.
 * @param nbHash Number of hash functions.
 * @param hashType Type of hash function.
 */
","* Constructor.
   * <p>
   * Builds a hash function that must obey to a given maximum number of returned values and a highest value.
   * @param maxValue The maximum highest returned value.
   * @param nbHash The number of resulting hashed values.
   * @param hashType type of the hashing function (see {@link Hash}).",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,<init>,org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>(),102,102,"/**
 * Constructs a new RetouchedBloomFilter instance.
 */
",Default constructor - use with readFields,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,write,org.apache.hadoop.util.bloom.DynamicBloomFilter:write(java.io.DataOutput),248,257,"/**
 * Writes data to the output stream, including nr, record count,
 * and matrix dimensions, then writes each matrix element.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,write,org.apache.hadoop.util.bloom.RetouchedBloomFilter:write(java.io.DataOutput),407,427,"/**
* Writes data to the output stream, including fpVector, keyVector, and ratio.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,add,org.apache.hadoop.util.bloom.CountingBloomFilter:add(org.apache.hadoop.util.bloom.Key),104,127,"/**
* Increments hash values based on the provided key.
* @param key The key used to update hash values.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.CountingBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),178,200,"/**
* Checks if key is present in the hash structure.
* @param key The key to check for existence.
* @return True if key is present, false otherwise.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,approximateCount,org.apache.hadoop.util.bloom.CountingBloomFilter:approximateCount(org.apache.hadoop.util.bloom.Key),220,238,"/**
* Finds the minimum value in hash buckets based on key.
* Returns the minimum value or 0 if no value is found.
*/","* This method calculates an approximate count of the key, i.e. how many
   * times the key was added to the filter. This allows the filter to be
   * used as an approximate <code>key -&gt; count</code> map.
   * <p>NOTE: due to the bucket size of this filter, inserting the same
   * key more than 15 times will cause an overflow at all filter positions
   * associated with this key, and it will significantly increase the error
   * rate for this and other keys. For this reason the filter can only be
   * used to store small count values <code>0 &lt;= N &lt;&lt; 15</code>.
   * @param key key to be tested
   * @return 0 if the key is not present. Otherwise, a positive value v will
   * be returned such that <code>v == count</code> with probability equal to the
   * error rate of this filter, and <code>v &gt; count</code> otherwise.
   * Additionally, if the filter experienced an underflow as a result of
   * {@link #delete(Key)} operation, the return value may be lower than the
   * <code>count</code> with the probability of the false negative rate of such
   * filter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,add,org.apache.hadoop.util.bloom.BloomFilter:add(org.apache.hadoop.util.bloom.Key),116,128,"/**
 * Processes a key by hashing and setting bits.
 * @param key The key to process; must not be null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.BloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),142,156,"/**
 * Checks if a key satisfies all hash conditions.
 * @param key The key to check. Throws NullPointerException if null.
 * @return True if all hash conditions are met, false otherwise.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,add,org.apache.hadoop.util.bloom.RetouchedBloomFilter:add(org.apache.hadoop.util.bloom.Key),118,131,"/**
 * Processes a key by hashing and updating bit vectors and key vectors.
 * @param key The key to process. Throws NullPointerException if null.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key),139,150,"/**
 * Processes a key by hashing and applying transformations.
 * @param key The key to be processed. Throws NullPointerException if null.
 */","* Adds a false positive information to <i>this</i> retouched Bloom filter.
   * <p>
   * <b>Invariant</b>: if the false positive is <code>null</code>, nothing happens.
   * @param key The false positive key to add.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,removeKey,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:removeKey(org.apache.hadoop.util.bloom.Key,java.util.List[])",351,365,"/**
 * Processes a key by applying hashing and updating vector elements.
 * @param k The key to process.
 * @param vector The array of lists to update.
 */
","* Removes a given key from <i>this</i> filer.
   * @param k The key to remove.
   * @param vector The counting vector associated to the key.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,equals,org.apache.hadoop.util.bloom.Key:equals(java.lang.Object),137,143,"/**
 * Checks if an object is a Key and its m1 value is 0.
 * @param o Object to check; must be a Key.
 * @return True if object is a Key and m1 value is 0.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,minimumFnRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:minimumFnRemove(int[]),250,265,"/**
 * Finds the index of the element with the minimum key weight.
 * @param h array of hash indices
 * @return Index of the element with the minimum key weight.
 */","* Chooses the bit position that minimizes the number of false negative generated.
   * @param h The different bit positions.
   * @return The position that minimizes the number of false negative generated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,maximumFpRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:maximumFpRemove(int[]),272,286,"/**
 * Finds the index with the maximum FP weight.
 * @param h array of hash indices
 * @return Index with the highest FP weight.
 */","* Chooses the bit position that maximizes the number of false positive removed.
   * @param h The different bit positions.
   * @return The position that maximizes the number of false positive removed.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,computeRatio,org.apache.hadoop.util.bloom.RetouchedBloomFilter:computeRatio(),370,379,"/**
 * Calculates the ratio of keyWeight to fpWeight for each element.
 */",* Computes the ratio A/FP.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,dumpResource,org.apache.hadoop.util.FindClass:dumpResource(java.lang.String),187,209,"/**
* Reads and prints resource content from URL.
* @param name Resource name to load.
* @return SUCCESS, E_NOT_FOUND, or E_LOAD_FAILED.
*/","* Dump a resource to out
   * @param name resource name
   * @return the status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,usage,org.apache.hadoop.util.FindClass:usage(java.lang.String[]),342,361,"/**
 * Prints usage instructions and returns E_USAGE.
 * Explains available commands and return codes.
 */","* Print a usage message
   * @param args the command line arguments
   * @return an exit code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,run,org.apache.hadoop.util.GcTimeMonitor:run(),153,172,"/**
* Periodically checks data, updates timestamp, and triggers alerts.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,put,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:put(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor),3510,3520,"/**
 * Checks compression settings and calls super.m4.
 * @param stream SegmentDescriptor containing compression info.
 * @throws IOException if compression settings are inconsistent.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,insert,org.apache.hadoop.util.PriorityQueue:insert(java.lang.Object),73,85,"/**
 * Adds an element if space is available or replaces if valid.
 * @param element The element to add or replace.
 * @return True if added/replaced, false otherwise.
 */
","* Adds element to the PriorityQueue in log(size) time if either
   * the PriorityQueue is not full, or not lessThan(element, top()).
   * @param element element.
   * @return true if element is added, false otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newTreeSet,org.apache.hadoop.util.Sets:newTreeSet(java.lang.Iterable),154,159,"/**
 * Creates a TreeSet from the given elements.
 * @param elements Iterable of comparable elements
 * @return TreeSet containing the elements
 */","* Creates a <i>mutable</i> {@code TreeSet} instance containing the given
   * elements sorted by their natural ordering.
   *
   * <p><b>Note:</b> if mutability is not required, use
   * ImmutableSortedSet#copyOf(Iterable) instead.
   *
   * <p><b>Note:</b> If {@code elements} is a {@code SortedSet} with an
   * explicit comparator, this method has different behavior than
   * {@link TreeSet#TreeSet(SortedSet)}, which returns a {@code TreeSet}
   * with that comparator.
   *
   * <p><b>Note for Java 7 and later:</b> this method is now unnecessary and
   * should be treated as deprecated. Instead, use the {@code TreeSet}
   * constructor directly, taking advantage of the new
   * <a href=""http://goo.gl/iz2Wi"">""diamond"" syntax</a>.
   *
   * <p>This method is just a small convenience for creating an empty set and
   * then calling Iterables#addAll. This method is not very useful and will
   * likely be deprecated in the future.
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain
   * @return a new {@code TreeSet} containing those elements (minus duplicates)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.lang.Iterable),123,127,"/**
* Converts an iterable to a HashSet. Handles Collection types.
*/","* Creates a <i>mutable</i> {@code HashSet} instance containing the given
   * elements. A very thin convenience for creating an empty set then calling
   * {@link Collection#addAll} or Iterables#addAll.
   *
   * <p><b>Note:</b> if mutability is not required and the elements are
   * non-null, use ImmutableSet#copyOf(Iterable) instead. (Or, change
   * {@code elements} to be a FluentIterable and call {@code elements.toSet()}.)</p>
   *
   * <p><b>Note:</b> if {@code E} is an {@link Enum} type, use
   * newEnumSet(Iterable, Class) instead.</p>
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain.
   * @return a new, empty thread-safe {@code Set}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.lang.Object[]),100,105,"/**
 * Creates a HashSet from the provided elements.
 * @param elements elements to be added to the HashSet
 * @return A HashSet containing the input elements.
 */
","* Creates a <i>mutable</i> {@code HashSet} instance initially containing
   * the given elements.
   *
   * <p><b>Note:</b> if elements are non-null and won't be added or removed
   * after this point, use ImmutableSet#of() or ImmutableSet#copyOf(Object[])
   * instead. If {@code E} is an {@link Enum} type, use
   * {@link EnumSet#of(Enum, Enum[])} instead. Otherwise, strongly consider
   * using a {@code LinkedHashSet} instead, at the cost of increased memory
   * footprint, to get deterministic iteration behavior.</p>
   *
   * <p>This method is just a small convenience, either for
   * {@code newHashSet(}{@link Arrays#asList}{@code (...))}, or for creating an
   * empty set then calling {@link Collections#addAll}.</p>
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain.
   * @return a new, empty thread-safe {@code Set}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,run,org.apache.hadoop.util.ProgramDriver:run(java.lang.String[]),120,146,"/**
* Executes a program based on the provided arguments.
* @param args Command-line arguments; first is program name.
* @return 0 on success, -1 on failure.
*/","* This is a driver for the example programs.
   * It looks at the first command line argument and tries to find an
   * example program with that name.
   * If it is found, it calls the main method in that class with the rest 
   * of the command line arguments.
   * @param args The argument from the user. args[0] is the command to run.
   * @return -1 on error, 0 on success
   * @throws NoSuchMethodException  when a particular method cannot be found.
   * @throws SecurityException security manager to indicate a security violation.
   * @throws IllegalAccessException for backward compatibility.
   * @throws IllegalArgumentException if the arg is invalid.
   * @throws Throwable Anything thrown by the example program's main",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)",151,155,"/**
 * Adds a column with the given title, justification, and wrap.
 * @param title Column title.
 * @param justification Column justification.
 * @param wrap Whether to wrap the column text.
 * @return This builder instance.
 */
","* Add a new field to the Table under construction.
     *
     * @param title Field title.
     * @param justification Right or left justification. Defaults to left.
     * @param wrap Width at which to auto-wrap the content of the cell.
     *        Defaults to Integer.MAX_VALUE.
     * @return This Builder object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,logDeprecationOnce,"org.apache.hadoop.conf.Configuration:logDeprecationOnce(java.lang.String,java.lang.String)",1465,1470,"/**
 * Logs deprecation information if keyInfo exists and is not deprecated.
 * @param name Key name. @param source Source of the key.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurationHelper,"org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)",1936,1938,"/**
* Calls m1 with the provided name, value string, and unit.
* @param name Name identifier.
* @param vStr Value string.
* @param unit Time unit.
*/","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param vStr The string value with time unit suffix to be converted.
   * @param unit Unit to convert the stored property, if it exists.
   * @return time duration in given time unit.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStreamReader,"org.apache.hadoop.conf.Configuration:getStreamReader(org.apache.hadoop.conf.Configuration$Resource,boolean)",3149,3177,"/**
 * Creates an XMLStreamReader2 from a Resource wrapper.
 * @param wrapper Resource wrapper object
 * @param quiet Suppress logging if true
 * @return XMLStreamReader2 or null if creation fails
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleStartElement,org.apache.hadoop.conf.Configuration$Parser:handleStartElement(),3240,3265,"/**
* Parses XML stream events and routes processing to other methods.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,appendXMLProperty,"org.apache.hadoop.conf.Configuration:appendXMLProperty(org.w3c.dom.Document,org.w3c.dom.Element,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3695,3733,"/**
* Adds a property to the configuration element, redacting if needed.
* @param doc XML document, conf config element, propertyName property name
* @param redactor ConfigRedactor for value redaction
*/","*  Append a property with its attributes to a given {#link Document}
   *  if the property is found in configuration.
   *
   * @param doc
   * @param conf
   * @param propertyName",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecations,org.apache.hadoop.conf.Configuration:addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]),566,572,"/**
 * Processes deprecation deltas, updating deprecation contexts.
 */","* Adds a set of deprecated keys to the global deprecations.
   *
   * This method is lockless.  It works by means of creating a new
   * DeprecationContext based on the old one, and then atomically swapping in
   * the new context.  If someone else updated the context in between us reading
   * the old context and swapping in the new one, we try again until we win the
   * race.
   *
   * @param deltas   The deprecations to add.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleEndElement,org.apache.hadoop.conf.Configuration$Parser:handleEndElement(),3374,3413,"/**
 * Processes configuration tokens based on the reader's state.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,bindForPortRange,"org.apache.hadoop.http.HttpServer2:bindForPortRange(org.eclipse.jetty.server.ServerConnector,int)",1502,1531,"/**
 * Attempts to bind a connector to an available port.
 * Tries ports in `portRanges`, retrying on bind failures.
 */","* Bind using port ranges. Keep on looking for a free port in the port range
   * and throw a bind exception if no port in the configured range binds.
   * @param listener jetty listener.
   * @param startPort initial port which is set in the listener.
   * @throws Exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,fatalError,org.apache.hadoop.ha.ActiveStandbyElector:fatalError(java.lang.String),768,772,"/**
* Logs an error message, performs cleanup, and notifies the client.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToActive,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),89,95,"/**
 * Transitions a request to the active state via RPC.
 * @param reqInfo Request info object.
 * @throws IOException if RPC call fails.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToStandby,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),97,103,"/**
 * Sends TransitionToStandbyRequestProto via RPC.
 * @param reqInfo StateChangeRequestInfo object
 * @throws IOException if RPC call fails
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToObserver,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),105,112,"/**
 * Sends a TransitionToObserverRequest via RPC.
 * @param reqInfo Request info containing necessary data.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToActive,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToActive(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto)",107,117,"/**
* Processes transition to active request.
* Delegates to server, returns response or throws exception.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToStandby,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToStandby(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto)",119,129,"/**
 * Handles transition to standby request.
 * Delegates to server, returns response or throws exception.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToObserver,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToObserver(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto)",131,141,"/**
 * Processes transition request, calls server, and returns response.
 * @param request TransitionToObserverRequestProto object
 * @return TransitionToObserverResponseProto object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,<init>,org.apache.hadoop.ha.SshFenceByTcpPort$Args:<init>(java.lang.String),237,256,"/**
 * Parses arguments to configure user and SSH port.
 * @param arg Argument string containing user and port info.
 * @throws BadFencingConfigurationException if parsing fails.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String)",151,153,"/**
* Calls m1 with default USAGE.
* @param pStr PrintStream for output.
* @param cmd Command string.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,parseOpts,"org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[],java.util.Map)",491,503,"/**
 * Parses command-line arguments using GnuParser.
 * @param cmdName Command name, @param opts Options, @param argv Arguments
 * @return Parsed CommandLine object or null on error.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,doFence,"org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)",130,171,"/**
* Kills a process listening on a port via SSH.
* @param session SSH session.
* @param serviceAddr Service address.
* @return True if successful, false otherwise.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,addTargetInfoAsEnvVars,"org.apache.hadoop.ha.ShellCommandFencer:addTargetInfoAsEnvVars(org.apache.hadoop.ha.HAServiceTarget,java.util.Map)",220,243,"/**
* Sets environment variables based on target state and properties.
* @param target The HAServiceTarget object.
* @param environment The environment map to update.
*/
","* Add information about the target to the the environment of the
   * subprocess.
   * 
   * @param target
   * @param environment",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setAclsWithRetries,org.apache.hadoop.ha.ActiveStandbyElector:setAclsWithRetries(java.lang.String),1124,1138,"/**
 * Updates ACL for the given path, synchronizing with the server.
 * @param path ZNode path to update ACL for.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,zkDoWithRetries,org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction),1140,1143,"/**
* Executes a ZKAction and returns the result.
* @param action ZKAction to execute; returns T.
* @return Result of the ZKAction.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readNonByteBufferPositionedReadable,"org.apache.hadoop.fs.VectoredReadUtils:readNonByteBufferPositionedReadable(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer)",151,170,"/**
 * Reads data from a stream into a buffer, using direct buffer if available.
 * @param stream Readable stream to read from.
 * @param range File range to read.
 * @param buffer ByteBuffer to write to.
 */","* Read into a direct tor indirect buffer using {@code PositionedReadable.readFully()}.
   * @param stream stream
   * @param range file range
   * @param buffer destination buffer
   * @throws IOException IO problems.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateVectoredReadRanges,org.apache.hadoop.fs.VectoredReadUtils:validateVectoredReadRanges(java.util.List),82,85,"/**
* Processes a list of file ranges using m2.
* @param ranges List of FileRange objects to process.
*/","* Validate a list of vectored read ranges.
   * @param ranges list of ranges.
   * @throws EOFException any EOF exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setCaching,org.apache.hadoop.fs.impl.prefetch.BufferData:setCaching(java.util.concurrent.Future),195,201,"/**
 * Initiates caching action, validates future, and updates state.
 * @param actionFuture Future representing the caching action.
 */
","* Indicates that a caching operation is in progress.
   *
   * @param actionFuture the {@code Future} of a caching action.
   *
   * @throws IllegalArgumentException if actionFuture is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,updateState,"org.apache.hadoop.fs.impl.prefetch.BufferData:updateState(org.apache.hadoop.fs.impl.prefetch.BufferData$State,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",243,250,"/**
* Updates the state to newState, validates inputs, and calls m2.
*/","* Updates the current state to the specified value.
   * Asserts that the current state is as expected.
   * @param newState the state to transition to.
   * @param expectedCurrentState the collection of states from which
   *        transition to {@code newState} is allowed.
   *
   * @throws IllegalArgumentException if newState is null.
   * @throws IllegalArgumentException if expectedCurrentState is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExistsAsDir,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsDir(java.nio.file.Path,java.lang.String)",357,364,"/**
 * Checks if a path is a directory, providing a custom error message.
 * @param path The path to check.
 * @param argName Argument name for error message.
 */
","* Validates that the given path exists and is a directory.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExistsAsFile,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsFile(java.nio.file.Path,java.lang.String)",371,375,"/**
 * Checks if the given path points to a file, throwing an exception if not.
 * @param path The path to check.
 * @param argName Argument name for error message.
 */
","* Validates that the given path exists and is a file.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,isLastBlock,org.apache.hadoop.fs.impl.prefetch.BlockData:isLastBlock(int),127,135,"/**
 * Checks if the given block number is the last block.
 * @param blockNumber The block number to check.
 * @return True if it's the last block, false otherwise.
 */
","* Indicates whether the given block is the last block in the associated file.
   * @param blockNumber the id of the desired block.
   * @return true if the given block is the last block in the associated file, false otherwise.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getStartOffset,org.apache.hadoop.fs.impl.prefetch.BlockData:getStartOffset(int),181,185,"/**
 * Calculates a masked value based on the block number.
 * @param blockNumber The block number to process.
 * @return A long value representing the masked block number.
 */
","* Gets the start offset of the given block.
   * @param blockNumber the id of the given block.
   * @return the start offset of the given block.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getState,org.apache.hadoop.fs.impl.prefetch.BlockData:getState(int),206,210,"/**
 * Retrieves the state of a block.
 * @param blockNumber The block's index.
 * @return The state of the specified block.
 */
","* Gets the state of the given block.
   * @param blockNumber the id of the given block.
   * @return the state of the given block.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,setState,"org.apache.hadoop.fs.impl.prefetch.BlockData:setState(int,org.apache.hadoop.fs.impl.prefetch.BlockData$State)",218,222,"/**
 * Updates the state of a block with the given state.
 * @param blockNumber Block index to update.
 * @param blockState The new state for the block.
 */","* Sets the state of the given block to the given value.
   * @param blockNumber the id of the given block.
   * @param blockState the target state.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getBlockNumber,org.apache.hadoop.fs.impl.prefetch.BlockData:getBlockNumber(long),143,147,"/**
* Calculates the block index from an offset.
* @param offset The offset value.
* @return The block index as an integer.
*/
","* Gets the id of the block that contains the given absolute offset.
   * @param offset the absolute offset to check.
   * @return the id of the block that contains the given absolute offset.
   * @throws IllegalArgumentException if offset is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_aggregate,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)",94,107,"/**
 * Processes a snapshot with optional statistics.
 * @param snapshot The snapshot to process.
 * @param statistics Optional IOStatistics object.
 * @return True if processing is successful, false otherwise.
 */","* Aggregate an existing {@link IOStatisticsSnapshot} with
   * the supplied statistics.
   * @param snapshot snapshot to update
   * @param statistics IOStatistics to add
   * @return true if the snapshot was updated.
   * @throws IllegalArgumentException if the {@code statistics} argument is not
   * null but not an instance of IOStatistics, or if  {@code snapshot} is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_save,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",162,171,"/**
 * Saves a snapshot to the filesystem, overwriting if needed.
 * @param snapshot The snapshot to save.
 * @param fs Filesystem to use.
 * @param path Path to save to.
 * @param overwrite Whether to overwrite existing files.
 */
","* Save IOStatisticsSnapshot to a Hadoop filesystem as a JSON file.
   * @param snapshot statistics
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten?
   * @throws UncheckedIOException Any IO exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_counters,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_counters(java.io.Serializable),203,206,"/**
* Extracts counters from a source using m1.
* @param source Serializable source object
* @return Map of string counters.
*/
","* Get the counters of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of counters.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_gauges,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_gauges(java.io.Serializable),213,216,"/**
* Extracts gauges from a source using m1 and IOStatisticsSnapshot.
* @param source Serializable source data.
* @return Map of gauges.
*/
","* Get the gauges of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of gauges.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_minimums,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_minimums(java.io.Serializable),223,226,"/**
 * Extracts minimum statistics from a source using m1.
 * @param source Serializable data source
 * @return Map of string keys to Long values.
 */","* Get the minimums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of minimums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_maximums,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_maximums(java.io.Serializable),233,236,"/**
* Extracts maximum statistics from a serializable source.
* @param source The serializable data source.
* @return A map of string keys to long values.
*/
","* Get the maximums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of maximums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_means,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_means(java.io.Serializable),245,253,"/**
 * Transforms source data into a map of string keys and Long tuples.
 */","* Get the means of an IOStatisticsSnapshot.
   * Each value in the map is the (sample, sum) tuple of the values;
   * the mean is then calculated by dividing sum/sample wherever sample count is non-zero.
   * @param source source of statistics.
   * @return a map of mean key to (sample, sum) tuples.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,copy,org.apache.hadoop.fs.impl.FlagSet:copy(),253,255,"/**
 * Creates a new FlagSet instance with the provided parameters.
 */","* Create a copy of the FlagSet.
   * @return a new mutable instance with a separate copy of the flags",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,createFlagSet,"org.apache.hadoop.fs.impl.FlagSet:createFlagSet(java.lang.Class,java.lang.String,java.util.EnumSet)",276,281,"/**
 * Creates a FlagSet instance with the given enum class, prefix, and flags.
 */","* Create a FlagSet.
   * @param enumClass class of enum
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param flags flags
   * @param <E> enum type
   * @return a mutable FlagSet",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,close,org.apache.hadoop.fs.HarFileSystem:close(),733,744,"/**
 * Calls super.m1() and attempts to call fs.m1(), handling IOException.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,close,org.apache.hadoop.fs.RawLocalFileSystem:close(),895,898,"/**
* Calls the m1 method of the superclass.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,closeAll,org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:closeAll(),155,163,"/**
 * Closes all child file systems in the map, handling potential IO errors.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,close,org.apache.hadoop.fs.FsShell:close(),371,376,"/**
* Calls m1 on the file system if it exists, then sets it to null.
*/","*  Performs any necessary cleanup
   * @throws IOException upon error",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,close,org.apache.hadoop.fs.FilterFileSystem:close(),527,531,"/**
* Calls super.m1() and fs.m1().
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(boolean),3799,3831,"/**
 * Processes keys, performing actions based on 'onlyAutomatic'.
 * Throws MultipleIOException if any IOExceptions occur.
 */","* Close all FileSystem instances in the Cache.
     * @param onlyAutomatic only close those that are marked for automatic closing
     * @throws IOException a problem arose closing one or more FileSystem.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(org.apache.hadoop.security.UserGroupInformation),3844,3868,"/**
* Executes a task on FileSystems accessible by the user.
* @param ugi UserGroupInformation to check access.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,compareTo,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus),117,120,"/**
 * Delegates the method call to the superclass implementation.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,readFully,"org.apache.hadoop.fs.FSInputStream:readFully(long,byte[],int,int)",118,133,"/**
* Reads data from a stream into a buffer.
* @param position file position, buffer, offset, length
* @throws IOException if an I/O error occurs
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,read,"org.apache.hadoop.fs.BufferedFSInputStream:read(long,byte[],int,int)",115,118,"/**
 * Delegates read operation to the underlying FSInputStream.
 * @param position Starting position for reading.
 * @param buffer Buffer to store read data.
 * @param offset Offset within the buffer.
 * @param length Number of bytes to read.
 * @return Number of bytes read.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_toJsonString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable),180,184,"/**
* Converts a snapshot to a JSON string using m2 and toJson.
* @param snapshot The snapshot to convert; may be null.
*/","* Save IOStatisticsSnapshot to a JSON string.
   * @param snapshot statistics; may be null or of an incompatible type
   * @return JSON string value
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,byte[])",1878,1890,"/**
 * Writes byte array to a file in the fileContext.
 * @param fileContext Hadoop file context.
 * @param path Path to write to.
 * @param bytes Data to write.
 * @return The fileContext.
 */
","* Writes bytes to a file. This utility method opens the file for writing,
   * creating the file if it does not exist, or overwrites an existing file. All
   * bytes in the byte array are written to the file.
   *
   * @param fileContext the file context with which to create the file
   * @param path the path to the file
   * @param bytes the byte array with the bytes to write
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)",1948,1966,"/**
 * Writes lines to a file in the file system.
 * @param fileContext FileContext object
 * @param path Path to the file
 * @param lines Lines to write
 * @param cs Charset to use for encoding
 * @return FileContext object
 */","* Write lines of text to a file. Each line is a char sequence and is written
   * to the file in sequence with each line terminated by the platform's line
   * separator, as defined by the system property {@code
   * line.separator}. Characters are encoded into bytes using the specified
   * charset. This utility method opens the file for writing, creating the file
   * if it does not exist, or overwrites an existing file.
   *
   * @param fileContext the file context with which to create the file
   * @param path the path to the file
   * @param lines a Collection to iterate over the char sequences
   * @param cs the charset to use for encoding
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)",2014,2028,"/**
 * Writes a CharSequence to a file using a FileSystem and Charset.
 * @param fs FileSystem object
 * @param path Path to write to
 * @param charseq CharSequence to write
 * @param cs Charset to use for encoding
 * @return FileSystem object
 */","* Write a line of text to a file. Characters are encoded into bytes using the
   * specified charset. This utility method opens the file for writing, creating
   * the file if it does not exist, or overwrites an existing file.
   *
   * @param fs the file context with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   * @param cs the charset to use for encoding
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createFile,org.apache.hadoop.fs.FileSystem:createFile(org.apache.hadoop.fs.Path),4732,4735,"/**
* Creates an FSDataOutputStreamBuilder for the given path.
* @param path Path to create the output stream for.
*/
","* Create a new FSDataOutputStreamBuilder for the file with path.
   * Files are overwritten by default.
   *
   * @param path file path
   * @return a FSDataOutputStreamBuilder object to build the file
   *
   * HADOOP-14384. Temporarily reduce the visibility of method before the
   * builder interface becomes stable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createFile,org.apache.hadoop.fs.ChecksumFileSystem:createFile(org.apache.hadoop.fs.Path),1109,1112,"/**
* Creates an FSDataOutputStreamBuilder for the given path.
* @param path Path for the output stream builder.
*/
","* This is overridden to ensure that this class's create() method is
   * ultimately called.
   *
   * {@inheritDoc}",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,appendFile,org.apache.hadoop.fs.FileSystem:appendFile(org.apache.hadoop.fs.Path),4742,4744,"/**
* Creates an FSDataOutputStreamBuilder for the given path.
* @param path The path for the output stream builder.
*/","* Create a Builder to append a file.
   * @param path file path.
   * @return a {@link FSDataOutputStreamBuilder} to build file append request.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,appendFile,org.apache.hadoop.fs.ChecksumFileSystem:appendFile(org.apache.hadoop.fs.Path),1120,1122,"/**
* Creates an FSDataOutputStreamBuilder for the given path.
* @param path Path for the output stream builder.
*/
","* This is overridden to ensure that this class's create() method is
   * ultimately called.
   *
   * {@inheritDoc}",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)",155,159,"/**
 * Constructor for BlockLocation. Calls the full constructor.
 * @param names Block names.
 * @param hosts Hostnames.
 * @param cachedHosts Cached hostnames.
 * @param topologyPaths Topology paths.
 * @param offset Offset.
 * @param length Length.
 * @param corrupt Corrupt flag.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,toString,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:toString(),559,562,"/**
* Delegates method call to the realStatus object.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpConfigKeys.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpConfigKeys:getServerDefaults(),60,71,"/**
 * Creates and returns an FsServerDefaults object with default values.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalConfigKeys.java,getServerDefaults,org.apache.hadoop.fs.local.LocalConfigKeys:getServerDefaults(),60,71,"/**
 * Creates and returns a FsServerDefaults object with default values.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(),75,80,"/**
 * Calls m1 with oneByte and returns a value or -1.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,next,org.apache.hadoop.fs.FileSystem$DirListingIterator:next(),2332,2342,"/**
* Returns the next element from the iterator.
* Throws NoSuchElementException if no more elements exist.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processPath,org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData),102,118,"/**
 * Writes file data to output, either extended attributes or value.
 * @param item PathData object containing file system and path.
 * @throws IOException if an I/O error occurs.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,listStatus,org.apache.hadoop.fs.ChecksumFileSystem:listStatus(org.apache.hadoop.fs.Path),959,962,"/**
* Lists status of files/directories under the given path.
* @param f Path to list. Returns FileStatus[] or throws IOException.
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   *
   * @param f
   *          given path
   * @return the statuses of the files/directories in the given path
   * @throws IOException if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[]),2140,2143,"/**
* Lists status of files.
* @param files array of file paths
* @return FileStatus array or null if empty
*/
","* Filter files/directories in the given list of paths using default
   * path filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param files
   *          a list of paths
   * @return a list of statuses for the files under the given paths after
   *         applying the filter default Path filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",103,108,"/**
 * Creates and registers a MutableCounterInt with the given info.
 * @param info MetricsInfo object.
 * @param iVal Initial value for the counter.
 * @return The created MutableCounterInt.
 */
","* Create a mutable integer counter
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new counter object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",127,133,"/**
 * Creates and registers a MutableCounterLong with the given info.
 * @param info MetricsInfo object
 * @param iVal Initial value for the counter
 * @return The created MutableCounterLong object
 */
","* Create a mutable long integer counter
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new counter object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",176,181,"/**
 * Creates and registers a MutableGaugeLong metric.
 * @param info MetricsInfo object; @param iVal initial value
 * @return The created MutableGaugeLong object
 */
","* Create a mutable long integer gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",200,205,"/**
 * Creates a MutableGaugeFloat, registers it, and returns it.
 * @param info MetricsInfo object
 * @param iVal Initial float value
 * @return MutableGaugeFloat object
 */
","* Create a mutable float gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",152,157,"/**
 * Creates and registers a MutableGaugeInt with the given info.
 * @param info MetricsInfo object
 * @param iVal Initial integer value
 * @return The created MutableGaugeInt object
 */
","* Create a mutable integer gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addCounter,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",102,109,"/**
 * Updates metrics with a long value if acceptable and filtered.
 * @param info MetricsInfo object. @param value The long value.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",120,127,"/**
 * Adds a long metric to the metrics collection if acceptable.
 * @param info MetricsInfo object
 * @param value Long value of the metric
 * @return MetricsRecordBuilderImpl instance
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addCounter,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",93,100,"/**
 * Updates metrics counter if conditions are met.
 * @param info MetricsInfo object. @param value int value to record.
 * @return MetricsRecordBuilderImpl instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",129,136,"/**
 * Adds a float metric gauge if acceptable and filtered.
 * @param info MetricsInfo object. @param value float value.
 * @return MetricsRecordBuilderImpl instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",138,145,"/**
 * Adds a double metric gauge to the metrics collection if acceptable.
 * @param info MetricsInfo object
 * @param value Double value of the metric
 * @return MetricsRecordBuilderImpl instance
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",111,118,"/**
* Adds an integer metric to the metrics collection if acceptable.
* @param info MetricsInfo object
* @param value Integer value for the metric
* @return MetricsRecordBuilderImpl instance
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getFS,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFS(),54,57,"/**
* Delegates the call to the superclass's m1() method.
* Returns a FileSystem object.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FsLinkResolution.java,resolve,"org.apache.hadoop.fs.impl.FsLinkResolution:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction)",91,96,"/**
 * Resolves a link using the provided function.
 * @param fileContext FileContext object.
 * @param path Path to resolve.
 * @param fn Link resolution function.
 */
","* Apply the given function to the resolved path under the the supplied
   * FileContext.
   * @param fileContext file context to resolve under
   * @param path path to resolve
   * @param fn function to invoke
   * @param <T> return type.
   * @return the return value of the function as revoked against the resolved
   * path.
   * @throws UnresolvedLinkException link resolution failure
   * @throws IOException other IO failure.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,createGlobber,org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileContext),413,415,"/**
 * Creates a new GlobBuilder instance with the given FileContext.
 * @param fileContext The context for file operations.
 * @return A new GlobBuilder object.
 */
","* Create a builder for a Globber, bonded to the specific file
   * context.
   * @param fileContext file context.
   * @return the builder to finish configuring.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,createGlobber,org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileSystem),403,405,"/**
 * Creates a new GlobBuilder instance for the given filesystem.
 * @param filesystem The filesystem to use for globbing.
 * @return A new GlobBuilder object.
 */
","* Create a builder for a Globber, bonded to the specific filesystem.
   * @param filesystem filesystem
   * @return the builder to finish configuring.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isDone,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:isDone(),246,266,"/**
 * Processes a call return, logs it, and handles retry/async states.
 */","@return true if the call is done; otherwise, return false.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,getAsyncReturn,org.apache.hadoop.io.retry.AsyncCallHandler:getAsyncReturn(),56,66,"/**
 * Returns an AsyncGet object, prioritizing ASYNC_RETURN.m1().
 * Returns a fallback AsyncGet if m1() returns null.
 */","* @return the async return value from {@link AsyncCallHandler}.
   * @param <T> T.
   * @param <R> R.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String,java.lang.String)",441,443,"/**
 * Constructs a DeprecationDelta with a single replacement key.
 * @param key Original key.
 * @param newKey Replacement key.
 * @param customMessage Custom deprecation message.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String)",445,447,"/**
 * Creates a DeprecationDelta with a single replacement key.
 * @param key The deprecated key.
 * @param newKey The replacement key.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedString,"org.apache.hadoop.io.WritableUtils:writeCompressedString(java.io.DataOutput,java.lang.String)",94,96,"/**
 * Writes string to output, encoding it using m1.
 * @param out DataOutput to write to.
 * @param s String to write.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,concat,"org.apache.hadoop.fs.RawLocalFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",612,622,"/**
* Copies data from multiple source paths to a target path.
* @param trg Target path.
* @param psrcs Array of source paths to copy from.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,invoke,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:invoke(),279,316,"/**
 * Invokes a lower layer call, handling async operations and returns.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,equals,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:equals(java.lang.Object),122,125,"/**
* Delegates the call to the superclass's m1 method.
* @param o The object passed to the super method.
* @return The result of the superclass's m1 method.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,hashCode,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:hashCode(),127,130,"/**
* Calls the m1 method of the superclass and returns its result.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,main,org.apache.hadoop.fs.DUHelper:main(java.lang.String[]),85,90,"/**
 * Prints a platform-specific message with the result of DUHelper.m1.
 * @param args Command line arguments; args[0] is used.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/WindowsGetSpaceUsed.java,refresh,org.apache.hadoop.fs.WindowsGetSpaceUsed:refresh(),45,48,"/**
* Calls used.m3 with the result of DUHelper.m2(m1()).
*/",* Override to hook in DUHelper class.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,<init>,org.apache.hadoop.fs.statistics.MeanStatistic:<init>(org.apache.hadoop.fs.statistics.MeanStatistic),107,111,"/**
 * Creates a new MeanStatistic by copying an existing one.
 * @param that The MeanStatistic to copy.
 */
","* Create from another statistic.
   * @param that source",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",245,251,"/**
* Updates the MeanStatistic for a given key.
* @param key The key for the statistic.
* @param value The new value to update with.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsSourceToString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsSourceToString(java.lang.Object),63,70,"/**
 * Extracts a string from the source, handling RuntimeExceptions.
 * @param source The source object to extract from, may be null.
 * @return A string or """" if an error occurs.
 */
","* Extract the statistics from a source object -or """"
   * if it is not an instance of {@link IOStatistics},
   * {@link IOStatisticsSource} or the retrieved
   * statistics are null.
   * <p>
   * Exceptions are caught and downgraded to debug logging.
   * @param source source of statistics.
   * @return a string for logging.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,toString,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:toString(),103,106,"/**
* Delegates to the wrapped object's m1 method.
* @return The result of calling m1 on the wrapped object.
*/
","* Return the statistics dump of the wrapped statistics.
   * @return the statistics for logging.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsLogging$StatisticsToString:toString(),325,330,"/**
* Returns statistics or a null source if statistics is null.
*/","* Evaluate and stringify the statistics.
     * @return a string value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:toString(),253,256,"/**
* Delegates to m1 with the current object as an argument.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsToPrettyString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToPrettyString(org.apache.hadoop.fs.statistics.IOStatistics),102,121,"/**
* Generates a statistics report string based on provided IOStatistics.
* @param statistics IOStatistics object to report on, or null.
* @return Statistics report string or empty string if null.
*/","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * This is more expensive than the simple conversion, so should only
   * be used for logging/output where it's known/highly likely that the
   * caller wants to see the values. Not for debug logging.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,createTracker,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:createTracker(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String)",672,678,"/**
 * Returns a DurationTracker, using factory or a stub.
 * @param factory Factory to create tracker, or null.
 * @param statistic Statistic name.
 * @return DurationTracker object.
 */
","* Create the tracker. If the factory is null, a stub
   * tracker is returned.
   * @param factory tracker factory
   * @param statistic statistic to track
   * @return a duration tracker.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,deleteBlockFileAndEvictCache,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:deleteBlockFileAndEvictCache(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),446,471,"/**
 * Purges a cache entry, attempting to acquire a write lock first.
 * @param elementToPurge Entry to purge from the cache.
 */","* Delete cache file as part of the block cache LRU eviction.
   *
   * @param elementToPurge Block entry to evict.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.util.concurrent.Callable),128,138,"/**
 * Executes a Callable, acquiring permits and handling interruption.
 * @param task Callable to execute; returns a Future<T>.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable,java.lang.Object)",140,150,"/**
 * Executes a task with permit release, tracking duration.
 * @param task Runnable task to execute
 * @param result Result value for the future
 * @return Future representing the task execution
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable),152,162,"/**
 * Executes a task, acquiring permits and handling interruption.
 * @param task Runnable task to execute.
 * @return Future representing task completion.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,execute,org.apache.hadoop.util.SemaphoredDelegatingExecutor:execute(java.lang.Runnable),164,173,"/**
 * Executes command, acquiring permits and releasing them.
 * @param command Runnable to execute, permit released after.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,iterator,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:iterator(),56,59,"/**
 * Returns an iterator for LongStatistic objects.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,java.time.Duration)",447,450,"/**
* Calls m2 with the prefix and the duration's m1() value.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,fromStorageStatistics,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:fromStorageStatistics(org.apache.hadoop.fs.StorageStatistics),72,83,"/**
 * Creates IO statistics from storage statistics.
 * @param storageStatistics Source statistics to process.
 * @return IOStatistics object built from the input.
 */
","* Create  IOStatistics from a storage statistics instance.
   *
   * This will be updated as the storage statistics change.
   * @param storageStatistics source data.
   * @return an IO statistics source.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongCounter(java.lang.String,java.util.concurrent.atomic.AtomicLong)",87,91,"/**
* Updates statistics for a key using an AtomicLong source.
* @param key Identifier for the statistics.
* @param source AtomicLong to update statistics from.
*/","* Add a counter statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerCounter(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",100,104,"/**
 * Updates statistics for a given key using the provided AtomicInteger.
 * @param key statistic key
 * @param source AtomicInteger to update the statistic with
 */
","* Add a counter statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withMutableCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMutableCounter(java.lang.String,org.apache.hadoop.metrics2.lib.MutableCounterLong)",113,117,"/**
 * Records a metric from the source counter for the given key.
 * @param key Metric key
 * @param source Counter to record from
 * @return The builder instance.
 */
","* Build a dynamic counter statistic from a
   * {@link MutableCounterLong}.
   * @param key key of this statistic
   * @param source mutable long counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongGauge(java.lang.String,java.util.concurrent.atomic.AtomicLong)",138,142,"/**
 * Updates statistics for a given key using the provided AtomicLong.
 * @param key Identifier for the statistics.
 * @param source AtomicLong to update.
 */
","* Add a gauge statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long gauge
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerGauge(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",151,155,"/**
* Updates statistics for a key using an AtomicInteger.
* @param key Identifier for the statistics.
* @param source AtomicInteger to update.
*/
","* Add a gauge statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int gauge
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMinimum(java.lang.String,java.util.concurrent.atomic.AtomicLong)",176,180,"/**
* Updates statistics for a key using the provided AtomicLong.
* @param key Statistics key. @param source AtomicLong to update.
*/","* Add a minimum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long minimum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMinimum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",189,193,"/**
 * Updates statistics for a key using an AtomicInteger source.
 * @param key Statistics key.
 * @param source AtomicInteger to update.
 */
","* Add a minimum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int minimum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMaximum(java.lang.String,java.util.concurrent.atomic.AtomicLong)",215,219,"/**
 * Updates statistics for a key using the provided AtomicLong.
 * @param key Statistic key. @param source AtomicLong to update.
 */","* Add a maximum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long maximum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMaximum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",228,232,"/**
* Updates statistics for a key using an AtomicInteger.
* @param key Identifier for the statistics.
* @param source AtomicInteger to update.
*/
","* Add a maximum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int maximum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,registerFailureHandling,org.apache.hadoop.service.launcher.ServiceLauncher:registerFailureHandling(),760,772,"/**
 * Initializes interrupt handling and sets uncaught exception handler.
 */","* Override point: register this class as the handler for the control-C
   * and SIGINT interrupts.
   *
   * Subclasses can extend this with extra operations, such as
   * an exception handler:
   * <pre>
   *  Thread.setDefaultUncaughtExceptionHandler(
   *     new YarnUncaughtExceptionHandler());
   * </pre>",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,accept,org.apache.hadoop.net.unix.DomainSocket:accept(),233,243,"/**
 * Creates a DomainSocket instance.
 * @param path Socket path.
 * @return DomainSocket object.
 * @throws IOException If an I/O error occurs.
 */
","* Accept a new UNIX domain connection.
   *
   * This method can only be used on sockets that were bound with bind().
   *
   * @return                The new connection.
   * @throws IOException    If there was an I/O error performing the accept--
   *                        such as the socket being closed from under us.
   *                        Particularly when the accept is timed out, it throws
   *                        SocketTimeoutException.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,setAttribute,"org.apache.hadoop.net.unix.DomainSocket:setAttribute(int,int)",308,317,"/**
 * Performs an operation with error handling.
 * @param type operation type, @param size operation size.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,getAttribute,org.apache.hadoop.net.unix.DomainSocket:getAttribute(int),321,332,"/**
 * Retrieves an attribute based on the given type.
 * @param type Attribute type; returns the attribute value.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,shutdown,org.apache.hadoop.net.unix.DomainSocket:shutdown(),395,404,"/**
 * Executes m3 and updates refCount based on success/failure.
 */","* Call shutdown(SHUT_RDWR) on the UNIX domain socket.
   *
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,sendFileDescriptors,"org.apache.hadoop.net.unix.DomainSocket:sendFileDescriptors(java.io.FileDescriptor[],byte[],int,int)",421,431,"/**
* Reads data from file descriptors into a byte array.
* @param descriptors file descriptors to read from
* @param jbuf buffer to store the read data
* @param offset start offset in the buffer
* @param length number of bytes to read
* @throws IOException if an I/O error occurs
*/
","* Send some FileDescriptor objects to the process on the other side of this
   * socket.
   * 
   * @param descriptors       The file descriptors to send.
   * @param jbuf              Some bytes to send.  You must send at least
   *                          one byte.
   * @param offset            The offset in the jbuf array to start at.
   * @param length            Length of the jbuf array to use.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,recvFileInputStreams,"org.apache.hadoop.net.unix.DomainSocket:recvFileInputStreams(java.io.FileInputStream[],byte[],int,int)",448,487,"/**
* Reads data from multiple file streams into a buffer.
* @param streams Input streams, streams[i] will be set to null.
* @param buf Buffer to store the data.
* @param offset Offset in the buffer.
* @param length Number of bytes to read.
* @return Number of bytes read.
*/","* Receive some FileDescriptor objects from the process on the other side of
   * this socket, and wrap them in FileInputStream objects.
   *
   * @param streams input stream.
   * @param buf input buf.
   * @param offset input offset.
   * @param length input length.
   * @return wrap them in FileInputStream objects.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,createNewInstance,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:createNewInstance(java.lang.Long),102,107,"/**
 * Creates and returns an IOStatisticsContext instance for a key.
 * @param key The key used to identify the statistics context.
 * @return An IOStatisticsContextImpl object.
 */
","* Creating a new IOStatisticsContext instance for a FS to be used.
   * @param key Thread ID that represents which thread the context belongs to.
   * @return an instance of IOStatisticsContext.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunkedSums,"org.apache.hadoop.util.DataChecksum:verifyChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer,java.lang.String,long)",401,427,"/**
* Calculates and verifies checksums for data, using native or fallback method.
* @param data ByteBuffer containing data
* @param checksums ByteBuffer for checksums
* @param fileName file name
* @param basePos base position
*/","* Verify that the given checksums match the given data.
   * 
   * The 'mark' of the ByteBuffer parameters may be modified by this function,.
   * but the position is maintained.
   *  
   * @param data the DirectByteBuffer pointing to the data to verify.
   * @param checksums the DirectByteBuffer pointing to a series of stored
   *                  checksums
   * @param fileName the name of the file being read, for error-reporting
   * @param basePos the file position to which the start of 'data' corresponds
   * @throws ChecksumException if the checksums do not match",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,afterDecryption,"org.apache.hadoop.crypto.CryptoInputStream:afterDecryption(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,long,byte[])",271,286,"/**
 * Processes decryption padding.
 * @param decryptor Decryptor object
 * @param inBuffer Input buffer
 * @param position Current position
 * @param iv Initialization vector
 * @return Padding byte
 */","* This method is executed immediately after decryption. Check whether 
   * decryptor should be updated and recalculate padding if needed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,resetStreamOffset,org.apache.hadoop.crypto.CryptoInputStream:resetStreamOffset(long),309,317,"/**
 * Initializes stream with given offset, resets buffers, and decrypts.
 * @param offset Offset for stream initialization.
 */
","* Reset the underlying stream offset; clear {@link #inBuffer} and 
   * {@link #outBuffer}. This Typically happens during {@link #seek(long)} 
   * or {@link #skip(long)}.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,write,"org.apache.hadoop.crypto.CryptoOutputStream:write(byte[],int,int)",151,172,"/**
* Reads data from input buffer to byte array.
* @param b byte array to fill, off offset, len number of bytes
*/","* Encryption is buffer based.
   * If there is enough room in {@link #inBuffer}, then write to this buffer.
   * If {@link #inBuffer} is full, then do encryption and write data to the
   * underlying stream.
   * @param b the data.
   * @param off the start offset in the data.
   * @param len the number of bytes to write.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,flush,org.apache.hadoop.crypto.CryptoOutputStream:flush(),262,269,"/**
 * Calls m1, then super.m2, if not closed.
 */","* To flush, we need to encrypt the data in the buffer and write to the 
   * underlying stream, then do the flush.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,compile,org.apache.hadoop.fs.GlobPattern:compile(java.lang.String),57,59,"/**
* Creates a Pattern object from a glob pattern string.
* @param globPattern The glob pattern string.
* @return A Pattern object.
*/
","* Compile glob pattern string
   * @param globPattern the glob pattern
   * @return the pattern object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,init,"org.apache.hadoop.fs.GlobFilter:init(java.lang.String,org.apache.hadoop.fs.PathFilter)",64,73,"/**
* Initializes file pattern and filter.
* @param filePattern Glob pattern for files.
* @param filter PathFilter to apply.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,prepare,org.apache.hadoop.fs.shell.find.Name:prepare(),73,80,"/**
 * Initializes the glob pattern based on a derived argument pattern.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingTar,"org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.InputStream,java.io.File,boolean)",1035,1051,"/**
 * Extracts a tar archive from an InputStream to a directory.
 * @param inputStream Input stream containing the tar archive.
 * @param untarDir Directory to extract the archive to.
 * @param gzipped True if the archive is gzipped.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,close,org.apache.hadoop.fs.sftp.SFTPFileSystem:close(),710,722,"/**
 * Calls super.m2(), returns if closed, releases connection.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,create,"org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",723,729,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to the file; permission, overwrite, etc.
 * @return FSDataOutputStream object
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,resetChecksumBufSize,org.apache.hadoop.fs.FSOutputSummer:resetChecksumBufSize(),263,265,"/**
* Calls m2 with the result of sum.m1() multiplied by BUFFER_NUM_CHUNKS.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getAllStatistics,org.apache.hadoop.fs.AbstractFileSystem:getAllStatistics(),234,244,"/**
 * Creates a map of URI to Statistics, copying from STATISTICS_TABLE.
 * Returns the new map.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,<init>,"org.apache.hadoop.fs.FileSystemStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)",118,125,"/**
 * Constructs a FileSystemStorageStatistics with a name and statistics.
 * @param name Statistics name.
 * @param stats FileSystem statistics object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getLongStatistics,org.apache.hadoop.fs.FileSystemStorageStatistics:getLongStatistics(),132,135,"/**
 * Returns an iterator for LongStatistic objects.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getLong,org.apache.hadoop.fs.FileSystemStorageStatistics:getLong(java.lang.String),137,140,"/**
* Returns a Long value based on the key, using internal methods.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesReadByDistance,org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadByDistance(int),4411,4430,"/**
 * Reads bytes based on distance. Returns the number of bytes read.
 */","* In the common network topology setup, distance value should be an even
     * number such as 0, 2, 4, 6. To make it more general, we group distance
     * by {1, 2}, {3, 4} and {5 and beyond} for accounting. So if the caller
     * ask for bytes read for distance 2, the function will return the value
     * for group {1, 2}.
     * @param distance the network distance
     * @return the total number of bytes read by the network distance",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,reset,org.apache.hadoop.fs.FileSystemStorageStatistics:reset(),157,160,"/**
* Delegates the m1() call to the stats object.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,clearStatistics,org.apache.hadoop.fs.AbstractFileSystem:clearStatistics(),218,222,"/**
* Iterates through statistics and calls the m1() method on each.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,primitiveCreate,"org.apache.hadoop.fs.FileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",1334,1359,"/**
 * Creates a FSDataOutputStream with specified flags and permissions.
 * @param f Path to create the output stream on.
 */","* This create has been added to support the FileContext that processes
   * the permission with umask before calling this method.
   * This a temporary method added to support the transition from FileSystem
   * to FileContext for user applications.
   *
   * @param f path.
   * @param absolutePermission permission.
   * @param flag create flag.
   * @param bufferSize buffer size.
   * @param replication replication.
   * @param blockSize block size.
   * @param progress progress.
   * @param checksumOpt check sum opt.
   * @return output stream.
   * @throws IOException IO failure",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,<init>,"org.apache.hadoop.fs.AbstractFileSystem:<init>(java.net.URI,java.lang.String,boolean,int)",278,283,"/**
 * Constructs a FileSystem with URI, scheme, authority flag, and default port.
 */
","* Constructor to be called by subclasses.
   * 
   * @param uri for this file system.
   * @param supportedScheme the scheme supported by the implementor
   * @param authorityNeeded if true then theURI must have authority, if false
   *          then the URI must have null authority.
   * @param defaultPort default port to use if port is not specified in the URI.
   * @throws URISyntaxException <code>uri</code> has syntax error",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(byte[][],byte[][])",117,127,"/**
* Encodes data from inputs to outputs; returns if dataLen is 0.
*/","* Encode with inputs and generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param outputs output buffers to put the encoded data into, read to read
   *                after the call
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",68,99,"/**
 * Encodes data into output buffers, updating input buffer positions.
 */","* Encode with inputs and generates outputs.
   *
   * Note, for both inputs and outputs, no mixing of on-heap buffers and direct
   * buffers are allowed.
   *
   * If the coder option ALLOW_CHANGE_INPUTS is set true (false by default), the
   * content of input buffers may change after the call, subject to concrete
   * implementation. Anyway the positions of input buffers will move forward.
   *
   * @param inputs input buffers to read data from. The buffers' remaining will
   *               be 0 after encoding
   * @param outputs output buffers to put the encoded data into, ready to read
   *                after the call
   * @throws IOException if the encoder is closed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Object),122,124,"/**
 * Constructs a new ArrayPrimitiveWritable with the given value.
 */","* Wrap an existing array of primitives
   * @param value - array of primitives",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getCanonicalUri,org.apache.hadoop.fs.HarFileSystem:getCanonicalUri(),318,321,"/**
* Delegates m1() call to the underlying file system.
* @return URI returned by the file system's m1() method.
*/
","* Used for delegation token related functionality. Must delegate to
   * underlying file system.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getCanonicalUri,org.apache.hadoop.fs.FilterFileSystem:getCanonicalUri(),113,116,"/**
* Delegates m1() call to the underlying file system.
* @return URI returned by the file system's m1() method.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFsStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus(),148,151,"/**
* Delegates to the underlying file system implementation.
* @return Status object from the file system implementation.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasCapability,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:hasCapability(java.lang.String),687,693,"/**
 * Checks if capability is handled; delegates if not.
 * @param capability The capability to check.
 * @return True if capability is handled, false otherwise.
 */","* Probe the inner stream for a capability.
     * Syncable operations are rejected before being passed down.
     * @param capability string to query the stream support for.
     * @return true if a capability is known to be supported.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hasCapability,org.apache.hadoop.io.SequenceFile$Writer:hasCapability(java.lang.String),1410,1416,"/**
 * Delegates to the wrapped object's m1 method.
 * @param capability The capability string to pass.
 * @return True if the wrapped object's m1 returns true, false otherwise.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasCapability,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:hasCapability(java.lang.String),484,487,"/**
 * Delegates capability check to the underlying data store.
 * @param capability The capability to check.
 * @return True if capability exists, false otherwise.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,addToCacheAndRelease,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:addToCacheAndRelease(org.apache.hadoop.fs.impl.prefetch.BufferData,java.util.concurrent.Future,java.time.Instant)",484,552,"/**
* Processes buffer data, potentially caching it after a block future.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,release,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData),215,226,"/**
 * Processes data using ops and buffer pool.
 * @param data BufferData object to be processed.
 */","* Releases resources allocated to the given block.
   *
   * @throws IllegalArgumentException if data is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,releaseDoneBlocks,org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseDoneBlocks(),192,198,"/**
 * Processes BufferData objects where m1(DONE) returns true.
 * Iterates through data from m3() and calls m2() for each.
 */",* Releases resources for any blocks marked as 'done'.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getSummary(boolean),232,250,"/**
 * Processes operations, appending debug info or executing them.
 * @param showDebugInfo if true, shows debug info; otherwise, executes ops.
 * @return String representation of processed operations.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,<init>,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters),113,137,"/**
 * Constructs a CachingBlockManager with provided parameters.
 * @param blockManagerParameters Configuration parameters for the manager.
 */
","* Constructs an instance of a {@code CachingBlockManager}.
   *
   * @param blockManagerParameters params for block manager.
   * @throws IllegalArgumentException if bufferPoolSize is zero or negative.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,<init>,"org.apache.hadoop.util.BlockingThreadPoolExecutorService:<init>(int,java.util.concurrent.ThreadPoolExecutor)",104,108,"/**
 * Initializes the BlockingThreadPoolExecutorService.
 * @param permitCount Initial number of permits.
 * @param eventProcessingExecutor Executor for event processing.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,get,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:get(int,java.nio.ByteBuffer)",265,283,"/**
* Reads data from a block into a buffer.
* @param blockNumber Block number to read.
* @param buffer ByteBuffer to store the data.
*/
","* Gets the block having the given {@code blockNumber}.
   *
   * @throws IllegalArgumentException if buffer is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,toString,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:toString(),635,647,"/**
 * Generates a string describing cache and buffer pool status.
 * Returns a formatted string with cache and pool details.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,absolute,org.apache.hadoop.fs.impl.prefetch.FilePosition:absolute(),145,148,"/**
 * Calculates a masked value by adding m2 to bufferStartOffset.
 */","* Gets the current absolute position within this file.
   *
   * @return the current absolute position within this file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,bufferFullyRead,org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferFullyRead(),241,246,"/**
* Checks buffer status: start offset, m2() value, and bytes read.
*/","* Determines whether the current buffer has been fully read.
   *
   * @return true if the current buffer has been fully read, false otherwise.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,setAbsolute,org.apache.hadoop.fs.impl.prefetch.FilePosition:setAbsolute(long),157,165,"/**
 * Updates buffer at given position if conditions are met.
 * @param pos Position to update in the buffer.
 * @return True if updated, false otherwise.
 */
","* If the given {@code pos} lies within the current buffer, updates the current position to
   * the specified value and returns true; otherwise returns false without changing the position.
   *
   * @param pos the absolute position to change the current position to if possible.
   * @return true if the given current position was updated, false otherwise.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,getCurrentIOStatisticsContext,org.apache.hadoop.fs.statistics.IOStatisticsContext:getCurrentIOStatisticsContext(),71,77,"/**
* Creates an IOStatisticsContext using m2 with provided arguments.
*/","* Get the context's IOStatisticsContext.
   *
   * @return instance of IOStatisticsContext for the context.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,setThreadIOStatisticsContext,org.apache.hadoop.fs.statistics.IOStatisticsContext:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext),84,88,"/**
* Delegates IO statistics processing to the integration layer.
* @param statisticsContext Context for IO statistics.
*/","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getInstanceConfigs,org.apache.hadoop.metrics2.impl.MetricsConfig:getInstanceConfigs(java.lang.String),157,171,"/**
 * Extracts metrics configurations for a given type.
 * @param type The type of metrics to extract.
 * @return A map of instance to MetricsConfig.
 */
","* Return sub configs for instance specified in the config.
   * Assuming format specified as follows:<pre>
   * [type].[instance].[option] = [value]</pre>
   * Note, '*' is a special default instance, which is excluded in the result.
   * @param type  of the instance
   * @return  a map with [instance] as key and config object as value",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,applyItem,org.apache.hadoop.fs.shell.find.Find:applyItem(org.apache.hadoop.fs.shell.PathData),412,419,"/**
 * Processes PathData item based on conditions and results.
 * @param item The PathData object to process.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processArguments,org.apache.hadoop.fs.shell.find.Find:processArguments(java.util.LinkedList),421,429,"/**
* Processes path data, configures expression, and calls super.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,processOptions,org.apache.hadoop.fs.shell.Test:processOptions(java.util.LinkedList),59,75,"/**
* Parses command flags from arguments.
* @param args Command line arguments to parse.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rm:processOptions(java.util.LinkedList),81,90,"/**
 * Parses command arguments and sets flags for file operations.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processOptions,org.apache.hadoop.fs.shell.Display$Checksum:processOptions(java.util.LinkedList),189,195,"/**
 * Processes command arguments, sets display block size.
 * @param args Command line arguments to process.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Df:processOptions(java.util.LinkedList),85,92,"/**
 * Processes command arguments, formats, and handles separators.
 * @param args List of command line arguments.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,processOptions,org.apache.hadoop.fs.shell.Head:processOptions(java.util.LinkedList),50,54,"/**
* Processes command arguments using CommandFormat.
* @param args List of command arguments to process.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processOptions,org.apache.hadoop.fs.shell.Ls:processOptions(java.util.LinkedList),132,153,"/**
 * Processes command-line arguments and configures listing options.
 * @param args Command-line arguments to parse.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,processOptions,org.apache.hadoop.fs.shell.Tail:processOptions(java.util.LinkedList),63,78,"/**
* Processes command arguments, potentially setting a follow delay.
* @param args List of command arguments to process.
* @throws IOException if an I/O error occurs.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Expunge:processOptions(java.util.LinkedList),235,242,"/**
 * Configures command format with filesystem option and arguments.
 * @param args Command line arguments to be processed.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rmdir:processOptions(java.util.LinkedList),197,203,"/**
 * Processes command arguments and sets ignoreNonEmpty flag.
 * @param args Command line arguments to process.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Get:processOptions(java.util.LinkedList),234,249,"/**
* Executes a command sequence using provided arguments.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,processOptions,org.apache.hadoop.fs.shell.Count:processOptions(java.util.LinkedList),124,178,"/**
* Configures and displays quota information based on command args.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processOptions,org.apache.hadoop.fs.shell.Mkdir:processOptions(java.util.LinkedList),51,56,"/**
 * Processes command arguments, sets flags, and creates parents.
 * @param args Command arguments to process.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processOptions,org.apache.hadoop.fs.shell.TouchCommands$Touch:processOptions(java.util.LinkedList),134,146,"/**
 * Parses command arguments and sets flags.
 * @param args Command line arguments to parse.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Put:processOptions(java.util.LinkedList),276,292,"/**
* Executes a command sequence using provided arguments.
* @param args Command line arguments to be processed.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Cp:processOptions(java.util.LinkedList),175,189,"/**
* Processes command arguments, configures format, and executes actions.
* @param args List of command line arguments.
* @throws IOException if an I/O error occurs.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processOptions(java.util.LinkedList),377,390,"/**
 * Processes arguments, validates input, and executes commands.
 * @param args List of arguments passed to the method.
 * @throws IOException if destination argument is missing.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processOptions,org.apache.hadoop.fs.shell.Display$Cat:processOptions(java.util.LinkedList),80,86,"/**
 * Processes command arguments, formats, and verifies checksum.
 * @param args Command arguments to process.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processOptions,org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processOptions(java.util.LinkedList),64,75,"/**
 * Processes command arguments, validates input, and calls m3.
 * @param args Command line arguments to process.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,processOptions,org.apache.hadoop.fs.shell.Stat:processOptions(java.util.LinkedList),82,89,"/**
 * Processes arguments, formats data, and sets the format.
 * @param args List of arguments to process.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processOptions,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processOptions(java.util.LinkedList),61,65,"/**
* Initializes CommandFormat with parameters and calls m1.
* @param args List of arguments passed to m1.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,parse,"org.apache.hadoop.fs.shell.CommandFormat:parse(java.lang.String[],int)",87,92,"/**
 * Modifies and returns a list of strings from the input array.
 */","Parse parameters starting from the given position
   * Consider using the variant that directly takes a List
   * 
   * @param args an array of input arguments
   * @param pos the position at which starts to parse
   * @return a list of parameters",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processOptions,org.apache.hadoop.fs.shell.MoveCommands$Rename:processOptions(java.util.LinkedList),104,109,"/**
* Processes command arguments.
* @param args List of command arguments to process.
* @throws IOException if an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processOptions,org.apache.hadoop.fs.shell.Truncate:processOptions(java.util.LinkedList),51,66,"/**
 * Processes command arguments, parses length, and validates it.
 * @param args Command line arguments to process.
 * @throws IOException If an I/O error occurs.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processOptions,org.apache.hadoop.fs.shell.SetReplication:processOptions(java.util.LinkedList),56,72,"/**
* Processes command arguments, sets replication factor, validates input.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Du:processOptions(java.util.LinkedList),183,192,"/**
 * Processes command arguments, configures format, and sets flags.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processOptions,org.apache.hadoop.fs.FsShellPermissions$Chown:processOptions(java.util.LinkedList),144,150,"/**
 * Processes command arguments, formats, and calls related methods.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayError,org.apache.hadoop.fs.shell.Command:displayError(java.lang.String),501,504,"/**
 * Logs an error message and increments the error count.
 * @param message The error message to log.
 */
","* Display an error string prefaced with the command name.  Also increments
   * the error count for the command which will result in a non-zero exit
   * code.
   * @param message error message to display",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInstanceUsage,"org.apache.hadoop.fs.FsShell:printInstanceUsage(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)",249,251,"/**
* Logs a combined message to the output stream.
* @param out PrintStream to write to.
* @param instance Command instance to use.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,addRecord,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(java.lang.String),52,55,"/**
* Creates a MetricsRecordBuilderImpl with a given name.
* Uses m1 to create a MetricDefinition.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processArguments,org.apache.hadoop.fs.shell.FsUsage$Df:processArguments(java.util.LinkedList),94,105,"/**
 * Processes filesystem data, writes header, calls super, 
 * and flushes output if needed.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processArguments,org.apache.hadoop.fs.shell.FsUsage$Du:processArguments(java.util.LinkedList),194,207,"/**
 * Processes path data, potentially writing headers and data.
 * @param args LinkedList of PathData objects to process.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createPathHandle,"org.apache.hadoop.fs.RawLocalFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",1148,1172,"/**
 * Creates a PathHandle for a file, validating file system and options.
 * @param stat FileStatus object
 * @param opts Handle options
 * @return LocalFileSystemPathHandle object
 */","* Hook to implement support for {@link PathHandle} operations.
   * @param stat Referent in the target FileSystem
   * @param opts Constraints that determine the validity of the
   *            {@link PathHandle} reference.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,exact,org.apache.hadoop.fs.Options$HandleOpt:exact(),384,386,"/**
 * Returns an array containing the results of m1 and m2 (both false).
 */","* Handle is valid iff the referent is neither moved nor changed.
     * Equivalent to changed(false), moved(false).
     * @return Options requiring that the content and location of the entity
     * be unchanged between calls.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,content,org.apache.hadoop.fs.Options$HandleOpt:content(),394,396,"/**
* Returns an array containing the results of m1(false) and m2(true).
*/","* Handle is valid iff the content of the referent is the same.
     * Equivalent to changed(false), moved(true).
     * @return Options requiring that the content of the entity is unchanged,
     * but it may be at a different location.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,path,org.apache.hadoop.fs.Options$HandleOpt:path(),404,406,"/**
* Returns an array containing the results of m1(true) and m2(false).
*/","* Handle is valid iff the referent is unmoved in the namespace.
     * Equivalent to changed(true), moved(false).
     * @return Options requiring that the referent exist in the same location,
     * but its content may have changed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,reference,org.apache.hadoop.fs.Options$HandleOpt:reference(),414,416,"/**
* Returns an array containing the results of m1(true) and m2(true).
*/","* Handle is valid iff the referent exists in the namespace.
     * Equivalent to changed(true), moved(true).
     * @return Options requiring that the implementation resolve a reference
     * to this entity regardless of changes to content or location.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long,boolean)",1248,1259,"/**
 * Creates a ShellCommandExecutor with given command, directory, env, timeout.
 */","* Create a new instance of the ShellCommandExecutor to execute a command.
     *
     * @param execString The command to execute with arguments
     * @param dir If not-null, specifies the directory which should be set
     *            as the current working directory for the command.
     *            If null, the current working directory is not modified.
     * @param env If not-null, environment of the command will include the
     *            key-value pairs specified in the map. If null, the current
     *            environment is not modified.
     * @param timeout Specifies the time in milliseconds, after which the
     *                command will be killed and the status marked as timed-out.
     *                If 0, the command will not be timed out.
     * @param inheritParentEnv Indicates if the process should inherit the env
     *                         vars from the parent process or not.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,initRefreshThread,org.apache.hadoop.fs.CachingGetSpaceUsed:initRefreshThread(boolean),108,118,"/**
 * Starts or stops the refresh thread based on refreshInterval.
 * @param runImmediately Whether to start immediately or schedule.
 */
","* RunImmediately should set true, if we skip the first refresh.
   * @param runImmediately The param default should be false.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,privateClone,org.apache.hadoop.security.token.Token:privateClone(org.apache.hadoop.io.Text),243,245,"/**
 * Creates a new PrivateToken wrapping the given service.
 * @param newService The service to wrap in a PrivateToken.
 * @return A new PrivateToken instance.
 */
","* Create a private clone of a public token.
   * @param newService the new service name
   * @return a private token",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ViewFileSystem:getAllStoragePolicies(),1125,1139,"/**
 * Retrieves all BlockStoragePolicySpi instances from FileSystems.
 * @return Collection of BlockStoragePolicySpi instances.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,initAsyncCall,"org.apache.hadoop.io.retry.AsyncCallHandler:initAsyncCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue)",333,353,"/**
 * Executes an async call and returns the result.
 * @param asyncCall The async call to execute.
 * @param asyncCallReturn Async value containing the call result.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,process,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:process(java.nio.ByteBuffer,java.nio.ByteBuffer)",164,182,"/**
 * Encrypts data from inBuffer to outBuffer.
 * @param inBuffer Input buffer containing data to encrypt.
 * @param outBuffer Output buffer for encrypted data.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,initializeInterceptors,org.apache.hadoop.fs.viewfs.RegexMountPoint:initializeInterceptors(),98,115,"/**
* Processes interceptor settings, creates interceptors, and adds them.
* @throws IOException if settings are invalid or processing fails.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,main,org.apache.hadoop.fs.DF:main(java.lang.String[]),216,223,"/**
 * Executes a DataFormat scan of the specified path (or ""."").
 * @param args Command-line arguments; first is the path.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,initialize,"org.apache.hadoop.fs.Path:initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",258,266,"/**
 * Constructs a URI from scheme, authority, path, and fragment.
 * @param scheme URI scheme (e.g., http)
 * @param authority URI authority (e.g., example.com)
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,uriToString,"org.apache.hadoop.fs.shell.PathData:uriToString(java.net.URI,boolean)",466,487,"/**
 * Constructs a URI string.
 * @param uri The URI object.
 * @param inferredSchemeFromPath Flag to infer scheme from path.
 * @return URI string representation.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,checkNotSchemeWithRelative,org.apache.hadoop.fs.Path:checkNotSchemeWithRelative(),85,90,"/**
 * Throws exception if m1().m2() is true and m3() is false.
 */","* Test whether this Path uses a scheme and is relative.
   * Pathnames with scheme and relative path are illegal.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isAbsoluteAndSchemeAuthorityNull,org.apache.hadoop.fs.Path:isAbsoluteAndSchemeAuthorityNull(),377,380,"/**
* Checks a condition based on m1() and null checks on uri fields.
*/","* Returns true if the path component (i.e. directory) of this URI is
   * absolute <strong>and</strong> the scheme is null, <b>and</b> the authority
   * is null.
   *
   * @return whether the path is absolute and the URI has no scheme nor
   * authority parts",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isAbsolute,org.apache.hadoop.fs.Path:isAbsolute(),399,401,"/**
* Delegates to m1() and returns its boolean result.
*/","* Returns true if the path component (i.e. directory) of this URI is
   * absolute.  This method is a wrapper for {@link #isUriPathAbsolute()}.
   *
   * @return whether this URI's path is absolute",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,checkPath,org.apache.hadoop.fs.AbstractFileSystem:checkPath(org.apache.hadoop.fs.Path),369,413,"/**
 * Validates a Path object against the current filesystem's configuration.
 */","* Check that a Path belongs to this FileSystem.
   * 
   * If the path is fully qualified URI, then its scheme and authority
   * matches that of this file system. Otherwise the path must be 
   * slash-relative name.
   * @param path the path.
   * @throws InvalidPathException if the path is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,write,org.apache.hadoop.fs.FileStatus:write(java.io.DataOutput),530,537,"/**
 * Writes the file status to an output stream.
 * @param out Output stream to write to.
 * @throws IOException if an I/O error occurs.
 */
","* Write instance encoded as protobuf to stream.
   * @param out Output stream
   * @see PBHelper#convert(FileStatus)
   * @deprecated Use the {@link PBHelper} and protobuf serialization directly.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPutArguments,"org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPutArguments(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)",114,124,"/**
 * Validates input parameters for file upload processing.
 * @param filePath Path to the file.
 * @param inputStream Input stream for the file part.
 * @param partNumber Part number of the upload.
 * @param uploadId Upload handle identifier.
 * @param lengthInBytes Length of the file part in bytes.
 */","* Check all the arguments to the
   * {@link MultipartUploader#putPart(UploadHandle, int, Path, InputStream, long)}
   * operation.
   * @param filePath Target path for upload (as {@link #startUpload(Path)}).
   * @param inputStream Data for this part. Implementations MUST close this
   * stream after reading in the data.
   * @param partNumber Index of the part relative to others.
   * @param uploadId Identifier from {@link #startUpload(Path)}.
   * @param lengthInBytes Target length to read from the stream.
   * @throws IllegalArgumentException invalid argument",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,abortUploadsUnderPath,org.apache.hadoop.fs.impl.AbstractMultipartUploader:abortUploadsUnderPath(org.apache.hadoop.fs.Path),132,138,"/**
 * Returns a CompletableFuture that is immediately completed with -1.
 * @param path Path object (unused in this method)
 * @return CompletableFuture<Integer> - immediately completed with -1
 */
","* {@inheritDoc}.
   * @param path path to abort uploads under.
   * @return a future to -1.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",1452,1456,"/**
* Throws an IOException when attempting to append to a path.
* @param f the path to append to
* @param bufferSize the buffer size
* @param progress progress tracker
* @throws IOException if append operation is attempted
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)",1498,1503,"/**
* Deletes a file or directory. Throws exceptions on failure.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1739,1745,"/**
* Renames a file from src to dst, throwing an AccessControlException or IOException.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)",1747,1750,"/**
* Truncates a file to the specified length. Throws an exception.
* @param f the file to truncate
* @param newLength the new length of the file
* @throws IOException if an I/O error occurs
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1752,1757,"/**
* Sets file owner; throws AccessControlException or IOException.
* @param f file path
* @param username username
* @param groupname group name
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1759,1764,"/**
* Sets file permission, throws exception if fails.
* @param f Path to file. @param permission Permission to set.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)",1766,1771,"/**
* Sets replication factor for a file path.
* @param f file path. @param replication replication factor.
* @throws AccessControlException, IOException
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)",1773,1778,"/**
* Sets file modification times.
* @param f The file path.
* @param mtime Modification time.
* @param atime Access time.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1800,1805,"/**
* Modifies ACL entries for a path, throwing an exception.
* @param path Path to modify. @param aclSpec ACL entries.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1807,1812,"/**
 * Removes ACL entries for a given path.
 * @param path Path to remove ACL entries from.
 * @param aclSpec ACL entries to remove (unused).
 * @throws IOException if an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),1814,1818,"/**
* Removes the default ACL for a given path.
* @param path The path for which to remove the ACL.
* @throws IOException if an I/O error occurs.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path),1820,1824,"/**
* Removes ACL for a given path.
* @param path the path to remove ACL from.
* @throws IOException if an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",1826,1830,"/**
* Sets ACL for a path, throwing an IOException if failed.
* @param path Path to set ACL on.
* @param aclSpec ACL entries to apply.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",1841,1846,"/**
* Sets an extended attribute; throws an IOException on failure.
* @param path Path to set the attribute on.
* @param name Attribute name.
* @param value Attribute value.
* @param flag Attribute set flags.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1869,1873,"/**
* Removes an X attribute from a path.
* @param path Path to operate on.
* @param name Attribute name.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1875,1880,"/**
 * Creates a snapshot of the given path, throws an IOException.
 * @param path Path to snapshot.
 * @param snapshotName Snapshot name.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1882,1887,"/**
* Renames a snapshot. Throws an IOException after m1 and m2.
* @param path Path to the snapshot.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1889,1894,"/**
 * Deletes a snapshot. Deletes path and throws an exception.
 * @param path Path to delete.
 * @param snapshotName Snapshot name (unused).
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1901,1905,"/**
* Processes a Path, calls m1, then throws an IOException.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1907,1912,"/**
* Sets storage policy for a path.
* @param src Path to apply policy to.
* @param policyName Policy name (unused).
* @throws IOException if an error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),1914,1918,"/**
* Applies a storage policy and throws an exception.
* @param src The Path to apply the policy to.
* @throws IOException if an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,delete,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)",1044,1049,"/**
 * Deletes a file or directory. Throws exceptions on failure.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)",1308,1313,"/**
* Truncates a file to the specified length.
* @param f The file to truncate. @param newLength The new length.
* @throws IOException If an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1315,1321,"/**
* Renames a file from src to dst, throwing exceptions if needed.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1328,1332,"/**
* Creates a symbolic link at the target path.
* @param target Link target path.
* @param link Link path.
* @param createParent Whether to create parent directories.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1340,1345,"/**
* Sets file owner.
* @param f file path
* @param username new owner username
* @param groupname new groupname
* @throws AccessControlException, IOException
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1347,1352,"/**
* Sets file permission, throwing an exception if failed.
* @param f Path to file. @param permission Permission to set.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)",1354,1359,"/**
* Sets replication factor for a file path.
* @param f Path to the file. @param replication Replication factor.
* @throws AccessControlException, IOException
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)",1361,1366,"/**
* Sets file modification times.
* @param f Path to the file.
* @param mtime Modification time.
* @param atime Access time.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1374,1379,"/**
 * Sets ACL entries for a path. Throws an exception.
 * @param path The path to modify.
 * @param aclSpec ACL entries to apply.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1381,1386,"/**
 * Removes ACL entries for a path.
 * @param path The path to remove ACLs from.
 * @param aclSpec ACL entry specifications (unused).
 * @throws IOException If an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),1388,1392,"/**
* Removes the default ACL for a given path.
* @param path the path to remove the default ACL from
* @throws IOException if an I/O error occurs
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path),1394,1398,"/**
* Removes ACL for a path.
* @param path The path to remove ACL from.
* @throws IOException if an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",1400,1404,"/**
 * Sets ACL for a path, throwing an IOException if failed.
 * @param path Path to set ACL on.
 * @param aclSpec ACL entries to apply.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",1415,1420,"/**
* Sets an extended attribute; throws an IOException if failed.
* @param path Path to set the attribute on.
* @param name Attribute name.
* @param value Attribute value.
* @param flag Attribute set flags.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1443,1447,"/**
* Removes an X attribute from a path.
* @param path The path to modify.
* @param name Attribute name.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1449,1454,"/**
* Creates a snapshot of the given path.
* @param path Path to snapshot.
* @param snapshotName Snapshot name.
* @throws IOException If an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1456,1461,"/**
* Renames a snapshot.
* @param path Path to snapshot.
* @param snapshotOldName Old snapshot name.
* @param snapshotNewName New snapshot name.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1463,1468,"/**
* Deletes a snapshot by path.
* @param path Path to snapshot directory.
* @param snapshotName Name of the snapshot.
* @throws IOException if an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1470,1473,"/**
* Throws an IOException with a message related to the path.
* @param path The path to be processed.
* @throws IOException If an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1475,1479,"/**
* Sets storage policy for a path.
* @param path Path to set policy on.
* @param policyName Policy name.
* @throws IOException If an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",340,364,"/**
* Creates a data output stream at the specified path.
* @param f Path to create stream at; creates parent if needed.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",651,667,"/**
 * Resolves link and creates symlink, throwing exceptions on failure.
 * @param target symlink target path
 * @param link link path
 * @param createParent create parent directories if needed
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:equals(java.lang.Object),3923,3929,"/**
 * Checks if object is a LinkedSegmentsDescriptor, then calls super.m1.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getCredentialEntry,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getCredentialEntry(java.lang.String),173,198,"/**
 * Retrieves a credential entry by alias.
 * @param alias credential alias
 * @return CredentialEntry object or null if not found
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getAliases,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getAliases(),206,226,"/**
 * Retrieves a list of aliases from the keystore.
 * @return List of aliases or an empty list if none exist.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,skip,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:skip(long),291,299,"/**
 * Reads 'n' bytes, adjusting if beyond file length.
 * @param n number of bytes to read
 * @return number of bytes actually read
 */
","* Skips over and discards <code>n</code> bytes of data from the
     * input stream.
     *
     * The <code>skip</code> method skips over some smaller number of bytes
     * when reaching end of file before <code>n</code> bytes have been skipped.
     * The actual number of bytes skipped is returned.  If <code>n</code> is
     * negative, no bytes are skipped.
     *
     * @param      n   the number of bytes to be skipped.
     * @return     the actual number of bytes skipped.
     * @exception  IOException  if an I/O error occurs.
     *             ChecksumException if the chunk to skip to is corrupted",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getPermissions,org.apache.hadoop.fs.ftp.FTPFileSystem:getPermissions(org.apache.commons.net.ftp.FTPFile),455,461,"/**
 * Creates an FsPermission object from FTPFile access levels.
 * @param ftpFile the FTPFile containing access permissions
 * @return An FsPermission object representing the permissions
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,applyUMask,org.apache.hadoop.fs.permission.FsPermission:applyUMask(org.apache.hadoop.fs.permission.FsPermission),297,301,"/**
 * Calculates permission mask based on a given umask.
 * @param umask The umask to calculate the permission mask from.
 * @return A new FsPermission object representing the mask.
 */
","* Apply a umask to this permission and return a new one.
   *
   * The umask is used by create, mkdir, and other Hadoop filesystem operations.
   * The mode argument for these operations is modified by removing the bits
   * which are set in the umask.  Thus, the umask limits the permissions which
   * newly created files and directories get.
   *
   * @param umask              The umask to use
   * 
   * @return                   The effective permission",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FsPermissionProto),38,41,"/**
 * Creates an FsPermission from a FsPermissionProto.
 * @param proto The FsPermissionProto to convert.
 * @return An FsPermission object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getPermissions,org.apache.hadoop.fs.sftp.SFTPFileSystem:getPermissions(com.jcraft.jsch.ChannelSftp$LsEntry),305,307,"/**
* Creates an FsPermission from the given LsEntry's mode.
*/","* Return file permission.
   *
   * @param sftpFile
   * @return file permission",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processPath,org.apache.hadoop.fs.FsShellPermissions$Chmod:processPath(org.apache.hadoop.fs.shell.PathData),98,110,"/**
* Updates file permissions if they have changed.
* @param item PathData object containing file information.
* @throws IOException if an error occurs while changing permissions.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(int),127,129,"/**
 * Constructs a FsPermission with the given octal mode.
 * @param mode Octal representation of file permissions.
 */
","* Construct by the given mode.
   *
   * octal mask is applied.
   *
   *<pre>
   *              before mask     after mask    file type   sticky bit
   *
   *    octal     100644            644         file          no
   *    decimal    33188            420
   *
   *    octal     101644           1644         file          yes
   *    decimal    33700           1420
   *
   *    octal      40644            644         directory     no
   *    decimal    16804            420
   *
   *    octal      41644           1644         directory     yes
   *    decimal    17316           1420
   *</pre>
   *
   * 100644 becomes 644 while 644 remains as 644
   *
   * @param mode Mode is supposed to come from the result of native stat() call.
   *             It contains complete permission information: rwxrwxrwx, sticky
   *             bit, whether it is a directory or a file, etc. Upon applying
   *             mask, only permission and sticky bit info will be kept because
   *             they are the only parts to be used for now.
   * @see #FsPermission(short mode)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getDefault,org.apache.hadoop.fs.permission.FsPermission:getDefault(),416,418,"/**
 * Returns a default FsPermission with read, write, and execute permissions.
 */","* Get the default permission for directory and symlink.
   * In previous versions, this default permission was also used to
   * create files, so files created end up with ugo+x permission.
   * See HADOOP-9155 for detail. 
   * Two new methods are added to solve this, please use 
   * {@link FsPermission#getDirDefault()} for directory, and use
   * {@link FsPermission#getFileDefault()} for file.
   * This method is kept for compatibility.
   *
   * @return Default FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getDirDefault,org.apache.hadoop.fs.permission.FsPermission:getDirDefault(),425,427,"/**
 * Returns a FsPermission object representing the FUNC_MASK (00777).
 */","* Get the default permission for directory.
   *
   * @return DirDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getFileDefault,org.apache.hadoop.fs.permission.FsPermission:getFileDefault(),434,436,"/**
 * Returns a FsPermission object representing the default file permissions (0666).
 */","* Get the default permission for file.
   *
   * @return FileDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getCachePoolDefault,org.apache.hadoop.fs.permission.FsPermission:getCachePoolDefault(),443,445,"/**
 * Returns a FsPermission representing the FUNC_MASK (00755).
 */","* Get the default permission for cache pools.
   *
   * @return CachePoolDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,valueOf,org.apache.hadoop.fs.permission.FsPermission:valueOf(java.lang.String),452,475,"/**
 * Parses a Unix symbolic permission string into an FsPermission.
 * @param unixSymbolicPermission Unix permission string (e.g., ""rwxrwxrwx"")
 * @return FsPermission representing the parsed permission.
 */","* Create a FsPermission from a Unix symbolic permission string
   * @param unixSymbolicPermission e.g. ""-rw-rw-rw-""
   * @return FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission:<init>(short),479,481,"/**
 * Constructs a new ImmutableFsPermission with the given permission value.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processOptions,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processOptions(java.util.LinkedList),188,248,"/**
 * Validates arguments, checks flags, and processes ACL entries.
 * @param args Command-line arguments to validate and process.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,printAclEntriesForSingleScope,"org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printAclEntriesForSingleScope(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,java.util.List)",112,126,"/**
* Processes AclEntry list based on conditions.
* @param aclStatus Acl status object
* @param fsPerm FsPermission object
* @param entries List of AclEntry objects to process
*/
","* Prints all the ACL entries in a single scope.
     * @param aclStatus AclStatus for the path
     * @param fsPerm FsPermission for the path
     * @param entries List<AclEntry> containing ACL entries of file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,aclSpecToString,org.apache.hadoop.fs.permission.AclEntry:aclSpecToString(java.util.List),330,337,"/**
 * Converts a list of AclEntry objects to a comma-separated string.
 */","* Convert a List of AclEntries into a string - the reverse of parseAclSpec.
   * @param aclSpec List of AclEntries to convert
   * @return String representation of aclSpec",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(java.lang.String),148,150,"/**
 * Creates a FsPermission with the given mode string.
 * @param mode String representing the file permissions.
 */
","* Construct by given mode, either in octal or symbolic format.
   * @param mode mode as a string, either in octal or symbolic format
   * @throws IllegalArgumentException if <code>mode</code> is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processOptions,org.apache.hadoop.fs.FsShellPermissions$Chmod:processOptions(java.util.LinkedList),81,96,"/**
 * Parses chmod arguments, creates a parser, and validates mode.
 * @param args Command line arguments for chmod operation.
 * @throws IOException If an I/O error occurs.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,read,org.apache.hadoop.fs.store.ByteBufferInputStream:read(),94,100,"/**
 * Returns a masked byte from the buffer if m1() > 0, else -1.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,skip,org.apache.hadoop.fs.store.ByteBufferInputStream:skip(long),102,114,"/**
 * Seeks to a new position, adjusting for offset.
 * @param offset Offset from current position; must be non-negative.
 * @return New position after seeking.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,mark,org.apache.hadoop.fs.store.ByteBufferInputStream:mark(int),140,145,"/**
* Marks, clears, and processes the buffer up to readlimit.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,read,"org.apache.hadoop.fs.store.ByteBufferInputStream:read(byte[],int,int)",169,189,"/**
* Reads up to 'length' bytes from the source into the provided buffer.
* @param b buffer to read into, offset start index, length to read
* @return number of bytes read, or -1 on failure
*/","* Read in data.
   * @param b destination buffer.
   * @param offset offset within the buffer.
   * @param length length of bytes to read.
   * @throws EOFException if the position is negative
   * @throws IndexOutOfBoundsException if there isn't space for the
   * amount of data requested.
   * @throws IllegalArgumentException other arguments are invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:startUpload(),887,897,"/**
 * Uploads a block of data and returns BlockUploadData.
 * Uses super.m1(), then closes out and sets it to null.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:startUpload(),593,600,"/**
* Retrieves block upload data from the buffer.
* @return BlockUploadData object containing buffer data.
* @throws IOException if an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:startUpload(),722,731,"/**
 * Uploads a block of data.
 * Reads data size, copies data, and returns BlockUploadData.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,close,org.apache.hadoop.fs.store.DataBlocks$DataBlock:close(),475,481,"/**
 * Closes the resource if m1() returns true, logs closure.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,checkAndWriteSync,org.apache.hadoop.io.SequenceFile$Writer:checkAndWriteSync(),1445,1450,"/**
 * Synchronizes data if sync is valid and interval has elapsed.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCompressedSize(),170,173,"/**
* Calculates a value by subtracting posStart from the result of m1().
*/
","* Current size of compressed data.
       * 
       * @return
       * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getFileLength,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:getFileLength(),507,512,"/**
 * Returns the length of the file.
 * @return File length, or -1 if not initialized.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPathArgument,org.apache.hadoop.fs.shell.Ls:processPathArgument(org.apache.hadoop.fs.shell.PathData),232,246,"/**
 * Processes a PathData item, throwing exception if erasure coding is unsupported.
 * Recursively processes directories if dirRecurse is true.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,adjustColumnWidths,org.apache.hadoop.fs.shell.Ls:adjustColumnWidths(org.apache.hadoop.fs.shell.PathData[]),327,355,"/**
 * Updates maxRepl, maxLen, maxOwner, maxGroup based on PathData.
 * Calculates lineFormat string for displaying PathData items.
 */","* Compute column widths and rebuild the format string
   * @param items to find the max field width for each column",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPath,org.apache.hadoop.fs.shell.FsUsage$Du:processPath(org.apache.hadoop.fs.shell.PathData),219,230,"/**
 * Updates stats for a PathData item, adjusting for snapshots.
 * @param item The PathData object to process.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getQuotaUsage,org.apache.hadoop.fs.FileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),1952,1954,"/**
* Delegates quota usage retrieval to m1.
* @param f Path to the file.
* @return QuotaUsage object.
*/
","Return the {@link QuotaUsage} of a given {@link Path}.
   * @param f path to use
   * @return the quota usage
   * @throws IOException IO failure",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUsed,org.apache.hadoop.fs.FileSystem:getUsed(org.apache.hadoop.fs.Path),2730,2732,"/**
* Delegates to m1(path) and returns the result of m2().
*/","* Return the total size of all files from a specified path.
   * @param path the path.
   * @throws IOException IO failure
   * @return the number of path content summary.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/AbstractLaunchableService.java,<init>,org.apache.hadoop.service.launcher.AbstractLaunchableService:<init>(java.lang.String),48,50,"/**
 * Constructs a new AbstractLaunchableService with the given name.
 */","* Construct an instance with the given name.
   *
   * @param name input name.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,<init>,org.apache.hadoop.service.CompositeService:<init>(java.lang.String),53,55,"/**
 * Constructs a CompositeService with the given name.
 * @param name The name of the composite service.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,<init>,org.apache.hadoop.util.JvmPauseMonitor:<init>(),70,72,"/**
 * Constructs a JvmPauseMonitor with the class name as the name.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,enterState,org.apache.hadoop.service.ServiceStateModel:enterState(org.apache.hadoop.service.Service$STATE),113,119,"/**
 * Transitions service state, returning the previous state.
 * @param proposed The new state to transition to.
 * @return The service's previous state.
 */
","* Enter a state -thread safe.
   *
   * @param proposed proposed new state
   * @return the original state
   * @throws ServiceStateException if the transition is not permitted",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,instantiateService,org.apache.hadoop.service.launcher.ServiceLauncher:instantiateService(org.apache.hadoop.conf.Configuration),658,694,"/**
 * Creates and returns a Service instance using the provided configuration.
 */","* @return Instantiate the service defined in {@code serviceClassName}.
   *
   * Sets the {@code configuration} field
   * to the the value of {@code conf},
   * and the {@code service} field to the service created.
   *
   * @param conf configuration to use",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.Throwable,java.lang.String,java.lang.Object[])",1094,1098,"/**
 * Constructs a KerberosDiagsFailure with a category, message, args, and cause.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceStop,org.apache.hadoop.service.CompositeService:serviceStop(),128,136,"/**
* Stops a specified number of services and calls super.m6().
* @param numOfServicesToStop Number of services to stop.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,run,org.apache.hadoop.service.CompositeService$CompositeServiceShutdownHook:run(),184,187,"/**
* Calls ServiceOperations.m1 with the compositeService.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,progressable,org.apache.hadoop.io.MapFile$Writer:progressable(org.apache.hadoop.util.Progressable),308,310,"/**
* Delegates to SequenceFile.Writer.m1, setting progress reporting.
* @param value Progressable object to track progress.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,valueClass,org.apache.hadoop.io.MapFile$Writer:valueClass(java.lang.Class),293,295,"/**
 * Delegates to SequenceFile.Writer.m1, setting a value.
 * @param value The value to set for the SequenceFile Writer.
 * @return A SequenceFile.Writer.Option object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,equals,org.apache.hadoop.io.BinaryComparable:equals(java.lang.Object),74,82,"/**
 * Compares this object with another BinaryComparable object.
 * @param other The object to compare to.
 * @return True if objects are equal, false otherwise.
 */
",* Return true if bytes from {#getBytes()} match.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,hashCode,org.apache.hadoop.io.BytesWritable:hashCode(),207,210,"/**
 * Calls the m1 method of the superclass and returns its result.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,hashCode,org.apache.hadoop.io.Text:hashCode(),422,425,"/**
* Calls the m1 method of the superclass and returns its result.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,hashCode,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:hashCode(),95,98,"/**
* Delegates m1() call to the token object.
* @return Integer value returned by token.m1()
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,set,"org.apache.hadoop.io.BytesWritable:set(byte[],int,int)",178,182,"/**
* Copies a portion of newData to bytes.
* @param newData Data to copy from.
* @param offset Start offset in newData.
* @param length Number of bytes to copy.
*/
","* Set the value to a copy of the given byte range.
   *
   * @param newData the new values to copy in
   * @param offset the offset in newData to start at
   * @param length the number of bytes to copy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,readFields,org.apache.hadoop.io.BytesWritable:readFields(java.io.DataInput),184,189,"/**
* Reads data from input stream, calls m1, and writes bytes.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(org.apache.hadoop.io.BytesWritable),1690,1694,"/**
* Processes a BytesWritable key by applying transformations.
* Recursively calls itself with a derived key. Returns an int.
*/","* Copy the key into BytesWritable. The input BytesWritable will be
         * automatically resized to the actual key size.
         * 
         * @param key
         *          BytesWritable to hold the key.
         * @throws IOException raised on errors performing I/O.
         * @return the key into BytesWritable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,list,org.apache.hadoop.fs.FileUtil:list(java.io.File),1619,1630,"/**
 * Lists file names in a directory.
 * @param dir The directory to list.
 * @return String array of file names.
 * @throws IOException if access is denied or error occurs.
 */","* A wrapper for {@link File#list()}. This java.io API returns null
   * when a dir is not a directory or for any I/O error. Instead of having
   * null check everywhere File#list() is used, we will add utility API
   * to get around this problem. For the majority of cases where we prefer
   * an IOException to be thrown.
   * @param dir directory for which listing should be performed
   * @return list of file names or empty string list
   * @exception AccessDeniedException for unreadable directory
   * @exception IOException for invalid directory or for bad disk",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkAccessByFileMethods,org.apache.hadoop.util.DiskChecker:checkAccessByFileMethods(java.io.File),153,174,"/**
 * Checks if a directory is valid, readable, writable, and executable.
 * @param dir The directory to check.
 * @throws DiskErrorException if any check fails.
 */
","* Checks that the current running process can read, write, and execute the
   * given directory by using methods of the File object.
   * 
   * @param dir File to check
   * @throws DiskErrorException if dir is not readable, not writable, or not
   *   executable",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,mlock,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:mlock(java.nio.ByteBuffer,long)",470,477,"/**
 * Locks a ByteBuffer, checks if direct, and processes it.
 * @param buffer ByteBuffer to lock and process.
 * @param len Length associated with the ByteBuffer.
 */
","* Locks the provided direct ByteBuffer into memory, preventing it from
     * swapping out. After a buffer is locked, future accesses will not incur
     * a page fault.
     * 
     * See the mlock(2) man page for more information.
     * 
     * @throws NativeIOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.java,create,"org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:create(java.lang.String,java.lang.String[])",74,100,"/**
 * Creates a SharedFileDescriptorFactory, throwing IOException on failure.
 * @param prefix Prefix for file paths.
 * @param paths Array of paths to try.
 */
","* Create a new SharedFileDescriptorFactory.
   *
   * @param prefix       The prefix to prepend to all the file names created
   *                       by this factory.
   * @param paths        An array of paths to use.  We will try each path in 
   *                       succession, and return a factory using the first 
   *                       usable path.
   * @return             The factory.
   * @throws IOException If a factory could not be created for any reason.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getMemlockLimit,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getMemlockLimit(),285,287,"/**
* Delegates to NativeIO.m1() and returns the result.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,writeChecksumChunks,"org.apache.hadoop.fs.FSOutputSummer:writeChecksumChunks(byte[],int,int)",212,228,"/**
* Processes data in chunks, updating checksum and tracing.
* @param b byte array to process
* @param off offset in the array
* @param len number of bytes to process
*/","Generate checksums for the given data chunks and output chunks & checksums
   * to the underlying output stream.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,calculateChunkedSums,"org.apache.hadoop.util.DataChecksum:calculateChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer)",534,564,"/**
 * Processes data and checksums, handling recursive calls or CRC checks.
 */","* Calculate checksums for the given data.
   * 
   * The 'mark' of the ByteBuffer parameters may be modified by this function,
   * but the position is maintained.
   * 
   * @param data the DirectByteBuffer pointing to the data to checksum.
   * @param checksums the DirectByteBuffer into which checksums will be
   *                  stored. Enough space must be available in this
   *                  buffer to put the checksums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,freeBuffers,org.apache.hadoop.crypto.CryptoInputStream:freeBuffers(),809,813,"/**
 * Processes input and output buffers using CryptoStreamUtils and m2.
 */",Forcibly free the direct buffers.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,unbuffer,org.apache.hadoop.crypto.CryptoInputStream:unbuffer(),853,858,"/**
* Calls m1, m2, and StreamCapabilitiesPolicy.m3 with 'in'.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int),45,47,"/**
 * Constructs a BoundedByteArrayOutputStream with specified capacity.
 * @param capacity initial buffer size, also the maximum size.
 */
","* Create a BoundedByteArrayOutputStream with the specified
   * capacity
   * @param capacity The capacity of the underlying byte array",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeFromUrlString,org.apache.hadoop.security.token.Token:decodeFromUrlString(java.lang.String),382,384,"/**
* Calls m1 with the current object and the provided newValue.
* @param newValue Value passed to m1 method.
*/
","* Decode the given url safe string into this token.
   * @param newValue the encoded string
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),89,107,"/**
 * Filters ECBlocks from a block group based on m3() status.
 * @param blockGroup The ECBlockGroup to filter.
 * @return An array of filtered ECBlocks.
 */","* Which blocks were erased ?
   * @param blockGroup blockGroup.
   * @return output blocks to recover",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",36,49,"/**
 * Initializes a ByteBufferDecodingState with provided decoding data.
 * @param decoder Erasure decoder, inputs, indexes, outputs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,byte[][],int[],byte[][])",37,52,"/**
 * Initializes a ByteArrayDecodingState with provided decoding data.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,41,"/**
 * Creates a RawErasureDecoder using the provided options.
 * @param coderOptions ErasureCoderOptions for decoder creation
 * @return A RawErasureDecoder instance
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,41,"/**
 * Creates a RawErasureDecoder using the provided options.
 * @param coderOptions ErasureCoderOptions for decoder creation.
 * @return A RawErasureDecoder instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,53,"/**
 * Constructs a RSLegacyRawDecoder with given options.
 * @param coderOptions ErasureCoderOptions object
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,53,"/**
 * Constructs a RSLegacyRawEncoder with provided options.
 * Initializes the generating polynomial based on data/parity units.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),33,36,"/**
 * Creates and returns a NativeXORRawEncoder with given options.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),33,36,"/**
 * Creates and returns a NativeRSRawEncoder with the given options.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),39,62,"/**
* Decodes data, XORing inputs into the output buffer.
* @param decodingState State containing inputs, outputs, and indexes.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),39,60,"/**
* Copies/XORs input data into the output buffer.
* @param encodingState State containing input/output buffers.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),167,217,"/**
 * Decodes data based on erasure coding parameters.
 * @param decodingState State object containing decoding information.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),55,88,"/**
 * Combines outputs and inputs, then applies Reed-Solomon encoding.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),63,68,"/**
 * Processes input data using specified tables and outputs.
 * @param encodingState State containing inputs, outputs, and length.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),64,86,"/**
* XORs inputs into output, skipping erased index.
* @param decodingState State containing inputs, outputs, offsets.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),62,85,"/**
* Copies and XORs input data into the output buffer.
* @param encodingState State object containing input/output buffers.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),111,165,"/**
* Decodes data based on the decoding state and adjusts outputs.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),90,128,"/**
* Encodes data using the provided encoding state and RSUtil.
* @param encodingState State containing inputs, outputs, and offsets.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),70,79,"/**
* Processes data using CoderUtil.m1 and RSUtil.m2.
* @param encodingState State object containing input/output data.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
 * Creates and returns a RawErasureDecoder instance.
 * @param coderOptions ErasureCoderOptions for decoder configuration.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
* Creates and returns a RawErasureEncoder using the provided options.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,processErasures,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:processErasures(int[]),117,140,"/**
* Initializes erasure coding data structures based on erased indexes.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,skipToNextMarker,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextMarker(long,int)",217,257,"/**
* Searches for a marker in the compressed stream.
* @param marker The marker to search for.
* @param markerBitLength Length of the marker in bits.
* @return True if marker found, false otherwise.
*/","* This method tries to find the marker (passed to it as the first parameter)
   * in the stream. It can find bit patterns of length &lt;= 63 bits.
   * Specifically this method is used in CBZip2InputStream to find the end of
   * block (EOB) delimiter in the stream, starting from the current position
   * of the stream. If marker is found, the stream position will be at the
   * byte containing the starting bit of the marker.
   * @param marker The bit pattern to be found in the stream
   * @param markerBitLength No of bits in the marker
   * @return true if the marker was found otherwise false
   * @throws IOException raised on errors performing I/O.
   * @throws IllegalArgumentException if marketBitLength is greater than 63",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetUByte,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetUByte(),660,662,"/**
 * Reads and returns a character from input stream.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetInt,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetInt(),664,666,"/**
 * Calculates a masked integer value using m1(8) calls.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,getAndMoveToFrontDecode0,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode0(int),1006,1037,"/**
* Reads data and returns a value based on groupNo and bitstream.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,recvDecodingTables,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:recvDecodingTables(),709,788,"/**
 * Initializes decoding tables and selectors based on internal state.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream,int)",627,643,"/**
 * Creates a CBZip2OutputStream with the specified output stream and block size.
 * @param out output stream to write to
 * @param blockSize block size (1-9)
 * @throws IOException if an I/O error occurs
 */
","* Constructs a new <tt>CBZip2OutputStream</tt> with specified blocksize.
  *
  * <p>
  * <b>Attention: </b>The caller is resonsible to write the two BZip2 magic
  * bytes <tt>""BZ""</tt> to the specified stream prior to calling this
  * constructor.
  * </p>
  *
  *
  * @param out
  *            the destination stream.
  * @param blockSize
  *            the blockSize as 100k units.
  *
  * @throws IOException
  *             if an I/O error occurs in the specified stream.
  * @throws IllegalArgumentException
  *             if {@code (blockSize < 1) || (blockSize > 9)}
  * @throws NullPointerException
  *             if {@code out == null}.
  *
  * @see #MIN_BLOCKSIZE
  * @see #MAX_BLOCKSIZE",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,moveToFrontCodeAndSend,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:moveToFrontCodeAndSend(),1391,1395,"/**
* Executes a sequence of internal operations.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,blockSort,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:blockSort(),1605,1629,"/**
 * Initializes state, performs work, and finds the original pointer.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",69,71,"/**
* Creates a BlockCompressorStream with default buffer sizes.
* @param out Output stream to write compressed data.
* @param compressor Compressor to use for compression.
*/
","* Create a {@link BlockCompressorStream} with given output-stream and 
   * compressor.
   * Use default of 512 as bufferSize and compressionOverhead of 
   * (1% of bufferSize + 12 bytes) =  18 bytes (zlib algorithm).
   * 
   * @param out stream
   * @param compressor compressor to be used",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",74,77,"/**
 * Constructs a DecompressorStream with default buffer size.
 * @param in Input stream to decompress.
 * @param decompressor Decompressor to use.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",48,51,"/**
 * Creates a BlockDecompressorStream with given input, decompressor, and buffer size.
 */
","* Create a {@link BlockDecompressorStream}.
   * 
   * @param in input stream
   * @param decompressor decompressor to use
   * @param bufferSize size of buffer
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,createInputStream,"org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",138,142,"/**
 * Creates a PassthroughDecompressorStream using the input stream.
 * @param in The input stream.
 * @param decompressor The decompressor (unused).
 * @return A PassthroughDecompressorStream.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,read,"org.apache.hadoop.io.compress.DecompressorStream:read(byte[],int,int)",95,106,"/**
 * Reads data from a byte array with bounds checking.
 * @param b byte array to read from
 * @param off start offset
 * @param len number of bytes to read
 * @return Number of bytes read
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,write,"org.apache.hadoop.io.compress.BlockCompressorStream:write(byte[],int,int)",81,134,"/**
 * Writes data to the compressor, handling size limits and errors.
 * @param b the data to write
 * @param off offset in the buffer
 * @param len number of bytes to write
 */","* Write the data provided to the compression codec, compressing no more
   * than the buffer size less the compression overhead as specified during
   * construction for each block.
   *
   * Each block contains the uncompressed length for the block, followed by
   * one or more length-prefixed blocks of compressed data.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>(),70,72,"/**
 * Default constructor. Initializes with stream size from getStreamSize().
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:<init>(int),298,300,"/**
 * Creates a ZStandardDirectDecompressor with the given buffer size.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int)",90,92,"/**
 * Constructs a ZStandardCompressor with specified level and buffer sizes.
 */
","* Creates a new compressor with the default compression level.
   * Compressed data will be generated in ZStandard format.
   * @param level level.
   * @param bufferSize bufferSize.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getCompressor,org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec),167,169,"/**
 * Creates a Compressor with the given codec.
 * @param codec CompressionCodec to use; null is not allowed.
 * @return A Compressor instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getDecompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:getDecompressor(),319,343,"/**
 * Obtains a Decompressor from the CodecPool.
 * Returns null if no codec is available.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodec.java,createOutputStreamWithCodecPool,"org.apache.hadoop.io.compress.CompressionCodec$Util:createOutputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.OutputStream)",128,143,"/**
 * Creates a CompressionOutputStream using the given codec and output stream.
 * @param codec CompressionCodec to use
 * @param conf Configuration for the codec
 * @param out Output stream to compress
 * @return CompressionOutputStream instance
 */","* Create an output stream with a codec taken from the global CodecPool.
     *
     * @param codec       The codec to use to create the output stream.
     * @param conf        The configuration to use if we need to create a new codec.
     * @param out         The output stream to wrap.
     * @return            The new output stream
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionOutputStream.java,close,org.apache.hadoop.io.compress.CompressionOutputStream:close(),61,75,"/**
 * Executes m3 and releases trackedCompressor to CodecPool.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Writer:close(),1422,1443,"/**
 * Releases resources used by the compressor and output stream.
 */",Close the file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,returnCompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnCompressor(org.apache.hadoop.io.compress.Compressor),310,317,"/**
* Releases a compressor back to the CodecPool for reuse.
* @param compressor The compressor to release.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodec.java,createInputStreamWithCodecPool,"org.apache.hadoop.io.compress.CompressionCodec$Util:createInputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.InputStream)",154,169,"/**
 * Creates a CompressionInputStream using the given codec and stream.
 * @param codec Compression codec
 * @param conf Configuration object
 * @param in Input stream to compress
 * @return CompressionInputStream object
 */","* Create an input stream with a codec taken from the global CodecPool.
     *
     * @param codec       The codec to use to create the input stream.
     * @param conf        The configuration to use if we need to create a new codec.
     * @param in          The input stream to wrap.
     * @return            The new input stream
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionInputStream.java,close,org.apache.hadoop.io.compress.CompressionInputStream:close(),65,75,"/**
 * Calls the m2 method of the input stream and releases resources.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Reader:close(),2169,2188,"/**
* Releases resources and calls deserializer cleanup methods.
*/",Close the file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,returnDecompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnDecompressor(org.apache.hadoop.io.compress.Decompressor),345,352,"/**
 * Releases a Decompressor back to the CodecPool.
 * @param decompressor The Decompressor to release.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createCompressor,org.apache.hadoop.io.compress.GzipCodec:createCompressor(),62,67,"/**
 * Returns a compressor based on configuration.
 * Returns GzipZlibCompressor or BuiltInGzipCompressor.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibCompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressor(org.apache.hadoop.conf.Configuration),109,113,"/**
 * Creates a compressor based on configuration.
 * @param conf Configuration object to determine compressor type.
 * @return Compressor instance.
 */
","* Return the appropriate implementation of the zlib compressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib compressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDirectDecompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDirectDecompressor(org.apache.hadoop.conf.Configuration),144,147,"/**
 * Creates a ZlibDirectDecompressor if m1(conf) is true, else null.
 */","* Return the appropriate implementation of the zlib direct decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib decompressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.GzipCodec:createDirectDecompressor(),109,114,"/**
* Creates a DirectDecompressor based on configuration; null if disabled.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDecompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressor(org.apache.hadoop.conf.Configuration),133,136,"/**
 * Creates a Decompressor based on the configuration.
 * @param conf Configuration object to determine compressor type.
 * @return A Decompressor instance.
 */
","* Return the appropriate implementation of the zlib decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib decompressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createDecompressor,org.apache.hadoop.io.compress.GzipCodec:createDecompressor(),95,100,"/**
* Returns a Decompressor based on configuration.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,decompress,"org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:decompress(byte[],int,int)",189,243,"/**
 * Decompresses data into the provided byte array.
 * @param b buffer to write decompressed data
 * @param off offset in buffer to start writing
 * @param len maximum number of bytes to decompress
 * @return number of decompressed bytes
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,write,org.apache.hadoop.io.SequenceFile$Metadata:write(java.io.DataOutput),759,769,"/**
 * Writes metadata to the output stream.
 * Writes the size and then iterates through entries.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,writeImpl,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:writeImpl(java.io.DataOutput),205,215,"/**
 * Writes data to the output stream.
 * @param out DataOutput to write to.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,write,org.apache.hadoop.security.token.Token:write(java.io.DataOutput),323,331,"/**
 * Writes data to the output stream: identifier, password, kind, service.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),333,345,"/**
 * Stores the delegation key in the SQL secret manager.
 * @param key The delegation key to store.
 * @throws IOException If an IO error occurs.
 */
","* Persists a DelegationKey into the SQL database. The delegation keyId
   * is expected to be unique and any duplicate key attempts will result
   * in an IOException.
   * @param key DelegationKey to persist into the SQL database.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),351,363,"/**
* Updates a delegation key, writing to stream and calling super.
* @param key The delegation key to update.
* @throws IOException If an IO error occurs during update.
*/","* Updates an existing DelegationKey in the SQL database.
   * @param key Updated DelegationKey.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,addOrUpdateDelegationKey,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)",691,725,"/**
* Stores or updates a ZKDTSMDelegationKey in Zookeeper.
* @param key DelegationKey to store/update
* @param isUpdate Flag indicating if it's an update operation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,readFields,org.apache.hadoop.io.VIntWritable:readFields(java.io.DataInput),49,52,"/**
* Reads a value from the input stream and assigns it to 'value'.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readStringSafely,"org.apache.hadoop.io.WritableUtils:readStringSafely(java.io.DataInput,int)",484,497,"/**
 * Reads a string from input, masking length to maxLength.
 * @param in DataInput to read from
 * @param maxLength Max allowed string length
 * @return String read from input
 */","* Read a string, but check it for sanity. The format consists of a vint
   * followed by the given number of bytes.
   * @param in the stream to read from
   * @param maxLength the largest acceptable length of the encoded string
   * @return the bytes as a string
   * @throws IOException if reading from the DataInput fails
   * @throws IllegalArgumentException if the encoded byte size for string 
             is negative or larger than maxSize. Only the vint is read.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readFields,org.apache.hadoop.io.Text:readFields(java.io.DataInput),346,350,"/**
* Reads data from input stream and processes it.
* @param in DataInput stream to read from.
* @throws IOException if an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readFields,"org.apache.hadoop.io.Text:readFields(java.io.DataInput,int)",352,362,"/**
 * Deserializes data from input stream, validating length.
 * @param in DataInput stream to read from.
 * @param maxLength Max allowed length of data.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,skip,org.apache.hadoop.io.Text:skip(java.io.DataInput),369,372,"/**
* Reads data from input stream.
* @param in DataInput stream to read from.
*/","* Skips over one Text in the input.
   * @param in input in.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readBuffer,"org.apache.hadoop.io.SequenceFile$Reader:readBuffer(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.compress.CompressionInputStream)",2274,2291,"/**
 * Copies data from input buffer to target buffer, then filters.
 * @param buffer Target buffer to copy data to.
 * @param filter Compression filter to apply.
 */",Read a compressed buffer,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,readFields,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:readFields(java.io.DataInput),749,758,"/**
 * Reads renewal date, password, and tracking ID from input stream.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readString,"org.apache.hadoop.io.Text:readString(java.io.DataInput,int)",566,572,"/**
 * Reads a fixed-length byte array from input.
 * @param in DataInput to read from.
 * @param maxLength Maximum length of the read data.
 * @return Byte array read from input.
 */
","* @return Read a UTF8 encoded string with a maximum size.
   * @param in input datainput.
   * @param maxLength input maxLength.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,readFields,org.apache.hadoop.security.token.delegation.DelegationKey:readFields(java.io.DataInput),108,119,"/**
 * Reads key data from input stream.
 * @param in Input stream to read key ID, expiry date, and key bytes.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,writeString,"org.apache.hadoop.io.file.tfile.Utils:writeString(java.io.DataOutput,java.lang.String)",256,266,"/**
 * Writes a string to the output or -1 if the string is null.
 * @param out DataOutput to write to
 * @param s String to write
 */","* Write a String as a VInt n, followed by n Bytes as in Text format.
   * 
   * @param out out.
   * @param s s.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,getCredentialEntry,org.apache.hadoop.security.alias.UserProvider:getCredentialEntry(java.lang.String),54,62,"/**
 * Retrieves a CredentialEntry by alias.
 * @param alias Credential alias.
 * @return CredentialEntry or null if not found.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,createCredentialEntry,"org.apache.hadoop.security.alias.UserProvider:createCredentialEntry(java.lang.String,char[])",64,75,"/**
* Adds a credential entry.
* @param name Credential name.
* @param credential Credential value (password).
* @throws IOException if credential already exists.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,deleteCredentialEntry,org.apache.hadoop.security.alias.UserProvider:deleteCredentialEntry(java.lang.String),77,87,"/**
 * Retrieves credentials for a given name.
 * @param name The name for which to retrieve credentials.
 * @throws IOException if credentials do not exist.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildTokenService,org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.InetSocketAddress),474,487,"/**
 * Creates a Text object containing host and port from an address.
 * @param addr The InetSocketAddress to extract host and port from.
 * @return A Text object representing the host and port.
 */","* Construct the service key for a token
   * @param addr InetSocketAddress of remote connection with a token
   * @return ""ip:port"" or ""host:port"" depending on the value of
   *          hadoop.security.token.service.use_ip",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.UserProvider:getKeyVersion(java.lang.String),58,66,"/**
 * Retrieves KeyVersion based on versionName.
 * Returns null if credentials are unavailable.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getMetadata,org.apache.hadoop.crypto.key.UserProvider:getMetadata(java.lang.String),68,80,"/**
 * Retrieves metadata by name, caching the result.
 * @param name Metadata name.
 * @return Metadata object or null if not found.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,createKey,"org.apache.hadoop.crypto.key.UserProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",82,100,"/**
* Creates a new key version with given name and material.
* @param name Key name
* @param material Key material
* @param options Key options
* @return KeyVersion object
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDtService,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDtService(java.net.URI),430,441,"/**
 * Creates a Text object from the URI, using fragment or scheme.
 * @param uri The URI to process.
 * @return A Text object representing the service.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,tokenFromProto,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto),123,131,"/**
 * Creates a Token object from a TokenProto.
 * @param tokenProto TokenProto object to create Token from
 * @return Token object
 */
","* Create a hadoop token from a protobuf token.
   * @param tokenProto token
   * @return a new token",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,find,org.apache.hadoop.io.Text:find(java.lang.String),171,173,"/**
 * Overloads m1 with a default value for the second parameter.
 * @param what The input string.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeEnum,"org.apache.hadoop.io.WritableUtils:writeEnum(java.io.DataOutput,java.lang.Enum)",432,435,"/**
* Writes enum value to output using Text.m2.
* @param out Output stream
* @param enumVal Enum value to write
*/
","* writes String value of enum to DataOutput. 
   * @param out Dataoutput stream
   * @param enumVal enum value
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,write,"org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput,java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",128,135,"/**
 * Writes username, groupname, and permission to the output stream.
 */","* Serialize a {@link PermissionStatus} from its base components.
   * @param out out.
   * @param username username.
   * @param groupname groupname.
   * @param permission FsPermission.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(byte[]),112,114,"/**
 * Constructs a Text object from a byte array (UTF-8 encoded).
 */","* Construct from a byte array.
   *
   * @param utf8 input utf8.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(org.apache.hadoop.io.Text),103,105,"/**
 * Constructs a new Text object by copying from an existing Text.
 */","* Construct from another text.
   * @param utf8 input utf8.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,"org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int,int)",180,187,"/**
 * Determines line length based on record delimiter.
 * @param str Input text.
 * @param maxLineLength Max line length.
 * @param maxBytesToConsume Max bytes to consume.
 */
","* Read one line from the InputStream into the given Text.
   *
   * @param str the object to store the given line (without newline)
   * @param maxLineLength the maximum number of bytes to store into str;
   *  the rest of the line is silently discarded.
   * @param maxBytesToConsume the maximum number of bytes to consume
   *  in this call.  This is only a hint, because if the line cross
   *  this threshold, we allow it to happen.  It can overshoot
   *  potentially by as much as one buffer length.
   *
   * @return the number of bytes read including the (longest) newline
   * found.
   *
   * @throws IOException if the underlying stream throws",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,getTextLength,org.apache.hadoop.io.Text:getTextLength(),146,151,"/**
* Returns the text length, initializing if negative.
*/","* @return Returns the length of this text. The length is equal to the number of
   * Unicode code units in the text.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,toString,org.apache.hadoop.io.SequenceFile$Metadata:toString(),828,840,"/**
 * Generates a string representation of metadata.
 * Returns a formatted string containing metadata details.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setRenewer,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text),105,116,"/**
 * Updates the renewer Text object. Uses renewer if not null.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,getAliases,org.apache.hadoop.security.alias.UserProvider:getAliases(),111,119,"/**
 * Extracts alias names from credentials and returns them as a list.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getCanonicalServiceName,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCanonicalServiceName(),1020,1023,"/**
* Delegates to canonicalService's m1() method and returns the result.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getCanonicalServiceName,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCanonicalServiceName(),246,249,"/**
 * Delegates to canonicalService's m1() method and returns the result.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,<init>,org.apache.hadoop.io.SortedMapWritable:<init>(org.apache.hadoop.io.SortedMapWritable),55,58,"/**
* Constructs a new SortedMapWritable, copying from another.
* @param other The SortedMapWritable to copy from.
*/
","* Copy constructor.
   * 
   * @param other the map to copy from",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,<init>,org.apache.hadoop.io.MapWritable:<init>(org.apache.hadoop.io.MapWritable),53,56,"/**
 * Constructs a new MapWritable by copying another MapWritable.
 */
","* Copy constructor.
   * 
   * @param other the map to copy from",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,compression,org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType),297,300,"/**
 * Creates a SequenceFile writer option with the given compression type.
 * @param type CompressionType for the SequenceFile writer.
 * @return SequenceFile.Writer.Option object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerializationComparator.java,<init>,org.apache.hadoop.io.serializer.JavaSerializationComparator:<init>(),42,45,"/**
 * Constructs a JavaSerializationComparator using a deserializer.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumTimeWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumTimeWithFixedSleep(long,long,java.util.concurrent.TimeUnit)",114,116,"/**
 * Creates a RetryPolicy with a max time and fixed sleep duration.
 * @param maxTime Maximum time allowed for retries.
 * @param sleepTime Sleep time between retries.
 * @param timeUnit Time unit for maxTime and sleepTime.
 */
","* <p>
   * Keep trying for a maximum time, waiting a fixed time between attempts,
   * and then fail by re-throwing the exception.
   * </p>
   *
   * @param timeUnit timeUnit.
   * @param sleepTime sleepTime.
   * @param maxTime maxTime.
   * @return RetryPolicy.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,run,org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:run(),977,1053,"/**
 * Renews the Ticket Granting Ticket (TGT) until the loop terminates.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int)",205,208,"/**
 * Overloads m1 with default retry count and delay.
 * @param fallbackPolicy Fallback policy to use on failure.
 * @param maxFailovers Max retries before fallback.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,newCall,"org.apache.hadoop.io.retry.RetryInvocationHandler:newCall(java.lang.reflect.Method,java.lang.Object[],boolean,int)",349,356,"/**
 * Creates a Call object, either async or sync based on Client.m1().
 * @param method Method to be called.
 * @param args Arguments for the method.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)",50,57,"/**
 * Constructs an InstrumentedWriteLock with provided dependencies.
 * @param name Lock name, logger, lock, timer for metrics.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long)",81,85,"/**
 * Constructs an InstrumentedLock with a default Timer.
 * @param name Lock name, logger, lock, gap, threshold.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)",56,63,"/**
 * Constructs an InstrumentedReadLock with provided parameters.
 * @param name Lock name, logger, readWriteLock, timers, thresholds.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,tryLock,org.apache.hadoop.util.InstrumentedLock:tryLock(),117,124,"/**
 * Executes m2 if lock.m1() returns true; otherwise, returns false.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",58,65,"/**
 * Creates a dynamic proxy implementing the given interface.
 * @param iface Interface to proxy.
 * @param proxyProvider Provider for the base proxy.
 * @param retryPolicy Retry policy to apply.
 */
","* Create a proxy for an interface of implementations of that interface using
   * the given {@link FailoverProxyProvider} and the same retry policy for each
   * method in the interface.
   * 
   * @param iface the interface that the retry will implement
   * @param proxyProvider provides implementation instances whose methods should be retried
   * @param retryPolicy the policy for retrying or failing over method call failures
   * @param <T> T.
   * @return the retry proxy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:<init>(int,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",35,39,"/**
 * Constructs a LossyRetryInvocationHandler.
 * @param numToDrop Number of retries to drop.
 * @param proxyProvider Provider for failover proxies.
 * @param retryPolicy Policy for retry attempts.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,java.util.Map)",79,85,"/**
 * Creates a proxy with default failover provider.
 * @param iface Interface to proxy.
 * @param implementation Implementation of the interface.
 * @param methodNameToPolicyMap Retry policies for methods.
 */","* Create a proxy for an interface of an implementation class
   * using the a set of retry policies specified by method name.
   * If no retry policy is defined for a method then a default of
   * {@link RetryPolicies#TRY_ONCE_THEN_FAIL} is used.
   * 
   * @param iface the interface that the retry will implement
   * @param <T> T.
   * @param implementation the instance whose methods should be retried
   * @param methodNameToPolicyMap a map of method names to retry policies
   * @return the retry proxy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,processRetryInfo,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processRetryInfo(),151,159,"/**
 * Increments retry counters and invokes failover if retryInfo permits.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,handleException,"org.apache.hadoop.io.retry.RetryInvocationHandler:handleException(java.lang.reflect.Method,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,long,java.lang.Exception)",376,396,"/**
* Executes a method with retry policy, logs reason if not retried.
* @param method Method to execute, policy, counters, exception.
* @return RetryInfo object containing retry details.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,write,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(int),327,340,"/**
 * Writes data 'd' to output streams and handles potential IOExceptions.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,write,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(byte[],int,int)",348,361,"/**
* Writes data to multiple output streams, handling potential IOExceptions.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,flush,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:flush(),363,376,"/**
 * Flushes output streams, handles exceptions, and reports errors.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProxyCombiner.java,close,org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:close(),133,149,"/**
 * Calls m2 on each Closeable proxy, aggregating IOExceptions.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,java.lang.String,org.apache.hadoop.conf.Configuration)",285,297,"/**
 * Creates a Writer with a given output stream, compression name, and config.
 */","* Constructor
     * 
     * @param fout
     *          FS output stream.
     * @param compressionName
     *          Name of the compression algorithm, which will be used for all
     *          data blocks.
     * @throws IOException
     * @see Compression#getSupportedAlgorithms",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:<init>(java.io.DataInput),809,822,"/**
 * Constructs a MetaIndexEntry from a DataInput.
 * @param in DataInput to read the entry from.
 * @throws IOException if input is corrupted.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.io.DataInput),2060,2068,"/**
 * Constructs a TFileMeta from a DataInput, validating file version.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.io.DataInput),864,875,"/**
 * Constructs a DataIndex from a DataInput stream.
 * Reads compression algorithm and block regions.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$TFileIndex:<init>(int,java.io.DataInput,org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator)",2145,2179,"/**
 * Constructs a TFileIndex with entries from the input stream.
 * @param entryCount number of entries to read
 * @param in input stream for reading index data
 * @param comparator comparator for comparing bytes
 * @throws IOException if an I/O error occurs
 */
","* For reading from file.
     * 
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,checkEOF,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:checkEOF(),119,126,"/**
 * Determines completion status based on internal logic.
 * Returns true if complete, false otherwise.
 */","* Check whether we reach the end of the stream.
     * 
     * @return false if the chunk encoded stream has more data to read (in which
     *         case available() will be greater than 0); true otherwise.
     * @throws java.io.IOException
     *           on I/O errors.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,flushBuffer,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flushBuffer(),296,301,"/**
 * Writes buffered data to the output stream if data exists.
 */","* Flush the internal buffer.
     * 
     * Is this the last call to flushBuffer?
     * 
     * @throws java.io.IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,close,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:close(),338,348,"/**
 * Processes buffered data, releases resources.
 * Clears buf and out if data exists.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[],int,int)",316,330,"/**
* Copies data to the internal buffer.
* @param b input byte array
* @param off offset in the array
* @param len number of bytes to copy
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:write(java.io.DataOutput),2272,2290,"/**
* Writes index data to the output stream.
* Uses DataOutputBuffer for intermediate data.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(org.apache.hadoop.io.file.tfile.RawComparable),1957,1961,"/**
* Delegates to reader.m5 with prepared arguments.
*/","* Compare an entry with a RawComparable object. This is useful when
         * Entries are stored in a collection, and we want to compare a user
         * supplied key.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister:close(),432,473,"/**
* Appends a key to the block appender, handling length checks & sorting.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8:<init>(java.lang.String),70,72,"/**
 * Initializes a UTF8 object with the given string.
 * @param string The string to be set as the initial value.
 */
","* Construct from a given string.
   * @param string input string.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getBytes,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getBytes(),80,83,"/**
* Returns byte array representation of the object.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(java.lang.String),186,188,"/**
* Calculates MD5 hash of a string using UTF-8 encoding.
* @param string The string to hash.
* @return MD5Hash object representing the hash.
*/
","* Construct a hash value for a String.
   * @param string string.
   * @return MD5Hash.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:sync(),1638,1665,"/**
 * Flushes buffered records to the output stream.
 * Clears the buffered records counter after flushing.
 */",Compress and flush contents to dfs,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/TokenIdentifier.java,getTrackingId,org.apache.hadoop.security.token.TokenIdentifier:getTrackingId(),78,83,"/**
 * Returns the tracking ID, generating it if it's null.
 */","* Returns a tracking identifier that can be used to associate usages of a
   * token across multiple client sessions.
   *
   * Currently, this function just returns an MD5 of {{@link #getBytes()}.
   *
   * @return tracking identifier",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,encodeToUrlString,org.apache.hadoop.security.token.Token:encodeToUrlString(),373,375,"/**
* Delegates to m1, passing 'this'.
* @return String result from m1.
* @throws IOException if m1 throws IOException
*/
","* Encode this token as a url safe string.
   * @return the encoded string
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,cloneWritableInto,"org.apache.hadoop.util.ReflectionUtils:cloneWritableInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",363,371,"/**
* Copies data from src to dst using internal buffers.
* @param dst Destination Writable object.
* @param src Source Writable object.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:hashCode(),162,166,"/**
* Calls the m1 method of the superclass and returns its result.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,add,org.apache.hadoop.net.NetworkTopology:add(org.apache.hadoop.net.Node),133,169,"/**
 * Adds a node to the topology, validating its depth and location.
 */","Add a leaf node
   * Update node counter &amp; rack counter if necessary
   * @param node node to be added; can be null
   * @exception IllegalArgumentException if add a node to a leave 
                                         or node to be added is not a leaf",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection)",497,554,"/**
 * Selects a node based on scope, exclusions, and node counts.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,remove,org.apache.hadoop.net.NetworkTopology:remove(org.apache.hadoop.net.Node),221,241,"/**
 * Removes a node from the cluster map.
 * @param node The node to remove. Throws exception if inner node.
 */","Remove a node
   * Update node counter and rack counter if necessary
   * @param node node to be removed; can be null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,decommissionNode,org.apache.hadoop.net.NetworkTopology:decommissionNode(org.apache.hadoop.net.Node),1061,1076,"/**
 * Removes a node. Throws exception if inner node. Acquires/releases netlock.
 */","* Update empty rack number when remove a node like decommission.
   * @param node node to be added; can be null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)",912,915,"/**
 * Sorts nodes using a secondary sort function.
 * @param reader Node reader.
 * @param nodes Array of nodes to sort.
 * @param activeLen Active length of the nodes array.
 * @param secondarySort Consumer for secondary sorting.
 */
","* Sort nodes array by network distance to <i>reader</i> with secondary sort.
   * <p>
   * In a three-level topology, a node can be either local, on the same rack,
   * or on a different rack from the reader. Sorting the nodes based on network
   * distance from the reader reduces network traffic and improves
   * performance.
   * </p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param secondarySort a secondary sorting strategy which can inject into
   *     that point from outside to help sort the same distance.
   * @param <T> Generics Type T",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistanceUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)",953,956,"/**
 * Calls m1 with the 'true' flag for secondary sorting.
 * @param reader Node reader.
 * @param nodes Array of nodes.
 * @param activeLen Active length of the nodes array.
 * @param secondarySort Consumer for secondary sorting.
 */","* Sort nodes array by network distance to <i>reader</i>.
   * <p> using network location. This is used when the reader
   * is not a datanode. Sorting the nodes based on network distance
   * from the reader reduces network traffic and improves
   * performance.
   * </p>
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param secondarySort a secondary sorting strategy which can inject into
   *     that point from outside to help sort the same distance.
   * @param <T> Generics Type T.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream:<init>(java.nio.channels.ReadableByteChannel,long)",72,76,"/**
 * Creates a SocketInputStream with a timeout for reading.
 * @param channel ReadableByteChannel to read from.
 * @param timeout Timeout in milliseconds.
 */
","* Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @param channel 
   *        Channel for reading, should also be a {@link SelectableChannel}.
   *        The channel will be configured to be non-blocking.
   * @param timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream:<init>(java.nio.channels.WritableByteChannel,long)",77,81,"/**
 * Creates a SocketOutputStream with a specified timeout.
 * @param channel WritableByteChannel to write to.
 * @param timeout Timeout in milliseconds.
 */
","* Create a new ouput stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @param channel 
   *        Channel for writing, should also be a {@link SelectableChannel}.  
   *        The channel will be configured to be non-blocking.
   * @param timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,createSocket,org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:createSocket(),184,196,"/**
 * Initializes a DatagramSocket and packet for sending data.
 * Throws IOException if initialization fails.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getRpcResponse,"org.apache.hadoop.ipc.Client:getRpcResponse(org.apache.hadoop.ipc.Client$Call,org.apache.hadoop.ipc.Client$Connection,long,java.util.concurrent.TimeUnit)",1566,1598,"/**
* Waits for a call to complete, handling errors and timeouts.
* @param call The call to wait for.
* @param connection The connection used by the call.
* @param timeout Timeout value.
* @param unit Timeout unit.
* @return Writable result or null if timeout.
*/","@return the rpc response or, in case of timeout, null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(),87,89,"/**
 * Default constructor, uses RawScriptBasedMapping internally.
 */
","* Create an instance with the default configuration.
   * <p>
   * Calling {@link #setConf(Configuration)} will trigger a
   * re-evaluation of the configuration settings and so be used to
   * set up the mapping script.
   *",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,<init>,org.apache.hadoop.net.ScriptBasedMappingWithDependency:<init>(),59,61,"/**
 * Constructs a ScriptBasedMappingWithDependency, using RawScriptBasedMapping.
 */","* Create an instance with the default configuration.
   * <p>
   * Calling {@link #setConf(Configuration)} will trigger a
   * re-evaluation of the configuration settings and so be used to
   * set up the mapping script.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,newInnerNode,org.apache.hadoop.net.InnerNodeImpl$Factory:newInnerNode(java.lang.String),32,35,"/**
 * Creates a new InnerNodeImpl with the given path.
 * @param path The path for the new node.
 * @return A new InnerNodeImpl instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,<init>,org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:<init>(java.lang.String),306,308,"/**
 * Constructs an InnerNodeWithNodeGroup with the given path.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:<init>(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode),127,129,"/**
 * Constructs an MRNflyNode using data from another NflyNode.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,createParentNode,org.apache.hadoop.net.InnerNodeImpl:createParentNode(java.lang.String),184,187,"/**
* Creates a new InnerNodeImpl with provided parent name and derived data.
*/","* Creates a parent node to be added to the list of children.
   * Creates a node using the InnerNode four argument constructor specifying
   * the name, location, parent, and level of this node.
   *
   * <p>To be overridden in subclasses for specific InnerNode implementations,
   * as alternative to overriding the full {@link #add(Node)} method.
   *
   * @param parentName The name of the parent node
   * @return A new inner node
   * @see InnerNodeImpl(String, String, InnerNode, int)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,add,"org.apache.hadoop.net.unix.DomainSocketWatcher:add(org.apache.hadoop.net.unix.DomainSocket,org.apache.hadoop.net.unix.DomainSocketWatcher$Handler)",304,332,"/**
 * Processes a domain socket, handling closed sockets and adding entries.
 * @param sock The domain socket to process.
 * @param handler Handler for socket-related events.
 */","* Add a socket.
   *
   * @param sock     The socket to add.  It is an error to re-add a socket that
   *                   we are already watching.
   * @param handler  The handler to associate with this socket.  This may be
   *                   called any time after this function is called.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,remove,org.apache.hadoop.net.unix.DomainSocketWatcher:remove(org.apache.hadoop.net.unix.DomainSocket),339,354,"/**
 * Processes domain socket, removing it if necessary.
 * @param sock The DomainSocket to process.
 */","* Remove a socket.  Its handler will be called.
   *
   * @param sock     The socket to remove.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,<init>,"org.apache.hadoop.net.unix.DomainSocketWatcher:<init>(int,java.lang.String)",241,259,"/**
 * Initializes a DomainSocketWatcher with a period and source name.
 * @param interruptCheckPeriodMs Period for interrupt checks (ms)
 * @param src Source name for the watcher thread.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,select,"org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:select(java.nio.channels.SelectableChannel,int,long)",321,376,"/**
 * Selects a key on a channel with a timeout.
 * @param channel The channel to select on.
 * @param ops The selection ops.
 * @param timeout Timeout in milliseconds.
 * @return Selection result or 0 if timeout.
 */","* Waits on the channel with the given timeout using one of the 
     * cached selectors. It also removes any cached selectors that are
     * idle for a few seconds.
     * 
     * @param channel
     * @param ops
     * @param timeout
     * @return
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultIP,org.apache.hadoop.net.DNS:getDefaultIP(java.lang.String),224,228,"/**
 * Retrieves the first IP address from the interface.
 * @param strInterface Interface name.
 * @return First IP address or null if not found.
 */
","* Returns the first available IP address associated with the provided
   * network interface or the local host IP if ""default"" is given.
   *
   * @param strInterface
   *            The name of the network interface or subinterface to query
   *             (e.g. eth0 or eth0:0) or the string ""default""
   * @return The IP address in text form, the local host IP is returned
   *         if the interface name ""default"" is specified
   * @throws UnknownHostException
   *             If the given interface is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getHosts,org.apache.hadoop.net.DNS:getHosts(java.lang.String),340,343,"/**
* Calls m1 with null timeout and false flag.
* @param strInterface Interface string to process.
* @throws UnknownHostException if host resolution fails.
*/
","* Returns all the host names associated by the default nameserver with the
   * address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @return The list of host names associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface
   *",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,"org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String,boolean)",360,374,"/**
 * Resolves hostname based on interface and nameserver.
 * @param strInterface Interface string, null for default.
 * @param nameserver Nameserver string, null for default.
 * @return Resolved hostname.
 */
","* Returns the default (first) host name associated by the provided
   * nameserver with the address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @param nameserver
   *            The DNS host name
   * @param tryfallbackResolution
   *            Input tryfallbackResolution.
   * @return The default host names associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream),134,136,"/**
* Calls m1 with the provided PrintStream and default usage.
* @param pStr PrintStream to use for output.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkParameterValidity,"org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[],java.util.Map)",363,385,"/**
 * Validates the command line arguments.
 * @param argv Command line arguments.
 * @param helpEntries Help entries for commands.
 * @return True if command is valid, false otherwise.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,help,"org.apache.hadoop.ha.HAAdmin:help(java.lang.String[],java.util.Map)",512,537,"/**
 * Processes commands, displays help, and handles errors based on input.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addContext,"org.apache.hadoop.http.HttpServer2:addContext(org.eclipse.jetty.servlet.ServletContextHandler,boolean)",997,1001,"/**
 * Configures ServletContextHandler, applying filters if specified.
 * @param ctxt ServletContextHandler to configure
 * @param isFiltered Whether to apply filtering
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getPlugin,org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin(java.lang.String),202,216,"/**
 * Creates and configures a plugin of type T by name.
 * @param name Plugin name, used for configuration.
 * @return Plugin instance or null if creation fails.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,loadFirst,"org.apache.hadoop.metrics2.impl.MetricsConfig:loadFirst(java.lang.String,java.lang.String[])",113,142,"/**
 * Loads MetricsConfig from specified files, or creates a default.
 * @param prefix Config prefix.
 * @param fileNames File names to load.
 * @return MetricsConfig object.
 */","* Load configuration from a list of files until the first successful load
   * @param conf  the configuration object
   * @param files the list of filenames to try
   * @return  the configuration object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,toString,org.apache.hadoop.metrics2.impl.MetricsConfig:toString(),282,285,"/**
* Delegates to the overloaded method with the current object.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,putMetrics,org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),108,193,"/**
* Processes a MetricsRecord, extracts metrics, and sends them.
* @param record The MetricsRecord to process.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,putMetrics,org.apache.hadoop.metrics2.sink.GraphiteSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),70,108,"/**
 * Sends metrics record to Graphite, constructing the path.
 * @param record The MetricsRecord containing metrics to send.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/PrometheusServlet.java,doGet,"org.apache.hadoop.http.PrometheusServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",40,46,"/**
 * Executes a masked function, logs metrics and sets response header.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,publishMetricsFromQueue,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:publishMetricsFromQueue(),128,166,"/**
* Processes queue items, retrying on errors with exponential backoff.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,incrCacheClearedCounter,org.apache.hadoop.ipc.RetryCache:incrCacheClearedCounter(),221,223,"/**
 * Records a metric using the retryCacheMetrics object.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,requeueCall,org.apache.hadoop.ipc.Server$Handler:requeueCall(org.apache.hadoop.ipc.Server$Call),3232,3240,"/**
 * Processes a call, handles RPC exceptions, and updates metrics.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,getRecords,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:getRecords(),57,66,"/**
 * Collects MetricsRecordImpl objects from MetricsRecordBuilderImpl.
 * @return List of MetricsRecordImpl objects.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,int)",62,65,"/**
* Logs an Integer metric using provided info and value.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,long)",67,70,"/**
* Records a metric value for a given info object.
* @param info Metric info; @param value The value to record.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,float)",72,75,"/**
* Records a float metric value using provided info.
* @param info MetricsInfo object containing metadata.
* @param value The float value to record.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,double)",77,80,"/**
* Logs a metric value using provided info.
* @param info MetricsInfo object; @param value Metric value
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,counter,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,int)",82,85,"/**
* Records Integer metrics using provided info and value.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,counter,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,long)",87,90,"/**
* Records a metric value for a given info object.
* @param info MetricsInfo object containing metric details.
* @param value The value to be recorded.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateInfoCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateInfoCache(java.lang.Iterable),243,248,"/**
 * Updates the info cache using the provided metrics records.
 * @param lastRecs Iterable of MetricsRecordImpl objects.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newMBeanName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newMBeanName(java.lang.String),108,111,"/**
 * Returns an ObjectName based on the provided name.
 * @param name The name to use for creating the ObjectName.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,sourceName,"org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:sourceName(java.lang.String,boolean)",123,126,"/**
 * Masks a name using a private instance.
 * @param name Name to mask.
 * @param dupOK Whether duplicates are acceptable.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,load,org.apache.hadoop.security.Groups$GroupCacheLoader:load(java.lang.String),342,370,"/**
 * Fetches group list for a user, caches negatively if empty, throws exception.
 */","* This method will block if a cache entry doesn't exist, and
     * any subsequent requests for the same user will wait on this
     * request to return. If a user already exists in the cache,
     * and when the key expires, the first call to reload the key
     * will block, but subsequent requests will return the old
     * value until the blocking thread returns.
     * If reloadGroupsInBackground is true, then the thread that
     * needs to refresh an expired key will not block either. Instead
     * it will return the old cache value and schedule a background
     * refresh
     * @param user key of cache
     * @return List of groups belonging to user
     * @throws IOException to prevent caching negative entries",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,shutdownSingleton,org.apache.hadoop.metrics2.source.JvmMetrics:shutdownSingleton(),139,141,"/**
 * Calls the m1 method of the Singleton instance.
 */","* Shutdown the JvmMetrics singleton. This is not necessary if the JVM itself
   * is shutdown, but may be necessary for scenarios where JvmMetrics instance
   * needs to be re-created while the JVM is still around. One such scenario
   * is unit-testing.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reattach,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:reattach(),151,153,"/**
 * Initializes the 'metrics' object using UgiMetrics.m1().
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,stop,org.apache.hadoop.ipc.Server:stop(),3696,3719,"/**
* Stops the server, shutting down handlers, listener, and responder.
*/",Stops the service.  No new calls will be handled after this is called.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,stopMBeans,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stopMBeans(),226,231,"/**
 * Releases the mbeanName by invoking MBeans.m1 if it exists.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,unregisterSource,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:unregisterSource(java.lang.String),896,902,"/**
 * Records metrics for the DecayRpcScheduler with the given namespace.
 * @param namespace The namespace for the metrics.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2Metrics.java,create,"org.apache.hadoop.http.HttpServer2Metrics:create(org.eclipse.jetty.server.handler.StatisticsHandler,int)",151,159,"/**
 * Creates and registers HttpServer2 metrics with the given handler and port.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,stop,org.apache.hadoop.http.HttpServer2:stop(),1559,1609,"/**
 * Stops the web application context and related components.
 * Handles exceptions and aggregates them into a MultiException.
 */","* stop the server.
   *
   * @throws Exception exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,getMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",569,581,"/**
 * Records metrics using the provided builder, optionally including all.
 * @param builder MetricsCollector to build the record
 * @param all Whether to include all sinks in the metrics
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,<init>,"org.apache.hadoop.metrics2.lib.MutableQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",87,106,"/**
 * Constructs a MutableQuantiles object with given parameters.
 * @param interval Interval for sample rollover.
 */","* Instantiates a new {@link MutableQuantiles} for a metric that rolls itself
   * over on the specified time interval.
   * 
   * @param name
   *          of the metric
   * @param description
   *          long-form textual description of the metric
   * @param sampleName
   *          type of items in the stream (e.g., ""Ops"")
   * @param valueName
   *          type of the values
   * @param interval
   *          rollover interval (in seconds) of the estimator",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:<init>(),993,1000,"/**
 * Initializes the metrics registry and IO statistics.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,<init>,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:<init>(org.apache.hadoop.ipc.RetryCache),42,48,"/**
 * Initializes RetryCacheMetrics with a given RetryCache.
 * @param retryCache The RetryCache to associate metrics with.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Field)",129,131,"/**
 * Delegates metrics info creation to m2(annotation, Field).
 * @param annotation Metric annotation
 * @param field Field to derive metrics from
 * @return MetricsInfo object
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Method)",137,139,"/**
 * Delegates metrics info creation to m2(annotation, MethodContext).
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newStat,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)",262,269,"/**
 * Creates and registers a MutableStat.
 * @param name Stat name.
 * @param extended Extended flag.
 * @return The created MutableStat.
 */
","* Create a mutable metric with stats
   * @param name  of the metric
   * @param desc  metric description
   * @param sampleName  of the metric (e.g., ""Ops"")
   * @param valueName   of the metric (e.g., ""Time"" or ""Latency"")
   * @param extended    produce extended stat (stdev, min/max etc.) if true.
   * @return a new mutable stat metric object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",94,97,"/**
 * Constructs a MutableStat with a flag initially set to false.
 * @param name Stat name.
 * @param description Stat description.
 * @param sampleName Sample name.
 * @param valueName Value name.
 */
","* Construct a snapshot stat metric with extended stat off by default
   * @param name        of the metric
   * @param description of the metric
   * @param sampleName  of the metric (e.g. ""Ops"")
   * @param valueName   of the metric (e.g. ""Time"", ""Latency"")",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRate.java,<init>,"org.apache.hadoop.metrics2.lib.MutableRate:<init>(java.lang.String,java.lang.String,boolean)",31,33,"/**
* Constructs a MutableRate object.
* @param name Rate name.
* @param description Rate description.
* @param extended Whether the rate is extended.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getGcUsage,org.apache.hadoop.metrics2.source.JvmMetrics:getGcUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder),180,207,"/**
 * Populates metrics record with GC stats from beans & monitors.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,setContext,org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:setContext(java.lang.String),147,150,"/**
* Calls m1 with a context and value.
* @param value The value to pass to m1.
* @return A MetricsRecordBuilderImpl instance.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,setContext,org.apache.hadoop.metrics2.lib.MetricsRegistry:setContext(java.lang.String),380,382,"/**
* Creates a MetricsRegistry with the given name.
* @param name The name of the registry.
* @return A MetricsRegistry object.
*/
","* Set the metrics context tag
   * @param name of the context
   * @return the registry itself as a convenience",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String,boolean)",403,406,"/**
* Creates a MetricsRegistry with an interned name/description.
* @param name Metrics name.
* @param description Metrics description.
* @param value Metrics value.
* @param override Whether to override existing registry.
* @return The created MetricsRegistry.
*/
","* Add a tag to the metrics
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @param override  existing tag if true
   * @return the registry (for keep adding tags)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",422,424,"/**
 * Creates a MetricsRegistry with a boolean flag.
 * @param info MetricsInfo object
 * @param value String value
 * @return MetricsRegistry object
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,add,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:add(java.lang.String,long)",100,114,"/**
 * Records elapsed time for a given name into thread-local stats.
 * @param name Identifier for the recorded elapsed time.
 * @param elapsed Elapsed time value to record.
 */","* Add a rate sample for a rate metric.
   * @param name of the rate metric
   * @param elapsed time",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,publishMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,boolean)",435,449,"/**
 * Sends metrics to sinks, optionally immediately.
 * @param buffer MetricsBuffer to send.
 * @param immediate If true, sends immediately; otherwise, asynchronously.
 */","* Publish a metrics snapshot to all the sinks
   * @param buffer  the metrics snapshot to publish
   * @param immediate  indicates that we should publish metrics immediately
   *                   instead of using a separate thread.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,copyTo,org.apache.hadoop.metrics2.util.SampleStat:copyTo(org.apache.hadoop.metrics2.util.SampleStat),59,61,"/**
* Calls the m1 method of the provided SampleStat object.
* @param other The SampleStat object to call m1 on.
*/","* Copy the values to other (saves object creation and gc.)
   * @param other the destination to hold our values",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,<init>,"org.apache.hadoop.metrics2.lib.MethodMetric:<init>(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.annotation.Metric$Type)",46,53,"/**
 * Initializes a MethodMetric with provided object, method, info, and type.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,toString,org.apache.hadoop.metrics2.lib.MutableStat:toString(),187,190,"/**
* Delegates to the result of m1().m2(), returning a String.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,logSlowRpcCalls,"org.apache.hadoop.ipc.Server:logSlowRpcCalls(java.lang.String,org.apache.hadoop.ipc.Server$Call,org.apache.hadoop.ipc.ProcessingDetails)",593,615,"/**
* Logs slow RPC calls exceeding defined thresholds.
* @param methodName RPC method name.
* @param call Call object.
* @param details Processing details.
*/","* Logs a Slow RPC Request.
   *
   * @param methodName - RPC Request method name
   * @param details - Processing Detail.
   *
   * If a request took significant more time than other requests,
   * and its processing time is at least `logSlowRPCThresholdMs` we consider that as a slow RPC.
   *
   * The definition rules for calculating whether the current request took too much time
   * compared to other requests are as follows:
   * 3 is a magic number that comes from 3 sigma deviation.
   * A very simple explanation can be found by searching for 68-95-99.7 rule.
   * We flag an RPC as slow RPC if and only if it falls above 99.7% of requests.
   * We start this logic only once we have enough sample size.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,toString,org.apache.hadoop.metrics2.util.SampleQuantiles:toString(),280,288,"/**
 * Generates a string representation of quantile data.
 * Returns ""[no samples]"" if no data is available.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addTopNCallerSummary,org.apache.hadoop.ipc.DecayRpcScheduler:addTopNCallerSummary(org.apache.hadoop.metrics2.MetricsRecordBuilder),1079,1096,"/**
* Records top callers and their priorities using a MetricsRecordBuilder.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,cacheGroupsRefresh,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh(),78,83,"/**
 * Retrieves netgroups, refreshes cache, and processes them.
 */",* Refresh the netgroup cache,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupsSet,org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroupsSet(java.lang.String),1585,1592,"/**
 * Retrieves a set of groups for a user, using fallback implementation.
 * @param user The username to lookup.
 * @return A set of group names, or null if not found.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,endln,org.apache.hadoop.security.KDiag:endln(),875,878,"/**
 * Calls m1 with and without a string argument.
 */",* Print something at the end of a section,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,title,"org.apache.hadoop.security.KDiag:title(java.lang.String,java.lang.Object[])",886,891,"/**
* Logs messages using m1, formats a string, and calls m1 again.
*/","* Print a title entry.
   *
   * @param format format string
   * @param args any arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,fail,"org.apache.hadoop.security.KDiag:fail(java.lang.String,java.lang.String,java.lang.Object[])",942,946,"/**
 * Logs a message and throws a KerberosDiagsFailure.
 * @param category Message category.
 * @param message Message to log.
 * @param args Arguments for the message.
 */
","* Format and raise a failure.
   *
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @throws KerberosDiagsFailure containing the formatted text",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullMaps,org.apache.hadoop.security.ShellBasedIdMapping:loadFullMaps(),386,389,"/**
 * Executes m1 and m2 methods sequentially in a synchronized block.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createRemoteUser,"org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",1451,1462,"/**
 * Creates a UserGroupInformation object for a given user and auth method.
 * @param user the user's name
 * @param authMethod the authentication method to use
 * @return UserGroupInformation object
 */
","* Create a user from a login name. It is intended to be used for remote
   * users in RPC, since it won't have any credentials.
   * @param user the full user principal name, must not be empty or null
   * @param authMethod authMethod.
   * @return the UserGroupInformation for the remote user.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getDefault,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getDefault(),1052,1067,"/**
* Returns a SocketFactory, creating one if it doesn't exist.
* Uses keyStoreLocation and trustStoreLocation for SSL config.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,retrievePassword,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),548,552,"/**
* Retrieves byte array from TokenIdent using m1 and m2.
* @param identifier TokenIdent object
* @return byte[] retrieved from TokenIdent
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,startThreads,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:startThreads(),169,177,"/**
 * Starts the expired token removal process.
 * Ensures !running, then initializes and starts the removal thread.
 */","* should be called before this object is used.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,rollMasterKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:rollMasterKey(),463,476,"/**
 * Updates the current key and performs related operations.
 */","* Update the current master key for generating delegation tokens 
   * It should be called only by tokenRemoverThread.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(),150,152,"/**
 * Constructs a DelegationTokenAuthenticatedURL with null parameters.
 */","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   * <p>
   * An instance of the default {@link DelegationTokenAuthenticator} will be
   * used.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator),160,163,"/**
 * Constructs a DelegationTokenAuthenticatedURL with an authenticator.
 * @param authenticator The DelegationTokenAuthenticator to use.
 */
","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   *
   * @param authenticator the {@link DelegationTokenAuthenticator} instance to
   * use, if <code>null</code> the default one will be used.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.authentication.client.ConnectionConfigurator),171,174,"/**
 * Constructs a DelegationTokenAuthenticatedURL with a null base URL.
 * @param connConfigurator Configures the connection.
 */
","* Creates an <code>DelegationTokenAuthenticatedURL</code> using the default
   * {@link DelegationTokenAuthenticator} class.
   *
   * @param connConfigurator a connection configurator.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(byte[]),225,228,"/**
* Reads bytes from a byte array.
* @param b the byte array to read from
* @return number of bytes read
*/","* Reads up to <code>b.length</code> bytes of data from this input stream into
   * an array of bytes.
   * <p>
   * The <code>read</code> method of <code>InputStream</code> calls the
   * <code>read</code> method of three arguments with the arguments
   * <code>b</code>, <code>0</code>, and <code>b.length</code>.
   * 
   * @param b
   *          the buffer into which the data is read.
   * @return the total number of bytes read into the buffer, or <code>-1</code>
   *         is there is no more data because the end of the stream has been
   *         reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isFromKeytab,org.apache.hadoop.security.UserGroupInformation:isFromKeytab(),834,838,"/**
* Checks if m1 and m2 are true and m3 is not null.
*/","* Is this user logged in from a keytab file managed by the UGI?
   * @return true if the credentials are from a keytab file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isFromTicket,org.apache.hadoop.security.UserGroupInformation:isFromTicket(),844,846,"/**
* Checks if m1 is true, m2 is true, and m3 is null.
*/","*  Is this user logged in from a ticket (but no keytab) managed by the UGI?
   * @return true if the credentials are from a ticket cache.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,shouldRelogin,org.apache.hadoop.security.UserGroupInformation:shouldRelogin(),869,873,"/**
* Returns true if both m1() and m2() return true.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,toString,org.apache.hadoop.fs.FileSystem$Cache$Key:toString(),3915,3918,"/**
* Constructs a string representation of the object.
* Returns a formatted string combining ugi, scheme, and authority.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,toString,org.apache.hadoop.security.UserGroupInformation$RealUser:toString(),497,500,"/**
 * Delegates m1() call to the realUser.
 * @return String returned by realUser's m1()
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeIdle,org.apache.hadoop.ipc.Server$ConnectionManager:closeIdle(boolean),4130,4149,"/**
 * Closes idle connections, optionally scanning all.
 * @param scanAll if true, scans all connections; otherwise, limited.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeAll,org.apache.hadoop.ipc.Server$ConnectionManager:closeAll(),4151,4157,"/**
* Iterates through connections and applies operation m1 to each.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeConnection,org.apache.hadoop.ipc.Server:closeConnection(org.apache.hadoop.ipc.Server$Connection),3493,3495,"/**
* Delegates connection handling to the connectionManager.
* @param connection The database connection to handle.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,javax.net.ssl.SSLSocket)",275,278,"/**
* Calls m1 with an array containing the host.
* @param host The hostname to use.
* @param ssl The SSLSocket to use.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,initializeDefaultFactory,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeDefaultFactory(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),103,108,"/**
 * Creates or retrieves a DelegatingSSLSocketFactory instance.
 * @param preferredMode Preferred SSL channel mode.
 */","* Initialize a singleton SSL socket factory.
   *
   * @param preferredMode applicable only if the instance is not initialized.
   * @throws IOException if an error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,<init>,org.apache.hadoop.fs.shell.FsCommand:<init>(org.apache.hadoop.conf.Configuration),78,80,"/**
 * Constructs a FsCommand object with the given configuration.
 * @param conf The Hadoop configuration to use.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,<init>,org.apache.hadoop.fs.shell.CommandFactory:<init>(),45,47,"/**
 * Default constructor. Initializes the CommandFactory with null config.
 */
",Factory constructor for commands,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,org.apache.hadoop.fs.HarFileSystem:<init>(),83,85,"/**
 * Constructs a HarFileSystem object.
 * Must initialize the underlying file system.
 */
",* public construction of harfilesystem,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,org.apache.hadoop.fs.HarFileSystem:<init>(org.apache.hadoop.fs.FileSystem),103,106,"/**
 * Constructs a HarFileSystem with the given FileSystem.
 * @param fs The FileSystem to associate with this HarFileSystem.
 */
","* Constructor to create a HarFileSystem with an
   * underlying filesystem.
   * @param fs underlying file system",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,<init>,org.apache.hadoop.fs.FilterFileSystem:<init>(),71,72,"/**
 * Constructs a new FilterFileSystem object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,<init>,org.apache.hadoop.fs.FilterFileSystem:<init>(org.apache.hadoop.fs.FileSystem),74,77,"/**
 * Constructs a FilterFileSystem with the given FileSystem and stats.
 * @param fs The FileSystem to filter.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell:<init>(),66,68,"/**
 * Default constructor for FsShell, calls the parameterized constructor.
 */","* Default ctor with no configuration.  Be sure to invoke
   * {@link #setConf(Configuration)} with a valid configuration prior
   * to running commands.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,<init>,org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration),43,45,"/**
 * Constructs a GetGroupsBase with a Configuration and default output.
 * @param conf The Configuration object used for initialization.
 */
","* Create an instance of this tool using the given configuration.
   * @param conf configuration.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,<init>,org.apache.hadoop.fs.shell.Command:<init>(),76,79,"/**
 * Initializes the Command object with System.out and System.err.
 */
",Constructor,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),39,43,"/**
 * Constructs an ErasureEncoder with specified options.
 * @param options ErasureCoderOptions object configuring the encoder.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,42,"/**
 * Constructs an ErasureDecoder with the given options.
 * @param options ErasureCoderOptions object containing configuration.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,<init>,"org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)",48,51,"/**
 * Creates a WritableDeserializer for the given class.
 * @param conf Configuration object.
 * @param c Writable class to deserialize.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,<init>,org.apache.hadoop.log.LogLevel$CLI:<init>(org.apache.hadoop.conf.Configuration),105,107,"/**
 * Constructor. Initializes the CLI with the given configuration.
 * @param conf Configuration object for CLI setup.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,org.apache.hadoop.security.KDiag:<init>(),186,187,"/**
 * Default constructor for the KDiag class.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,<init>,org.apache.hadoop.ha.HAAdmin:<init>(),99,101,"/**
 * Default constructor. Calls the superclass constructor.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByName,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByName(java.lang.String),647,685,"/**
 * Resolves a host name to an InetAddress. Throws UnknownHostException if resolution fails.
 */","* Create an InetAddress with a fully qualified hostname of the given
     * hostname.  InetAddress does not qualify an incomplete hostname that
     * is resolved via the domain search list.
     * {@link InetAddress#getCanonicalHostName()} will fully qualify the
     * hostname, but it always return the A record whereas the given hostname
     * may be a CNAME.
     * 
     * @param host a hostname or ip address
     * @return InetAddress with the fully qualified hostname or ip
     * @throws UnknownHostException if host does not exist",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateStaticMapping,org.apache.hadoop.security.ShellBasedIdMapping:updateStaticMapping(),304,336,"/**
 * Loads/initializes static UID/GID mapping from file or defaults.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,write,org.apache.hadoop.security.authorize.AccessControlList:write(java.io.DataOutput),317,321,"/**
* Writes the ACL string to the output stream.
* @param out DataOutput to write the ACL string to.
*/",* Serializes the AccessControlList object,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createZooKeeper,org.apache.hadoop.ha.ActiveStandbyElector:createZooKeeper(),752,762,"/**
 * Creates and configures a ZooKeeper client.
 * Uses truststoreKeystore if provided, then returns the client.
 */","* Get a new zookeeper client instance. protected so that test class can
   * inherit and pass in a mock object for zookeeper
   *
   * @return new zookeeper client instance
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,"org.apache.hadoop.security.UserGroupInformation:relogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)",1334,1345,"/**
 * Updates login context, optionally ignoring the last login time.
 * @param login HadoopLoginContext object
 * @param ignoreLastLoginTime flag to ignore last login time
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],boolean)",34,40,"/**
 * Constructs a CryptoFSDataOutputStream.
 * @param out Output stream, codec, buffer size, key, iv, close flag
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)",91,95,"/**
 * Creates a CryptoOutputStream.
 * @param out Output stream.
 * @param codec CryptoCodec.
 * @param bufferSize Buffer size.
 * @param key Encryption key.
 * @param iv Initialization vector.
 * @param streamOffset Stream offset.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite,java.lang.String)",124,128,"/**
 * Initializes an OpensslCtrCipher with given mode, suite, and engine.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,getInstance,org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String),111,114,"/**
 * Creates an OpensslCipher with the given transformation.
 * @param transformation Cipher transformation string (e.g., ""AES-256-CBC"")
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,<init>,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:<init>(),39,48,"/**
 * Initializes the codec, throwing exception if OpenSSL fails to load or SM4 CTR is unsupported.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONEncKeyVersion,"org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersion(java.lang.String,java.util.Map)",157,183,"/**
* Creates an EncryptedKeyVersion object from a value map.
* @param keyName Key name.
* @param valueMap Map containing encryption details.
* @return EncryptedKeyVersion object.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$CreateCommand:execute(),456,474,"/**
* Creates a key, handling exceptions and logging success/failure.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,createKey,"org.apache.hadoop.crypto.key.KeyProviderExtension:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",71,75,"/**
* Delegates KeyVersion retrieval to the key provider.
* @param name Key name. @param options Retrieval options.
* @return KeyVersion object.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String),148,154,"/**
 * Retrieves a KeyVersion by name, calls m3, and returns it.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,rollNewVersion,org.apache.hadoop.crypto.key.KeyProviderExtension:rollNewVersion(java.lang.String),77,81,"/**
* Delegates KeyVersion retrieval to keyProvider.
* @param name Key name.
* @return KeyVersion object.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$RollCommand:execute(),308,328,"/**
* Rolls a key within the KeyProvider, handling potential exceptions.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getSize,org.apache.hadoop.crypto.key.kms.ValueQueue:getSize(java.lang.String),323,340,"/**
 * Retrieves queue size for a given key.
 * @param keyName The key to retrieve the queue size for.
 * @return The size of the queue or 0 if not found.
 */
","* Get size of the Queue for keyName. This is only used in unit tests.
   * @param keyName the key name
   * @return int queue size. Zero means the queue is empty or the key does not exist.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getAtMost,"org.apache.hadoop.crypto.key.kms.ValueQueue:getAtMost(java.lang.String,int)",354,399,"/**
 * Retrieves a list of elements from a queue, handling refill policies.
 * @param keyName Key for the queue.
 * @param num Number of elements to retrieve.
 * @return List of retrieved elements.
 */","* This removes the ""num"" values currently at the head of the Queue for the
   * provided key. Will immediately fire the Queue filler function if key
   * does not exist
   * How many values are actually returned is governed by the
   * <code>SyncGenerationPolicy</code> specified by the user.
   * @param keyName String key name
   * @param num Minimum number of values to return.
   * @return {@literal List<E>} values returned
   * @throws IOException raised on errors performing I/O.
   * @throws ExecutionException execution exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,drain,org.apache.hadoop.crypto.key.kms.ValueQueue:drain(java.lang.String),302,316,"/**
 * Processes tasks from the queue for a given key, then cleans up.
 * @param keyName The key to process tasks for.
 */","* Drains the Queue for the provided key.
   *
   * @param keyName the key to drain the Queue for",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,refresh,"org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:refresh(java.lang.String,java.lang.String[])",57,68,"/**
 * Refreshes data based on identifier and arguments.
 * @param identifier Identifier for the refresh operation.
 * @param args Arguments for the refresh operation.
 * @return Collection of RefreshResponse objects.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,newEntry,"org.apache.hadoop.ipc.RetryCache:newEntry(java.lang.Object,long,byte[],int)",341,345,"/**
 * Creates a CacheEntryWithPayload with given data and expiration.
 * @param payload The data to be cached.
 * @param expirationTime Expiration time in milliseconds.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long,boolean)",161,165,"/**
 * Constructs a CacheEntryWithPayload.
 * @param payload The data associated with the cache entry.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,put,org.apache.hadoop.ipc.CallQueueManager:put(java.lang.Object),289,299,"/**
 * Processes an element 'e' based on internal conditions.
 */","* Insert e into the backing queue or block until we can.  If client
   * backoff is enabled this method behaves like add which throws if
   * the queue overflows.
   * If we block and the queue changes on us, we will insert while the
   * queue is drained.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,add,org.apache.hadoop.ipc.CallQueueManager:add(java.lang.Object),301,304,"/**
* Delegates to m1 with the 'enable' flag set to true.
* @param e The element to pass to m1.
* @return The result of calling m1.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,internalQueueCall,"org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call,boolean)",3111,3141,"/**
* Enqueues a call to the call queue, timing the operation.
* Handles CallQueueOverflowException and re-throws the cause.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,ensureInitialized,org.apache.hadoop.ipc.WritableRpcEngine:ensureInitialized(),71,75,"/**
 * Initializes resources if not already initialized, calls m1().
 */",* Initialize this class if it isn't already.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,get,org.apache.hadoop.util.LightWeightCache:get(java.lang.Object),186,199,"/**
 * Retrieves an entry by key, updates expiry if enabled.
 * @param key The key of the entry to retrieve.
 * @return The entry or null if not found.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,close,org.apache.hadoop.util.StopWatch:close(),116,121,"/**
* Calls m1() if isStarted is true.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,now,org.apache.hadoop.util.StopWatch:now(java.util.concurrent.TimeUnit),97,100,"/**
* Converts m1 to the specified time unit.
* @param timeUnit The TimeUnit to convert to.
*/","* now.
   *
   * @param timeUnit timeUnit.
   * @return current elapsed time in specified timeunit.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,toString,org.apache.hadoop.util.StopWatch:toString(),111,114,"/**
* Concatenates the result of m1() with a constant string.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Invocation:<init>(java.lang.reflect.Method,java.lang.Object[])",104,120,"/**
 * Creates an Invocation object.
 * @param method The method being invoked.
 * @param parameters Method parameters.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(int,long,java.lang.Class)",210,223,"/**
 * Checks protocol signature based on hashcode and server version.
 * @param clientMethodsHashCode Client's methods hashcode.
 * @param serverVersion Server version.
 * @param protocol Protocol class.
 * @return ProtocolSignature object.
 */
","* Get a server protocol's signature
   * 
   * @param clientMethodsHashCode client protocol methods hashcode
   * @param serverVersion server protocol version
   * @param protocol protocol
   * @return the server's protocol signature",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(java.lang.String,long)",225,229,"/**
 * Retrieves the signature of a protocol class by name and version.
 * @param protocolName Protocol class name.
 * @param version Protocol version.
 * @return ProtocolSignature object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:shouldRetry(java.lang.Exception,int,int,boolean)",697,750,"/**
 * Determines retry action based on exception, retries, and flags.
 * @param e exception to analyze
 * @return RetryAction object defining retry strategy
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,shouldRetry,"org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:shouldRetry(java.lang.Exception,int,int,boolean)",98,129,"/**
 * Determines retry policy based on exception type.
 * @param e Exception to evaluate.
 * @return RetryAction based on retry policy.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,doHealthChecks,org.apache.hadoop.ha.HealthMonitor:doHealthChecks(),195,227,"/**
 * Monitors service health, updates state, and handles exceptions.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,org.apache.hadoop.ipc.Server$Call:<init>(),980,983,"/**
 * Constructs a Call object with default invalid values.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,addResponseTime,"org.apache.hadoop.ipc.CallQueueManager:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",256,258,"/**
* Delegates scheduling to the scheduler.
* @param name Schedule name.
* @param e Schedulable object to schedule.
* @param details Processing details for the schedule.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doAccept,org.apache.hadoop.ipc.Server$Listener:doAccept(java.nio.channels.SelectionKey),1616,1639,"/**
* Accepts incoming connections on a ServerSocketChannel.
* Handles connection setup and assigns a Connection object.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,add,org.apache.hadoop.ipc.FairCallQueue:add(org.apache.hadoop.ipc.Schedulable),194,213,"/**
* Attempts to process an element. Throws exception if processing fails.
* @param e the element to process
* @return true if processing was successful
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,put,org.apache.hadoop.ipc.FairCallQueue:put(org.apache.hadoop.ipc.Schedulable),215,222,"/**
 * Processes an element, adjusting queue size if processing fails.
 * @param e The element to process.
 * @throws InterruptedException if interrupted.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$RpcCall:run(),1234,1272,"/**
 * Executes RPC, handles errors, and logs timing details.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setDeferredError,org.apache.hadoop.ipc.Server$RpcCall:setDeferredError(java.lang.Throwable),1367,1391,"/**
* Handles a thrown exception, logs errors, and sets up deferred responses.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolVersions,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersions(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto)",44,68,"/**
* Gets protocol versions for a given protocol.
* @param controller RPC controller
* @param request Request containing the protocol
* @return Response containing protocol versions
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolProxy.java,fetchServerMethods,org.apache.hadoop.ipc.ProtocolProxy:fetchServerMethods(java.lang.reflect.Method),57,78,"/**
 * Checks server version and fetches server methods.
 * Throws VersionMismatch if versions don't match.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProtocolImpl,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:getProtocolImpl(org.apache.hadoop.ipc.RPC$Server,java.lang.String,long)",510,527,"/**
* Retrieves ProtoClassProtoImpl, throws VersionMismatch if needed.
* @param server RPC server, protoName protocol name, clientVersion version
* @return ProtoClassProtoImpl object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.lang.String)",1990,1992,"/**
 * Constructs a FatalRpcServerException with error code and message.
 * @param errCode The error code.
 * @param message The exception message.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer:<init>(int),37,39,"/**
 * Constructs a ResponseBuffer with the specified capacity.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection$RpcRequestSender:run(),1115,1150,"/**
 * Processes RPC requests from the request queue until closed.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,decayCurrentCosts,org.apache.hadoop.ipc.DecayRpcScheduler:decayCurrentCosts(),479,540,"/**
 * Decays current costs based on decayFactor, updates totals, and cleans zero costs.
 */","* Decay the stored costs for each user and clean as necessary.
   * This method should be called periodically in order to keep
   * costs current.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.ipc.Schedulable),677,684,"/**
 * Calculates a masked value based on the object's identity.
 * @param obj The object to process.
 * @return A masked integer value.
 */","* Compute the appropriate priority for a schedulable based on past requests.
   * @param obj the schedulable obj to query and remember
   * @return the level index which we recommend scheduling in",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),686,691,"/**
 * Extracts and processes an identity string from UserGroupInformation.
 * @param ugi UserGroupInformation object
 * @return An integer value derived from the identity.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setPriorityLevel,"org.apache.hadoop.ipc.Server:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",732,735,"/**
* Delegates m1 call to the callQueue.
* @param ugi UserGroupInformation for authentication.
* @param priority Priority level for the operation.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invoke,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke(),161,163,"/**
* Wraps the result of m1() in a CallReturn object.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,getValue,org.apache.hadoop.ipc.RpcWritable$Buffer:getValue(java.lang.Object),190,192,"/**
* Wraps value with RpcWritable, applies transformations.
* @param value The value to be wrapped and transformed.
* @return Transformed value.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,setResponse,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:setResponse(com.google.protobuf.Message),405,410,"/**
 * Processes a message: calculates time, calls RPC, and logs metrics.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,setResponse,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:setResponse(org.apache.hadoop.thirdparty.protobuf.Message),437,442,"/**
 * Processes a message, logs processing time, and calls RPC.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseForWritable,"org.apache.hadoop.ipc.Server:setupResponseForWritable(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3564,3580,"/**
 * Creates a byte array representing the RPC response.
 * @param header RPC response header
 * @param rv Writable data; null if no data.
 * @return Byte array of the response.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,removeNextElement,org.apache.hadoop.ipc.FairCallQueue:removeNextElement(),165,178,"/**
 * Retrieves an element from the queues based on priority.
 * Returns the element or null if no element is found.
 */","* Returns an element first non-empty queue equal to the priority returned
   * by the multiplexer or scans from highest to lowest priority queue.
   *
   * Caller must always acquire a semaphore permit before invoking.
   *
   * @return the first non-empty queue with less priority, or null if
   * everything was empty",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client$Connection:close(),1266,1304,"/**
* Closes the IPC connection, handling exceptions and logging.
*/",Close the connection.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRunLoop,org.apache.hadoop.ipc.Server$Responder:doRunLoop(),1727,1796,"/**
 * Handles asynchronous writes and checks for old call responses.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,sendResponse,org.apache.hadoop.ipc.Server$Connection:sendResponse(org.apache.hadoop.ipc.Server$RpcCall),3062,3064,"/**
 * Delegates the RPC call to the responder's m1 method.
 * @param call The RPC call object.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",56,63,"/**
 * Constructs a Globber with a file system, pattern, and filter.
 * @param fs Filesystem to operate on.
 * @param pathPattern Path pattern to match.
 * @param filter Filter to apply to matched paths.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)",81,91,"/**
 * Constructs a Globber with the given file system, pattern, filter, and symlink resolution.
 */
","* Filesystem constructor for use by {@link GlobBuilder}.
   * @param fs filesystem
   * @param pathPattern path pattern
   * @param filter optional filter
   * @param resolveSymlinks should symlinks be resolved.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,org.apache.hadoop.util.MachineList:<init>(java.lang.String),73,75,"/**
 * Constructs a MachineList with the given host entries and default InetAddressFactory.
 * @param hostEntries Comma-separated string of host entries.
 */
","* 
   * @param hostEntries comma separated ip/cidr/host addresses",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,<init>,org.apache.hadoop.util.FileBasedIPList:<init>(java.lang.String),52,65,"/**
 * Constructs IP list from file.
 * @param fileName File containing IP addresses, one per line.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfo.java,newInstance,org.apache.hadoop.util.SysInfo:newInstance(),36,44,"/**
 * Returns SysInfo object based on OS.
 * @return SysInfo instance; throws UnsupportedOperationException if OS is unknown.
 */","* Return default OS instance.
   * @throws UnsupportedOperationException If cannot determine OS.
   * @return Default instance for the detected OS.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getPhysicalMemorySize,org.apache.hadoop.util.SysInfoLinux:getPhysicalMemorySize(),594,600,"/**
 * Calculates available RAM size in KB.
 * Uses ramSize, hardwareCorruptSize, hugePageTotal, hugePageSize.
 */",{@inheritDoc},,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getAvailableVirtualMemorySize,org.apache.hadoop.util.SysInfoLinux:getAvailableVirtualMemorySize(),619,622,"/**
* Calculates a masked value by adding swapSizeFree to m1().
*/",{@inheritDoc},,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,<init>,org.apache.hadoop.fs.FSDataInputStream:<init>(java.io.InputStream),58,64,"/**
 * Creates an FSDataInputStream from an InputStream.
 * Requires InputStream to implement Seekable & PositionedReadable.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,read,"org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)",196,212,"/**
 * Delegates to the wrapped input or creates a buffer.
 * @param pool Buffer pool, maxLength, opts. Returns ByteBuffer.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,put,org.apache.hadoop.util.LightWeightResizableGSet:put(java.lang.Object),91,96,"/**
 * Adds an element, calls m2(), and returns the existing element.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,org.apache.hadoop.util.LightWeightGSet$SetIterator:remove(),346,357,"/**
* Removes the current element. Throws exception if no current element.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,remove,org.apache.hadoop.util.LightWeightResizableGSet:remove(java.lang.Object),103,106,"/**
* Calls the parent class's m1 method.
* @param key The key to pass to the parent's m1 method.
* @return The result of the parent's m1 method.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evict,org.apache.hadoop.util.LightWeightCache:evict(),155,161,"/**
 * Retrieves and removes an element from the queue.
 * Returns the polled element.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,transform,"org.apache.hadoop.util.XMLUtils:transform(java.io.InputStream,java.io.InputStream,java.io.Writer)",79,95,"/**
* Transforms XML using XSLT, writing output to a Writer.
* @param styleSheet XSLT stylesheet input stream
* @param xml XML input stream
* @param out Writer to write the transformed output
*/
","* Transform input xml given a stylesheet.
   * 
   * @param styleSheet the style-sheet
   * @param xml input xml data
   * @param out output
   * @throws TransformerConfigurationException synopsis signals a problem
   *         creating a transformer object.
   * @throws TransformerException this is used for throwing processor
   *          exceptions before the processing has started.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,boolean,java.util.List)",446,469,"/**
* Generates a summary string based on options and storage types.
* Returns a formatted string; options control summary content.
*/","Return the string representation of the object in the output format.
   * if qOption is false, output directory count, file count, and content size;
   * if qOption is true, output quota and remaining quota as well.
   * if hOption is false, file sizes are returned in bytes
   * if hOption is true, file sizes are returned in human readable
   * if tOption is true, display the quota by storage types
   * if tOption is false, same logic with #toString(boolean,boolean)
   * if xOption is false, output includes the calculation from snapshots
   * if xOption is true, output excludes the calculation from snapshots
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output is to be used
   * @param tOption a flag indicating if display quota by storage types
   * @param xOption a flag indicating if calculation from snapshots is to be
   *                included in the output
   * @param types Storage types to display
   * @return the string representation of the object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toSnapshot,org.apache.hadoop.fs.ContentSummary:toSnapshot(boolean),498,503,"/**
 * Formats snapshot information into a string.
 * @param hOption boolean flag, affects formatting.
 * @return Formatted snapshot information string.
 */
","* Return the string representation of the snapshot counts in the output
   * format.
   * @param hOption flag indicating human readable or not
   * @return String representation of the snapshot counts",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,getQuotaUsage,org.apache.hadoop.fs.QuotaUsage:getQuotaUsage(boolean),329,346,"/**
* Formats quota and space quota strings based on provided options.
* @param hOption boolean option for formatting
* @return Formatted quota string.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,getTypesQuotaUsage,"org.apache.hadoop.fs.QuotaUsage:getTypesQuotaUsage(boolean,java.util.List)",348,366,"/**
* Generates a summary string for each storage type.
* @param hOption boolean flag
* @param types list of storage types
* @return summary string
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,computeCapacity,"org.apache.hadoop.util.LightWeightGSet:computeCapacity(double,java.lang.String)",374,377,"/**
* Calls m3 with a calculated value, percentage, and mapName.
*/","* Let t = percentage of max memory.
   * Let e = round(log_2 t).
   * Then, we choose capacity = 2^e/(size of reference),
   * unless it is outside the close interval [1, 2^30].
   *
   * @param mapName mapName.
   * @param percentage percentage.
   * @return compute capacity.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,fill,org.apache.hadoop.fs.FSInputChecker:fill(),217,222,"/**
 * Reads data into the buffer.
 * Updates `count` based on `m1` result, ensuring non-negativity.
 */","* Fills the buffer with a chunk data. 
   * No mark is supported.
   * This method assumes that all data in the buffer has already been read in,
   * hence pos > count.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,readAndDiscard,org.apache.hadoop.fs.FSInputChecker:readAndDiscard(int),231,245,"/**
 * Reads up to 'len' bytes from the input stream.
 * @param len The maximum number of bytes to read.
 * @return The actual number of bytes read.
 */","* Like read(byte[], int, int), but does not provide a dest buffer,
   * so the read data is discarded.
   * @param      len maximum number of bytes to read.
   * @return     the number of bytes read.
   * @throws     IOException  if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,toString,org.apache.hadoop.io.UTF8:toString(),163,175,"/**
 * Reads data from IBUF, processes it, and returns as a string.
 */",Convert to a String.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,toStringChecked,org.apache.hadoop.io.UTF8:toStringChecked(),183,190,"/**
* Reads data into a buffer and returns it as a string.
* Uses IBUF for reading and buffer for string construction.
*/","* Convert to a string, checking for valid UTF8.
   * @return the converted string
   * @throws UTFDataFormatException if the underlying bytes contain invalid
   * UTF8 data.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,fromBytes,org.apache.hadoop.io.UTF8:fromBytes(byte[]),257,263,"/**
 * Converts byte array to String using internal helper methods.
 */","* @return Convert a UTF-8 encoded byte array back into a string.
   *
   * @param bytes input bytes.
   * @throws IOException if the byte array is invalid UTF8",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,readString,org.apache.hadoop.io.UTF8:readString(java.io.DataInput),272,277,"/**
* Reads a string from the input stream.
* @param in DataInput stream to read from
* @return The read string
*/
","* @return Read a UTF-8 encoded string.
   *
   * @see DataInput#readUTF()
   * @param in DataInput.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,checkResponse,org.apache.hadoop.ipc.Client:checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto),252,267,"/**
* Validates RPC response header client ID.
* @param header RpcResponseHeaderProto containing client ID
* @throws IOException if client IDs do not match
*/
",Check the rpc response header.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,toString,org.apache.hadoop.ipc.Client:toString(),1353,1357,"/**
* Concatenates m1().m2() with clientId using a hyphen as separator.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,org.apache.hadoop.util.StringUtils:byteToHexString(byte),210,212,"/**
* Wraps a single byte in a byte array and passes it to m1.
* @param b The byte to be wrapped.
*/","* Convert a byte to a hex string.
   * @see #byteToHexString(byte[])
   * @see #byteToHexString(byte[], int, int)
   * @param b byte
   * @return byte's hex value as a String",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,toString,org.apache.hadoop.ha.ActiveStandbyElector:toString(),1274,1280,"/**
 * Generates a masked string with elector ID, appData, and cb.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,uncaughtException,"org.apache.hadoop.service.launcher.ServiceLauncher:uncaughtException(java.lang.Thread,java.lang.Throwable)",779,783,"/**
 * Handles uncaught exceptions in a thread, logs error, and propagates.
 * @param thread The thread where exception occurred.
 * @param exception The exception that was thrown.
 */
","* Handler for uncaught exceptions: terminate the service.
   * @param thread thread
   * @param exception exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exitWithUsageMessage,org.apache.hadoop.service.launcher.ServiceLauncher:exitWithUsageMessage(),1033,1035,"/**
* Prints usage information to the console.
*/","* Exit with the usage exit code {@link #EXIT_USAGE}
   * and message {@link #USAGE_MESSAGE}.
   * @throws ExitUtil.ExitException if exceptions are disabled",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/HadoopUncaughtExceptionHandler.java,uncaughtException,"org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable)",83,127,"/**
 * Handles thread exceptions during shutdown or normal operation.
 * Logs the exception and potentially initiates shutdown.
 */","* Uncaught exception handler.
   * If an error is raised: shutdown
   * The state of the system is unknown at this point -attempting
   * a clean shutdown is dangerous. Instead: exit
   * @param thread thread that failed
   * @param exception the raised exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exit,"org.apache.hadoop.service.launcher.ServiceLauncher:exit(int,java.lang.String)",856,858,"/**
* Logs an exit message with the given exit code and message.
*/","* Exit the JVM.
   *
   * This is method can be overridden for testing, throwing an 
   * exception instead. Any subclassed method MUST raise an 
   * {@code ExitException} instance/subclass.
   * The service launcher code assumes that after this method is invoked,
   * no other code in the same method is called.
   * @param exitCode code to exit
   * @param message input message.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,org.apache.hadoop.util.ExitUtil:terminate(int),368,370,"/**
* Throws an ExitException with a default message.
* @param status Exit status code.
*/","* Like {@link #terminate(int, Throwable)} without a message.
   *
   * @param status exit code
   * @throws ExitException if {@link System#exit(int)} is disabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Classpath.java,terminate,"org.apache.hadoop.util.Classpath:terminate(int,java.lang.String)",121,124,"/**
 * Logs an error message and exits the program with a status code.
 * @param status Exit status code.
 * @param msg Error message to log.
 */
","* Prints a message to stderr and exits with a status code.
   *
   * @param status exit code
   * @param msg message",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,interrupted,org.apache.hadoop.service.launcher.InterruptEscalator:interrupted(org.apache.hadoop.service.launcher.IrqHandler$InterruptData),103,135,"/**
 * Handles interrupt, logs, escalates if repeated, and shuts down service.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,org.apache.hadoop.util.ExitUtil:halt(int),389,391,"/**
* Calls m1 with default exception message.
* @param status status code to pass to m1
* @throws HaltException if an error occurs
*/
","* Forcibly terminates the currently running Java virtual machine.
   * @param status status code
   * @throws HaltException if {@link Runtime#halt(int)} is disabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sort,"org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)",58,61,"/**
* Calls m1 with a null comparison object.
* @param s IndexedSortable object to sort.
* @param p Starting index of the sort range.
* @param r Ending index of the sort range.
*/
","* Sort the given range of items using quick sort.
   * {@inheritDoc} If the recursion depth falls below {@link #getMaxDepth},
   * then switch to {@link HeapSort}.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,<init>,"org.apache.hadoop.fs.permission.AclStatus:<init>(java.lang.String,java.lang.String,boolean,java.lang.Iterable,org.apache.hadoop.fs.permission.FsPermission)",216,223,"/**
 * Constructs an AclStatus object with the given parameters.
 * @param owner The owner of the file/directory.
 * @param group The group of the file/directory.
 */
","* Private constructor.
   *
   * @param file Path file associated to this ACL
   * @param owner String file owner
   * @param group String file group
   * @param stickyBit the sticky bit
   * @param entries the ACL entries
   * @param permission permission of the path",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,parseACLs,org.apache.hadoop.util.ZKUtil:parseACLs(java.lang.String),95,122,"/**
 * Parses an ACL string into a list of ACL objects.
 * @param aclString Comma-separated ACL string.
 * @return List of ACL objects or an empty list if null.
 */","* Parse comma separated list of ACL entries to secure generated nodes, e.g.
   * <code>sasl:hdfs/host1@MY.DOMAIN:cdrwa,sasl:hdfs/host2@MY.DOMAIN:cdrwa</code>
   *
   * @param aclString aclString.
   * @return ACL list
   * @throws BadAclFormatException if an ACL is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,parseAuth,org.apache.hadoop.util.ZKUtil:parseAuth(java.lang.String),133,154,"/**
 * Parses auth string into a list of ZKAuthInfo objects.
 * @param authString Comma-separated auth string, scheme:auth
 * @return List of ZKAuthInfo objects or empty list if null.
 */","* Parse a comma-separated list of authentication mechanisms. Each
   * such mechanism should be of the form 'scheme:auth' -- the same
   * syntax used for the 'addAuth' command in the ZK CLI.
   * 
   * @param authString the comma-separated auth mechanisms
   * @return a list of parsed authentications
   * @throws BadAuthFormatException if the auth format is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,preserveAttributes,"org.apache.hadoop.fs.shell.CommandWithDestination:preserveAttributes(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData,boolean)",445,490,"/**
* Copies file attributes from src to target, optionally preserving Xattrs.
* @param src Source PathData
* @param target Target PathData
* @param preserveRawXAttrs Preserve raw Xattrs
*/","* Preserve the attributes of the source to the target.
   * The method calls {@link #shouldPreserve(FileAttribute)} to check what
   * attribute to preserve.
   * @param src source to preserve
   * @param target where to preserve attributes
   * @param preserveRawXAttrs true if raw.* xattrs should be preserved
   * @throws IOException if fails to preserve attributes",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,add,org.apache.hadoop.util.ChunkedArrayList:add(java.lang.Object),132,146,"/**
* Adds an element to the list, resizing chunks as needed.
* @param e element to add
* @return true if added successfully
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclUtil.java,getMinimalAcl,org.apache.hadoop.fs.permission.AclUtil:getMinimalAcl(org.apache.hadoop.fs.permission.FsPermission),99,116,"/**
 * Creates ACL entries for user, group, and other based on permission.
 * @param perm FsPermission object containing permission details
 * @return List of AclEntry objects
 */","* Translates the given permission bits to the equivalent minimal ACL.
   *
   * @param perm FsPermission to translate
   * @return List&lt;AclEntry&gt; containing exactly 3 entries representing the
   *         owner, group and other permissions",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:trackDuration(java.lang.String,long)",461,468,"/**
 * Returns a DurationTracker based on the key.
 * @param key The key to check.
 * @param count The count value.
 * @return A DurationTracker instance.
 */
","* If the store is tracking the given key, return the
   * duration tracker for it. If not tracked, return the
   * stub tracker.
   * @param key statistic key prefix
   * @param count  #of times to increment the matching counter in this
   * operation.
   * @return a tracker.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,<init>,"org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String)",58,62,"/**
 * Constructs a DurationTracker with a default interval.
 * @param iostats IOStatisticsStore for tracking.
 * @param key Identifier for the tracked duration.
 */
","* Constructor -increments the counter by 1.
   * @param iostats statistics to update
   * @param key prefix of values.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,<init>,"org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,java.lang.String,java.lang.Object[])",57,59,"/**
 * Constructs a DurationInfo with logging enabled.
 * @param log Logger instance for logging.
 * @param format Format string for duration.
 * @param args Arguments to format the duration.
 */
","* Create the duration text from a {@code String.format()} code call;
   * log output at info level.
   * @param log log to write to
   * @param format format string
   * @param args list of arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.concurrent.CompletableFuture),116,126,"/**
 * Waits for CompletableFuture completion, handling exceptions.
 * @param future The CompletableFuture to wait for.
 * @throws IOException if a CancellationException occurs.
 */
","* Wait for a single of future to complete, extracting IOEs afterwards.
   *
   * @param <T> Generics Type T.
   * @param future future to wait for.
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletionIgnoringExceptions,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletionIgnoringExceptions(java.util.concurrent.CompletableFuture),133,143,"/**
 * Waits for future completion, logs duration, ignores exceptions.
 * @param future CompletableFuture to wait for, can be null.
 */
","* Wait for a single of future to complete, ignoring exceptions raised.
   * @param future future to wait for.
   * @param <T> Generics Type T.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,toString,org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:toString(),107,112,"/**
 * Returns duration string, appends failure suffix if applicable.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,toString,org.apache.hadoop.util.DurationInfo:toString(),89,92,"/**
* Concatenates result of m1() and super.m2() with a separator.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,newCrcComposer,"org.apache.hadoop.util.CrcComposer:newCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long)",60,64,"/**
 * Creates a CrcComposer with a maximum hint length.
 * @param type checksum type, @param bytesPerCrcHint max hint bytes
 * @return CrcComposer instance
 */
","* Returns a CrcComposer which will collapse all ingested CRCs into a single
   * value.
   *
   * @param type type.
   * @param bytesPerCrcHint bytesPerCrcHint.
   * @throws IOException raised on errors performing I/O.
   * @return a CrcComposer which will collapse all ingested CRCs into a single value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(int,long)",167,192,"/**
 * Updates composite CRC based on bytesPerCrc, potentially using hint.
 * @param crcB current CRC value
 * @param bytesPerCrc bytes processed for CRC update
 */","* Updates with a single additional CRC which corresponds to an underlying
   * data size of {@code bytesPerCrc}.
   *
   * @param crcB crcB.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,lock,org.apache.hadoop.util.InstrumentedLock:lock(),101,107,"/**
 * Executes a sequence of actions, timing and logging as needed.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,lockInterruptibly,org.apache.hadoop.util.InstrumentedLock:lockInterruptibly(),109,115,"/**
 * Executes a sequence of actions, including locking and timing.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,tryLock,"org.apache.hadoop.util.InstrumentedLock:tryLock(long,java.util.concurrent.TimeUnit)",126,136,"/**
 * Attempts to acquire the lock and executes m3 if successful.
 * @param time wait time
 * @param unit time unit
 * @return True if lock acquired, false otherwise.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,unlock,org.apache.hadoop.util.InstrumentedLock:unlock(),138,144,"/**
 * Releases a lock, records timings, and calls m3 with a flag.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getFormattedTimeWithDiff,"org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(org.apache.commons.lang3.time.FastDateFormat,long,long)",375,379,"/**
 * Formats finish time and calls m2 with formatted time and other params.
 */","* Formats time in ms and appends difference (finishTime - startTime)
   * as returned by formatTimeDiff().
   * If finish time is 0, empty string is returned, if start time is 0
   * then difference is not appended to return value.
   *
   * @param dateFormat date format to use
   * @param finishTime finish time
   * @param startTime  start time
   * @return formatted value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,org.apache.hadoop.util.StringUtils:escapeString(java.lang.String),665,667,"/**
* Delegates to overloaded method with default escape and comma.
* @param str The input string to process.
*/","* Escape commas in the string using the default escape char
   * @param str a string
   * @return an escaped string",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String),723,725,"/**
* Calls m1 with default escape and comma separators.
* @param str The input string to be processed.
*/","* Unescape commas in the string using the default escape char
   * @param str a string
   * @return an unescaped string",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,startupShutdownMessage,"org.apache.hadoop.service.launcher.ServiceLauncher:startupShutdownMessage(java.lang.String,java.util.List)",1010,1016,"/**
* Masks class name and arguments with hostname.
* @param classname Class name to mask.
* @param args Arguments to mask.
* @return Masked string.
*/
","* @return Build a log message for starting up and shutting down.
   * @param classname the class of the server
   * @param args arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceNext,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceNext(),489,499,"/**
 * Retrieves a value, throwing exception if element is absent.
 */","* Get the next source value.
     * This calls {@link #sourceHasNext()} first to verify
     * that there is data.
     * @return the next value
     * @throws IOException failure
     * @throws NoSuchElementException no more data",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceHasNext,org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:sourceHasNext(),790,793,"/**
* Continues work, returning true if both calls succeed.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,awaitFuture,org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future),65,69,"/**
 * Delegates to FutureIO.m1 to retrieve a result from a Future.
 * @param future The Future to retrieve the result from.
 * @return The result wrapped in the Future.
 * @throws InterruptedIOException, IOException, RuntimeException
 */
","* Given a future, evaluate it. Raised exceptions are
   * extracted and handled.
   * See {@link FutureIO#awaitFuture(Future, long, TimeUnit)}.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException future was interrupted
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitAllFutures,org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection),162,169,"/**
 * Collects results from a collection of Futures into a List.
 * @param collection Futures to retrieve results from.
 * @return List of results, or an empty list if input is empty.
 */
","* Evaluates a collection of futures and returns their results as a list.
   * <p>
   * This method blocks until all futures in the collection have completed.
   * If any future throws an exception during its execution, this method
   * extracts and rethrows that exception.
   * </p>
   * @param collection collection of futures to be evaluated
   * @param <T> type of the result.
   * @return the list of future's result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,awaitFuture,"org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)",86,93,"/**
 * Retrieves result from future within a timeout.
 * @param future Future to retrieve from.
 * @param timeout Timeout duration.
 * @param unit Timeout unit (e.g., SECONDS, MILLISECONDS)
 * @return Result of the future.
 */
","* Given a future, evaluate it. Raised exceptions are
   * extracted and handled.
   * See {@link FutureIO#awaitFuture(Future, long, TimeUnit)}.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @param timeout timeout.
   * @param unit unit.
   * @return the result, if all went well.
   * @throws InterruptedIOException future was interrupted
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitAllFutures,"org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection,java.time.Duration)",188,197,"/**
 * Collects results from a list of Futures within a given duration.
 * @param collection Futures to retrieve results from.
 * @param duration Timeout duration for each future.
 * @return List of results from the futures.
 */
","* Evaluates a collection of futures and returns their results as a list,
   * but only waits up to the specified timeout for each future to complete.
   * <p>
   * This method blocks until all futures in the collection have completed or
   * the timeout expires, whichever happens first. If any future throws an
   * exception during its execution, this method extracts and rethrows that exception.
   * @param collection collection of futures to be evaluated
   * @param duration timeout duration
   * @param <T> type of the result.
   * @return the list of future's result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,cancelAllFuturesAndAwaitCompletion,"org.apache.hadoop.util.functional.FutureIO:cancelAllFuturesAndAwaitCompletion(java.util.Collection,boolean,java.time.Duration)",211,239,"/**
* Retrieves results from a collection of Futures, handling exceptions.
* @param collection Futures to process
* @return List of retrieved results
*/
","* Cancels a collection of futures and awaits the specified duration for their completion.
   * <p>
   * This method blocks until all futures in the collection have completed or
   * the timeout expires, whichever happens first.
   * All exceptions thrown by the futures are ignored. as is any TimeoutException.
   * @param collection collection of futures to be evaluated
   * @param interruptIfRunning should the cancel interrupt any active futures?
   * @param duration total timeout duration
   * @param <T> type of the result.
   * @return all futures which completed successfully.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,newInstance,"org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.Class[],java.lang.Object[])",138,161,"/**
* Creates an instance of theClass using provided arguments.
* @param theClass Class to instantiate
* @param conf Configuration object
* @param argTypes Argument types for the constructor
* @param values Constructor arguments
* @return Instance of theClass
*/
","Create an object for the given class and initialize it from conf
   *
   * @param theClass class of which an object is created
   * @param conf Configuration
   * @param argTypes the types of the arguments
   * @param values the values of the arguments
   * @param <T> Generics Type.
   * @return a new object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getKeyClass,org.apache.hadoop.io.SequenceFile$Reader:getKeyClass(),2196,2205,"/**
 * Returns the key class, fetching it if not already loaded.
 * Throws RuntimeException on IO error.
 */",@return Returns the class of keys in this file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getValueClass,org.apache.hadoop.io.SequenceFile$Reader:getValueClass(),2213,2222,"/**
 * Lazily initializes and returns the valClass.
 * Uses WritableName.m2() to fetch it from conf.
 */",@return Returns the class of values in this file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,readFields,org.apache.hadoop.io.EnumSetWritable:readFields(java.io.DataInput),117,133,"/**
* Populates the EnumSet value from the DataInput stream.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadClass,org.apache.hadoop.util.FindClass:loadClass(java.lang.String),244,259,"/**
 * Loads a class by name and handles potential loading errors.
 * @param name Class name to load. Returns success/failure code.
 */","* Loads the class of the given name
   * @param name classname
   * @return outcome code",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,createClassInstance,org.apache.hadoop.util.FindClass:createClassInstance(java.lang.String),278,302,"/**
 * Creates an instance of the class named 'name'.
 * Returns SUCCESS, E_NOT_FOUND, or E_CREATE_FAILED.
 */","* Create an instance of a class
   * @param name classname
   * @return the outcome",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.Path),115,117,"/**
 * Initializes the builder with a path, no initial file set.
 * @param path The base path for the file set.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.PathHandle),119,121,"/**
 * Constructs an AbstractFSBuilderImpl with an empty root and given path.
 * @param pathHandle The initial path handle for the builder.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandler.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandler:<init>(),42,44,"/**
 * Constructs a new FsUrlStreamHandler, initializing the configuration.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,createConfiguration,org.apache.hadoop.service.launcher.ServiceLauncher:createConfiguration(),399,401,"/**
 * Creates and returns a new Configuration object.
 */","* Override point: create the base configuration for the service.
   *
   * Subclasses can override to create HDFS/YARN configurations etc.
   * @return the configuration to use as the service initializer.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,loadConf,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:loadConf(),437,449,"/**
 * Returns the configuration, using suppliedConf if available.
 */","* Return the supplied configuration for testing or otherwise load a new
   * configuration.
   *
   * @return the configuration to use",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,<init>,org.apache.hadoop.util.FindClass:<init>(),123,125,"/**
 * Constructs a FindClass object using a default Configuration.
 */","* Empty constructor; passes a new Configuration
   * object instance to its superclass's constructor",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,<init>,org.apache.hadoop.conf.ReconfigurableBase:<init>(),75,77,"/**
 * Constructs a ReconfigurableBase with a default Configuration.
 */",* Construct a ReconfigurableBase.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,<init>,org.apache.hadoop.conf.ReconfigurableBase:<init>(org.apache.hadoop.conf.Configuration),84,86,"/**
 * Constructs a ReconfigurableBase with a Configuration object.
 * Uses a default config if none is provided.
 */
","* Construct a ReconfigurableBase with the {@link Configuration}
   * conf.
   * @param conf configuration.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,"org.apache.hadoop.util.DataChecksum:newDataChecksum(org.apache.hadoop.util.DataChecksum$Type,int)",135,150,"/**
 * Creates a DataChecksum object based on the given type.
 * @param type checksum type (NULL, CRC32, CRC32C)
 * @param bytesPerChecksum bytes per checksum
 * @return DataChecksum object or null if type is invalid
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBinPath,org.apache.hadoop.util.Shell:getQualifiedBinPath(java.lang.String),701,704,"/**
* Executes a command and returns its output as a string.
*/","*  Fully qualify the path to a binary that should be in a known hadoop
   *  bin location. This is primarily useful for disambiguating call-outs
   *  to executable sub-components of Hadoop to avoid clashes with other
   *  executables that may be in the path.  Caveat:  this call doesn't
   *  just format the path to the bin directory.  It also checks for file
   *  existence of the composed path. The output of this call should be
   *  cached by callers.
   *
   * @param executable executable
   * @return executable file reference
   * @throws FileNotFoundException if the path does not exist
   * @throws IOException on path canonicalization failures",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,runCommand,org.apache.hadoop.util.Shell:runCommand(),967,1098,"/**
 * Launches a shell process with configured parameters.
 * Configures environment, directory, and handles timeouts.
 */","* Run the command.
   *
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(java.lang.String),61,65,"/**
* Creates and updates a Progress object with the given status.
*/","* Adds a named node to the tree.
   * @param status status.
   * @return Progress.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,createRootDirRecursively,org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String),361,363,"/**
* Calls m1 with the given path and a null value for the second parameter.
* @param path The path to be passed to the overloaded method.
*/","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   * @param path Path of the znode to create.
   * @throws Exception If it cannot create the file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,invoke,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invoke(java.lang.Object,java.lang.Object[])",77,82,"/**
 * Calls m2 with arguments, casts result to R.
 * @param target Target object, must be null.
 * @param args Arguments to pass to m2.
 * @return Result of m2, cast to type R.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,pathCapabilities_hasPathCapability,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:pathCapabilities_hasPathCapability(java.lang.Object,org.apache.hadoop.fs.Path,java.lang.String)",349,356,"/**
 * Checks if a path has the specified capability.
 * @param fs filesystem, path the path, capability the capability
 * @return True if path has capability, false otherwise.
 */
","* Does a path have a given capability?
   * Calls {@code PathCapabilities#hasPathCapability(Path, String)},
   * mapping IOExceptions to false.
   * @param fs filesystem
   * @param path path to query the capability of.
   * @param capability non-null, non-empty string to query the path for support.
   * @return true if the capability is supported
   * under that part of the FS
   * false if the method is not loaded or the path lacks the capability.
   * @throws IllegalArgumentException invalid arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,streamCapabilities_hasCapability,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:streamCapabilities_hasCapability(java.lang.Object,java.lang.String)",367,372,"/**
* Checks if a stream has a capability for a given object.
* @param object The object to check.
* @param capability Capability string to check for.
* @return True if capability exists, false otherwise.
*/
","* Does an object implement {@code StreamCapabilities} and, if so,
   * what is the result of the probe for the capability?
   * Calls {@code StreamCapabilities#hasCapability(String)},
   * @param object object to probe
   * @param capability capability string
   * @return true iff the object implements StreamCapabilities and the capability is
   * declared available.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_counters,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_counters(java.io.Serializable),611,614,"/**
* Delegates counter retrieval to iostatisticsCountersMethod.
* @param source The source object for counter retrieval.
* @return A map of string keys to long values.
*/","* Get the counters of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of counters.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_gauges,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_gauges(java.io.Serializable),621,625,"/**
* Delegates gauge retrieval to iostatisticsGaugesMethod.
* @param source The source object for gauge retrieval.
* @return A map of string keys to long values.
*/
","* Get the gauges of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of gauges.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_minimums,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_minimums(java.io.Serializable),632,635,"/**
* Delegates to m1 to retrieve minimum statistics.
* @param source The source object for analysis.
* @return A map of string keys to long values.
*/","* Get the minimums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of minimums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_maximums,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_maximums(java.io.Serializable),642,645,"/**
* Delegates to m1 to get maximum statistics for a source.
* @param source The source data for statistics.
* @return A map of string keys to long values.
*/
","* Get the maximums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of maximums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_means,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_means(java.io.Serializable),654,657,"/**
* Delegates to m1 to process a source and return a map.
* @param source Serializable data to be processed.
* @return Map containing processed data.
*/
","* Get the means of an IOStatisticsSnapshot.
   * Each value in the map is the (sample, sum) tuple of the values;
   * the mean is then calculated by dividing sum/sample wherever sample is non-zero.
   * @param source source of statistics.
   * @return a map of mean key to (sample, sum) tuples.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invoke(java.lang.Object[]),219,221,"/**
* Delegates to method.m1, passing provided arguments.
* @param args Arguments to be passed to method.m1
* @return Result of method.m1
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeStatic,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeStatic(java.lang.Object[]),106,109,"/**
 * Delegates to m4 with null as the first argument and args as the second.
 */","* Invoke a static method.
     * @param args arguments.
     * @return result.
     * @param <R> type of result.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invoke(java.lang.Object[]),202,204,"/**
* Delegates to the underlying method 'm1' with provided arguments.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.Class[])",308,311,"/**
 * Calls m1 with provided class name and argument classes.
 * @param className Class name to set.
 * @param argClasses Argument classes.
 * @return This builder instance.
 */
","* Checks for an implementation, first finding the given class by name.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param className name of a class
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.Class[])",411,414,"/**
 * Calls m1 with provided class name and argument classes.
 * @param className Class name to use.
 * @param argClasses Argument classes.
 * @return This builder instance.
 */
","* Checks for an implementation, first finding the given class by name.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param className name of a class
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,loadStaticMethod,"org.apache.hadoop.util.dynamic.BindingUtils:loadStaticMethod(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])",139,149,"/**
 * Creates an unbound method.
 * @param source Class to search.
 * @param returnType Return type of method.
 * @param name Method name.
 * @param parameterTypes Method parameter types.
 * @return UnboundMethod object.
 */
","* Load a static method from the source class, which will be a noop() if
   * the class is null or the method isn't found.
   * If the class and method are not found, then an {@code IllegalStateException}
   * is raised on the basis that this means that the binding class is broken,
   * rather than missing/out of date.
   *
   * @param <T> return type
   * @param source source. If null, the method is a no-op.
   * @param returnType return type class (unused)
   * @param name method name
   * @param parameterTypes parameters
   *
   * @return the method or a no-op.
   * @throws IllegalStateException if the method is not static.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatisticsSource,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSource(java.lang.Object),394,397,"/**
* Checks if an object satisfies a condition using m1 and m2.
* @param object The object to check.
* @return True if both conditions are met, false otherwise.
*/
","* Probe for an object being an instance of {@code IOStatisticsSource}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatistics,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatistics(java.lang.Object),405,408,"/**
* Checks a condition based on m1() and isIOStatisticsMethod.m2().
*/","* Probe for an object being an instance of {@code IOStatisticsSource}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatisticsSnapshot,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSnapshot(java.io.Serializable),416,419,"/**
* Checks a condition based on m1() and isIOStatisticsSnapshotMethod.
* @param object Serializable object to be processed.
* @return True if both conditions are met, false otherwise.
*/
","* Probe for an object being an instance of {@code IOStatisticsSnapshot}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_enabled,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_enabled(),427,430,"/**
* Checks if m1() is true and iostatisticsContextEnabledMethod.m2() returns true.
*/","* Probe to check if the thread-level IO statistics enabled.
   * If the relevant classes and methods were not found, returns false
   * @return true if the IOStatisticsContext API was found
   * and is enabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,toString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:toString(),671,677,"/**
* Returns a string representation of DynamicWrappedStatistics.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_pageSize,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",263,268,"/**
* Deletes a file or directory.
* @param fileSystem The file system to operate on.
* @param path The path to delete.
* @throws IOException if an I/O error occurs.
*/
","* Get the maximum number of objects/files to delete in a single request.
   * @param fileSystem filesystem
   * @param path path to delete under.
   * @return a number greater than or equal to zero.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException path not valid.
   * @throws IOException problems resolving paths
   * @throws RuntimeException invocation failure.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_delete,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)",293,299,"/**
 * Deletes paths using a bulk delete method and returns results.
 * @param fs Filesystem, base path, and paths to delete.
 * @return List of deletion results.
 */","* Delete a list of files/objects.
   * <ul>
   *   <li>Files must be under the path provided in {@code base}.</li>
   *   <li>The size of the list must be equal to or less than the page size.</li>
   *   <li>Directories are not supported; the outcome of attempting to delete
   *       directories is undefined (ignored; undetected, listed as failures...).</li>
   *   <li>The operation is not atomic.</li>
   *   <li>The operation is treated as idempotent: network failures may
   *        trigger resubmission of the request -any new objects created under a
   *        path in the list may then be deleted.</li>
   *    <li>There is no guarantee that any parent directories exist after this call.
   *    </li>
   * </ul>
   * @param fs filesystem
   * @param base path to delete under.
   * @param paths list of paths which must be absolute and under the base path.
   * @return a list of all the paths which couldn't be deleted for a reason other than
   *          ""not found"" and any associated error message.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException if a path argument is invalid.
   * @throws IOException IO problems including networking, authentication and more.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,fileSystem_openFile,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)",323,335,"/**
* Opens a data input stream with specified options.
* @param fs Filesystem, path, policy, status, length, options
* @return FSDataInputStream or null on failure
*/","* OpenFile assistant, easy reflection-based access to
   * {@code FileSystem#openFile(Path)} and blocks
   * awaiting the operation completion.
   * @param fs filesystem
   * @param path path
   * @param policy read policy
   * @param status optional file status
   * @param length optional file length
   * @param options nullable map of other options
   * @return stream of the opened file
   * @throws IOException if the operation was attempted and failed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_readFully,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)",412,419,"/**
* Reads data from an input stream into a ByteBuffer at a specific position.
*/","* Delegate to {@code ByteBufferPositionedReadable#read(long, ByteBuffer)}.
   * @param in input stream
   * @param position position within file
   * @param buf the ByteBuffer to receive the results of the read operation.
   * @throws UnsupportedOperationException if the input doesn't implement
   * the interface or, if when invoked, it is raised.
   * Note: that is the default behaviour of {@code FSDataInputStream#readFully(long, ByteBuffer)}.
   * @throws IOException if the operation was attempted and failed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,checkIoStatisticsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsAvailable(),376,378,"/**
* Calls m1 with the iostatisticsSnapshotCreateMethod.
*/","* Require a IOStatistics to be available.
   * @throws UnsupportedOperationException if the method was not found.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,checkIoStatisticsContextAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsContextAvailable(),384,386,"/**
* Calls m1 with the iostatisticsContextEnabledMethod flag.
*/","* Require IOStatisticsContext methods to be available.
   * @throws UnsupportedOperationException if the classes/methods were not found",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,<init>,org.apache.hadoop.util.ComparableVersion:<init>(java.lang.String),355,358,"/**
 * Parses a version string and initializes the ComparableVersion object.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,<init>,"org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long)",112,118,"/**
 * Creates a LightWeightCache with default Timer.
 * @param recommendedLength Initial cache size.
 * @param sizeLimit Max cache size.
 * @param creationExpiration Period for creation expiration.
 * @param accessExpiration Period for access expiration.
 */
","* @param recommendedLength Recommended size of the internal array.
   * @param sizeLimit the limit of the size of the cache.
   *            The limit is disabled if it is &lt;= 0.
   * @param creationExpirationPeriod the time period C &gt; 0 in nanoseconds
   *            that the creation of an entry is expired if it is added to the
   *            cache longer than C.
   * @param accessExpirationPeriod the time period A &gt;= 0 in nanoseconds that
   *            the access of an entry is expired if it is not accessed
   *            longer than A.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,contains,org.apache.hadoop.util.LightWeightGSet$Values:contains(java.lang.Object),250,254,"/**
 * Delegates the method call to the superclass, casting the object.
 * @param o Object to pass to the superclass method.
 * @return The result of the superclass method call.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToMap,"org.apache.hadoop.util.HostsFileReader:readFileToMap(java.lang.String,java.lang.String,java.util.Map)",123,128,"/**
 * Processes a file based on type, using provided map.
 * @param type file type, filename, input stream, map
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,"org.apache.hadoop.util.HostsFileReader:refresh(java.io.InputStream,java.io.InputStream)",236,259,"/**
* Refreshes the host include/exclude lists from input streams.
* @param inFileInputStream Input stream for includes.
* @param exFileInputStream Input stream for excludes.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Filter.java,<init>,"org.apache.hadoop.util.bloom.Filter:<init>(int,int,int)",102,107,"/**
 * Constructs a Filter with specified vector size, hash count, and type.
 */
","* Constructor.
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash functions to consider.
   * @param hashType type of the hashing function (see {@link Hash}).",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Filter.java,readFields,org.apache.hadoop.util.bloom.Filter:readFields(java.io.DataInput),204,218,"/**
* Reads version and initializes hash function parameters from input.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,delete,org.apache.hadoop.util.bloom.CountingBloomFilter:delete(org.apache.hadoop.util.bloom.Key),135,160,"/**
 * Processes a key, validating it and updating hash buckets.
 * @param key The key to process; must not be null.
 */","* Removes a specified key from <i>this</i> counting Bloom filter.
   * <p>
   * <b>Invariant</b>: nothing happens if the specified key does not belong to <i>this</i> counter Bloom filter.
   * @param key The key to remove.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.DynamicBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),175,188,"/**
 * Checks if a key exists in the matrix.
 * @param key The key to search for. Returns true if null.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.Collection),156,164,"/**
 * Processes a collection of keys, recursively calling m1 for each.
 * @param coll Collection of keys to process; must not be null.
 */
","* Adds a collection of false positive information to <i>this</i> retouched Bloom filter.
   * @param coll The collection of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.List),170,178,"/**
 * Processes a list of keys, recursively calling m1 for each.
 * @param keys List of keys to process; must not be null.
 */
","* Adds a list of false positive information to <i>this</i> retouched Bloom filter.
   * @param keys The list of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key[]),184,192,"/**
 * Processes an array of keys, recursively calling m1 for each.
 * @param keys Array of keys to process; must not be null.
 */","* Adds an array of false positive information to <i>this</i> retouched Bloom filter.
   * @param keys The array of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,clearBit,org.apache.hadoop.util.bloom.RetouchedBloomFilter:clearBit(int),313,344,"/**
* Processes key and fp lists at the given index, updating data.
* @param index Index of the list to process.
*/","* Clears a specified bit in the bit vector and keeps up-to-date the KeyList vectors.
   * @param index The position of the bit to clear.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,ratioRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:ratioRemove(int[]),294,307,"/**
 * Finds the index of the minimum ratio value in the array.
 */","* Chooses the bit position that minimizes the number of false negative generated while maximizing.
   * the number of false positive removed.
   * @param h The different bit positions.
   * @return The position that minimizes the number of false negative generated while maximizing.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,driver,org.apache.hadoop.util.ProgramDriver:driver(java.lang.String[]),155,159,"/**
 * Calls m1 with argv, logs -1 if m1 returns -1.
 */","* API compatible with Hadoop 1.x.
   *
   * @param argv argv.
   * @throws Throwable Anything thrown
   *                   by the example program's main",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String),130,132,"/**
 * Sets the title for the element.
 * @param title The title to set.
 * @return This builder instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification)",134,136,"/**
 * Sets the title and justification. Overloads m1 with default flag.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,boolean)",138,140,"/**
 * Creates a builder with the given title and default justification.
 * @param title The title for the builder.
 * @param wrap Whether to wrap the text.
 * @return The builder instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getCredentialEntry,"org.apache.hadoop.conf.Configuration:getCredentialEntry(org.apache.hadoop.security.alias.CredentialProvider,java.lang.String)",2439,2469,"/**
* Retrieves a CredentialEntry, trying different names/keys.
* @param provider CredentialProvider to fetch from.
* @param name Name to search for. Returns entry or null.
*/","* Get the credential entry by name from a credential provider.
   *
   * Handle key deprecation.
   *
   * @param provider a credential provider
   * @param name alias of the credential
   * @return the credential entry or null if not found",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadResource,"org.apache.hadoop.conf.Configuration:loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)",3102,3147,"/**
 * Parses a resource, extracts properties, and returns a Resource object.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[],java.lang.String)",594,600,"/**
 * Logs a deprecation event with the given key, new keys, and message.
 */","* Adds the deprecated key to the global deprecation map.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If a key is deprecated in favor of multiple keys, they are all treated as 
   * aliases of each other, and setting any one of them resets all the others 
   * to the new value.
   *
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   * 
   * @param key to be deprecated
   * @param newKeys list of keys that take up the values of deprecated key
   * @param customMessage depcrication message
   * @deprecated use {@link #addDeprecation(String key, String newKey,
      String customMessage)} instead",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,parseNext,org.apache.hadoop.conf.Configuration$Parser:parseNext(),3448,3466,"/**
* Processes an XML event based on the reader's current event.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,openListeners,org.apache.hadoop.http.HttpServer2:openListeners(),1537,1552,"/**
 * Processes server listeners, assigning ports or handling them.
 */","* Open the main listener for the server
   * @throws Exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,checkArgs,org.apache.hadoop.ha.SshFenceByTcpPort:checkArgs(java.lang.String),73,78,"/**
 * Parses arguments from a string.
 * @param argStr Argument string to parse.
 * @throws BadFencingConfigurationException if parsing fails.
 */","* Verify that the argument, if given, in the conf is parseable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,parseOpts,"org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[])",505,507,"/**
* Calls m1 with default USAGE.
* @param cmdName Command name.
* @param opts Options object.
* @param argv Command line arguments.
* @return CommandLine object.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,clearParentZNode,org.apache.hadoop.ha.ActiveStandbyElector:clearParentZNode(),407,427,"/**
* Recursively deletes a znode from Zookeeper.
* Throws IOException if deletion fails.
*/","* Clear all of the state held within the parent ZNode.
   * This recursively deletes everything within the znode as well as the
   * parent znode itself. It should only be used when it's certain that
   * no electors are currently participating in the election.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,fenceOldActive,org.apache.hadoop.ha.ActiveStandbyElector:fenceOldActive(),1016,1047,"/**
 * Retrieves a Stat object and potentially fences an old node.
 * Returns null if no old node exists, otherwise returns stat.
 */","* If there is a breadcrumb node indicating that another node may need
   * fencing, try to fence that node.
   * @return the Stat of the breadcrumb node that was read, or null
   * if no breadcrumb node existed",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:createWithRetries(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode)",1082,1091,"/**
 * Creates a ZNode with given data, ACL, and mode.
 * @param path ZNode path
 * @param data ZNode data
 * @param acl Access control list
 * @param mode ZNode creation mode
 * @return ZNode path
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,getDataWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:getDataWithRetries(java.lang.String,boolean,org.apache.zookeeper.data.Stat)",1093,1101,"/**
 * Retrieves data from ZooKeeper.
 * @param path ZK path. @param watch Watch flag. @param stat Stat object.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setDataWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:setDataWithRetries(java.lang.String,byte[],int)",1103,1111,"/**
 * Executes a ZK action and returns a Stat object.
 * @param path ZK path. @param data Data to set. @param version Version.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,deleteWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:deleteWithRetries(java.lang.String,int)",1113,1122,"/**
 * Executes a ZooKeeper operation on the given path with a version.
 * @param path ZooKeeper path to operate on.
 * @param version Version for the operation.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readRangeFrom,"org.apache.hadoop.fs.VectoredReadUtils:readRangeFrom(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.util.function.IntFunction)",117,142,"/**
* Asynchronously reads data from a stream into a ByteBuffer.
* @param stream Input stream
* @param range File range to read
* @param allocate Allocates ByteBuffer
* @return CompletableFuture containing the ByteBuffer
*/","* Synchronously reads a range from the stream dealing with the combinations
   * of ByteBuffers buffers and PositionedReadable streams.
   * @param stream the stream to read from
   * @param range the range to read
   * @param allocate the function to allocate ByteBuffers
   * @return the CompletableFuture that contains the read data or an exception.
   * @throws IllegalArgumentException the range is invalid other than by offset or being null.
   * @throws EOFException the range offset is negative
   * @throws NullPointerException if the range is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,requestCaching,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestCaching(org.apache.hadoop.fs.impl.prefetch.BufferData),435,482,"/**
 * Processes buffer data, potentially caching it based on state.
 * @param data BufferData object to process.
 */","* Requests that the given block should be copied to the local cache.
   * The block must not be accessed by the caller after calling this method
   * because it will released asynchronously relative to the caller.
   *
   * @throws IllegalArgumentException if data is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setPrefetch,org.apache.hadoop.fs.impl.prefetch.BufferData:setPrefetch(java.util.concurrent.Future),181,186,"/**
 * Sets action future and updates state to PREFETCHING.
 * @param actionFuture Future object representing an action.
 */
","* Indicates that a prefetch operation is in progress.
   *
   * @param actionFuture the {@code Future} of a prefetch action.
   *
   * @throws IllegalArgumentException if actionFuture is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setReady,org.apache.hadoop.fs.impl.prefetch.BufferData:setReady(org.apache.hadoop.fs.impl.prefetch.BufferData$State[]),209,218,"/**
 * Updates internal state, calculates checksum, and transitions.
 * @param expectedCurrentState Expected current state array.
 */
","* Marks the completion of reading data into the buffer.
   * The buffer cannot be modified once in this state.
   *
   * @param expectedCurrentState the collection of states from which transition to READY is allowed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getSize,org.apache.hadoop.fs.impl.prefetch.BlockData:getSize(int),154,164,"/**
 * Calculates block size based on block number and file size.
 * @param blockNumber the block number
 * @return the size of the block
 */
","* Gets the size of the given block.
   * @param blockNumber the id of the desired block.
   * @return the size of the given block.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getRelativeOffset,"org.apache.hadoop.fs.impl.prefetch.BlockData:getRelativeOffset(int,long)",194,198,"/**
 * Calculates an offset value based on block number and offset.
 * @param blockNumber Block identifier.
 * @param offset Initial offset value.
 * @return Calculated offset as an integer.
 */
","* Gets the relative offset corresponding to the given block and the absolute offset.
   * @param blockNumber the id of the given block.
   * @param offset absolute offset in the file.
   * @return the relative offset corresponding to the given block and the absolute offset.
   * @throws IllegalArgumentException if either blockNumber or offset is invalid.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getStateString,org.apache.hadoop.fs.impl.prefetch.BlockData:getStateString(),225,241,"/**
* Generates a string describing consecutive blocks with same state.
* @return String containing the formatted block descriptions.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BlockData:<init>(long,int)",75,95,"/**
 * Initializes a BlockData object with file size and block size.
 * @param fileSize Size of the file in bytes.
 * @param blockSize Size of each block.
 */
","* Constructs an instance of {@link BlockData}.
   * @param fileSize the size of a file.
   * @param blockSize the file is divided into blocks of this size.
   * @throws IllegalArgumentException if fileSize is negative.
   * @throws IllegalArgumentException if blockSize is negative.
   * @throws IllegalArgumentException if blockSize is zero or negative.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,blockNumber,org.apache.hadoop.fs.impl.prefetch.FilePosition:blockNumber(),194,197,"/**
* Calls m1() and returns a value from blockData based on offset.
*/","* Gets the id of the current block.
   *
   * @return the id of the current block.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,run,org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer:run(),3834,3841,"/**
 * Closes all caches, handling potential IOExceptions and logging errors.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(),3790,3792,"/**
 * Calls m1 with 'false' as the argument.
 */","* Close all FileSystems in the cache, whether they are marked for
     * automatic closing or not.
     * @throws IOException a problem arose closing one or more FileSystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAllForUGI,org.apache.hadoop.fs.FileSystem:closeAllForUGI(org.apache.hadoop.security.UserGroupInformation),653,657,"/**
 * Closes resources and invalidates cache entries for the given UGI.
 * @param ugi UserGroupInformation object
 */","* Close all cached FileSystem instances for a given UGI.
   * Be sure those filesystems are not used anymore.
   * @param ugi user group info to close
   * @throws IOException a problem arose closing one or more filesystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,readFully,"org.apache.hadoop.fs.FSInputStream:readFully(long,byte[])",135,139,"/**
* Calls overloaded m1 with buffer length equal to buffer.length.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readFully,"org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[],int,int)",120,123,"/**
* Delegates read operation to the underlying FSInputStream.
* @param position File offset for reading.
* @param buffer Byte array to store read data.
* @param offset Offset within the buffer.
* @param length Number of bytes to read.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence)",2063,2066,"/**
 * Calls overloaded method with UTF-8 encoding.
 * @param fileContext FileContext object
 * @param path Path object
 * @param charseq CharSequence object
 * @return FileContext object
 */
","* Write a line of text to a file. Characters are encoded into bytes using
   * UTF-8. This utility method opens the file for writing, creating the file if
   * it does not exist, or overwrites an existing file.
   *
   * @param fileContext the files system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,createFile,org.apache.hadoop.fs.HarFileSystem:createFile(org.apache.hadoop.fs.Path),1305,1308,"/**
 * Delegates to the underlying FileSystem's m1 method.
 * @param path The path to create the output stream for.
 * @return An FSDataOutputStreamBuilder object.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[])",1851,1862,"/**
 * Writes byte array to a file in the given file system.
 * @param fs file system
 * @param path file path
 * @param bytes data to write
 * @return the file system
 */
","* Writes bytes to a file. This utility method opens the file for writing,
   * creating the file if it does not exist, or overwrites an existing file. All
   * bytes in the byte array are written to the file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param bytes the byte array with the bytes to write
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)",1910,1928,"/**
 * Writes lines to a file in a FileSystem, using the given charset.
 * @param fs FileSystem to write to
 * @param path Path to the file
 * @param lines Lines to write
 * @param cs Charset to use for encoding
 * @return FileSystem
 */","* Write lines of text to a file. Each line is a char sequence and is written
   * to the file in sequence with each line terminated by the platform's line
   * separator, as defined by the system property {@code
   * line.separator}. Characters are encoded into bytes using the specified
   * charset. This utility method opens the file for writing, creating the file
   * if it does not exist, or overwrites an existing file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param lines a Collection to iterate over the char sequences
   * @param cs the charset to use for encoding
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)",1983,1997,"/**
 * Writes a CharSequence to a file in a FileSystem, returns fs.
 * @param fs FileSystem to write to.
 * @param path Path to write the content.
 * @param charseq Content to write.
 * @param cs Charset for encoding.
 */
","* Write a line of text to a file. Characters are encoded into bytes using the
   * specified charset. This utility method opens the file for writing, creating
   * the file if it does not exist, or overwrites an existing file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   * @param cs the charset to use for encoding
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createFile,org.apache.hadoop.fs.FilterFileSystem:createFile(org.apache.hadoop.fs.Path),699,702,"/**
 * Delegates to the underlying FSDataOutputStreamBuilder.
 * @param path The path for the output stream.
 * @return FSDataOutputStreamBuilder instance.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,appendFile,org.apache.hadoop.fs.HarFileSystem:appendFile(org.apache.hadoop.fs.Path),1310,1313,"/**
* Delegates to the underlying FSDataOutputStreamBuilder for the path.
* @param path Path to create the output stream for.
* @return FSDataOutputStreamBuilder
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,appendFile,org.apache.hadoop.fs.FilterFileSystem:appendFile(org.apache.hadoop.fs.Path),704,707,"/**
* Delegates to the underlying FSDataOutputStreamBuilder.
* @param path Path for the output stream.
* @return FSDataOutputStreamBuilder instance.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)",150,153,"/**
 * Constructs a BlockLocation with null topology.
 * @param names Block names. @param hosts Hostnames.
 * @param offset Offset. @param length Length. @param corrupt Corrupt flag.
 */
","* Constructor with host, name, network topology, offset, length 
   * and corrupt flag.
   * @param names names.
   * @param hosts hosts.
   * @param topologyPaths topologyPaths.
   * @param offset offset.
   * @param length length.
   * @param corrupt corrupt.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults(),60,64,"/**
* Returns the FsServerDefaults from FtpConfigKeys.
* @return FsServerDefaults object
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults(org.apache.hadoop.fs.Path),66,69,"/**
* Returns FsServerDefaults using FtpConfigKeys.m1().
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,getServerDefaults,org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults(org.apache.hadoop.fs.Path),66,70,"/**
* Returns FsServerDefaults from LocalConfigKeys.
* No parameters, returns FsServerDefaults object.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,getServerDefaults,org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults(),72,76,"/**
* Returns the FsServerDefaults from LocalConfigKeys.
* @return FsServerDefaults object
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults(),1135,1139,"/**
* Returns the FsServerDefaults from LocalConfigKeys.
* @return FsServerDefaults object
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path),1141,1144,"/**
* Returns the FsServerDefaults from LocalConfigKeys.
* No parameters, returns FsServerDefaults object.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(),292,296,"/**
* Returns the FsServerDefaults from LocalConfigKeys.
* @return FsServerDefaults object
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(org.apache.hadoop.fs.Path),298,307,"/**
 * Resolves FsServerDefaults based on a Path.
 * @param f Path to resolve; returns FsServerDefaults.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,int)",93,95,"/**
 * Creates a MutableCounterInt with an interned name and description.
 * @param name Counter name.
 * @param desc Counter description.
 * @param iVal Initial counter value.
 * @return MutableCounterInt object.
 */
","* Create a mutable integer counter
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new counter object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,long)",117,119,"/**
* Creates a MutableCounterLong with an interned name and description.
* @param name Counter name.
* @param desc Counter description.
* @param iVal Initial counter value.
* @return MutableCounterLong object.
*/
","* Create a mutable long integer counter
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new counter object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,long)",166,168,"/**
* Creates a MutableGaugeLong with interned name/desc.
* @param name Gauge name.
* @param desc Gauge description.
* @param iVal Initial gauge value.
* @return MutableGaugeLong object.
*/
","* Create a mutable long integer gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,float)",190,192,"/**
* Creates a MutableGaugeFloat with interned name/desc.
* @param name Gauge name
* @param desc Gauge description
* @param iVal Initial float value
* @return MutableGaugeFloat instance
*/
","* Create a mutable float gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,int)",142,144,"/**
 * Creates a MutableGaugeInt with interned name and description.
 * @param name Gauge name.
 * @param desc Gauge description.
 * @param iVal Initial integer value.
 * @return MutableGaugeInt object.
 */
","* Create a mutable integer gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,checkCalls,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:checkCalls(),126,143,"/**
* Calculates the minimum wait time for async calls in the queue.
* Returns the minimum wait time found or Processor.MAX_WAIT_PERIOD.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedStringArray,"org.apache.hadoop.io.WritableUtils:writeCompressedStringArray(java.io.DataOutput,java.lang.String[])",149,158,"/**
 * Writes string array to output. Writes -1 if array is null.
 * @param out DataOutput to write to.
 * @param s String array to write.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,copy,org.apache.hadoop.fs.statistics.MeanStatistic:copy(),281,283,"/**
 * Creates a new MeanStatistic instance, using the current object.
 */","* Create a copy of this instance.
   * @return copy.
   *",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString:toString(),297,302,"/**
* Returns a value based on 'source', or a null source indicator.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtDebug,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(org.slf4j.Logger,java.lang.String,java.lang.Object)",227,238,"/**
* Logs a message with associated source stats if logging is enabled.
* @param log Logger instance
* @param message Message to log
* @param source Source object for stats
*/
","* Extract any statistics from the source and log at debug, if
   * the log is set to log at debug.
   * No-op if logging is not at debug or the source is null/of
   * the wrong type/doesn't provide statistics.
   * @param log log to log to
   * @param message message for log -this must contain ""{}"" for the
   * statistics report to actually get logged.
   * @param source source object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,toString,org.apache.hadoop.fs.FSInputStream:toString(),148,158,"/**
 * Extends the parent's m1() by adding IO statistics logging.
 */","* toString method returns the superclass toString, but if the subclass
   * implements {@link IOStatisticsSource} then those statistics are
   * extracted and included in the output.
   * That is: statistics of subclasses are automatically reported.
   * @return a string value.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_toPrettyString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_toPrettyString(java.lang.Object),324,328,"/**
 * Returns a string representation of statistics or """" if null.
 * @param statistics The statistics object to format.
 * @return A string representation of the statistics.
 */
","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * @param statistics A statistics instance; may be null
   * @return string value or the empty string if null",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,measureDurationOfInvocation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:measureDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)",484,507,"/**
 * Measures duration of input.m4() using tracker.
 * @param factory DurationTrackerFactory
 * @param statistic Statistic name
 * @param input Input object
 * @return Duration object
 */
","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic,
   * returning the measured duration.
   *
   * {@link #trackDurationOfInvocation(DurationTrackerFactory, String, InvocationRaisingIOE)}
   * with the duration returned for logging etc.; added as a new
   * method to avoid linking problems with any code calling the existing
   * method.
   *
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @return the duration of the operation, as measured by the duration tracker.
   * @throws IOException IO failure.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDurationOfSupplier,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfSupplier(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.function.Supplier)",642,663,"/**
 * Executes a supplier, tracks duration, and propagates exceptions.
 * @param factory DurationTrackerFactory, input Supplier<B>
 * @return Result of the supplier.
 */
","* Given a Java supplier, evaluate it while
   * tracking the duration of the operation and success/failure.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @param <B> return type.
   * @return the output of the supplier.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,addToLinkedListAndEvictIfRequired,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListAndEvictIfRequired(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),421,439,"/**
 * Processes an entry, potentially purging old entries.
 * @param entry The entry to process.
 */","* Add the given entry to the head of the linked list and if the LRU cache size
   * exceeds the max limit, evict tail of the LRU linked list.
   *
   * @param entry Block entry to add.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,<init>,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:<init>(java.util.List,java.util.List,java.util.List,java.util.List,java.util.List)",92,140,"/**
 * Initializes statistics store with counters, gauges, mins, maxes, means.
 * @param counters List of counter keys
 * @param gauges List of gauge keys
 * @param minimums List of minimum keys
 * @param maximums List of maximum keys
 * @param meanStatistics List of mean statistic keys
 */
","* Constructor invoked via the builder.
   * @param counters keys to use for the counter statistics.
   * @param gauges names of gauges
   * @param minimums names of minimums
   * @param maximums names of maximums
   * @param meanStatistics names of mean statistics.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(byte[],int,int)",162,220,"/**
 * Reads bytes from input to output buffer, handling validation & decryption.
 * @param b buffer to write to, @param off offset, @param len length
 * @return number of bytes read
 */","* Decryption is buffer based.
   * If there is data in {@link #outBuffer}, then read it out of this buffer.
   * If there is no data in {@link #outBuffer}, then read more from the 
   * underlying stream and do the decryption.
   * @param b the buffer into which the decrypted data is read.
   * @param off the buffer offset.
   * @param len the maximum number of decrypted data bytes to read.
   * @return int the total number of decrypted data bytes read into the buffer.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,byte[],int,int)",395,425,"/**
* Decrypts data into a buffer using a decryptor and IV.
* @param position Decryption start position.
*/","* Decrypt length bytes in buffer starting at offset. Output is also put 
   * into buffer starting at offset. It is thread-safe.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,java.nio.ByteBuffer,int,int)",456,499,"/**
 * Decrypts data from a ByteBuffer, using a Decryptor.
 * @param filePosition File position for decryption.
 */","* Decrypts the given {@link ByteBuffer} in place. {@code length} bytes are
   * decrypted from {@code buf} starting at {@code start}.
   * {@code buf.position()} and {@code buf.limit()} are unchanged after this
   * method returns. This method is thread-safe.
   *
   * <p>
   *   This method decrypts the input buf chunk-by-chunk and writes the
   *   decrypted output back into the input buf. It uses two local buffers
   *   taken from the {@link #bufferPool} to assist in this process: one is
   *   designated as the input buffer and it stores a single chunk of the
   *   given buf, the other is designated as the output buffer, which stores
   *   the output of decrypting the input buffer. Both buffers are of size
   *   {@link #bufferSize}.
   * </p>
   *
   * <p>
   *   Decryption is done by using a {@link Decryptor} and the
   *   {@link #decrypt(Decryptor, ByteBuffer, ByteBuffer, byte)} method. Once
   *   the decrypted data is written into the output buffer, is is copied back
   *   into buf. Both buffers are returned back into the pool once the entire
   *   buf is decrypted.
   * </p>
   *
   * @param filePosition the current position of the file being read
   * @param buf the {@link ByteBuffer} to decrypt
   * @param length the number of bytes in {@code buf} to decrypt
   * @param start the position in {@code buf} to start decrypting data from",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(java.nio.ByteBuffer,int,int)",649,670,"/**
* Decrypts data from a buffer, handling padding and updating state.
*/","* Decrypts the given {@link ByteBuffer} in place. {@code length} bytes are
   * decrypted from {@code buf} starting at {@code start}.
   * {@code buf.position()} and {@code buf.limit()} are unchanged after this
   * method returns.
   *
   * @see #decrypt(long, ByteBuffer, int, int)",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)",124,140,"/**
* Constructs a CryptoInputStream with given input, codec, buffer size, key, IV, and offset.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,seek,org.apache.hadoop.crypto.CryptoInputStream:seek(long),523,546,"/**
* Seeks to the specified position in the stream.
* @param pos The position to seek to.
*/",Seek to a position.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,skip,org.apache.hadoop.crypto.CryptoInputStream:skip(long),549,577,"/**
* Skips a specified number of bytes from the input stream.
* @param n number of bytes to skip, must be non-negative
* @return number of bytes actually skipped
*/",Skip n bytes,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,seekToNewSource,org.apache.hadoop.crypto.CryptoInputStream:seekToNewSource(long),693,705,"/**
 * Seeks to the specified position in the input stream.
 * @param targetPos The target position to seek to.
 * @return True if seek was successful, false otherwise.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,write,org.apache.hadoop.crypto.CryptoOutputStream:write(int),271,275,"/**
 * Writes a single byte to the buffer and calls m1.
 * @param b The byte to write.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,close,org.apache.hadoop.crypto.CryptoOutputStream:close(),238,256,"/**
 * Executes m1, calls m2, and closes resources if needed.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hflush,org.apache.hadoop.crypto.CryptoOutputStream:hflush(),294,300,"/**
 * Calls m1 and delegates m2 to Syncable out if applicable.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hsync,org.apache.hadoop.crypto.CryptoOutputStream:hsync(),302,308,"/**
* Calls m1 and delegates m2 to out if it's Syncable.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/GlobFilter.java,compile,org.apache.hadoop.metrics2.filter.GlobFilter:compile(java.lang.String),36,39,"/**
 * Delegates pattern creation to GlobPattern.
 * @param s Input string to create a pattern from.
 * @return A Pattern object.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,<init>,org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String),49,51,"/**
 * Constructs a GlobFilter with the given file pattern.
 * @param filePattern The glob pattern for file filtering.
 */
","* Creates a glob filter with the specified file pattern.
   *
   * @param filePattern the file pattern.
   * @throws IOException thrown if the file pattern is incorrect.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,<init>,"org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String,org.apache.hadoop.fs.PathFilter)",60,62,"/**
 * Initializes a GlobFilter with a file pattern and PathFilter.
 * @param filePattern Pattern for matching files.
 * @param filter PathFilter to apply.
 */
","* Creates a glob filter with the specified file pattern and an user filter.
   *
   * @param filePattern the file pattern.
   * @param filter user filter in addition to the glob pattern.
   * @throws IOException thrown if the file pattern is incorrect.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTar,"org.apache.hadoop.fs.FileUtil:unTar(java.io.InputStream,java.io.File,boolean)",985,1003,"/**
 * Extracts files from an InputStream to a directory.
 * @param inputStream Input stream containing archive data.
 * @param untarDir Directory to extract files to.
 * @param gzipped Whether the archive is gzipped.
 */
","* Given a Tar File as input it will untar the file in a the untar directory
   * passed as the second parameter
   *
   * This utility will untar "".tar"" files and "".tar.gz"",""tgz"" files.
   *
   * @param inputStream The tar file as input.
   * @param untarDir The untar directory where to untar the tar file.
   * @param gzipped The input stream is gzipped
   *                TODO Use magic number and PusbackInputStream to identify
   * @throws IOException an exception occurred
   * @throws InterruptedException command interrupted
   * @throws ExecutionException task submit failed",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAllStatistics,org.apache.hadoop.fs.FileContext:getAllStatistics(),2420,2422,"/**
* Delegates to AbstractFileSystem.m1() to retrieve statistics.
* Returns a map of URI to Statistics objects.
*/","* @return Map of uri and statistics for each filesystem instantiated. The uri
   *         consists of scheme and authority for the filesystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,clearStatistics,org.apache.hadoop.fs.FileContext:clearStatistics(),2404,2406,"/**
* Calls the m1 method of the AbstractFileSystem class.
*/","* Clears all the statistics stored in AbstractFileSystem, for all the file
   * systems.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,primitiveCreate,"org.apache.hadoop.fs.FilterFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",551,559,"/**
 * Delegates to the underlying FileSystem's create method.
 * @param f Path to create the output stream on.
 * @return FSDataOutputStream
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.fs.viewfs.InodeTree,org.apache.hadoop.conf.Configuration)",977,988,"/**
 * Constructs a new InternalDirOfViewFs.
 * @param dir Directory inode; others are ViewFS config.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,<init>,org.apache.hadoop.fs.FilterFs:<init>(org.apache.hadoop.fs.AbstractFileSystem),63,66,"/**
 * Constructs a FilterFs with the given AbstractFileSystem.
 * @param fs The AbstractFileSystem to filter.
 * @throws URISyntaxException if the URI is malformed.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,getPiggyBacksFromInput,"org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBacksFromInput(java.nio.ByteBuffer[],int[],int,int,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",64,120,"/**
 * Generates parity units based on input data and encoder.
 * @param inputs Input ByteBuffers
 * @return Array of parity ByteBuffers
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",146,150,"/**
* Processes ECChunk arrays, converting to ByteBuffers and calling m2.
* @param inputs ECChunk input array
* @param outputs ECChunk output array
*/
","* Encode with inputs and generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param outputs output buffers to put the encoded data into, read to read
   *                after the call
   * @throws IOException if the encoder is closed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>(java.lang.Object),163,165,"/**
 * Internal constructor, used for writing values.
 * @param value The value to be set.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,acquireHelper,"org.apache.hadoop.fs.impl.prefetch.BufferPool:acquireHelper(int,boolean)",161,187,"/**
* Retrieves or creates BufferData for a block.
* @param blockNumber Block identifier.
* @param canBlock Whether to block if buffer is unavailable.
* @return BufferData object or null if creation fails.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.BufferPool:numAvailable(),300,303,"/**
* Calls m1() and then returns the result of pool.m2().
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,newInstance,"org.apache.hadoop.util.BlockingThreadPoolExecutorService:newInstance(int,int,long,java.util.concurrent.TimeUnit,java.lang.String)",122,148,"/**
 * Creates a BlockingThreadPoolExecutorService with specified parameters.
 * @param activeTasks, waitingTasks, keepAliveTime, unit, prefixName
 * @return BlockingThreadPoolExecutorService instance
 */","* A thread pool that that blocks clients submitting additional tasks if
   * there are already {@code activeTasks} running threads and {@code
   * waitingTasks} tasks waiting in its queue.
   *
   * @param activeTasks maximum number of active tasks
   * @param waitingTasks maximum number of waiting tasks
   * @param keepAliveTime time until threads are cleaned up in {@code unit}
   * @param unit time unit
   * @param prefixName prefix of name for threads
   * @return BlockingThreadPoolExecutorService.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,setData,"org.apache.hadoop.fs.impl.prefetch.FilePosition:setData(org.apache.hadoop.fs.impl.prefetch.BufferData,long,long)",109,128,"/**
 * Processes buffer data, offsets, and calls helper methods.
 * @param bufferData Data buffer to process.
 * @param startOffset Start offset for processing.
 * @param readOffset Read offset for processing.
 */","* Associates a buffer with this file.
   *
   * @param bufferData the buffer associated with this file.
   * @param startOffset Start offset of the buffer relative to the start of a file.
   * @param readOffset Offset where reading starts relative to the start of a file.
   *
   * @throws IllegalArgumentException if bufferData is null.
   * @throws IllegalArgumentException if startOffset is negative.
   * @throws IllegalArgumentException if readOffset is negative.
   * @throws IllegalArgumentException if readOffset is outside the range [startOffset, buffer end].",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_getCurrent,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_getCurrent(),261,263,"/**
* Delegates execution to the m1() method.
*/","* Get the context's {@link IOStatisticsContext} which
   * implements {@link IOStatisticsSource}.
   * This is either a thread-local value or a global empty context.
   * @return instance of {@link IOStatisticsContext}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_reset,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_reset(),287,289,"/**
* Calls m2() on the result of m1().
*/","* Reset the context's IOStatistics.
   * {@link IOStatisticsContext#reset()}",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_snapshot,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_snapshot(),296,298,"/**
* Returns the result of chaining m1() and m2() calls.
*/","* Take a snapshot of the context IOStatistics.
   * {@link IOStatisticsContext#snapshot()}
   * @return an instance of {@link IOStatisticsSnapshot}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_aggregate,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_aggregate(java.lang.Object),308,316,"/**
* Processes a source object, returning true if successful, false otherwise.
*/","* Aggregate into the IOStatistics context the statistics passed in via
   * IOStatistics/source parameter.
   * <p>
   * Returns false if the source is null or does not contain any statistics.
   * @param source implementation of {@link IOStatisticsSource} or {@link IOStatistics}
   * @return true if the the source object was aggregated.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,runParallel,org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task),385,519,"/**
 * Executes tasks on items, handles failures, aborts, and reverts.
 * @param task The task to execute.
 * @return True if all tasks succeeded, false otherwise.
 */","* Parallel execution.
     * All tasks run within the same IOStatisticsContext as the
     * thread calling this method.
     * @param task task to execute
     * @param <E> exception which may be raised in execution.
     * @return true if the operation executed successfully
     * @throws E any exception raised.
     * @throws IOException IOExceptions raised by remote iterator or in execution.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_setThreadIOStatisticsContext,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object),270,273,"/**
 * Delegates processing to m1, casting statisticsContext to IOStatisticsContext.
 */","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,setStatisticsContext,org.apache.hadoop.util.functional.TaskPool$Builder:setStatisticsContext(),524,528,"/**
* Updates IO statistics using the provided context.
* @param ioStatisticsContext Context for IO statistics.
*/",* Set the statistics context for this thread.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,resetStatisticsContext,org.apache.hadoop.util.functional.TaskPool$Builder:resetStatisticsContext(),535,539,"/**
* Calls m1 on IOStatisticsContext if it exists.
*/","* Reset the statistics context if it was set earlier.
     * This unbinds the current thread from any statistics
     * context.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processPath,org.apache.hadoop.fs.shell.find.Find:processPath(org.apache.hadoop.fs.shell.PathData),394,401,"/**
* Processes a PathData item, skipping if m1().m2() is true.
* @param item The PathData object to process.
* @throws IOException if an I/O error occurs during processing.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,postProcessPath,org.apache.hadoop.fs.shell.find.Find:postProcessPath(org.apache.hadoop.fs.shell.PathData),403,410,"/**
* Processes a PathData item if a condition is met.
* @param item The PathData object to process.
* @throws IOException if an I/O error occurs during processing.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rmr:processOptions(java.util.LinkedList),174,178,"/**
* Calls m1 on args, then calls super.m2(args).
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processOptions,org.apache.hadoop.fs.shell.Ls$Lsr:processOptions(java.util.LinkedList),411,416,"/**
* Calls super.m2 with modified args: adds ""-R"" to the list.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Dus:processOptions(java.util.LinkedList),236,240,"/**
* Calls m1 on args and then calls super.m2(args).
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayError,org.apache.hadoop.fs.shell.Command:displayError(java.lang.Exception),474,493,"/**
 * Handles an exception, logs failure, and re-throws a message.
 * @param e The exception to handle.
 */","* Display an exception prefaced with the command name.  Also increments
   * the error count for the command which will result in a non-zero exit
   * code.
   * @param e exception to display",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getPathHandle,"org.apache.hadoop.fs.FileSystem:getPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",1050,1057,"/**
 * Creates a PathHandle. Uses default options if none are provided.
 * @param stat FileStatus object
 * @param opt HandleOpt array, options for PathHandle creation
 */
","* Create a durable, serializable handle to the referent of the given
   * entity.
   * @param stat Referent in the target FileSystem
   * @param opt If absent, assume {@link HandleOpt#path()}.
   * @throws IllegalArgumentException If the FileStatus does not belong to
   *         this FileSystem
   * @throws UnsupportedOperationException If {@link #createPathHandle}
   *         not overridden by subclass.
   * @throws UnsupportedOperationException If this FileSystem cannot enforce
   *         the specified constraints.
   * @return path handle.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long)",1227,1230,"/**
 * Constructs a ShellCommandExecutor with default logEnabled.
 * @param execString Command to execute. @param dir Working directory.
 * @param env Environment variables. @param timeout Timeout in milliseconds.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,init,org.apache.hadoop.fs.CachingGetSpaceUsed:init(),90,102,"/**
 * Handles a conditional refresh based on used.m1() value.
 * Sets used.m2(0) and calls m3/m4 based on shouldFirstRefresh.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addToken,"org.apache.hadoop.security.Credentials:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)",121,137,"/**
* Adds or updates tokens in the tokenMap based on alias and token.
*/","* Add a token in the storage (in memory).
   * @param alias the alias for the key
   * @param t the token object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,encrypt,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",148,152,"/**
* Copies data from `inBuffer` to `outBuffer`.
* @param inBuffer Input buffer containing data.
* @param outBuffer Output buffer to receive data.
*/
","* AES-CTR will consume all of the input data. It requires enough space in
     * the destination buffer to encrypt entire input buffer.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,decrypt,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",158,162,"/**
* Delegates processing to m1, copying data between buffers.
*/","*  AES-CTR will consume all of the input data. It requires enough space in
     * the destination buffer to decrypt entire input buffer.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,initialize,org.apache.hadoop.fs.viewfs.RegexMountPoint:initialize(),86,96,"/**
 * Initializes resources and prepares for processing, throws IOException on failure.
 */","* Initialize regex mount point.
   *
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",149,164,"/**
 * Constructs a Path object by resolving a child path relative to a parent.
 */","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,org.apache.hadoop.fs.Path:<init>(java.lang.String),184,223,"/**
 * Creates a Path from a string representation.
 * @param pathString The path string to be parsed.
 * @throws IllegalArgumentException if the path string is invalid.
 */
","* Construct a path from a String.  Path strings are URIs, but with
   * unescaped elements and some additional normalization.
   *
   * @param pathString the path string",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String,java.lang.String)",241,256,"/**
 * Constructs a Path with the given scheme, authority, and path.
 * @param scheme scheme of the path (e.g., ""http"")
 * @param authority authority of the path (e.g., ""example.com"")
 * @param path path component of the URI
 */
","* Construct a Path from components.
   *
   * @param scheme the scheme
   * @param authority the authority
   * @param path the path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,toString,org.apache.hadoop.fs.shell.PathData:toString(),461,464,"/**
* Calls m1 with the URI and inferred scheme.
* @return String result of calling m1.
*/
","* Returns the printable version of the path that is either the path
   * as given on the commandline, or the full path
   * @return String of the path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,checkNotRelative,org.apache.hadoop.fs.Path:checkNotRelative(),92,96,"/**
 * Throws exception if m1() is true or m2().m3() is null.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getUriPath,org.apache.hadoop.fs.AbstractFileSystem:getUriPath(org.apache.hadoop.fs.Path),423,431,"/**
 * Extracts and validates a path segment from a Path object.
 * @param p The Path object to extract from.
 * @return The validated path segment.
 */
","* Get the path-part of a pathname. Checks that URI matches this file system
   * and that the path-part is a valid name.
   * 
   * @param p path
   * 
   * @return path-part of the Path p",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,resolvePath,org.apache.hadoop.fs.AbstractFileSystem:resolvePath(org.apache.hadoop.fs.Path),506,510,"/**
* Processes a path, calls m1, then returns the result of m2(p).m3().
*/","* Return the fully-qualified path of path f resolving the path
   * through any internal symlinks or mount point
   * @param p path to be resolved
   * @return fully qualified path 
   * @throws FileNotFoundException when file not find throw.
   * @throws AccessControlException when accees control error throw.
   * @throws IOException raised on errors performing I/O.
   * @throws UnresolvedLinkException if symbolic link on path cannot be
   * resolved internally",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,create,"org.apache.hadoop.fs.AbstractFileSystem:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",530,640,"/**
 * Creates a FSDataOutputStream with specified options.
 * @param f the path
 * @param createFlag create flags
 * @param opts create options
 * @return FSDataOutputStream
 */","* The specification of this method matches that of
   * {@link FileContext#create(Path, EnumSet, Options.CreateOpts...)} except
   * that the Path f must be fully qualified and the permission is absolute
   * (i.e. umask has been applied).
   *
   * @param f the path.
   * @param createFlag create_flag.
   * @param opts create ops.
   * @throws AccessControlException access controll exception.
   * @throws FileAlreadyExistsException file already exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not dir exception.
   * @throws UnsupportedFileSystemException unsupported file system exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.
   * @return output stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,checkPath,org.apache.hadoop.fs.FilterFs:checkPath(org.apache.hadoop.fs.Path),184,187,"/**
* Delegates the Path processing to the underlying FileSystem.
* @param path The Path to be processed.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path),1505,1510,"/**
 * Calls m1 with the provided Path and default 'copy' flag.
 * @param f the Path to process
 * @return true if successful, false otherwise
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,applyUMask,"org.apache.hadoop.fs.permission.FsCreateModes:applyUMask(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",43,49,"/**
 * Combines a permission mode with a umask.
 * @param mode base permission mode
 * @param umask umask to apply
 * @return Combined FsPermission object
 */
","* Create from unmasked mode and umask.
   *
   * @param mode mode.
   * @param umask umask.
   * @return If the mode is already
   * an FsCreateModes object, return it.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfoByNativeIO,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNativeIO(),1063,1085,"/**
* Retrieves and sets file ownership details.
* Throws IOException if an error occurs.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,mkdirs,org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path),2495,2497,"/**
 * Sets access permissions for a file.
 * @param f The Path object representing the file.
 * @throws IOException if an I/O error occurs.
 */
","* Call {@link #mkdirs(Path, FsPermission)} with default permission.
   * @param f path
   * @return true if the directory was created
   * @throws IOException IO failure",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set)",160,190,"/**
 * Constructs a FileStatus object with the given file attributes.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,setPermission,org.apache.hadoop.fs.FileStatus:setPermission(org.apache.hadoop.fs.permission.FsPermission),367,370,"/**
* Sets the permission. Uses default if permission is null.
*/","* Sets permission.
   * @param permission if permission is null, default value is set",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,getPermission,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getPermission(),111,116,"/**
 * Returns the permission mask. Lazily initializes if null.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1432,1438,"/**
* Creates a data output stream, using default file permissions.
* @param f Path to the output file
* @return FSDataOutputStream
* @throws IOException if an I/O error occurs
*/
","* Opens an FSDataOutputStream at the indicated Path with write-progress
   * reporting. Same as create(), except fails if parent directory doesn't
   * already exist.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   * the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getPermission,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getPermission(),146,151,"/**
* Returns the permission mask. Lazily initializes if null.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,createImmutable,org.apache.hadoop.fs.permission.FsPermission:createImmutable(short),64,66,"/**
 * Creates an FsPermission object from a short permission value.
 * @param permission short representing the permission value
 * @return An ImmutableFsPermission object.
 */
","* Create an immutable {@link FsPermission} object.
   * @param permission permission.
   * @return FsPermission.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processPath,org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData),77,104,"/**
 * Writes file metadata (owner, group, permissions, ACLs) to output.
 * @param item PathData object containing file metadata.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,createPermissions,org.apache.hadoop.security.alias.KeyStoreProvider:createPermissions(java.lang.String),68,71,"/**
* Sets the permissions using the provided string representation.
* @param perms String representing the file permissions.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$Writer:append(java.lang.Object,java.lang.Object)",1469,1502,"/**
 * Writes a key-value pair to the output stream, validating types.
 */","* Append a key/value pair.
     * @param key input Object key.
     * @param val input Object val.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendRaw,"org.apache.hadoop.io.SequenceFile$Writer:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)",1504,1517,"/**
 * Writes key and value data to the output stream.
 * @param keyData Key data.
 * @param val Value data to be written.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:getCompressedSize(),242,244,"/**
 * Delegates to wBlkState's m1() method.
 * @return Long value returned by wBlkState.m1()
 * @throws IOException if wBlkState.m1() throws IOException
 */
","* Get the compressed size of the block in progress.
       * 
       * @return the number of compressed bytes written to the underlying FS
       *         file. The size may be smaller than actual need to compress the
       *         all data written due to internal buffering inside the
       *         compressor.
       * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,skip,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:skip(long),528,536,"/**
 * Reads 'n' bytes from the input stream, adjusting 'n' if needed.
 */","* Skips over and discards <code>n</code> bytes of data from the
     * input stream.
     *
     *The <code>skip</code> method skips over some smaller number of bytes
     * when reaching end of file before <code>n</code> bytes have been skipped.
     * The actual number of bytes skipped is returned.  If <code>n</code> is
     * negative, no bytes are skipped.
     *
     * @param      n   the number of bytes to be skipped.
     * @return     the actual number of bytes skipped.
     * @exception  IOException  if an I/O error occurs.
     *             ChecksumException if the chunk to skip to is corrupted",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,seek,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:seek(long),550,556,"/**
* Seeks to a specific position.
* Throws EOFException if pos is beyond the current file length.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPaths,"org.apache.hadoop.fs.shell.Ls:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])",270,283,"/**
 * Processes path data, logs item count, and calls other methods.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getUsed,org.apache.hadoop.fs.HarFileSystem:getUsed(org.apache.hadoop.fs.Path),1277,1280,"/**
* Delegates m1 call to the underlying filesystem.
* @param path Path to operate on.
* @return Long value returned by the filesystem's m1.
*/",Return the total size of all files from a specified path.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getUsed,org.apache.hadoop.fs.FilterFileSystem:getUsed(org.apache.hadoop.fs.Path),421,424,"/**
* Delegates m1 operation to the underlying filesystem.
* @param path The path to operate on.
* @return Long value returned by the filesystem's m1.
*/",Return the total size of all files from a specified path.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,main,org.apache.hadoop.util.JvmPauseMonitor:main(java.lang.String[]),221,231,"/**
 * Starts JVM pause monitoring and continuously adds strings to a list.
 */","* Simple 'main' to facilitate manual testing of the pause monitor.
   * 
   * This main function just leaks memory into a list. Running this class
   * with a 1GB heap will very quickly go into ""GC hell"" and result in
   * log messages about the GC pauses.
   *
   * @param args args.
   * @throws Exception Exception.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,start,org.apache.hadoop.service.AbstractService:start(),185,208,"/**
* Attempts to start the service, handling exceptions and logging.
*/","* {@inheritDoc}
   * @throws ServiceStateException if the current service state does not permit
   * this action",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,enterState,org.apache.hadoop.service.AbstractService:enterState(org.apache.hadoop.service.Service$STATE),440,449,"/**
 * Transitions to a new state, logs the change, and triggers actions.
 * @param newState The target state to transition to.
 * @return The actual state after the transition.
 */
","* Enter a state; record this via {@link #recordLifecycleEvent}
   * and log at the info level.
   * @param newState the proposed new state
   * @return the original state
   * it wasn't already in that state, and the state model permits state re-entrancy.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printDefaultRealm,org.apache.hadoop.security.KDiag:printDefaultRealm(),488,512,"/**
 * Retrieves and logs the default Kerberos realm. Handles exceptions.
 */","* Get the default realm.
   * <p>
   * Not having a default realm may be harmless, so is noted at info.
   * All other invocation failures are downgraded to warn, as
   * follow-on actions may still work.
   * Failure to invoke the method via introspection is considered a failure,
   * as it's a sign of JVM compatibility issues that may have other 
   * consequences",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,equals,org.apache.hadoop.io.BytesWritable:equals(java.lang.Object),200,205,"/**
 * Compares with another object, delegates to super if BytesWritable.
 */",* Are the two byte sequences equal?,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,equals,org.apache.hadoop.io.Text:equals(java.lang.Object),415,420,"/**
 * Checks if object is a Text; if so, calls super.m1(o).
 */","* Returns true iff <code>o</code> is a Text with the same length and same
   * contents.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,hashCode,org.apache.hadoop.security.token.Token$PrivateToken:hashCode(),299,304,"/**
 * Calculates a combined result based on superclass and service.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,set,org.apache.hadoop.io.BytesWritable:set(org.apache.hadoop.io.BytesWritable),167,169,"/**
 * Calls overloaded method with bytes, offset, and size from BytesWritable.
 */","* Set the BytesWritable to the contents of the given newData.
   *
   * @param newData the value to set this BytesWritable to.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirInternal,org.apache.hadoop.util.DiskChecker:checkDirInternal(java.io.File),94,101,"/**
 * Creates a directory and performs post-creation tasks.
 * @param dir The directory to create.
 * @throws DiskErrorException If directory creation fails.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,mlock,"org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:mlock(java.lang.String,java.nio.ByteBuffer,long)",280,283,"/**
 * Calls POSIX m1 with the provided buffer and length.
 * @param identifier Identifier (unused).
 * @param buffer ByteBuffer to be processed.
 * @param len Length of data in the buffer.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flushBuffer,"org.apache.hadoop.fs.FSOutputSummer:flushBuffer(boolean,boolean)",159,176,"/**
* Flushes a portion of the buffer, updating the count accordingly.
* @param keep Whether to keep partial data; flushPartial: flush partial data.
* @return The number of bytes flushed.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,close,org.apache.hadoop.crypto.CryptoInputStream:close(),319,329,"/**
 * Executes m1, m2, codec.m1, then sets closed to true.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",85,116,"/**
 * Decodes data from input buffers, handles erasure, and updates positions.
 */","* Decode with inputs and erasedIndexes, generates outputs.
   * How to prepare for inputs:
   * 1. Create an array containing data units + parity units. Please note the
   *    data units should be first or before the parity units.
   * 2. Set null in the array locations specified via erasedIndexes to indicate
   *    they're erased and no data are to read from;
   * 3. Set null in the array locations for extra redundant items, as they're
   *    not necessary to read when decoding. For example in RS-6-3, if only 1
   *    unit is really erased, then we have 2 extra items as redundant. They can
   *    be set as null to indicate no data will be used from them.
   *
   * For an example using RS (6, 3), assuming sources (d0, d1, d2, d3, d4, d5)
   * and parities (p0, p1, p2), d2 being erased. We can and may want to use only
   * 6 units like (d1, d3, d4, d5, p0, p2) to recover d2. We will have:
   *     inputs = [null(d0), d1, null(d2), d3, d4, d5, p0, null(p1), p2]
   *     erasedIndexes = [2] // index of d2 into inputs array
   *     outputs = [a-writable-buffer]
   *
   * Note, for both inputs and outputs, no mixing of on-heap buffers and direct
   * buffers are allowed.
   *
   * If the coder option ALLOW_CHANGE_INPUTS is set true (false by default), the
   * content of input buffers may change after the call, subject to concrete
   * implementation.
   *
   * @param inputs input buffers to read data from. The buffers' remaining will
   *               be 0 after decoding
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(byte[][],int[],byte[][])",135,145,"/**
 * Decodes data using ByteArrayDecodingState, returns if empty.
 * @param inputs Input byte array, erased indexes, output array.
 * @throws IOException if an I/O error occurs during decoding.
 */
","* Decode with inputs and erasedIndexes, generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException if the decoder is closed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
 * Creates a RawErasureDecoder using the provided options.
 * @param coderOptions ErasureCoderOptions for decoder configuration
 * @return A RawErasureDecoder instance
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
 * Creates a RawErasureEncoder using the provided options.
 * @param coderOptions Encoder options to use.
 * @return A RawErasureEncoder instance.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,prepareDecoding,"org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:prepareDecoding(java.lang.Object[],int[])",103,115,"/**
 * Updates cached indexes if input arrays differ.
 * @param inputs Input array. @param erasedIndexes Erased index array.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,skipToNextBlockMarker,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextBlockMarker(),328,332,"/**
* Checks a condition using m1 with specified delimiter and bit length.
*/","* Skips bytes in the stream until the start marker of a block is reached
   * or end of stream is reached. Used for testing purposes to identify the
   * start offsets of blocks.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,complete,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:complete(),591,599,"/**
 * Completes processing, updates state, and verifies CRC.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,getAndMoveToFrontDecode,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode(),821,1004,"/**
 * Decodes a block of data, handling runs and symbols.
 * @throws IOException if an I/O error occurs during decoding
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,<init>,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream),598,600,"/**
 * Creates a CBZip2OutputStream with default block size.
 * @param out The OutputStream to write compressed data to.
 */
","* Constructs a new <tt>CBZip2OutputStream</tt> with a blocksize of 900k.
  *
  * <p>
  * <b>Attention: </b>The caller is resonsible to write the two BZip2 magic
  * bytes <tt>""BZ""</tt> to the specified stream prior to calling this
  * constructor.
  * </p>
  *
  * @param out *
  *            the destination stream.
  *
  * @throws IOException
  *             if an I/O error occurs in the specified stream.
  * @throws NullPointerException
  *             if <code>out == null</code>.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,endBlock,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endBlock(),788,831,"/**
* Updates CRC values and performs internal processing steps.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",60,62,"/**
 * Creates a BlockDecompressorStream with an input stream and decompressor.
 * @param in Input stream to decompress.
 * @param decompressor Decompressor to use.
 */
","* Create a {@link BlockDecompressorStream}.
   * 
   * @param in input stream
   * @param decompressor decompressor to use
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,createInputStream,org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream),132,136,"/**
* Delegates to overloaded method with null options.
* @param in Input stream to compress
* @return CompressionInputStream object
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,read,org.apache.hadoop.io.compress.DecompressorStream:read(),89,93,"/**
* Reads a byte from oneByte and returns its integer value.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,skip,org.apache.hadoop.io.compress.DecompressorStream:skip(long),193,213,"/**
* Skips a specified number of bytes from the input stream.
* @param n number of bytes to skip; must be non-negative
* @return number of bytes actually skipped
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(),78,82,"/**
 * Constructs a ZStandardCompressor with default compression level and buffer size.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:getCompressor(),285,308,"/**
 * Obtains a Compressor from the CodecPool, handling nulls.
 * Returns the compressor or null if codec is null.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createOutputStream,org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream),53,58,"/**
* Creates a compression output stream using the codec.
* @param out The underlying output stream.
* @return A CompressionOutputStream.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createOutputStream,org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream),66,71,"/**
* Creates a compression output stream using the codec.
* @param out The underlying output stream.
* @return CompressionOutputStream instance.
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createOutputStream,org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream),105,110,"/**
* Creates a compression output stream using the codec.
* @param out The underlying output stream.
* @return A CompressionOutputStream.
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out        the location for the final output stream
   * @return a stream the user can write uncompressed data to, to have it 
   *         compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createOutputStream,org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream),121,126,"/**
* Creates a compression output stream using the provided output stream.
* @param out The underlying output stream.
* @return A CompressionOutputStream instance.
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createOutputStream,org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream),44,49,"/**
* Creates a compression output stream using the codec.
* @param out The underlying output stream.
* @return A CompressionOutputStream.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createOutputStream,org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream),66,71,"/**
* Creates a compression output stream using codec utilities.
* @param out The underlying output stream.
* @return A CompressionOutputStream instance.
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,close,org.apache.hadoop.io.compress.CompressorStream:close(),102,112,"/**
 * Calls super.m1() if not closed, then sets closed to true.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Writer:close(),385,389,"/**
* Calls m1() on data and index objects.
*/",Close the map.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,finish,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:finish(),178,188,"/**
* Releases resources held by 'out' and 'compressor'.
*/",* Finishing up the current block.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createInputStream,org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream),79,84,"/**
* Creates a compression input stream using the codec.
* @param in Input stream to compress.
* @return CompressionInputStream instance.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createInputStream,org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream),130,135,"/**
* Creates a compression input stream using the codec.
* @param in Input stream to compress.
* @return CompressionInputStream object.
*/","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream),160,165,"/**
* Creates a compression input stream using the codec.
* @param in Input stream to be compressed.
* @return CompressionInputStream instance.
*/","* Create a {@link CompressionInputStream} that will read from the given
   * input stream and return a stream for uncompressed data.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createInputStream,org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream),178,183,"/**
 * Creates a compression input stream using the provided input stream.
 * @param in The input stream to compress.
 * @return A CompressionInputStream.
 */","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createInputStream,org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream),76,81,"/**
* Creates a compression input stream using the codec.
* @param in Input stream to compress.
* @return CompressionInputStream for the given input stream.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createInputStream,org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream),127,132,"/**
* Creates a compression input stream using the codec.
* @param in Input stream to compress.
* @return CompressionInputStream
*/","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,close,org.apache.hadoop.io.compress.DecompressorStream:close(),221,230,"/**
 * Calls super.m1() if not closed, then sets closed to true.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,close,org.apache.hadoop.fs.shell.Display$TextRecordInputStream:close(),259,263,"/**
* Calls m1 of r and the superclass.
* Delegates to the wrapped object and parent.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:close(),3181,3191,"/**
 * Calls m1() on in, out, and indexOut if they are not null.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Reader:close(),881,887,"/**
 * Calls m1 on the index and data, if index is open.
 */","* Close the map.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:close(),3877,3880,"/**
* Delegates m1() call to the inner object and then nulls it.
*/",closes the underlying reader,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,finish,org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:finish(),532,539,"/**
 * Releases resources after decompression, handling potential errors.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createCompressor,org.apache.hadoop.io.compress.DefaultCodec:createCompressor(),74,77,"/**
* Returns a compressor instance using the provided configuration.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.DefaultCodec:createDirectDecompressor(),108,111,"/**
* Returns a DirectDecompressor instance using the provided configuration.
*/",* {@inheritDoc},,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createDecompressor,org.apache.hadoop.io.compress.DefaultCodec:createDecompressor(),100,103,"/**
 * Returns a Decompressor instance using the provided configuration.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeFileHeader,org.apache.hadoop.io.SequenceFile$Writer:writeFileHeader(),1274,1289,"/**
 * Writes data to the output stream in a specific format.
 */",Write and flush the file header.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,write,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:write(java.io.DataOutput),217,229,"/**
 * Serializes the object to the output stream, validating lengths.
 * @param out Output stream to write to. Throws IOException if invalid.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,write,org.apache.hadoop.security.Credentials:write(java.io.DataOutput),354,371,"/**
 * Writes data to the output stream, including token and secret key maps.
 */","* Stores all the keys to DataOutput.
   * @param out DataOutput.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),681,684,"/**
* Calls m1 with the given key and false as the second argument.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),686,689,"/**
* Calls m1 with the given key and true as the second argument.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readFields,org.apache.hadoop.io.SequenceFile$Metadata:readFields(java.io.DataInput),771,783,"/**
 * Reads file metadata from input.
 * @param in DataInput stream containing metadata.
 * @throws IOException if an I/O error occurs.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,readFields,org.apache.hadoop.security.token.Token:readFields(java.io.DataInput),307,321,"/**
 * Reads data from input stream to populate object fields.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,readFields,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:readFields(java.io.DataInput),189,203,"/**
 * Reads delegation token data from input stream.
 * Version is checked, then reads data for owner, renewer, etc.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readBlock,org.apache.hadoop.io.SequenceFile$Reader:readBlock(),2294,2330,"/**
 * Decompresses data, verifies sync, and prepares buffers.
 */",Read the next 'compressed' block,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,seekToCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:seekToCurrentValue(),2336,2369,"/**
* Decompresses or skips values based on compression state.
* Handles decompression and seeks based on flags and buffers.
*/","* Position valLenIn/valIn to the 'value' 
     * corresponding to the 'current' key",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,createTokenInfo,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenInfo(byte[]),262,271,"/**
 * Parses delegation token information from byte array.
 * @param tokenInfoBytes byte array containing token info
 * @return DelegationTokenInformation object
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,readFields,org.apache.hadoop.fs.permission.PermissionStatus:readFields(java.io.DataInput),96,101,"/**
 * Reads username, groupname, and permission from the input stream.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readString,org.apache.hadoop.io.Text:readString(java.io.DataInput),556,558,"/**
 * Reads a string from the input stream, with a maximum length.
 * @param in DataInput to read from
 * @throws IOException if an I/O error occurs
 */
","* @return Read a UTF8 encoded string from in.
   * @param in input in.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getDelegationKey(int),385,412,"/**
* Retrieves a DelegationKey by ID, fetching from SQL if missing.
* @param keyId The ID of the delegation key to retrieve.
* @return The DelegationKey object, or the one from super().
*/","* Obtains the DelegationKey from the SQL database.
   * @param keyId KeyId of the DelegationKey to obtain.
   * @return DelegationKey that matches the given keyId or null
   *         if it doesn't exist in the database.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processKeyAddOrUpdate,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processKeyAddOrUpdate(byte[]),391,397,"/**
* Reads delegation key from byte array and adds it to allKeys.
* @param data byte array containing the delegation key data.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getKeyFromZK,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getKeyFromZK(int),579,598,"/**
* Retrieves a DelegationKey by keyId from Zookeeper.
* @param keyId The ID of the delegation key to retrieve.
* @return DelegationKey object or null if not found.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:write(java.io.DataOutput),843,848,"/**
 * Writes data to the output stream: meta name, compression algo, region data.
 * @param out DataOutput stream to write to.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:write(java.io.DataOutput),2098,2102,"/**
* Writes data to the output stream.
* @param out DataOutput object to write to
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:write(java.io.DataOutput),897,905,"/**
 * Writes data to the output stream, including compression and regions.
 * @param out DataOutput stream to write to.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,selectDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:selectDelegationToken(java.net.URL,org.apache.hadoop.security.Credentials)",344,354,"/**
 * Retrieves a delegation token from credentials for a service.
 * @param url URL containing service address.
 * @param creds Credentials containing the token.
 * @return Delegation token.
 */
","* Select a delegation token from all tokens in credentials, based on url.
   *
   * @param url url.
   * @param creds credentials.
   * @return token.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getServerToken,org.apache.hadoop.security.SaslRpcClient:getServerToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),279,293,"/**
 * Retrieves a token selector based on the provided authentication type.
 * @param authType SaslAuth object representing the authentication type
 * @return TokenSelector object or null if token info is null
 */","* Try to locate the required token for the server.
   * 
   * @param authType of the SASL client
   * @return Token for server, or null if no token available
   * @throws IOException - token selector cannot be instantiated",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setTokenService,"org.apache.hadoop.security.SecurityUtil:setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress)",456,466,"/**
 * Processes a token for a service address.
 * @param token Token object, can be null.
 * @param addr Service address.
 */
","* Set the given token's service to the format expected by the RPC client 
   * @param token a delegation token
   * @param addr the socket for the rpc connection",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,deleteKey,org.apache.hadoop.crypto.key.UserProvider:deleteKey(java.lang.String),102,113,"/**
 * Processes a key, fetches credentials, and caches the name.
 * @param name The key to process.
 * @throws IOException if the key doesn't exist.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.UserProvider:rollNewVersion(java.lang.String,byte[])",115,131,"/**
* Creates a KeyVersion with given name and material, validates metadata.
* @param name Key name
* @param material Key material
* @return KeyVersion object
* @throws IOException if key not found or length mismatch
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.UserProvider:getKeyVersions(java.lang.String),167,181,"/**
 * Retrieves KeyVersion objects for a given name.
 * @param name The name to search for.
 * @return A list of KeyVersion objects.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,tokenFromProto,org.apache.hadoop.ipc.ProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto),114,117,"/**
 * Delegates token creation to ShadedProtobufHelper.
 * @param tokenProto TokenProto object to process
 * @return Token object of a specific type
 */","* Get a token from a TokenProto payload.
   * @param tokenProto marshalled token
   * @return the token.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeys,org.apache.hadoop.crypto.key.UserProvider:getKeys(),155,165,"/**
 * Extracts strings from Text objects lacking ""@"" symbol.
 * @return List of extracted strings.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,write,org.apache.hadoop.fs.FsServerDefaults:write(java.io.DataOutput),163,173,"/**
 * Writes metadata to the output stream.
 * Writes blockSize, bytesPerChecksum, writePacketSize, etc.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,write,org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput),103,106,"/**
 * Delegates to overloaded method with username, groupname, permission.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text),82,89,"/**
 * Retrieves a ByteString value associated with the given Text key.
 */","* Get the ByteString for frequently used fixed and small set strings.
   * @param key Hadoop Writable Text string
   * @return the ByteString for frequently used fixed and small set strings.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,org.apache.hadoop.security.token.Token:<init>(org.apache.hadoop.security.token.Token),107,112,"/**
 * Constructs a new Token by cloning another Token object.
 */
","* Clone a token.
   * @param other the token to clone",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,"org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int)",380,382,"/**
* Delegates to overloaded method with default maxWraps.
* @param str The input text string.
* @param maxLineLength Maximum line length.
*/
","* Read from the InputStream into the given Text.
   * @param str the object to store the given line
   * @param maxLineLength the maximum number of bytes to store into str.
   * @return the number of bytes read including the newline
   * @throws IOException if the underlying stream throws",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text),390,392,"/**
 * Calls m1 with default max and min values.
 * @throws IOException if an I/O error occurs
 */","* Read from the InputStream into the given Text.
   * @param str the object to store the given line
   * @return the number of bytes read including the newline
   * @throws IOException if the underlying stream throws",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)",166,188,"/**
 * Creates a delegation token with specified user, renewer, and service.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,<init>,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",55,61,"/**
 * Constructs a DelegationTokenIdentifier with owner, renewer, and realUser.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(int),201,203,"/**
* Creates a RetryPolicy with the specified maxFailovers.
* Uses TRY_ONCE_THEN_FAIL as the retry strategy.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)",43,48,"/**
 * Constructs an InstrumentedWriteLock with a default Timer.
 * @param name Lock name, logger, lock, gaps, threshold, timer.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,long,long)",75,79,"/**
 * Constructs an InstrumentedLock with a ReentrantLock.
 * @param name Lock name, logger, gap & threshold for logging.
 */
","* Create a instrumented lock instance which logs a warning message
   * when lock held time is above given threshold.
   *
   * @param name the identifier of the lock object
   * @param logger this class does not have its own logger, will log to the
   *               given logger instead
   * @param minLoggingGapMs  the minimum time gap between two log messages,
   *                         this is to avoid spamming to many logs
   * @param lockWarningThresholdMs the time threshold to view lock held
   *                               time as being ""too long""",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)",49,54,"/**
 * Constructs an InstrumentedReadLock with a default Timer.
 * @param name Lock name, logger, readWriteLock, gaps, threshold.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,org.apache.hadoop.io.retry.RetryPolicy)",40,45,"/**
 * Creates a proxied object with retry logic.
 * @param iface Interface to proxy, implementation, retry policy.
 */","* <p>
   * Create a proxy for an interface of an implementation class
   * using the same retry policy for each method in the interface. 
   * </p>
   * @param iface the interface that the retry will implement
   * @param implementation the instance whose methods should be retried
   * @param retryPolicy the policy for retrying method call failures
   * @param <T> T.
   * @return the retry proxy",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,processWaitTimeAndRetryInfo,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processWaitTimeAndRetryInfo(),129,149,"/**
* Waits specified time for retry, logs details, and returns RETRY.
* @throws InterruptedIOException if the wait is interrupted.
*/","* It first processes the wait time, if there is any,
     * and then invokes {@link #processRetryInfo()}.
     *
     * If the wait time is positive, it either sleeps for synchronous calls
     * or immediately returns for asynchronous calls.
     *
     * @return {@link CallReturn#RETRY} if the retryInfo is processed;
     *         otherwise, return {@link CallReturn#WAIT_RETRY}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,int,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)",281,292,"/**
 * Creates a Writer with provided stream, size, compression, comparator, and config.
 */","* Constructor
     * 
     * @param fsdos
     *          output stream for writing. Must be at position 0.
     * @param minBlockSize
     *          Minimum compressed block size in bytes. A compression block will
     *          not be closed until it reaches this size except for the last
     *          block.
     * @param compressName
     *          Name of the compression algorithm. Must be one of the strings
     *          returned by {@link TFile#getSupportedCompressionAlgorithms()}.
     * @param comparator
     *          Leave comparator as null or empty string if TFile is not sorted.
     *          Otherwise, provide the string name for the comparison algorithm
     *          for keys. Two kinds of comparators are supported.
     *          <ul>
     *          <li>Algorithmic comparator: binary comparators that is language
     *          independent. Currently, only ""memcmp"" is supported.
     *          <li>Language-specific comparator: binary comparators that can
     *          only be constructed in specific language. For Java, the syntax
     *          is ""jclass:"", followed by the class name of the RawComparator.
     *          Currently, we only support RawComparators that can be
     *          constructed through the default constructor (with no
     *          parameters). Parameterized RawComparators such as
     *          {@link WritableComparator} or
     *          {@link JavaSerializationComparator} may not be directly used.
     *          One should write a wrapper class that inherits from such classes
     *          and use its default constructor to perform proper
     *          initialization.
     *          </ul>
     * @param conf
     *          The configuration object.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:<init>(java.io.DataInput),772,780,"/**
 * Constructs MetaIndex from DataInput, populating the index.
 * @param in DataInput stream containing index data.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,isLastChunk,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:isLastChunk(),80,83,"/**
* Executes m1() and returns the value of lastChunk.
*/","* Have we reached the last chunk.
     * 
     * @return true if we have reached the last chunk.
     * @throws java.io.IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,getRemain,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:getRemain(),91,94,"/**
 * Executes m1() and returns the value of the remain variable.
 */","* How many bytes remain in the current chunk?
     * 
     * @return remaining bytes left in the current chunk.
     * @throws java.io.IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(),137,144,"/**
 * Reads a value from the stream. Returns -1 on failure.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,"org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[],int,int)",151,165,"/**
* Reads data from the input stream.
* @param b buffer to fill, @param off offset, @param len length.
* @return Number of bytes read or -1 if stream is corrupted.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,skip,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:skip(long),167,175,"/**
* Reads data from input stream. Returns 0 if m1() fails.
* @param n The requested number of bytes to read.
* @return The number of bytes actually read.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(int),303,309,"/**
* Adds a byte to the buffer. Resizes buffer if full.
* @param b The byte to add.
* @throws IOException If an I/O error occurs.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,flush,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flush(),332,336,"/**
 * Calls m1 and delegates to out.m2, throwing IOException if needed.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[]),311,314,"/**
* Calls m1 with the provided byte array and full length.
* @param b The byte array to process.
* @throws IOException if an I/O error occurs.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[],int,int)",1948,1950,"/**
* Delegates to a ByteArray-based m1 method.
* @param buf byte array
* @param offset start offset
* @param length length of the data
*/","* Compare the entry key to another key. Synonymous to compareTo(new
         * ByteArray(buf, offset, length)
         * 
         * @param buf
         *          The key buffer
         * @param offset
         *          offset into the key buffer.
         * @param length
         *          the length of the key.
         * @return comparison result between the entry key with the input key.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,<init>,org.apache.hadoop.io.ArrayWritable:<init>(java.lang.String[]),62,67,"/**
 * Creates an ArrayWritable from an array of strings.
 * @param strings array of strings to be stored
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:close(),1668,1674,"/**
 * Calls m1 if out is not null, then calls super.m2().
 */",Close the file.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:append(java.lang.Object,java.lang.Object)",1677,1707,"/**
 * Writes key-value pair to buffer, validates types, and flushes if full.
 */",Append a key/value pair.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendRaw,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)",1710,1733,"/**
* Writes key and value to buffer, handling block size limits.
* @param keyData Key data.
* @param val Value data.
*/",Append a key/value pair.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,delegationTokenToJSON,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:delegationTokenToJSON(org.apache.hadoop.security.token.Token),357,365,"/**
 * Extracts delegation token details from a token.
 * @param token The token to process.
 * @return A map containing delegation token information.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,doDelegationTokenOperation,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:doDelegationTokenOperation(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation,java.lang.String,org.apache.hadoop.security.token.Token,boolean,java.lang.String)",288,356,"/**
 * Executes a delegation token operation and returns a map.
 * @param url URL to call, token, operation, renewer, dToken, hasResponse, doAsUser
 * @return Map containing the result or null if no response.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,cloneInto,"org.apache.hadoop.io.WritableUtils:cloneInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",236,239,"/**
* Copies data from src to dst using reflection.
* @param dst Destination Writable object.
* @param src Source Writable object.
*/","* Make a copy of the writable object using serialization to a buffer.
   * @param dst the object to copy from
   * @param src the object to copy into, which is destroyed
   * @throws IOException raised on errors performing I/O.
   * @deprecated use ReflectionUtils.cloneInto instead.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.util.Collection)",483,495,"/**
 * Recursively finds a node based on scope, excluding specified nodes.
 */","* Randomly choose one node from <i>scope</i>.
   *
   * If scope starts with ~, choose one from the all nodes except for the
   * ones in <i>scope</i>; otherwise, choose one from <i>scope</i>.
   * If excludedNodes is given, choose a node that's not in excludedNodes.
   *
   * @param scope range of nodes from which a node will be chosen
   * @param excludedNodes nodes to be excluded from
   * @return the chosen node",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",886,892,"/**
* Overloaded method, calls the primary m1 with a null context.
*/","* Sort nodes array by network distance to <i>reader</i>.
   * <p>
   * In a three-level topology, a node can be either local, on the same rack,
   * or on a different rack from the reader. Sorting the nodes based on network
   * distance from the reader reduces network traffic and improves
   * performance.
   * <p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistanceUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",929,936,"/**
 * Overloads m1 with a null context.
 * @param reader Node reader
 * @param nodes Array of nodes
 * @param activeLen Active length of nodes array
 */","* Sort nodes array by network distance to <i>reader</i> with secondary sort.
   * <p> using network location. This is used when the reader
   * is not a datanode. Sorting the nodes based on network distance
   * from the reader reduces network traffic and improves
   * performance.
   * </p>
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket,long)",91,94,"/**
 * Creates a SocketInputStream associated with a Socket.
 * @param socket The Socket to associate with.
 * @param timeout Timeout value in milliseconds.
 */
","* Same as SocketInputStream(socket.getChannel(), timeout): <br><br>
   * 
   * Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @param timeout timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket),108,110,"/**
 * Creates a SocketInputStream associated with the given socket.
 * @param socket The Socket to associate with this stream.
 */
","* Same as SocketInputStream(socket.getChannel(), socket.getSoTimeout())
   * :<br><br>
   * 
   * Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream:<init>(java.net.Socket,long)",96,99,"/**
 * Creates a SocketOutputStream with a timeout.
 * @param socket The Socket instance.
 * @param timeout Timeout in milliseconds.
 */
","* Same as SocketOutputStream(socket.getChannel(), timeout):<br><br>
   * 
   * Create a new ouput stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @see SocketOutputStream#SocketOutputStream(WritableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @param timeout timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,write,org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:write(java.lang.String),198,205,"/**
 * Sends a metric message via the socket.
 * @param msg The metric message to send.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,<init>,org.apache.hadoop.net.NetworkTopology:<init>(),122,125,"/**
 * Initializes the NetworkTopology with a root node.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getNodeForNetworkLocation,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeForNetworkLocation(org.apache.hadoop.net.Node),42,55,"/**
 * Processes a node, potentially creating a node group.
 * @param node The node to process.
 * @return A processed Node object.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,add,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:add(org.apache.hadoop.net.Node),176,222,"/**
 * Adds a node to the network topology, handling node placement.
 */","Add a leaf node
   * Update node counter &amp; rack counter if necessary
   * @param node node to be added; can be null
   * @exception IllegalArgumentException if add a node to a leave 
   *                                     or node to be added is not a leaf",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,add,org.apache.hadoop.net.InnerNodeImpl:add(org.apache.hadoop.net.Node),129,170,"/**
* Adds a node to the tree, ensuring it's a descendant.
* @param n Node to add; throws exception if not a descendant.
* @return True if added successfully, false otherwise.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,doIO,"org.apache.hadoop.net.SocketIOWithTimeout:doIO(java.nio.ByteBuffer,int)",124,170,"/**
* Reads from a buffer, performs operations, and handles IO exceptions.
*/","* Performs one IO and returns number of bytes read or written.
   * It waits up to the specified timeout. If the channel is 
   * not read before the timeout, SocketTimeoutException is thrown.
   * 
   * @param buf buffer for IO
   * @param ops Selection Ops used for waiting. Suggested values: 
   *        SelectionKey.OP_READ while reading and SelectionKey.OP_WRITE while
   *        writing. 
   *        
   * @return number of bytes read or written. negative implies end of stream.
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,connect,"org.apache.hadoop.net.SocketIOWithTimeout:connect(java.nio.channels.SocketChannel,java.net.SocketAddress,int)",182,228,"/**
* Connects a channel to an endpoint with a timeout.
* @param channel SocketChannel to connect.
* @param endpoint Remote address to connect to.
* @param timeout Connection timeout in seconds.
*/","* The contract is similar to {@link SocketChannel#connect(SocketAddress)} 
   * with a timeout.
   * 
   * @see SocketChannel#connect(SocketAddress)
   * 
   * @param channel - this should be a {@link SelectableChannel}
   * @param endpoint
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,waitForIO,org.apache.hadoop.net.SocketIOWithTimeout:waitForIO(int),242,248,"/**
 * Throws SocketTimeoutException if selector returns 0.
 * Uses SelectorPool.m1 to check channel selection status.
 */","* This is similar to {@link #doIO(ByteBuffer, int)} except that it
   * does not perform any I/O. It just waits for the channel to be ready
   * for I/O as specified in ops.
   * 
   * @param ops Selection Ops used for waiting
   * 
   * @throws SocketTimeoutException 
   *         if select on the channel times out.
   * @throws IOException
   *         if any other I/O error occurs.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String),388,391,"/**
 * Calls m1 with a null timeout and false flag.
 * @param strInterface The interface string.
 * @throws UnknownHostException if hostname resolution fails.
 */
","* Returns the default (first) host name associated by the default
   * nameserver with the address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0).
   *            Must not be null.
   * @return The default host name associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,"org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String)",404,408,"/**
* Resolves hostname, using provided interface/nameserver.
* @param strInterface Network interface, null for default.
* @param nameserver Nameserver, null for default.
* @return Resolved hostname or null if resolution fails.
*/
","* @return Returns the default (first) host name associated by the provided
   * nameserver with the address bound to the specified network interface.
   *
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @param nameserver
   *            The DNS host name
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkParameterValidity,org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[]),387,389,"/**
* Calls m1 with default USAGE.
* @param argv command-line arguments
* @return boolean result of the call to m1
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,help,org.apache.hadoop.ha.HAAdmin:help(java.lang.String[]),508,510,"/**
* Calls m1 with default USAGE string.
* @param argv command-line arguments
* @return int result of m1
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getFilter,org.apache.hadoop.metrics2.impl.MetricsConfig:getFilter(java.lang.String),266,280,"/**
 * Creates a MetricsFilter based on the given prefix.
 * @param prefix filter prefix; returns a MetricsFilter.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,create,org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String),98,101,"/**
 * Loads MetricsConfig. Uses prefix to construct properties file name.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,create,"org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String,java.lang.String[])",103,105,"/**
* Delegates metrics config loading to m1.
* @param prefix Config file prefix.
* @param fileNames File names to load.
* @return MetricsConfig object.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,clear,org.apache.hadoop.ipc.RetryCache:clear(org.apache.hadoop.ipc.RetryCache),398,403,"/**
 * Executes m1 on the cache's set and calls cache.m2.
 * @param cache The RetryCache instance to operate on.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getMetrics,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMetrics(org.apache.hadoop.metrics2.impl.MetricsCollectorImpl,boolean)",196,210,"/**
 * Retrieves metrics records from the source, applies filters, and adds tags.
 * @param builder MetricsCollectorImpl instance
 * @param all boolean flag to fetch all metrics
 * @return Iterable of MetricsRecordImpl objects
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,getMBeanName,"org.apache.hadoop.metrics2.util.MBeans:getMBeanName(java.lang.String,java.lang.String,java.util.Map)",151,169,"/**
* Creates an ObjectName for metrics, combining service, name, and params.
* @param serviceName Service name.
* @param nameName Name of the metric.
* @param additionalParameters Additional parameters as key-value pairs.
* @return ObjectName or null on error.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reattachMetrics,org.apache.hadoop.security.UserGroupInformation:reattachMetrics(),259,261,"/**
 * Calls UgiMetrics.m1() for metric tracking.
 */",* Reattach the class's metrics to a new metric system.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopMetricsMBeans,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopMetricsMBeans(),341,346,"/**
 * Calls the 'm1' method on each MetricsSourceAdapter in 'sources'.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,stop,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stop(),212,214,"/**
 * Calls method m1 in a synchronized block.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,stop,org.apache.hadoop.ipc.DecayRpcScheduler:stop(),1156,1161,"/**
* Records detailed metrics for the current namespace.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,<init>,"org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",61,64,"/**
 * Constructs a MutableInverseQuantiles object.
 * @param intervalSecs Interval in seconds.
 */","* Instantiates a new {@link MutableInverseQuantiles} for a metric that rolls itself
   * over on the specified time interval.
   *
   * @param name          of the metric
   * @param description   long-form textual description of the metric
   * @param sampleName    type of items in the stream (e.g., ""Ops"")
   * @param valueName     type of the values
   * @param intervalSecs  rollover interval (in seconds) of the estimator",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newQuantiles,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",217,228,"/**
 * Creates and registers a MutableQuantiles object.
 * @param name Quantiles name, @param interval Interval size.
 * @return The newly created MutableQuantiles object.
 */","* Create a mutable metric that estimates quantiles of a stream of values
   * @param name of the metric
   * @param desc metric description
   * @param sampleName of the metric (e.g., ""Ops"")
   * @param valueName of the metric (e.g., ""Time"" or ""Latency"")
   * @param interval rollover interval of estimator in seconds
   * @return a new quantile estimator object
   * @throws MetricsException if interval is not a positive integer",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,create,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:create(),989,991,"/**
 * Creates and returns a DelegationTokenSecretManagerMetrics instance.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,create,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:create(org.apache.hadoop.ipc.RetryCache),52,55,"/**
 * Creates and registers metrics for the given retry cache.
 * @param cache The retry cache to create metrics for.
 * @return RetryCacheMetrics object for the cache.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newStat,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",279,282,"/**
* Creates a MutableStat with the given parameters and default flag.
* @param name Stat name
* @param desc Description
* @param sampleName Sample name
* @param valueName Value name
*/","* Create a mutable metric with stats
   * @param name  of the metric
   * @param desc  metric description
   * @param sampleName  of the metric (e.g., ""Ops"")
   * @param valueName   of the metric (e.g., ""Time"" or ""Latency"")
   * @return a new mutable metric object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,addMetricIfNotExists,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:addMetricIfNotExists(java.lang.String),163,172,"/**
 * Retrieves or creates a MutableRate metric by name.
 * @param name Metric name. Returns the metric object.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean,boolean)",314,329,"/**
 * Gets or creates a MutableRate. Returns existing if specified.
 * @param name Rate name.
 * @param desc Rate description.
 * @param extended Whether rate is extended.
 * @param returnExisting Whether to return existing rate.
 * @return MutableRate object.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getMetrics,"org.apache.hadoop.metrics2.source.JvmMetrics:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",143,155,"/**
 * Records JVM metrics using the provided collector.
 * @param collector MetricsCollector instance
 * @param all if true, record all metrics
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,initRegistry,org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:initRegistry(java.lang.Object),98,127,"/**
 * Retrieves or creates a MetricsRegistry from the given source object.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String)",391,393,"/**
 * Creates a MetricsRegistry with a boolean flag.
 * @param name Metric name.
 * @param description Metric description.
 * @param value Metric value.
 */
","* Add a tag to the metrics
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @return the registry (for keep adding tags)",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,add,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:add(java.lang.String,long)",212,214,"/**
 * Delegates the call to innerMetrics.m1 with provided name and value.
 */","* @param name
   *          name of metric
   * @param value
   *          value of metric",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,addQueueTime,"org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addQueueTime(int,long)",88,90,"/**
* Records queue rate for a given priority and queue time.
* @param priority Level of the queue.
* @param queueTime Time spent in the queue.
*/","* Instrument a Call queue time based on its priority.
   *
   * @param priority of the RPC call
   * @param queueTime of the RPC call in the queue of the priority",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,addProcessingTime,"org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addProcessingTime(int,long)",98,100,"/**
 * Records processing rate for a given priority and time.
 * @param priority Processing level priority.
 * @param processingTime Processing time in milliseconds.
 */","* Instrument a Call processing time based on its priority.
   *
   * @param priority of the RPC call
   * @param processingTime of the RPC call in the queue of the priority",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addProcessingTime(java.lang.String,long)",87,89,"/**
* Logs RPC call processing time using the rates service.
* @param rpcCallName Name of the RPC call.
* @param processingTime Processing time in milliseconds.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addDeferredProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addDeferredProcessingTime(java.lang.String,long)",91,93,"/**
* Delegates rate limiting to deferredRpcRates.
* @param name Name of the operation.
* @param processingTime Processing time in milliseconds.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addOverallProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addOverallProcessingTime(java.lang.String,long)",100,102,"/**
* Records RPC processing time.
* @param rpcCallName RPC call name.
* @param overallProcessingTime Processing time in milliseconds.
*/
","* Add an overall RPC processing time sample.
   * @param rpcCallName of the RPC call
   * @param overallProcessingTime  the overall RPC processing time",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableStat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",138,163,"/**
* Records metrics data to a builder, conditionally updating stats.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,newForMethod,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)",94,105,"/**
 * Creates/retrieves a MutableMetric, registers it, returns it.
 * @param source Object source, method, annotation, registry.
 * @return MutableMetric object.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getMetrics,"org.apache.hadoop.ipc.DecayRpcScheduler:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",1009,1027,"/**
 * Collects metrics using the provided collector, optionally all.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroups,org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroups(java.lang.String),1580,1583,"/**
* Wraps m1(user) in a List.
* @param user User identifier.
* @return List of strings returned by m1(user).
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKrb5File,org.apache.hadoop.security.KDiag:validateKrb5File(),561,588,"/**
 * Locates and sets the Kerberos configuration file path.
 * Uses system properties and environment variables to find it.
 */","* Locate the {@code krb5.conf} file and dump it.
   *
   * No-op on windows.
   * @throws IOException problems reading the file.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verify,"org.apache.hadoop.security.KDiag:verify(boolean,java.lang.String,java.lang.String,java.lang.Object[])",963,981,"/**
 * Returns false if condition is false, logs error if !nofail.
 * @param condition boolean condition to check
 * @param category error category
 * @param message error message
 * @param args message arguments
 */","* Assert that a condition must hold.
   *
   * If not, an exception is raised, or, if {@link #nofail} is set,
   * an error will be logged and the method return false.
   *
   * @param condition condition which must hold
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @return true if the verification succeeded, false if it failed but
   * an exception was not raised.
   * @throws KerberosDiagsFailure containing the formatted text
   *         if the condition was met",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,failif,"org.apache.hadoop.security.KDiag:failif(boolean,java.lang.String,java.lang.String,java.lang.Object[])",1034,1042,"/**
 * Logs a message if the condition is true.
 * @param condition Flag to determine if message is logged.
 * @param category Message category.
 * @param message Message to log.
 * @param args Arguments for the message.
 */
","* Conditional failure with string formatted arguments.
   * There is no chek for the {@link #nofail} value.
   * @param condition failure condition
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @throws KerberosDiagsFailure containing the formatted text
   *         if the condition was met",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createRemoteUser,org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String),1438,1442,"/**
 * Gets UserGroupInformation for a user, using SIMPLE authentication.
 * @param user User name.
 * @return UserGroupInformation object.
 */
","* Create a user from a login name. It is intended to be used for remote
   * users in RPC, since it won't have any credentials.
   * @param user the full user principal name, must not be empty or null
   * @return the UserGroupInformation for the remote user.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuthorizedUgi,org.apache.hadoop.ipc.Server$Connection:getAuthorizedUgi(java.lang.String),2161,2176,"/**
 * Obtains UserGroupInformation based on the authorized ID and auth method.
 * @param authorizedId ID of the authorized user.
 * @return UserGroupInformation object.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,verifyToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])",575,582,"/**
 * Verifies password against stored password for a token.
 * @param identifier Token identifier.
 * @param password Password to verify.
 * @throws InvalidToken if password doesn't match.
 */
","* Verifies that the given identifier and password are valid and match.
   * @param identifier Token identifier.
   * @param password Password in the token.
   * @throws InvalidToken InvalidToken.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:init(),143,152,"/**
 * Starts the secret manager if enabled, handling potential IO errors.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,startThreads,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:startThreads(),243,348,"/**
 * Initializes sequence and key counters using ZooKeeper.
 * Handles external clients and token watchers.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,run,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover:run(),829,859,"/**
 * Removes expired delegation tokens and updates master keys periodically.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(java.nio.ByteBuffer),367,384,"/**
* Reads data into the provided ByteBuffer.
* @param dst ByteBuffer to write data to.
* @return Number of bytes read, or -1 on error.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,spawnAutoRenewalThreadForKeytab,org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForKeytab(),905,918,"/**
 * Renews Kerberos ticket if conditions are met and ticket exists.
 */","* Spawn a thread to do periodic renewals of kerberos credentials from a
   * keytab file.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,<init>,org.apache.hadoop.fs.shell.Ls:<init>(org.apache.hadoop.conf.Configuration),122,124,"/**
 * Constructs a new Ls object with the given configuration.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,<init>,"org.apache.hadoop.fs.shell.Count:<init>(java.lang.String[],int,org.apache.hadoop.conf.Configuration)",118,122,"/**
 * Constructs a Count object.
 * @param cmd Command-line arguments, starting from 'pos'.
 * @param pos Starting position in the command array.
 * @param conf Configuration object.
 */
","Constructor
   * @deprecated invoke via {@link FsShell}
   * @param cmd the count command
   * @param pos the starting index of the arguments
   * @param conf configuration",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,<init>,org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:<init>(org.apache.hadoop.fs.FileSystem),495,497,"/**
 * Initializes a TargetFileSystem with the given FileSystem.
 * @param fs The FileSystem this TargetFileSystem uses.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,org.apache.hadoop.fs.ChecksumFileSystem:<init>(org.apache.hadoop.fs.FileSystem),79,81,"/**
 * Constructs a ChecksumFileSystem with the given FileSystem.
 * @param fs The FileSystem to associate with this ChecksumFileSystem.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,newShellInstance,org.apache.hadoop.fs.FsShell:newShellInstance(),398,400,"/**
 * Returns a new instance of FsShell.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,<init>,org.apache.hadoop.fs.shell.FsCommand:<init>(),76,76,"/**
 * Default constructor for FsCommand, protected for subclassing.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,39,"/**
 * Constructs a RSErasureEncoder with the given options.
 * @param options ErasureCoderOptions object for configuration
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),44,46,"/**
 * Constructs an HHXORErasureEncoder with provided options.
 * @param options ErasureCoderOptions object for configuration
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Constructs a DummyErasureEncoder with provided options.
 * @param options ErasureCoderOptions object for configuration.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,38,"/**
 * Constructs a XORErasureEncoder with provided options.
 * @param options ErasureCoderOptions object for configuration.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Constructs a DummyErasureDecoder with the given options.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,38,"/**
 * Constructs a XORErasureDecoder with provided options.
 * @param options ErasureCoderOptions object for configuration.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,39,"/**
 * Constructs an RSErasureDecoder with the given options.
 * @param options ErasureCoderOptions for decoder configuration.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),45,47,"/**
 * Constructs an HHXORErasureDecoder with provided options.
 * @param options ErasureCoderOptions object for configuration.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,getDeserializer,org.apache.hadoop.io.serializer.WritableSerialization:getDeserializer(java.lang.Class),120,124,"/**
 * Creates a WritableDeserializer for the given class.
 * @param c The class of Writable to deserialize.
 * @return A WritableDeserializer instance.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMaps,org.apache.hadoop.security.ShellBasedIdMapping:updateMaps(),343,357,"/**
 * Executes a conditional sequence of operations.
 * May construct a full map or execute m2/m3.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapIncr,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(java.lang.String,boolean)",465,504,"/**
 * Updates user/group mappings based on name and OS type.
 * @param name User/group name.
 * @param isGrp True for group, false for user.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapIncr,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(int,boolean)",506,540,"/**
* Updates user/group mapping based on OS and ID.
* @param id User/group ID. @param isGrp True for group, false for user.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,connectToZooKeeper,org.apache.hadoop.ha.ActiveStandbyElector:connectToZooKeeper(),723,743,"/**
 * Connects to ZooKeeper, configures watchers and auth.
 * @return ZooKeeper instance, configured and ready to use.
 */","* Get a new zookeeper client instance. protected so that test class can
   * inherit and mock out the zookeeper instance
   * 
   * @return new zookeeper client instance
   * @throws IOException raised on errors performing I/O.
   * @throws KeeperException zookeeper connectionloss exception",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,"org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean,boolean)",1272,1289,"/**
 * Authenticates with Hadoop using Kerberos, optionally checking TGT.
 * @param checkTGT Whether to check the Ticket Granting Ticket.
 * @param ignoreLastLoginTime Ignore the last login time.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache(boolean),1322,1332,"/**
* Performs authentication flow, throws exception if login fails.
* @param ignoreLastLoginTime Ignores last login time for auth.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",29,32,"/**
 * Creates a CryptoFSDataOutputStream.
 * @param out Output stream, codec, buffer size, key, and IV.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",86,89,"/**
 * Creates a CryptoOutputStream with default mode.
 * @param out Output stream.
 * @param codec CryptoCodec.
 * @param bufferSize Buffer size.
 * @param key Encryption key.
 * @param iv Initialization vector.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createEncryptor(),72,76,"/**
 * Creates and returns a new OpensslCtrCipher instance.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createDecryptor(),78,82,"/**
* Creates and returns a new OpensslCtrCipher for decryption.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite)",130,134,"/**
 * Initializes an OpensslCtrCipher with the given mode and CipherSuite.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONEncKeyVersions,"org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersions(java.lang.String,java.util.List)",143,155,"/**
 * Creates EncryptedKeyVersion list from valueList.
 * @param keyName Key name.
 * @param valueList List of values to process.
 * @return List of EncryptedKeyVersion objects.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getEncKeyQueueSize,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getEncKeyQueueSize(java.lang.String),966,969,"/**
* Retrieves the encryption key version for the given key name.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getNext,org.apache.hadoop.crypto.key.kms.ValueQueue:getNext(java.lang.String),292,295,"/**
 * Retrieves a value by key name, performing chained operations.
 */","* This removes the value currently at the head of the Queue for the
   * provided key. Will immediately fire the Queue filler function if key
   * does not exist.
   * If Queue exists but all values are drained, It will ask the generator
   * function to add 1 value to Queue and then drain it.
   * @param keyName String key name
   * @return E the next value in the Queue
   * @throws IOException raised on errors performing I/O.
   * @throws ExecutionException executionException.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,drain,org.apache.hadoop.crypto.key.kms.KMSClientProvider:drain(java.lang.String),961,964,"/**
 * Delegates the m1 call to the encKeyVersionQueue.
 * @param keyName The key name to pass to the queue.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,internalQueueCall,org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call),3106,3109,"/**
 * Calls a method with the provided Call object.
 * @param call The Call object to execute.
 * @throws IOException, InterruptedException if an error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByName,org.apache.hadoop.security.SecurityUtil:getByName(java.lang.String),569,588,"/**
 * Resolves a hostname to an InetAddress. Logs slow lookups.
 * @param hostname The hostname to resolve.
 * @return An InetAddress object.
 */
","* Resolves a host subject to the security requirements determined by
   * hadoop.security.token.service.use_ip. Optionally logs slow resolutions.
   * 
   * @param hostname host or ip to resolve
   * @return a resolved host
   * @throws UnknownHostException if the host doesn't exist",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,run,org.apache.hadoop.util.JvmPauseMonitor$Monitor:run(),181,208,"/**
 * Monitors JVM pauses, logs warnings/info if sleep interval exceeded.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(org.apache.hadoop.ipc.VersionedProtocol,java.lang.String,long,int)",241,254,"/**
 * Creates a ProtocolSignature using provided data.
 * @param server VersionedProtocol instance.
 * @param protocol Protocol name.
 * @param clientVersion Client version.
 * @param clientMethodsHash Hash of client methods.
 * @return ProtocolSignature object.
 */","* Get a server protocol's signature
   *
   * @param server server implementation
   * @param protocol server protocol
   * @param clientVersion client's version
   * @param clientMethodsHash client's protocol's hash code
   * @return the server protocol's signature
   * @throws IOException if any error occurs",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)",74,86,"/**
 * Creates a ProtocolSignature based on client version and hash.
 * @param protocol Protocol string
 * @param clientVersion Client version
 * @param clientMethodsHash Client methods hash
 * @return ProtocolSignature object
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)",186,198,"/**
 * Creates a ProtocolSignature if protocol is supported.
 * @param protocol Protocol string to validate.
 * @param clientVersion Client version.
 * @param clientMethodsHash Hash of client methods.
 * @return ProtocolSignature object.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolSignature(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto)",70,104,"/**
 * Retrieves protocol signatures based on request.
 * @param controller RPC controller
 * @param request Signature request
 * @return Response containing protocol signatures
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ExternalCall.java,<init>,org.apache.hadoop.ipc.ExternalCall:<init>(java.security.PrivilegedExceptionAction),36,38,"/**
 * Constructs an ExternalCall with the provided action.
 * @param action The PrivilegedExceptionAction to execute.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Listener:run(),1551,1600,"/**
* Main server loop, handles connections and events.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,add,org.apache.hadoop.ipc.FairCallQueue:add(java.lang.Object),194,213,"/**
 * Attempts to process an element. Throws an exception on failure.
 * @param e The element to process.
 * @return True if processing was successful.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,put,org.apache.hadoop.ipc.FairCallQueue:put(java.lang.Object),215,222,"/**
 * Processes an element. Decrements queue size if processing fails.
 * @param e The element to process.
 * @throws InterruptedException if interrupted.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolProxy.java,isMethodSupported,"org.apache.hadoop.ipc.ProtocolProxy:isMethodSupported(java.lang.String,java.lang.Class[])",95,117,"/**
* Checks if a server method is supported.
* @param methodName Method name to check.
* @param parameterTypes Method parameter types.
* @return True if the method is supported, false otherwise.
*/","* Check if a method is supported by the server or not.
   * 
   * @param methodName a method's name in String format
   * @param parameterTypes a method's parameter types
   * @return true if the method is supported by the server
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,checkRpcHeaders,org.apache.hadoop.ipc.Server$Connection:checkRpcHeaders(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto),2830,2852,"/**
 * Validates the RPC request header for essential fields.
 * Throws exception if validation fails.
 */","* Verify RPC header is valid
     * @param header - RPC request header
     * @throws RpcServerException - header contains invalid values",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer:<init>(),33,35,"/**
 * Constructs a ResponseBuffer with a default capacity of 1024.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,forceDecay,org.apache.hadoop.ipc.DecayRpcScheduler:forceDecay(),822,825,"/**
 * Calls the m1 method. Used for testing purposes.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,getPriorityLevel,org.apache.hadoop.ipc.CallQueueManager:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),265,270,"/**
 * Delegates m1 call to DecayRpcScheduler if scheduler is one.
 * @param user UserGroupInformation object
 * @return Result of m1 or 0 if not a DecayRpcScheduler.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getReturnMessage,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)",301,323,"/**
 * Creates a message using the buffer and method.
 * @param method The method to use.
 * @param buf The buffer to write to.
 * @return The created message.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,newInstance,"org.apache.hadoop.ipc.RpcWritable$Buffer:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",175,188,"/**
 * Creates an instance of valueClass, configures it, and returns it.
 * @param valueClass Class to instantiate.
 * @param conf Configuration object used for setup.
 * @return Instance of valueClass.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getMessage,"org.apache.hadoop.ipc.Server$Connection:getMessage(org.apache.hadoop.thirdparty.protobuf.Message,org.apache.hadoop.ipc.RpcWritable$Buffer)",3046,3057,"/**
 * Decodes a message buffer into a typed Message.
 * @param message The message to decode.
 * @param buffer The buffer containing the encoded message.
 * @return Decoded message of type T.
 */
","* Decode the a protobuf from the given input stream 
     * @return Message - decoded protobuf
     * @throws RpcServerException - deserialization failed",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getReturnMessage,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)",311,333,"/**
 * Creates a message using a buffer and method.
 * @param method The method to use.
 * @param buf The buffer to write to.
 * @return The created message.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponse,"org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3549,3562,"/**
* Sends a response to the RPC call.
* @param call RPC call object.
* @param header Response header.
* @param rv Writable response data.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,take,org.apache.hadoop.ipc.FairCallQueue:take(),292,296,"/**
* Acquires a semaphore and then calls m2().
* @return Value returned by m2()
* @throws InterruptedException if interrupted while waiting
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,poll,"org.apache.hadoop.ipc.FairCallQueue:poll(long,java.util.concurrent.TimeUnit)",298,301,"/**
 * Attempts to acquire a permit and returns a value, or null.
 * @param timeout Timeout for acquiring permit.
 * @param unit Time unit for the timeout.
 * @return Value if permit acquired, null otherwise.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,poll,org.apache.hadoop.ipc.FairCallQueue:poll(),307,310,"/**
* Returns m2() if semaphore.m1() is true, otherwise returns null.
*/","* poll() provides no strict consistency: it is possible for poll to return
   * null even though an element is in the queue.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Responder:run(),1711,1725,"/**
 * Executes a masked function, logs start/stop, and closes resources.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doSaslReply,org.apache.hadoop.ipc.Server$Connection:doSaslReply(org.apache.hadoop.thirdparty.protobuf.Message),2420,2426,"/**
 * Sends a SASL RPC call.
 * @param message The message to send with the call.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doSaslReply,org.apache.hadoop.ipc.Server$Connection:doSaslReply(java.lang.Exception),2428,2433,"/**
* Logs an authentication failure and cleans up resources.
* Logs error details from the Exception.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupBadVersionResponse,org.apache.hadoop.ipc.Server$Connection:setupBadVersionResponse(int),2636,2665,"/**
 * Handles version mismatch errors based on client version.
 * @param clientVersion Client version number.
 */","* Try to set up the response to indicate that the client version
     * is incompatible with the server. This can contain special-case
     * code to speak enough of past IPC protocols to pass back
     * an exception to the caller.
     * @param clientVersion the version the caller is using 
     * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupHttpRequestOnIpcPortResponse,org.apache.hadoop.ipc.Server$Connection:setupHttpRequestOnIpcPortResponse(),2667,2672,"/**
 * Simulates an RPC call with a request and processes it.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CombinedIPWhiteList.java,<init>,"org.apache.hadoop.util.CombinedIPWhiteList:<init>(java.lang.String,java.lang.String,long)",31,43,"/**
 * Constructs a CombinedIPWhiteList with fixed and variable IP lists.
 * @param fixedWhiteListFile Path to the fixed IP list file.
 * @param variableWhiteListFile Path to the variable IP list file (optional).
 * @param cacheExpiryInSeconds Cache expiry time for variable list.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CombinedIPList.java,<init>,"org.apache.hadoop.util.CombinedIPList:<init>(java.lang.String,java.lang.String,long)",33,44,"/**
 * Creates a CombinedIPList with fixed and variable blacklists.
 * @param fixedBlackListFile Path to the fixed IP blacklist file.
 * @param variableBlackListFile Optional path to variable IP list.
 * @param cacheExpiryInSeconds Cache expiry time in seconds.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,reload,org.apache.hadoop.util.FileBasedIPList:reload(),67,69,"/**
 * Creates and returns a FileBasedIPList instance using the fileName.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getVirtualMemorySize,org.apache.hadoop.util.SysInfoLinux:getVirtualMemorySize(),603,606,"/**
* Calculates a masked value by adding swapSize * 1024 to m1().
*/",{@inheritDoc},,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.InputStream)",495,499,"/**
 * Constructs a bounded input stream for reading from a file.
 * @param fs FileSystem object
 * @param file Path to the file
 * @param in Input stream to wrap
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
 * Opens an input stream for the given path.
 * @param path Path to open.
 * @param bufferSize Buffer size for the stream.
 * @return FSDataInputStream for reading data.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)",1109,1112,"/**
 * Creates a HarFSDataInputStream.
 * @param fs Filesystem, path, start, length, bufsize for input stream.
 */
","* constructors for har input stream.
     * @param fs the underlying filesystem
     * @param p The path in the underlying filesystem
     * @param start the start position in the part file
     * @param length the length of valid data in the part file
     * @param bufsize the buffer size
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,read,"org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int)",217,220,"/**
* Allocates a buffer from the pool, up to maxLength.
* @param bufferPool Pool to allocate from.
* @param maxLength Max buffer size.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evictExpiredEntries,org.apache.hadoop.util.LightWeightCache:evictExpiredEntries(),164,175,"/**
 * Evicts entries from the queue up to EVICTION_LIMIT.
 * Uses timer and checks eviction criteria (m3) before removal.
 */",Evict expired entries.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evictEntries,org.apache.hadoop.util.LightWeightCache:evictEntries(),178,184,"/**
 * Decrements a counter until it's below the size limit.
 */",Evict entries in order to enforce the size limit of the cache.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean)",408,410,"/**
* Overloads m1 with a default value for the 'z' parameter.
* @param qOption boolean option
* @param hOption boolean option
* @param xOption boolean option
*/
","Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output is to be used
   * @param xOption a flag indicating if calculation from snapshots is to be
   *                included in the output
   * @return the string representation of the object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,java.util.List)",423,426,"/**
 * Calls overloaded method with `false` for the last parameter.
 * @param qOption, hOption, tOption flags, types storage types
 */","* Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output if to be used
   * @param tOption a flag indicating if display quota by storage types
   * @param types Storage types to display
   * @return the string representation of the object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,"org.apache.hadoop.fs.QuotaUsage:toString(boolean,boolean,java.util.List)",321,327,"/**
 * Selects a method based on options.
 * @param hOption flag, @param tOption flag, @param types list of types
 */","* Return the string representation of the object in the output format.
   * if hOption is false file sizes are returned in bytes
   * if hOption is true file sizes are returned in human readable
   *
   * @param hOption a flag indicating if human readable output if to be used
   * @param tOption type option.
   * @param types storage types.
   * @return the string representation of the object.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read,org.apache.hadoop.fs.FSInputChecker:read(),150,159,"/**
 * Reads a byte from the buffer, advancing the position.
 * Returns -1 if buffer is exhausted after refill.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read1,"org.apache.hadoop.fs.FSInputChecker:read1(byte[],int,int)",251,275,"/**
* Reads up to 'len' bytes from the buffer into 'b', starting at 'off'.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readFields,org.apache.hadoop.io.ObjectWritable$NullInstance:readFields(java.io.DataInput),114,125,"/**
 * Resolves a class by name from input stream.
 * @param in DataInput stream containing class name.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,readFields,org.apache.hadoop.io.ArrayPrimitiveWritable:readFields(java.io.DataInput),205,251,"/**
 * Decodes an encoded array from the input stream.
 * @param in DataInput stream to read encoded data from.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.List),98,106,"/**
 * Recursively processes a list of CompletableFutures.
 * @param futures List of CompletableFutures to process.
 */","* Wait for a list of futures to complete. If the list is empty,
   * return immediately.
   *
   * @param futures list of futures.
   * @param <T> Generics Type T.
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,maybeAwaitCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:maybeAwaitCompletion(java.util.concurrent.CompletableFuture),152,157,"/**
 * Executes m1 with the provided CompletableFuture, if it exists.
 * @param future CompletableFuture to execute, or null.
 */","* Block awaiting completion for any non-null future passed in;
   * No-op if a null arg was supplied.
   * @param future future
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(byte[],int,int,long)",123,138,"/**
* Updates CRC values from a byte array in chunks.
* @param crcBuffer Byte array containing CRC data.
* @param offset Start offset in the byte array.
* @param length Number of bytes to process.
* @param bytesPerCrc Bytes per CRC value.
*/","* Composes length / CRC_SIZE_IN_BYTES more CRCs from crcBuffer, with
   * each CRC expected to correspond to exactly {@code bytesPerCrc} underlying
   * data bytes.
   *
   * @param crcBuffer crcBuffer.
   * @param offset offset.
   * @param length must be a multiple of the expected byte-size of a CRC.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(java.io.DataInputStream,long,long)",150,157,"/**
 * Reads checksums and processes them.
 * @param checksumIn Input stream for checksums.
 * @param numChecksumsToRead Number of checksums to read.
 * @param bytesPerCrc Bytes per CRC value.
 */
","* Composes {@code numChecksumsToRead} additional CRCs into the current digest
   * out of {@code checksumIn}, with each CRC expected to correspond to exactly
   * {@code bytesPerCrc} underlying data bytes.
   *
   * @param checksumIn checksumIn.
   * @param numChecksumsToRead numChecksumsToRead.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,hasNext,org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:hasNext(),780,783,"/**
 * Delegates to m1() and returns its boolean result.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,newInstance,"org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",124,127,"/**
 * Creates an instance of the specified class using the configuration.
 * @param theClass Class to instantiate.
 * @param conf Configuration object.
 */","Create an object for the given class and initialize it from conf
   * 
   * @param theClass class of which an object is created
   * @param conf Configuration
   * @param <T> Generics Type T.
   * @return a new object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getKeyClass,org.apache.hadoop.io.MapFile$Reader:getKeyClass(),465,465,"/**
* Delegates the call to data.m1() and returns the result.
*/","* Returns the class of keys in this file.
     *
     * @return keyClass.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getValueClass,org.apache.hadoop.io.MapFile$Reader:getValueClass(),472,472,"/**
* Delegates the call to data.m1() and returns the result.
*/","* Returns the class of values in this file.
     *
     * @return Value Class.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,run,org.apache.hadoop.util.FindClass:run(java.lang.String[]),310,335,"/**
* Processes arguments, delegates to other methods based on action.
* @param args Command-line arguments; action and name.
* @return Result code based on the action.
*/","* Run the class/resource find or load operation
   * @param args command specific arguments.
   * @return the outcome
   * @throws Exception if something went very wrong",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",78,84,"/**
 * Constructs a FutureDataInputStreamBuilderImpl.
 * @param fc The file context.
 * @param path The path to the file.
 * @throws IOException if an I/O error occurs.
 */
","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param path path.
   * @throws IOException failure",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,"org.apache.hadoop.util.DataChecksum:newDataChecksum(byte[],int)",160,181,"/**
 * Creates a DataChecksum from a byte array, offset, and bytesPerChecksum.
 * @param bytes byte array to process
 * @param offset offset into the byte array
 * @return DataChecksum object
 */","* Creates a DataChecksum from HEADER_LEN bytes from arr[offset].
   *
   * @param bytes bytes.
   * @param offset offset.
   * @return DataChecksum of the type in the array or null in case of an error.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,org.apache.hadoop.util.DataChecksum:newDataChecksum(java.io.DataInputStream),192,202,"/**
 * Creates a DataChecksum object.
 * @param in Input stream to read checksum parameters from.
 * @return DataChecksum object or throws InvalidChecksumSizeException.
 */
","* This constructs a DataChecksum by reading HEADER_LEN bytes from input
   * stream <i>in</i>.
   *
   * @param in data input stream.
   * @throws IOException raised on errors performing I/O.
   * @return DataChecksum by reading HEADER_LEN
   *         bytes from input stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell:run(),951,960,"/**
 * Executes a process if enough time has passed, logs if on macOS.
 */","* Check to see if a command needs to be executed and execute if needed.
   *
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>(java.lang.String),148,210,"/**
 * Constructs a DynamicWrappedIO instance using the provided class name.
 * @param classname Fully qualified name of the wrapped IO class.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>(java.lang.String),228,346,"/**
 * Initializes DynamicWrappedStatistics by loading and wrapping a class.
 * @param classname Name of the class to wrap.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_aggregate,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)",502,507,"/**
* Aggregates a snapshot using m2, passing snapshot and statistics.
*/","* Aggregate an existing {@code IOStatisticsSnapshot} with
   * the supplied statistics.
   * @param snapshot snapshot to update
   * @param statistics IOStatistics to add
   * @return true if the snapshot was updated.
   * @throws IllegalArgumentException if the {@code statistics} argument is not
   * null but not an instance of IOStatistics, or if  {@code snapshot} is invalid.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create(),514,518,"/**
* Creates an IStatisticsSnapshot using m2, after calling m1.
*/","* Create a new {@code IOStatisticsSnapshot} instance.
   * @return an empty IOStatisticsSnapshot.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create(java.lang.Object),527,532,"/**
 * Creates an IStatisticsSnapshot from a source object.
 * @param source The source object to create snapshot from.
 * @return An IStatisticsSnapshot or null on failure.
 */
","* Create a new {@code IOStatisticsSnapshot} instance.
   * @param source optional source statistics
   * @return an IOStatisticsSnapshot.
   * @throws ClassCastException if the {@code source} is not valid.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_toJsonString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable),541,545,"/**
 * Converts a snapshot to a JSON string.
 * @param snapshot Serializable snapshot object
 * @return JSON string representation of the snapshot
 */","* Save IOStatisticsSnapshot to a JSON string.
   * @param snapshot statistics; may be null or of an incompatible type
   * @return JSON string value or null if source is not an IOStatisticsSnapshot
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_fromJsonString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String),554,558,"/**
 * Parses a JSON string and returns an IostatisticsSnapshot.
 * @param json JSON string to parse.
 * @return IostatisticsSnapshot object.
 */
","* Load IOStatisticsSnapshot from a JSON string.
   * @param json JSON string value.
   * @return deserialized snapshot.
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_load,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",568,573,"/**
 * Loads an I/O statistics snapshot for a given file system path.
 * @param fs FileSystem object
 * @param path Path to load statistics for
 * @return Statistics snapshot
 */
","* Load IOStatisticsSnapshot from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return the loaded snapshot
   * @throws UncheckedIOException Any IO exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_retrieve,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object),581,585,"/**
 * Retrieves an I/O statistics snapshot from the source.
 * @param source The source object to retrieve stats from.
 * @return A Serializable object or null if unavailable.
 */","* Extract the IOStatistics from an object in a serializable form.
   * @param source source object, may be null/not a statistics source/instance
   * @return {@code IOStatisticsSnapshot} or null if the object is null/doesn't have statistics
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_save,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",596,604,"/**
* Saves a snapshot to the filesystem.
* @param snapshot The snapshot to save, null allowed.
* @param fs The filesystem.
* @param path The path to save to.
* @param overwrite Whether to overwrite existing files.
*/
","* Save IOStatisticsSnapshot to a Hadoop filesystem as a JSON file.
   * @param snapshot statistics
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten?
   * @throws UncheckedIOException Any IO exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_toPrettyString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_toPrettyString(java.lang.Object),666,669,"/**
* Converts statistics to a pretty string representation.
* Uses m2 method for conversion; statistics are passed.
*/","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_getCurrent,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_getCurrent(),439,443,"/**
* Executes m1 and returns the result of m2.
* @throws UnsupportedOperationException if an error occurs
*/
","* Get the context's {@code IOStatisticsContext} which
   * implements {@code IOStatisticsSource}.
   * This is either a thread-local value or a global empty context.
   * @return instance of {@code IOStatisticsContext}.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_setThreadIOStatisticsContext,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object),451,455,"/**
* Sets thread context using statistics context.
* @param statisticsContext Statistics context object.
* @throws UnsupportedOperationException if operation is not supported.
*/","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_reset,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_reset(),462,466,"/**
* Calls m1() and resets the iostatistics context using m2.
*/","* Reset the context's IOStatistics.
   * {@code IOStatisticsContext#reset()}
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_snapshot,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_snapshot(),474,478,"/**
* Retrieves a snapshot context using m2, after calling m1.
*/","* Take a snapshot of the context IOStatistics.
   * {@code IOStatisticsContext#snapshot()}
   * @return an instance of {@code IOStatisticsSnapshot}.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_aggregate,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_aggregate(java.lang.Object),487,490,"/**
* Delegates processing to m2, passing the source object.
*/","* Aggregate into the IOStatistics context the statistics passed in via
   * IOStatistics/source parameter.
   * <p>
   * Returns false if the source is null or does not contain any statistics.
   * @param source implementation of {@link IOStatisticsSource} or {@link IOStatistics}
   * @return true if the the source object was aggregated.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionUtil.java,compareVersions,"org.apache.hadoop.util.VersionUtil:compareVersions(java.lang.String,java.lang.String)",39,43,"/**
 * Compares two versions using ComparableVersion.
 * @param version1 First version string.
 * @param version2 Second version string.
 * @return Comparison result from ComparableVersion.
 */
","* Compares two version name strings using maven's ComparableVersion class.
   *
   * @param version1
   *          the first version to compare
   * @param version2
   *          the second version to compare
   * @return a negative integer if version1 precedes version2, a positive
   *         integer if version2 precedes version1, and 0 if and only if the two
   *         versions are equal.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refreshInternal,"org.apache.hadoop.util.HostsFileReader:refreshInternal(java.lang.String,java.lang.String,boolean)",200,225,"/**
 * Refreshes host details from include/exclude files.
 * @param includesFile Path to includes file, null if none.
 * @param excludesFile Path to excludes file, null if none.
 * @param lazy If true, lazy loads host details.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,<init>,"org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.io.InputStream,java.lang.String,java.io.InputStream)",67,75,"/**
 * Initializes the HostsFileReader with file paths and input streams.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.CountingBloomFilter:<init>(int,int,int)",93,96,"/**
 * Initializes a CountingBloomFilter with specified size, hashes, and hash type.
 */","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,<init>,"org.apache.hadoop.util.bloom.BloomFilter:<init>(int,int,int)",110,114,"/**
 * Initializes a Bloom filter with specified size, hash count, and type.
 */","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,readFields,org.apache.hadoop.util.bloom.CountingBloomFilter:readFields(java.io.DataInput),301,309,"/**
* Reads bucket data from input stream.
* @param in DataInput stream to read from.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,readFields,org.apache.hadoop.util.bloom.BloomFilter:readFields(java.io.DataInput),218,233,"/**
 * Reads bitset data from input stream.
 * @param in DataInput stream to read from.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,probablyHasKey,org.apache.hadoop.io.BloomMapFile$Reader:probablyHasKey(org.apache.hadoop.io.WritableComparable),264,272,"/**
 * Checks if a key might exist in the bloom filter.
 * @param key The key to check.
 * @return True if potentially present, false if definitely absent.
 */","* Checks if this MapFile has the indicated key. The membership test is
     * performed using a Bloom filter, so the result has always non-zero
     * probability of false positives.
     * @param key key to check
     * @return  false iff key doesn't exist, true if key probably exists.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,selectiveClearing,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:selectiveClearing(org.apache.hadoop.util.bloom.Key,short)",199,235,"/**
 * Clears a data element based on the key and scheme.
 * @param k The key to use for clearing.
 * @param scheme The clearing scheme to apply.
 */","* Performs the selective clearing for a given key.
   * @param k The false positive key to remove from <i>this</i> retouched Bloom filter.
   * @param scheme The selective clearing scheme to apply.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,createOptionTableListing,org.apache.hadoop.fs.FsShell:createOptionTableListing(),292,295,"/**
 * Creates a TableListing object with default values.
 * Returns a new TableListing instance.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadResources,"org.apache.hadoop.conf.Configuration:loadResources(java.util.Properties,java.util.ArrayList,int,boolean,boolean)",3082,3100,"/**
 * Loads resources, optionally reloads defaults, and updates properties.
 * @param properties Configuration properties.
 * @param resources List of resources to load.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String,java.lang.String)",616,619,"/**
 * Overloads m1 with a single new key.
 * @param key Original key.
 * @param newKey New key to add.
 * @param customMessage Optional custom message.
 */
","* Adds the deprecated key to the global deprecation map.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key to be deprecated
   * @param newKey key that take up the values of deprecated key
   * @param customMessage deprecation message",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[])",640,643,"/**
 * Calls overloaded method with null as the third argument.
 * @param key The key.
 * @param newKeys Array of new keys.
 */","* Adds the deprecated key to the global deprecation map when no custom
   * message is provided.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If a key is deprecated in favor of multiple keys, they are all treated as 
   * aliases of each other, and setting any one of them resets all the others 
   * to the new value.
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key Key that is to be deprecated
   * @param newKeys list of keys that take up the values of deprecated key
   * @deprecated use {@link #addDeprecation(String key, String newKey)} instead",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String)",659,661,"/**
* Calls m1 with a single new key, using the provided key.
* @param key The original key.
* @param newKey The new key to add.
*/
","* Adds the deprecated key to the global deprecation map when no custom
   * message is provided.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key Key that is to be deprecated
   * @param newKey key that takes up the value of deprecated key",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,start,org.apache.hadoop.http.HttpServer2:start(),1382,1434,"/**
 * Starts the HTTP server, including metrics and handler checks.
 */","* Start the server. Does not wait for the server to start.
   *
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,writeBreadCrumbNode,org.apache.hadoop.ha.ActiveStandbyElector:writeBreadCrumbNode(org.apache.zookeeper.data.Stat),963,977,"/**
 * Updates the ZNode to mark the local node as the most recent.
 * @param oldBreadcrumbStat Previous ZNode stat, or null if new.
 */","* Write the ""ActiveBreadCrumb"" node, indicating that this node may need
   * to be fenced on failover.
   * @param oldBreadcrumbStat",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,tryDeleteOwnBreadCrumbNode,org.apache.hadoop.ha.ActiveStandbyElector:tryDeleteOwnBreadCrumbNode(),985,1008,"/**
 * Deletes the breadcrumb for the active node, verifying data integrity.
 */","* Try to delete the ""ActiveBreadCrumb"" node when gracefully giving up
   * active status.
   * If this fails, it will simply warn, since the graceful release behavior
   * is only an optimization.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readVectored,"org.apache.hadoop.fs.VectoredReadUtils:readVectored(org.apache.hadoop.fs.PositionedReadable,java.util.List,java.util.function.IntFunction)",98,104,"/**
* Applies a FileRange to a stream using a provided allocator.
* @param stream Input stream. @param ranges Ranges to apply.
*/","* This is the default implementation which iterates through the ranges
   * to read each synchronously, but the intent is that subclasses
   * can make more efficient readers.
   * The data or exceptions are pushed into {@link FileRange#getData()}.
   * @param stream the stream to read the data from
   * @param ranges the byte ranges to read
   * @param allocate the byte buffer allocation
   * @throws IllegalArgumentException if there are overlapping ranges or a range is invalid
   * @throws EOFException the range offset is negative",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,cancelPrefetches,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:cancelPrefetches(),294,306,"/**
* Executes m1 operation on buffer data, transitioning state.
*/",* Requests cancellation of any previously issued prefetch requests.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.BlockManager:get(int),77,86,"/**
 * Reads block data into a buffer.
 * @param blockNumber Block number to read.
 * @return BufferData object containing block number and buffer.
 */","* Gets the block having the given {@code blockNumber}.
   *
   * The entire block is read into memory and returned as a {@code BufferData}.
   * The blocks are treated as a limited resource and must be released when
   * one is done reading them.
   *
   * @param blockNumber the number of the block to be read and returned.
   * @return {@code BufferData} having data from the given block.
   *
   * @throws IOException if there an error reading the given block.
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,readBlock,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:readBlock(org.apache.hadoop.fs.impl.prefetch.BufferData,boolean,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",331,395,"/**
 * Reads data from a buffer, potentially prefetching.
 * @param data BufferData object
 * @param isPrefetch Flag to indicate prefetch operation
 * @param expectedState Expected state of the buffer data
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,<init>,"org.apache.hadoop.fs.impl.prefetch.FilePosition:<init>(long,int)",83,95,"/**
 * Initializes a FilePosition with file size and block size.
 * @param fileSize size of the file in bytes
 * @param blockSize size of each block in bytes
 */
","* Constructs an instance of {@link FilePosition}.
   *
   * @param fileSize size of the associated file.
   * @param blockSize size of each block within the file.
   *
   * @throws IllegalArgumentException if fileSize is negative.
   * @throws IllegalArgumentException if blockSize is zero or negative.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,isLastBlock,org.apache.hadoop.fs.impl.prefetch.FilePosition:isLastBlock(),204,206,"/**
* Delegates m2 call to blockData, passing result of m1().
*/","* Determines whether the current block is the last block in this file.
   *
   * @return true if the current block is the last block in this file, false otherwise.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,toString,org.apache.hadoop.fs.impl.prefetch.FilePosition:toString(),275,296,"/**
* Logs buffer state, handling null buffer gracefully.
* Returns a string representation of the log message.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem:closeAll(),642,645,"/**
* Calls m1 with ""closeAll"" and then executes CACHE.m2().
*/","* Close all cached FileSystem instances. After this operation, they
   * may not be used in any operations.
   *
   * @throws IOException a problem arose closing one or more filesystem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readFully,"org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[])",125,128,"/**
 * Delegates m1 call to the underlying FSInputStream.
 * @param position File offset for reading.
 * @param buffer Buffer to store the read bytes.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence)",2044,2047,"/**
 * Delegates to overloaded method with UTF-8 charset.
 * @param fs FileSystem to use.
 * @param path Path to operate on.
 * @param charseq CharSequence to use.
 * @return FileSystem object.
 */
","* Write a line of text to a file. Characters are encoded into bytes using
   * UTF-8. This utility method opens the file for writing, creating the file if
   * it does not exist, or overwrites an existing file.
   *
   * @param fs the files system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long,boolean)",122,125,"/**
 * Constructor for BlockLocation with null filename.
 * @param names Block names, hosts, offset, length, corrupt flag.
 */
","* Constructor with host, name, offset, length and corrupt flag.
   * @param names names.
   * @param hosts hosts.
   * @param offset offset.
   * @param length length.
   * @param corrupt corrupt.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long)",135,138,"/**
 * Constructs a BlockLocation with specified names, hosts, paths, offset, and length.
 */","* Constructor with host, name, network topology, offset and length.
   * @param names names.
   * @param hosts hosts.
   * @param topologyPaths topologyPaths.
   * @param offset offset.
   * @param length length.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,clone,org.apache.hadoop.fs.statistics.MeanStatistic:clone(),271,274,"/**
 * Delegates to m1() and returns the MeanStatistic it produces.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,<init>,org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:<init>(),58,59,"/**
 * Initializes DynamicIOStatistics with default values.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,aggregateMeanStatistics,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMeanStatistics(org.apache.hadoop.fs.statistics.MeanStatistic,org.apache.hadoop.fs.statistics.MeanStatistic)",312,317,"/**
 * Combines two MeanStatistic objects.
 * Returns a new MeanStatistic containing combined data.
 */","* Aggregate the mean statistics.
   * This returns a new instance.
   * @param l left value
   * @param r right value
   * @return aggregate value",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,snapshot,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:snapshot(org.apache.hadoop.fs.statistics.IOStatistics),160,168,"/**
 * Processes IOStatistics to update internal counters and gauges.
 * @param source The IOStatistics object to process.
 */
","* Take a snapshot.
   *
   * This completely overwrites the map data with the statistics
   * from the source.
   * @param source statistics source.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtDebug,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(java.lang.String,java.lang.Object)",250,254,"/**
 * Calls the overloaded method with LOG as the source.
 * @param message The log message to be recorded.
 * @param source The object associated with the message.
 */
","* Extract any statistics from the source and log to
   * this class's log at debug, if
   * the log is set to log at debug.
   * No-op if logging is not at debug or the source is null/of
   * the wrong type/doesn't provide statistics.
   * @param message message for log -this must contain ""{}"" for the
   * statistics report to actually get logged.
   * @param source source object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtLevel,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtLevel(org.slf4j.Logger,java.lang.String,java.lang.Object)",263,281,"/**
 * Logs IO statistics based on level.
 * @param log Logger instance.
 * @param level Logging level.
 * @param source Source object.
 */","* A method to log IOStatistics from a source at different levels.
   *
   * @param log    Logger for logging.
   * @param level  LOG level.
   * @param source Source to LOG.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,cleanupRemoteIterator,org.apache.hadoop.util.functional.RemoteIterators:cleanupRemoteIterator(org.apache.hadoop.fs.RemoteIterator),297,304,"/**
 * Logs remote iterator stats and closes it if possible.
 * @param source The RemoteIterator to process.
 */
","* Clean up after an iteration.
   * If the log is at debug, calculate and log the IOStatistics.
   * If the iterator is closeable, cast and then cleanup the iterator
   * @param source iterator source
   * @param <T> type of source",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDurationOfInvocation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)",460,466,"/**
 * Delegates the statistic tracking to the provided factory.
 * @param factory DurationTrackerFactory instance
 * @param statistic Statistic name
 * @param input Input for tracking
 */
","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @throws IOException IO failure.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,build,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:build(),107,111,"/**
 * Creates and returns an IOStatisticsStore instance.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,org.apache.hadoop.crypto.CryptoInputStream:read(),779,782,"/**
* Reads the first byte from the buffer.
* Returns the byte as an integer or -1 on error.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(long,byte[],int,int)",332,348,"/**
* Reads data from input at a given position.
* @param position The position to read from.
* @param buffer The buffer to write to.
* @param offset Offset in the buffer.
* @param length Number of bytes to read.
* @return Number of bytes read.
*/",Positioned read. It is thread-safe,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[],int,int)",502,515,"/**
 * Reads data from the input stream at a specific position.
 * @param position byte offset, buffer, offset, and length.
 */",Positioned read fully. It is thread-safe,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(long,java.nio.ByteBuffer)",353,369,"/**
 * Reads data from input into buffer at given position.
 * @param position byte buffer position
 * @param buf byte buffer
 * @return number of bytes read
 */
",* Positioned read using {@link ByteBuffer}s. This method is thread-safe.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,java.nio.ByteBuffer)",374,389,"/**
* Reads data from input into a ByteBuffer, updating position.
* @param position Byte buffer position
* @param buf Byte buffer to read into
*/",* Positioned readFully using {@link ByteBuffer}s. This method is thread-safe.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,org.apache.hadoop.crypto.CryptoInputStream:read(java.nio.ByteBuffer),588,639,"/**
 * Reads data from the input stream into the provided ByteBuffer.
 * @param buf ByteBuffer to read data into
 * @return Number of bytes read, or -1 if EOF.
 */",ByteBuffer read.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)",707,736,"/**
* Reads data into a buffer, handling seekable and enhanced access.
* @param pool Buffer pool, maxLength, opts. Returns ByteBuffer.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",118,122,"/**
 * Creates a CryptoInputStream with specified input, codec, buffer, key, and IV.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,doEncode,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:doEncode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][])",101,118,"/**
 * Encodes input data and writes results to outputs, using parity units.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",50,54,"/**
* Encodes input chunks to output chunks using rawEncoder.
* @param inputChunks Input chunks to be encoded.
* @param outputChunks Output chunks to store encoded data.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,writeObject,"org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)",165,234,"/**
* Writes an object to a DataOutput, handling various types.
* @param out Output stream to write to.
*/","* Write a {@link Writable}, {@link String}, primitive type, or an array of
     * the preceding.  
     * 
     * @param allowCompactArrays - set true for RPC and internal or intra-cluster
     * usages.  Set false for inter-cluster, File, and other persisted output 
     * usages, to preserve the ability to interchange files with other clusters 
     * that may not be running the same version of software.  Sometime in ~2013 
     * we can consider removing this parameter and always using the compact format.
     *
     * @param conf configuration.
     * @param out dataoutput.
     * @param declaredClass declaredClass.
     * @param instance instance.
     * @throws IOException raised on errors performing I/O.
     *",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,tryAcquire,org.apache.hadoop.fs.impl.prefetch.BufferPool:tryAcquire(int),157,159,"/**
* Retrieves buffer data for a given block number.
* @param blockNumber The block number to retrieve data for.
*/
","* Acquires a buffer if one is immediately available. Otherwise returns null.
   * @param blockNumber the id of the block to try acquire.
   * @return the acquired block's {@code BufferData} or null.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numAvailable(),600,602,"/**
* Delegates to bufferPool's m1() method and returns the result.
*/","* Number of ByteBuffers available to be acquired.
   *
   * @return the number of available buffers.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,run,org.apache.hadoop.util.functional.TaskPool$Builder:run(org.apache.hadoop.util.functional.TaskPool$Task),268,282,"/**
 * Executes a task, using service if available, otherwise uses m3.
 * @param task The task to execute.
 * @return True if the task execution is successful.
 */
","* Execute the task across the data.
     * @param task task to execute
     * @param <E> exception which may be raised in execution.
     * @return true if the operation executed successfully
     * @throws E any exception raised.
     * @throws IOException IOExceptions raised by remote iterator or in execution.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPaths,"org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])",342,351,"/**
 * Processes a list of PathData objects, handling potential IOExceptions.
 */","*  Iterates over the given expanded paths and invokes
   *  {@link #processPath(PathData)} on each element.  If ""recursive"" is true,
   *  will do a post-visit DFS on directories.
   *  @param parent if called via a recurse, will be the parent dir, else null
   *  @param items a list of {@link PathData} objects to process
   *  @throws IOException if anything goes wrong...",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,getPathHandle,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:getPathHandle(org.apache.hadoop.fs.Path),163,166,"/**
 * Retrieves a PathHandle from a FileStatus.
 * @param filePath Path to the file.
 * @return PathHandle object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,resolve,"org.apache.hadoop.fs.Options$HandleOpt:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Options$HandleOpt[])",359,362,"/**
 * Creates a Function to get PathHandle from FileStatus.
 * @param fs FileSystem instance
 * @param opt Handle options
 * @return Function<FileStatus, PathHandle>
 */
","* Utility function for mapping {@link FileSystem#getPathHandle} to a
     * fixed set of handle options.
     * @param fs Target filesystem
     * @param opt Options to bind in partially evaluated function
     * @return Function reference with options fixed",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createPathHandle,"org.apache.hadoop.fs.FilterFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",177,180,"/**
* Calls fs.m1 to get a PathHandle based on FileStatus and options.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,createGroupExecutor,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupExecutor(java.lang.String),132,135,"/**
* Creates a ShellCommandExecutor with the given username.
* @param userName The username to use for the executor.
*/
","* Create a ShellCommandExecutor object using the user's name.
   *
   * @param userName user's name
   * @return a ShellCommandExecutor object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,createGroupIDExecutor,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupIDExecutor(java.lang.String),153,156,"/**
* Creates a ShellCommandExecutor with the provided username.
* @param userName The username to use for the executor.
*/
","* Create a ShellCommandExecutor object for fetch a user's group id list.
   *
   * @param userName the user's name
   * @return a ShellCommandExecutor object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map)",1222,1225,"/**
 * Constructs a ShellCommandExecutor with environment variables.
 * @param execString Command to execute.
 * @param dir Working directory.
 * @param env Environment variables.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,"org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[],long)",1373,1379,"/**
 * Executes a shell command with environment variables and timeout.
 * @param env Environment variables.
 * @param cmd Command to execute.
 * @param timeout Timeout in milliseconds.
 * @return Command output as a string.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readProto,org.apache.hadoop.security.Credentials:readProto(java.io.DataInput),403,413,"/**
 * Processes credentials from input, iterating and calling m6/m9.
 * @param in DataInput stream containing credentials data.
 */","* Populates keys/values from proto buffer storage.
   * @param in - stream ready to read a serialized proto buffer message",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addAll,"org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials,boolean)",463,476,"/**
* Copies secrets and tokens from another Credentials object.
* @param other The Credentials object to copy from.
* @param overwrite Whether to overwrite existing entries.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DelegationTokenIssuer.java,collectDelegationTokens,"org.apache.hadoop.security.token.DelegationTokenIssuer:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)",98,138,"/**
 * Retrieves or obtains a delegation token, potentially from issuer.
 * @param issuer Token issuer, renewer, credentials, tokens
 */","* NEVER call this method directly.
   *
   * @param issuer issuer.
   * @param renewer renewer.
   * @param credentials cache in which to add new delegation tokens.
   * @param tokens list of new delegation tokens.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addToken,"org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)",1712,1717,"/**
 * Calls m2 on m1, passing alias and token. Synchronized.
 */","* Add a named token to this UGI
   * 
   * @param alias Name of the token
   * @param token Token to be added
   * @return true on successful add of new token",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addRegexMountEntry,org.apache.hadoop.fs.viewfs.InodeTree:addRegexMountEntry(org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry),807,816,"/**
 * Adds a regex mount point to the list, logging details.
 * @param le LinkEntry containing mount point information.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,makeAbsolute,"org.apache.hadoop.fs.sftp.SFTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",176,181,"/**
 * Returns the path or a new Path based on path.m1() result.
 */","* Resolve against given working directory.
   *
   * @param workDir
   * @param path
   * @return absolute path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,makeAbsolute,"org.apache.hadoop.fs.ftp.FTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",269,274,"/**
 * Returns the path, or a new Path based on workDir if m1() is false.
 */","* Resolve against given working directory. *
   * 
   * @param workDir
   * @param path
   * @return",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,makeAbsolute,org.apache.hadoop.fs.RawLocalFileSystem:makeAbsolute(org.apache.hadoop.fs.Path),105,111,"/**
 * Returns the path, applying a mask if needed.
 * @param f The path to process.
 * @return The processed path.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,pathToFile,org.apache.hadoop.fs.RawLocalFileSystem:pathToFile(org.apache.hadoop.fs.Path),119,125,"/**
 * Creates a File object based on the provided Path, handling potential issues.
 */","* Convert a path to a File.
   *
   * @param path the path.
   * @return file.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,fixRelativePart,org.apache.hadoop.fs.FileSystem:fixRelativePart(org.apache.hadoop.fs.Path),2884,2890,"/**
 * Returns the path, potentially wrapping it with m2().
 * @param p The input path.
 * @return A modified Path object.
 */
","* See {@link FileContext#fixRelativePart}.
   * @param p the path.
   * @return relative part.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,makeAbsolute,org.apache.hadoop.fs.viewfs.ViewFileSystem:makeAbsolute(org.apache.hadoop.fs.Path),269,271,"/**
* Returns the path or a new Path if m1() is true.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),188,191,"/**
* Sets the working directory. Uses new_dir if m1() is true, otherwise combines paths.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,makeQualified,"org.apache.hadoop.fs.Path:makeQualified(java.net.URI,org.apache.hadoop.fs.Path)",562,598,"/**
 * Constructs a Path with a resolved URI, using default URI if needed.
 */","* Returns a qualified path object.
   *
   * @param defaultUri if this path is missing the scheme or authority
   * components, borrow them from this URI
   * @param workingDir if this path isn't absolute, treat it as relative to this
   * working directory
   * @return this path if it contains a scheme and authority and is absolute, or
   * a new path that includes a path and authority and is fully qualified",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,fixRelativePart,org.apache.hadoop.fs.FileContext:fixRelativePart(org.apache.hadoop.fs.Path),284,291,"/**
 * Returns the path, resolving it if necessary.
 * @param p The path to resolve.
 * @return The resolved path.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.HarFileSystem:getWorkingDirectory(),273,276,"/**
 * Creates a Path object from the URI's m1() value.
 */",* return the top level archive.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.HarFileSystem:getHomeDirectory(),809,812,"/**
 * Creates a Path object using the URI's m1() value.
 */",* return the top level archive path.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory(com.jcraft.jsch.ChannelSftp),680,686,"/**
 * Creates a Path object from channel.m1(), returns null on error.
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,org.apache.hadoop.fs.ChecksumFs:<init>(org.apache.hadoop.fs.AbstractFileSystem),58,63,"/**
 * Constructs a ChecksumFs with the given file system.
 * @param theFs The AbstractFileSystem to associate with this object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.RawLocalFileSystem:getHomeDirectory(),842,845,"/**
* Returns a Path representing the user's home directory.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.RawLocalFileSystem:getInitialWorkingDirectory(),861,864,"/**
* Returns the function mask path based on user.dir.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,abort,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:abort(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path)",245,261,"/**
 * Masks uploaded file, creates collector path, and initializes it.
 * @param uploadId Upload handle containing ID.
 * @param filePath Path to the uploaded file.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,lookupStat,"org.apache.hadoop.fs.shell.PathData:lookupStat(org.apache.hadoop.fs.FileSystem,java.lang.String,boolean)",175,186,"/**
 * Retrieves a FileStatus for the given path.
 * @param fs FileSystem instance
 * @param pathString Path to retrieve status for
 * @param ignoreFNF Ignore FileNotFoundException if true
 * @return FileStatus object or null if not found
 */","* Get the FileStatus info
   * @param ignoreFNF if true, stat will be null if the path doesn't exist
   * @return FileStatus for the given path
   * @throws IOException if anything goes wrong",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getPath,org.apache.hadoop.fs.PathIOException:getPath(),107,107,"/**
 * Returns a new Path object based on the existing 'path' field.
 */",@return Path that generated the exception,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getTargetPath,org.apache.hadoop.fs.PathIOException:getTargetPath(),110,112,"/**
 * Returns a Path object from targetPath, or null if targetPath is null.
 */","@return Path if the operation involved copying or moving, else null",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUsed,org.apache.hadoop.fs.FileSystem:getUsed(),2719,2722,"/**
 * Starts the recursive file processing from the root directory.
 * @return Long - total file count from the root.
 * @throws IOException if an I/O error occurs.
 */
","* Return the total size of all files in the filesystem.
   * @throws IOException IO failure
   * @return the number of path used.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ViewFileSystem:getHomeDirectory(),422,434,"/**
 * Returns the user's home directory path. Uses fsState and ugi.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getMountPoints,org.apache.hadoop.fs.viewfs.ViewFileSystem:getMountPoints(),1060,1070,"/**
 * Converts a list of MountPoints to a MountPoint array.
 * @return An array of MountPoint objects.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(org.apache.hadoop.fs.FileSystem,java.net.URI)",105,116,"/**
 * Creates a ChRootedFileSystem with the given FileSystem and URI.
 * @param fs the underlying FileSystem
 * @param uri the URI for the chrooted view
 * @throws IOException if an I/O error occurs
 */
","* Constructor
   * @param fs base file system
   * @param uri base uri
   * @throws IOException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getResolvedQualifiedPath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getResolvedQualifiedPath(org.apache.hadoop.fs.Path),177,181,"/**
 * Constructs a Path by combining root path and file path.
 * @param f The input Path object.
 * @return A new Path object.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ViewFs:getHomeDirectory(),314,326,"/**
 * Returns the user's home directory path.
 * Uses fsState and ugi to determine the path.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getMountPoints,org.apache.hadoop.fs.viewfs.ViewFs:getMountPoints(),720,730,"/**
 * Converts a list of MountPoints to a MountPoint array.
 * @return An array of MountPoint objects.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,getRemainingPathStr,"org.apache.hadoop.fs.viewfs.RegexMountPoint:getRemainingPathStr(java.lang.String,java.lang.String)",207,215,"/**
 * Constructs a Path object from relative path components.
 * @param srcPath Source path string.
 * @param resolvedPathStr Resolved path string.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,createLink,"org.apache.hadoop.fs.viewfs.InodeTree:createLink(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$LinkType,java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration)",423,505,"/**
* Creates a link at the specified path, handling mount points and link types.
* @param src Source path.
* @param target Target path.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRemainingPath,"org.apache.hadoop.fs.viewfs.InodeTree:getRemainingPath(java.lang.String[],int)",996,1008,"/**
 * Constructs a Path object from a string array, starting at startIndex.
 */","* Return remaining path from specified index to the end of the path array.
   * @param path An array of path components split by slash
   * @param startIndex the specified start index of the path array
   * @return remaining path.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getTargetLink,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetLink(),370,377,"/**
 * Constructs a Path object from a list of directory links.
 * Returns a Path representing the concatenated directory links.
 */","* Get the target of the link. If a merge link then it returned
     * as "","" separated URI list.
     *
     * @return the path.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)",119,121,"/**
 * Creates a Path object from parent and child path strings.
 */","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)",129,131,"/**
 * Creates a child path, delegating to a constructor.
 * @param parent The parent path.
 * @param child The name of the child path.
 */
","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,org.apache.hadoop.fs.Path)",139,141,"/**
 * Constructs a Path with a parent Path and a child string.
 */","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,rename,"org.apache.hadoop.io.MapFile:rename(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.String)",898,905,"/**
 * Renames a file or directory.
 * @param fs FileSystem object
 * @param oldName Old name
 * @param newName New name
 */","* Renames an existing map directory.
   * @param fs fs.
   * @param oldName oldName.
   * @param newName newName.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,insecureCreateForWrite,"org.apache.hadoop.io.SecureIOUtils:insecureCreateForWrite(java.io.File,int)",243,262,"/**
 * Creates a FileOutputStream with specified permissions.
 * @param f The file to create.
 * @param permissions File permissions to set.
 * @throws IOException If file exists or other I/O error occurs.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,fileToPath,org.apache.hadoop.security.token.DtFileOperations:fileToPath(java.io.File),93,95,"/**
 * Creates a Path object from the result of File's m1() and m2().
 */",Add the service prefix for a local filesystem.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,unnestUri,org.apache.hadoop.security.ProviderUtils:unnestUri(java.net.URI),80,101,"/**
 * Extracts the path from a URI, reconstructing it as a Path object.
 */","* Convert a nested URI to decode the underlying path. The translation takes
   * the authority and parses it into the underlying scheme and authority.
   * For example, ""myscheme://hdfs@nn/my/path"" is converted to
   * ""hdfs://nn/my/path"".
   * @param nestedUri the URI from the nested URI
   * @return the unnested path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,constructNewPath,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructNewPath(org.apache.hadoop.fs.Path),300,302,"/**
* Creates a new Path object by appending ""_NEW"" to the input path.
* @param path The original Path object.
* @return A new Path object with the modified name.
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,constructOldPath,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructOldPath(org.apache.hadoop.fs.Path),304,306,"/**
 * Creates a new Path with a modified name based on the input path.
 * @param path The original Path object.
 * @return A new Path object with a modified name.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,stringToPath,org.apache.hadoop.util.StringUtils:stringToPath(java.lang.String[]),273,282,"/**
 * Creates Path array from string array.
 * @param str array of strings to create Path objects from
 * @return Array of Path objects.
 */
","* stringToPath.
   * @param str str.
   * @return path array.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,makeQualified,org.apache.hadoop.fs.HarFileSystem:makeQualified(org.apache.hadoop.fs.Path),400,412,"/**
 * Constructs a Path object, potentially using an archive path.
 * @param path The input Path object.
 * @return A new Path object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getPathWithoutSchemeAndAuthority,org.apache.hadoop.fs.Path:getPathWithoutSchemeAndAuthority(org.apache.hadoop.fs.Path),104,111,"/**
* Modifies Path based on m1() result. Returns new Path or original.
*/","* Return a version of the given Path without the scheme information.
   *
   * @param path the source Path
   * @return a copy of this Path without the scheme information",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,mergePaths,"org.apache.hadoop.fs.Path:mergePaths(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",277,286,"/**
 * Combines paths, modifying path2 and incorporating it into the result.
 * @param path1 First path object.
 * @param path2 Second path object, modified during processing.
 * @return New Path object with combined data.
 */
","* Merge 2 paths such that the second path is appended relative to the first.
   * The returned path has the scheme and authority of the first path.  On
   * Windows, the drive specification in the second path is discarded.
   * 
   * @param path1 the first path
   * @param path2 the second path, to be appended relative to path1
   * @return the merged path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getParentUtil,org.apache.hadoop.fs.Path:getParentUtil(),444,459,"/**
 * Calculates the FUNC_MASK path based on URI components.
 * Returns a Path object or null if invalid.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,apply,"org.apache.hadoop.fs.shell.find.Print:apply(org.apache.hadoop.fs.shell.PathData,int)",59,63,"/**
* Executes a function with PathData and depth, returning PASS.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processPath,org.apache.hadoop.fs.shell.Display$Checksum:processPath(org.apache.hadoop.fs.shell.PathData),197,214,"/**
* Processes a PathData item, throwing exception if it's a directory.
* Calculates and displays checksum, optionally with block size.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,checkIfExists,org.apache.hadoop.fs.shell.PathData:checkIfExists(org.apache.hadoop.fs.shell.PathData$FileTypeRequirement),220,233,"/**
 * Validates path against specified file type requirement.
 * @param typeRequirement FileTypeRequirement to check against.
 */","* Ensure that the file exists and if it is or is not a directory
   * @param typeRequirement Set it to the desired requirement.
   * @throws PathIOException if file doesn't exist or the type does not match
   * what was specified in typeRequirement.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getStringForChildPath,org.apache.hadoop.fs.shell.PathData:getStringForChildPath(org.apache.hadoop.fs.Path),319,328,"/**
 * Constructs a masked path string based on the child path.
 * @param childPath Path object to mask
 * @return Masked path string
 */
","* Given a child of this directory, use the directory's path and the child's
   * basename to construct the string to the child.  This preserves relative
   * paths since Path will fully qualify.
   * @param childPath a path contained within this directory
   * @return String of the path relative to this directory",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPath,org.apache.hadoop.fs.shell.Ls:processPath(org.apache.hadoop.fs.shell.PathData),285,321,"/**
 * Writes PathData to output, conditionally displaying EC policy.
 * @param item The PathData object to write.
 * @throws IOException if an I/O error occurs.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,rename,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:rename(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",546,563,"/**
 * Renames a path to a target, deletes if exists, and cleans up source.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processPath,org.apache.hadoop.fs.shell.Delete$Rmdir:processPath(org.apache.hadoop.fs.shell.PathData),205,217,"/**
 * Checks if a path is a directory and optionally empty.
 * @param item PathData object containing path information.
 * @throws IOException if path is not a directory or not empty.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processPath,org.apache.hadoop.fs.shell.Mkdir:processPath(org.apache.hadoop.fs.shell.PathData),58,67,"/**
 * Checks if a path exists and throws exceptions if needed.
 * @param item PathData object containing path information.
 * @throws IOException if path exists or is not a directory.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),143,148,"/**
* Throws exception if PathData is not a directory.
* @param item The PathData object to check.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processNonexistentPath,org.apache.hadoop.fs.shell.Command:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),330,332,"/**
* Throws PathNotFoundException based on the provided PathData.
* @param item PathData object used to generate the exception.
*/","*  Provides a hook for handling paths that don't exist.  By default it
   *  will throw an exception.  Primarily overriden by commands that create
   *  paths such as mkdir or touch.
   *  @param item the {@link PathData} that doesn't exist
   *  @throws FileNotFoundException if arg is a path and it doesn't exist
   *  @throws IOException if anything else goes wrong...",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),57,62,"/**
 * Throws exception if PathData is not a directory.
 * @param item PathData object to validate.
 * @throws IOException if the path is not a directory.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processPath,"org.apache.hadoop.fs.shell.MoveCommands$Rename:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",111,128,"/**
 * Copies PathData, validating filesystem compatibility and existence.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),102,107,"/**
 * Throws exception if path is not a directory.
 * @param item PathData object containing path information.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processPath,org.apache.hadoop.fs.shell.Truncate:processPath(org.apache.hadoop.fs.shell.PathData),75,97,"/**
* Truncates a file to the specified length, handling directory and size limits.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processPath,org.apache.hadoop.fs.shell.SetReplication:processPath(org.apache.hadoop.fs.shell.PathData),81,103,"/**
* Handles path data, throwing exceptions or setting replication.
* @param item PathData object containing file system info.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processPath,"org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",61,68,"/**
 * Throws exception if target exists and m1() is true, else calls super.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,postProcessPath,org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:postProcessPath(org.apache.hadoop.fs.shell.PathData),70,78,"/**
* Checks if the path exists; throws PathIOException if not.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFSofPath,org.apache.hadoop.fs.FileContext:getFSofPath(org.apache.hadoop.fs.Path),325,337,"/**
 * Gets the AbstractFileSystem for the given path.
 * @param absOrFqPath Path to resolve; validates and resolves it.
 * @return AbstractFileSystem object.
 */","* Get the file system of supplied path.
   * 
   * @param absOrFqPath - absolute or fully qualified path
   * @return the file system of the path
   * 
   * @throws UnsupportedFileSystemException If the file system for
   *           <code>absOrFqPath</code> is not supported.
   * @throws IOException If the file system for <code>absOrFqPath</code> could
   *         not be instantiated.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFs:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)",102,122,"/**
 * Creates a ChRootedFs instance with the given filesystem and root path.
 * @param fs the AbstractFileSystem
 * @param theRoot the root path for the chroot
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getUriPath,org.apache.hadoop.fs.FilterFs:getUriPath(org.apache.hadoop.fs.Path),189,192,"/**
* Delegates the Path m1 call to the underlying FileSystem.
* @param p The Path object to process.
* @return String result from the FileSystem's m1 method.
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,resolvePath,org.apache.hadoop.fs.viewfs.ViewFs:resolvePath(org.apache.hadoop.fs.Path),328,338,"/**
 * Resolves a Path by delegating to the target file system.
 * @param f The Path to resolve.
 * @return Resolved Path or null if resolution fails.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,resolvePath,org.apache.hadoop.fs.FilterFs:resolvePath(org.apache.hadoop.fs.Path),168,172,"/**
* Delegates path retrieval to the underlying FileSystem.
* @param p The Path to retrieve.
* @return The retrieved Path.
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createInternal,"org.apache.hadoop.fs.FilterFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",88,97,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to the file. Returns FSDataOutputStream.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,delete,"org.apache.hadoop.fs.FilterFs:delete(org.apache.hadoop.fs.Path,boolean)",99,104,"/**
 * Delegates file processing to FileSystem.
 * @param f Path to the file. @param recursive Recursive processing flag.
 * @return True if successful, false otherwise.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileBlockLocations,"org.apache.hadoop.fs.FilterFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",106,111,"/**
 * Delegates block location retrieval to FileSystem.
 * @param f Path to file. @param start Start offset. @param len Length.
 * @return BlockLocation[] array.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileChecksum,org.apache.hadoop.fs.FilterFs:getFileChecksum(org.apache.hadoop.fs.Path),113,118,"/**
 * Calculates checksum of a file using the file system.
 * @param f Path to the file.
 * @return FileChecksum object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileStatus,org.apache.hadoop.fs.FilterFs:getFileStatus(org.apache.hadoop.fs.Path),120,125,"/**
 * Retrieves a FileStatus for the given path.
 * Delegates to the underlying FileSystem's m2 method.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileLinkStatus,org.apache.hadoop.fs.FilterFs:getFileLinkStatus(org.apache.hadoop.fs.Path),139,144,"/**
 * Retrieves FileStatus for a given path.
 * Delegates to the underlying FileSystem.
 * @param f the path to get the FileStatus for
 * @return FileStatus object
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listStatus,org.apache.hadoop.fs.FilterFs:listStatus(org.apache.hadoop.fs.Path),194,199,"/**
 * Retrieves FileStatus for a given path.
 * @param f Path to retrieve status for.
 * @return An array of FileStatus objects.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listLocatedStatus,org.apache.hadoop.fs.FilterFs:listLocatedStatus(org.apache.hadoop.fs.Path),201,207,"/**
 * Gets a remote iterator for files located at the given path.
 * @param f the path to iterate over
 * @return RemoteIterator of LocatedFileStatus objects
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,mkdir,"org.apache.hadoop.fs.FilterFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",215,221,"/**
 * Creates a directory.
 * @param dir Path to the directory.
 * @param permission Permissions for the directory.
 * @param createParent Whether to create parent directories.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,open,org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path),223,228,"/**
 * Opens an input stream for the file at the given path.
 * @param f Path to the file
 * @return FSDataInputStream for reading the file
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,open,"org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path,int)",230,235,"/**
 * Opens an input stream for the given file.
 * @param f Path to the file.
 * @param bufferSize Buffer size for the stream.
 * @return FSDataInputStream object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,truncate,"org.apache.hadoop.fs.FilterFs:truncate(org.apache.hadoop.fs.Path,long)",237,243,"/**
 * Resizes a file to a new length using the file system.
 * @param f Path to the file. @param newLength New file length.
 * @return True if successful, false otherwise.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setOwner,"org.apache.hadoop.fs.FilterFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",261,267,"/**
* Calls m1 and then delegates to myFs.m2 with provided parameters.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setPermission,"org.apache.hadoop.fs.FilterFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",269,274,"/**
 * Sets file permissions using the file system.
 * @param f Path to the file.
 * @param permission File system permission object.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setReplication,"org.apache.hadoop.fs.FilterFs:setReplication(org.apache.hadoop.fs.Path,short)",276,281,"/**
 * Delegates file replication to the file system.
 * @param f Path to the file. @param replication Replication factor.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setTimes,"org.apache.hadoop.fs.FilterFs:setTimes(org.apache.hadoop.fs.Path,long,long)",283,288,"/**
 * Updates file metadata (mtime, atime) using the file system.
 * @param f Path to the file.
 * @param mtime Modification time.
 * @param atime Access time.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,mkdirs,"org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",764,771,"/**
 * Sets permissions on a directory.
 * @param fs Filesystem object.
 * @param dir Path to the directory.
 * @param permission FsPermission to set.
 * @return True if operation was successful.
 */
","* Create a directory with the provided permission.
   * The permission of the directory is set to be the provided permission as in
   * setPermission, not permission{@literal &~}umask
   *
   * @see #create(FileSystem, Path, FsPermission)
   *
   * @param fs FileSystem handle
   * @param dir the name of the directory to be created
   * @param permission the permission of the directory
   * @return true if the directory creation succeeds; false otherwise
   * @throws IOException A problem creating the directories.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,mkdirs,org.apache.hadoop.fs.ChecksumFileSystem:mkdirs(org.apache.hadoop.fs.Path),986,989,"/**
* Delegates m1 operation to the file system.
* @param f The Path object to operate on.
* @return True if successful, false otherwise.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,mkdirs,org.apache.hadoop.fs.FilterFileSystem:mkdirs(org.apache.hadoop.fs.Path),339,342,"/**
 * Delegates m1 operation to the underlying file system.
 * @param f The path to operate on.
 * @return True if successful, false otherwise.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FileStatusProto),49,106,"/**
 * Creates a FileStatus from a FileStatusProto.
 * @param proto FileStatusProto containing file status data
 * @return FileStatus object representing the file status
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean)",151,158,"/**
 * Constructs a FileStatus with default attributes.
 * @param length File length, isdir, replication, blocksize, etc.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set,org.apache.hadoop.fs.BlockLocation[])",142,149,"/**
 * Constructs a LocatedFileStatus with file details and block locations.
 * @param locations Array of BlockLocation objects.
 */
","* Constructor.
   *
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param attr Attribute flags (See {@link FileStatus.AttrFlags}).
   * @param locations a file's block locations",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getPermission,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getPermission(),59,62,"/**
* Delegates the call to the parent class's m1() method.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$Writer:append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",1458,1461,"/**
* Delegates to the object-based m1 method.
* @param key The key object.
* @param val The value object.
*/","* Append a key/value pair.
     * @param key input Writable key.
     * @param val input Writable val.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeFile,"org.apache.hadoop.io.SequenceFile$Sorter:writeFile(org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator,org.apache.hadoop.io.SequenceFile$Writer)",3431,3438,"/**
* Writes records from iterator to writer.
* @param records Iterator of raw key-value records.
* @param writer Writer to write the records to.
*/
","* Writes records from RawKeyValueIterator into a file represented by the 
     * passed writer.
     * @param records the RawKeyValueIterator
     * @param writer the Writer created earlier 
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,init,org.apache.hadoop.service.AbstractService:init(org.apache.hadoop.conf.Configuration),152,178,"/**
* Initializes the service with the given configuration.
* Throws ServiceStateException if config is null or init fails.
*/","* {@inheritDoc}
   * This invokes {@link #serviceInit}
   * @param conf the configuration of the service. This must not be null
   * @throws ServiceStateException if the configuration was null,
   * the state change not permitted, or something else went wrong",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,stop,org.apache.hadoop.service.AbstractService:stop(),213,240,"/**
* Stops the service, handling re-entry and potential exceptions.
*/",* {@inheritDoc},,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Metadata:equals(org.apache.hadoop.io.SequenceFile$Metadata),797,820,"/**
 * Compares this Metadata object with another.
 * @param other The Metadata object to compare to.
 * @return True if objects are equal, false otherwise.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,handleKind,org.apache.hadoop.security.token.Token$TrivialRenewer:handleKind(org.apache.hadoop.io.Text),528,531,"/**
* Checks a Text kind against a derived value.
* @param kind The Text kind to check.
* @return True if the kind matches, false otherwise.
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSelector.java,selectToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)",46,60,"/**
 * Finds a token matching service and kind.
 * @param service The service to match.
 * @param tokens Tokens to search through.
 * @return Matching token or null if not found.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,isPrivateCloneOf,org.apache.hadoop.security.token.Token$PrivateToken:isPrivateCloneOf(org.apache.hadoop.io.Text),279,282,"/**
* Delegates processing of a Text object to publicService.
* @param thePublicService The Text object to process.
* @return True if processing was successful, false otherwise.
*/
","* Whether this is a private clone of a public token.
     * @param thePublicService the public service name
     * @return true when the public service is the same as specified",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,equals,org.apache.hadoop.security.token.Token:equals(java.lang.Object),386,400,"/**
 * Checks if this Token is equal to the given Token.
 * @param right The Token to compare with.
 * @return True if tokens are equal, false otherwise.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,matchAlias,"org.apache.hadoop.security.token.DtFileOperations:matchAlias(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",73,75,"/**
 * Checks if the token's alias matches the given alias.
 * @param token The token to check.
 * @param alias The alias to compare.
 * @return True if alias is null or matches; otherwise false.
 */
",Match token service field to alias text.  True if alias is null.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,matchService,"org.apache.hadoop.security.token.DtFileOperations:matchService(org.apache.hadoop.security.token.DtFetcher,org.apache.hadoop.io.Text,java.lang.String)",78,83,"/**
 * Checks service/URL based on fetcher's protocol.
 * @param fetcher DtFetcher object.
 * @param service Text service, can be null.
 * @param url URL string.
 * @return True if condition is met.
 */
",Match fetcher's service name to the service text and/or url prefix.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,selectDelegationToken,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text)",1008,1018,"/**
 * Selects a token based on credentials and service.
 * @param creds Credentials object.
 * @param service Service identifier.
 * @return Token object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,handleKind,org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:handleKind(org.apache.hadoop.io.Text),179,182,"/**
* Checks if the given text kind matches the defined token kind.
* @param kind The text kind to check.
* @return True if kinds match, false otherwise.
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDir,org.apache.hadoop.util.DiskChecker:checkDir(java.io.File),76,78,"/**
 * Processes a directory.
 * @param dir The directory to process.
 * @throws DiskErrorException if an error occurs during processing.
 */
","* Create the directory if it doesn't exist and check that dir is readable,
   * writable and executable
   *  
   * @param dir dir.
   * @throws DiskErrorException disk problem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirWithDiskIo,org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(java.io.File),88,92,"/**
 * Processes a directory by calling m1 and m2.
 * @param dir The directory to process.
 * @throws DiskErrorException if a disk error occurs.
 */
","* Create the directory if it doesn't exist and check that dir is
   * readable, writable and executable. Perform some disk IO to
   * ensure that the disk is usable for writes.
   *
   * @param dir dir.
   * @throws DiskErrorException disk problem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flushBuffer,org.apache.hadoop.fs.FSOutputSummer:flushBuffer(),145,147,"/**
* Calls m1 with default values for enable and proceed.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flush,org.apache.hadoop.fs.FSOutputSummer:flush(),183,185,"/**
 * Calls m1 with false arguments.
 */","* Checksums all complete data chunks and flushes them to the underlying
   * stream. If there is a trailing partial chunk, it is not flushed and is
   * maintained in the buffer.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,doDecodeSingle,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeSingle(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,boolean)",123,222,"/**
 * Decodes data with Reed-Solomon, handling erased locations.
 * @param inputs Input ByteBuffer array
 * @param outputs Output ByteBuffer array
 * @param erasedLocationToFix Location of erased data to fix
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,doDecodeMultiAndParity,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeMultiAndParity(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int[],int)",265,351,"/**
 * Decodes data, handles erasure, and piggybacks parity information.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingValidator.java,validate,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",73,128,"/**
 * Processes input buffers, erases data, and decodes with validation.
 */","* Validate outputs decoded from inputs, by decoding an input back from
   * the outputs and comparing it with the original one.
   *
   * For instance, in RS (6, 3), let (d0, d1, d2, d3, d4, d5) be sources
   * and (p0, p1, p2) be parities, and assume
   *  inputs = [d0, null (d1), d2, d3, d4, d5, null (p0), p1, null (p2)];
   *  erasedIndexes = [1, 6];
   *  outputs = [d1, p0].
   * Then
   *  1. Create new inputs, erasedIndexes and outputs for validation so that
   *     the inputs could contain the decoded outputs, and decode them:
   *      newInputs = [d1, d2, d3, d4, d5, p0]
   *      newErasedIndexes = [0]
   *      newOutputs = [d0']
   *  2. Compare d0 and d0'. The comparison will fail with high probability
   *     when the initial outputs are wrong.
   *
   * Note that the input buffers' positions must be the ones where data are
   * read: If the input buffers have been processed by a decoder, the buffers'
   * positions must be reset before being passed into this method.
   *
   * This method does not change outputs and erasedIndexes.
   *
   * @param inputs input buffers used for decoding. The buffers' position
   *               are moved to the end after this method.
   * @param erasedIndexes indexes of erased units used for decoding
   * @param outputs decoded output buffers, which are ready to be read after
   *                the call
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])",168,173,"/**
* Processes EC chunks, converting to ByteBuffers and calling m2.
* @param inputs ECChunk array, @param erasedIndexes erasure indices
* @param outputs ECChunk array for results
*/","* Decode with inputs and erasedIndexes, generates outputs. More see above.
   *
   * Note, for both input and output ECChunks, no mixing of on-heap buffers and
   * direct buffers are allowed.
   *
   * @param inputs input buffers to read data from
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException if the decoder is closed",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",55,68,"/**
 * Processes inputs, calls m1 and super.m2 with modified arrays.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(byte[][],int[],byte[][])",70,83,"/**
* Processes inputs, calls m1, and invokes super.m2 with new arrays.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),73,84,"/**
 * Decodes data using Reed-Solomon, preparing inputs and outputs.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),86,101,"/**
 * Decodes data using provided state, inputs, and RSUtil.
 * @param decodingState State containing input data and offsets.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,initBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:initBlock(),511,570,"/**
 * Processes a data block based on read mode and magic number check.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,internalReset,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:internalReset(),274,280,"/**
 * Resets the needsReset flag and reinitializes the output stream.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,writeRun,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:writeRun(),654,706,"/**
* Processes data blocks based on runLength, updating data & CRC.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Merger:close(),1148,1157,"/**
 * Closes all input readers and the output writer, releasing resources.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:close(),256,271,"/**
 * Registers a block, handling errors and ensuring closure.
 */","* Signaling the end of write to the block. The block register will be
       * called for registering the finished block.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cleanup,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:cleanup(),3887,3892,"/**
* Executes m1 and optionally deletes the segment file.
* @throws IOException if an I/O error occurs
*/
","* The default cleanup. Subclasses can override this with a custom
       * cleanup.
       * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:close(),557,569,"/**
 * Executes a masked function, ensuring it's not closed.
 * Sets closed to true after execution, regardless of success.
 */",* Finishing reading the block. Release all resources.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeWritableOutputStream,org.apache.hadoop.security.Credentials:writeWritableOutputStream(java.io.DataOutputStream),321,326,"/**
 * Writes header information to the DataOutputStream.
 * @param os Output stream to write the header to.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readFields,org.apache.hadoop.security.Credentials:readFields(java.io.DataInput),420,443,"/**
* Reads token and secret key data from input stream.
* Populates tokenMap and secretKeysMap with read data.
*/","* Loads all the keys.
   * @param in DataInput.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,createTokenIdent,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenIdent(byte[]),253,260,"/**
 * Parses a token identifier from byte array.
 * @param tokenIdentBytes byte array containing the token identifier
 * @return TokenIdent object parsed from the byte array
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processTokenAddOrUpdate,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenAddOrUpdate(byte[]),411,427,"/**
* Parses token data, creates a token, and stores it.
* @param data byte array containing token information
* @return TokenIdent object or null if parsing fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processTokenRemoved,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenRemoved(org.apache.curator.framework.recipes.cache.ChildData),429,435,"/**
 * Processes ChildData, reads tokens from stream, and adds them.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(java.lang.String,boolean)",652,679,"/**
 * Retrieves DelegationTokenInformation from ZooKeeper node.
 * @param nodePath Node path in ZooKeeper.
 * @param quiet If true, suppresses error logging.
 * @return DelegationTokenInformation object or null if not found.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawKey,org.apache.hadoop.io.SequenceFile$Reader:nextRawKey(org.apache.hadoop.io.DataOutputBuffer),2662,2697,"/**
 * Reads a key from the input buffer and writes it to the output buffer.
 * @param key DataOutputBuffer to write the key to.
 * @return Key length or -1 if error.
 */","* Read 'raw' keys.
     * @param key - The buffer into which the key is read
     * @return Returns the key length or -1 for end of file
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(org.apache.hadoop.io.Writable),2376,2408,"/**
* Processes a Writable value, potentially configuring it and handling data.
*/","* Get the 'value' corresponding to the last read 'key'.
     * @param val : The 'value' to be read.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(java.lang.Object),2415,2448,"/**
* Masks a value, potentially configuring it and handling compression.
* @param val The value to mask.
* @return The masked value.
*/
","* @return Get the 'value' corresponding to the last read 'key'.
     * @param val : The 'value' to be read.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRaw,"org.apache.hadoop.io.SequenceFile$Reader:nextRaw(org.apache.hadoop.io.DataOutputBuffer,org.apache.hadoop.io.SequenceFile$ValueBytes)",2603,2654,"/**
 * Reads key and value from input stream, decompresses if needed.
 * @param key DataOutputBuffer for the key
 * @param val ValueBytes object for the value
 * @return Length of the read data or -1 if error
 */","* Read 'raw' records.
     * @param key - The buffer into which the key is read
     * @param val - The 'raw' value
     * @return Returns the total record length or -1 for end of file
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawValue,org.apache.hadoop.io.SequenceFile$Reader:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes),2765,2790,"/**
* Processes data bytes, decompressing if needed, and returns length.
*/","* Read 'raw' values.
     * @param val - The 'raw' value
     * @return Returns the value length
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getTokenInfoFromSQL,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getTokenInfoFromSQL(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),238,251,"/**
 * Retrieves delegation token information from the SQL secret manager.
 * @param ident TokenIdent object containing token identifiers.
 * @throws RuntimeException if token retrieval fails.
 */
","* Obtains the DelegationTokenInformation associated with the given
   * TokenIdentifier in the SQL database.
   * @param ident Existing TokenIdentifier in the SQL database.
   * @return DelegationTokenInformation that matches the given TokenIdentifier or
   *         null if it doesn't exist in the database.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,read,org.apache.hadoop.fs.permission.PermissionStatus:read(java.io.DataInput),114,118,"/**
 * Creates and populates a PermissionStatus object from DataInput.
 * @param in DataInput stream to read permission data from.
 * @return Populated PermissionStatus object.
 */
","* Create and initialize a {@link PermissionStatus} from {@link DataInput}.
   * @param in data input.
   * @throws IOException raised on errors performing I/O.
   * @return PermissionStatus.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readEnum,"org.apache.hadoop.io.WritableUtils:readEnum(java.io.DataInput,java.lang.Class)",422,425,"/**
* Reads an enum value from DataInput.
* @param in DataInput to read from; @param enumType Enum type.
* @return Enum value or null.
*/
","* Read an Enum value from DataInput, Enums are read and written 
   * using String values. 
   * @param <T> Enum type
   * @param in DataInput to read from 
   * @param enumType Class type of Enum
   * @return Enum represented by String read from DataInput
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,readFields,org.apache.hadoop.security.authorize.AccessControlList:readFields(java.io.DataInput),326,330,"/**
* Processes an ACL string from input.
* @param in DataInput stream containing ACL string
* @throws IOException if an I/O error occurs
*/
",* Deserializes the AccessControlList object,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getDelegationKey(int),561,577,"/**
 * Retrieves a DelegationKey by ID, fetching from ZK if needed.
 * @param keyId The ID of the DelegationKey to retrieve.
 * @return The DelegationKey object or null if not found.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:write(java.io.DataOutput),790,796,"/**
 * Writes index data to the output stream.
 * @param out DataOutput to write to.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text),85,87,"/**
* Delegates to ShadedProtobufHelper.m1 to create a ByteString.
* @param key The input Text key.
* @return ByteString generated by the helper method.
*/
","* Get the ByteString for frequently used fixed and small set strings.
   * @param key string
   * @return the ByteString for frequently used fixed and small set strings.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,protoFromToken,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token),142,149,"/**
 * Creates a TokenProto from a Token object.
 * Populates fields from Token properties.
 */
","* Create a {@code TokenProto} instance
   * from a hadoop token.
   * This builds and caches the fields
   * (identifier, password, kind, service) but not
   * renewer or any payload.
   * @param tok token
   * @return a marshallable protobuf class.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,copyToken,org.apache.hadoop.security.token.Token:copyToken(),114,116,"/**
 * Creates a new token associated with the current object.
 * @return A new Token object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",160,164,"/**
 * Delegates to overloaded method with null delegation token.
 * @param ugi UserGroupInformation object
 * @param renewer Renewer string
 * @return Token object
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenIdentifier.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",49,53,"/**
 * Constructs a DelegationTokenIdentifier with kind, owner, renewer, realUser.
 */
","* Create a new delegation token identifier
   *
   * @param kind token kind
   * @param owner the effective username of the token owner
   * @param renewer the username of the renewer
   * @param realUser the real username of the token owner",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>(),51,53,"/**
 * Default constructor. Initializes with null values for all identifiers.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadWriteLock:<init>(boolean,java.lang.String,org.slf4j.Logger,long,long)",40,47,"/**
 * Constructs an InstrumentedReadWriteLock with specified fairness.
 * @param fair fairness of the lock
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeOnce,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce(),89,117,"/**
 * Executes a call, retries if needed, handling failures and exceptions.
 * @return CallReturn object containing result or exception.
 */",Invoke the call once without retrying.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,checkKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:checkKey(),1596,1611,"/**
 * Reads a key-value pair. Throws EOFException if no data.
 */","* check whether we have already successfully obtained the key. It also
       * initializes the valueInputStream.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(org.apache.hadoop.io.BytesWritable),1706,1720,"/**
 * Copies data from BytesWritable to DataInputStream, returns size.
 */","* Copy the value into BytesWritable. The input BytesWritable will be
         * automatically resized to the actual value size. The implementation
         * directly uses the buffer inside BytesWritable for storing the value.
         * The call does not require the value length to be known.
         * 
         * @param value value.
         * @throws IOException raised on errors performing I/O.
         * @return long value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,writeValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:writeValue(java.io.OutputStream),1747,1763,"/**
* Copies data from input stream to output stream in chunks.
* @param out Output stream to write data to.
* @return Total number of bytes transferred.
*/
","* Writing the value to the output stream. This method avoids copying
         * value data from Scanner into user buffer, then writing to the output
         * stream. It does not require the value length to be known.
         * 
         * @param out
         *          The output stream
         * @return the length of the value
         * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[]),146,149,"/**
* Calls overloaded method with full byte array.
* @param b byte array to process; returns int.
* @throws IOException if an I/O error occurs
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,close,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:close(),186,197,"/**
 * Executes a process if not closed, then sets closed to true.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[]),1932,1934,"/**
* Calls overloaded method with full buffer.
* @param buf byte array to process
* @return Result of processing the byte array
*/
","* Compare the entry key to another key. Synonymous to compareTo(key, 0,
         * key.length).
         * 
         * @param buf
         *          The key buffer.
         * @return comparison result between the entry key with the input key.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,equals,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:equals(java.lang.Object),1966,1971,"/**
 * Checks if two Entry objects are equal by comparing m2 results.
 */",* Compare whether this and other points to the same key value.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String,java.lang.String)",188,203,"/**
* Retrieves and validates a delegation token from a URL.
* @param url URL to fetch token from, renewer, doAsUser
* @return A delegation token object.
*/","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return abstract delegation token identifier.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)",237,245,"/**
 * Renews a delegation token and returns the new expiration timestamp.
 * @param url URL to renew token.
 * @param token Authentication token.
 */
","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param doAsUser the user to do as, which will be the token owner.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)",275,286,"/**
 * Cancels a delegation token using the provided URL, token, and user.
 * @param url URL for cancellation, token, dToken, doAsUser
 * @throws IOException if an I/O error occurs
 */
","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param dToken abstract delegation token identifier.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String),468,470,"/**
 * Calls m1 with the given scope and a null value for the data.
 * @param scope The scope to use.
 * @return A Node object.
 */
","* Randomly choose a node.
   *
   * @param scope range of nodes from which a node will be chosen
   * @return the chosen node
   *
   * @see #chooseRandom(String, Collection)",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,sortByDistance,"org.apache.hadoop.net.NetworkTopologyWithNodeGroup:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",285,300,"/**
* Adjusts the reader based on node group, then calls super.m5.
*/","* Sort nodes array by their distances to <i>reader</i>.
   * <p>
   * This is the same as {@link NetworkTopology#sortByDistance(Node, Node[],
   * int)} except with a four-level network topology which contains the
   * additional network distance of a ""node group"" which is between local and
   * same rack.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getInputStream,"org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket,long)",496,503,"/**
 * Creates a SocketInputWrapper with a timeout.
 * @param socket The socket to wrap.
 * @param timeout Timeout in milliseconds.
 * @return SocketInputWrapper object.
 */
","* Return a {@link SocketInputWrapper} for the socket and set the given
   * timeout. If the socket does not have an associated channel, then its socket
   * timeout will be set to the specified value. Otherwise, a
   * {@link SocketInputStream} will be created which reads with the configured
   * timeout.
   * 
   * Any socket created using socket factories returned by {@link #NetUtils},
   * must use this interface instead of {@link Socket#getInputStream()}.
   * 
   * In general, this should be called only once on each socket: see the note
   * in {@link SocketInputWrapper#setTimeout(long)} for more information.
   *
   * @see Socket#getChannel()
   * 
   * @param socket socket.
   * @param timeout timeout in milliseconds. zero for waiting as
   *                long as necessary.
   * @return SocketInputWrapper for reading from the socket.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getOutputStream,"org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket,long)",550,554,"/**
 * Returns OutputStream; uses socket's if not null, else creates new.
 * @param socket The socket to use.
 * @param timeout Timeout value for the OutputStream.
 * @return OutputStream object.
 */
","* Returns OutputStream for the socket. If the socket has an associated
   * SocketChannel then it returns a 
   * {@link SocketOutputStream} with the given timeout. If the socket does not
   * have a channel, {@link Socket#getOutputStream()} is returned. In the later
   * case, the timeout argument is ignored and the write will wait until 
   * data is available.<br><br>
   * 
   * Any socket created using socket factories returned by {@link NetUtils},
   * must use this interface instead of {@link Socket#getOutputStream()}.
   * 
   * @see Socket#getChannel()
   * 
   * @param socket socket.
   * @param timeout timeout in milliseconds. This may not always apply. zero
   *        for waiting as long as necessary.
   * @return OutputStream for writing to the socket.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,writeMetric,org.apache.hadoop.metrics2.sink.StatsDSink:writeMetric(java.lang.String),150,157,"/**
 * Sends a metric to StatsD, handling potential IO exceptions.
 * @param line The metric string to send.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,<init>,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:<init>(),38,40,"/**
 * Constructs a NetworkTopologyWithNodeGroup with root node.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,connect,"org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,java.net.SocketAddress,int)",590,636,"/**
* Connects a socket to an endpoint, handling timeouts and loopback detection.
*/","* Like {@link NetUtils#connect(Socket, SocketAddress, int)} but
   * also takes a local address and port to bind the socket to. 
   * 
   * @param socket socket.
   * @param endpoint the remote address
   * @param localAddr the local address to bind the socket to
   * @param timeout timeout in milliseconds
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,long,org.apache.hadoop.metrics2.impl.MetricsConfig)",90,98,"/**
 * Constructs a MetricsSourceAdapter with default filters and startup flag.
 * @param prefix Metric name prefix.
 * @param name Metric name.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,snapshotMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:snapshotMetrics(org.apache.hadoop.metrics2.impl.MetricsSourceAdapter,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder)",420,427,"/**
 * Snapshots metrics source data to buffer, updates stats, logs.
 * @param sa Source adapter.
 * @param bufferBuilder Buffer builder.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateJmxCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateJmxCache(),160,194,"/**
* Refreshes JMX metrics cache based on TTL and clears records.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,register,"org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.util.Map,java.lang.Object)",89,114,"/**
 * Registers an MBean with the MBeanServer, returning the ObjectName.
 * @param serviceName MBean service name
 * @param nameName MBean name
 * @param properties MBean properties
 * @param theMbean The MBean to register
 * @return ObjectName of registered MBean, or null on failure.
 */
","* Register the MBean using our standard MBeanName format
   * ""hadoop:service={@literal <serviceName>,name=<nameName>}""
   * Where the {@literal <serviceName> and <nameName>} are the supplied
   * parameters.
   *
   * @param serviceName serviceName.
   * @param nameName nameName.
   * @param properties - Key value pairs to define additional JMX ObjectName
   *                     properties.
   * @param theMbean    - the MBean to register
   * @return the named used to register the MBean",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,unregisterSource,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:unregisterSource(java.lang.String),245,258,"/**
 * Processes a named resource, performing actions based on source.
 * @param name The name of the resource to process.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopSources,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSources(),460,469,"/**
 * Stops all metrics sources and clears the sources list.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newInverseQuantiles,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newInverseQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",240,251,"/**
 * Creates and registers a MutableQuantiles object.
 * @param name Quantiles name, @param interval Interval size.
 * @return Registered MutableQuantiles object.
 */","* Create a mutable inverse metric that estimates inverse quantiles of a stream of values
   * @param name of the metric
   * @param desc metric description
   * @param sampleName of the metric (e.g., ""Ops"")
   * @param valueName of the metric (e.g., ""Rate"")
   * @param interval rollover interval of estimator in seconds
   * @return a new inverse quantile estimator object
   * @throws MetricsException if interval is not a positive integer",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,<init>,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:<init>(),53,71,"/**
 * Initializes the read/write disk validator metrics with quantile gauges.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache:<init>(java.lang.String,double,long)",196,204,"/**
 * Creates a RetryCache with specified name, percentage, and expiration.
 */","* Constructor
   * @param cacheName name to identify the cache by
   * @param percentage percentage of total java heap space used by this cache
   * @param expirationTime time for an entry to expire in nanoseconds",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class),71,81,"/**
 * Processes a protocol class, logging and triggering actions for methods.
 * @param protocol The class representing the protocol to process.
 */
","* Initialize the registry with all the methods in a protocol
   * so they all show up in the first snapshot.
   * Convenient for JMX implementations.
   * @param protocol the protocol class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.String[]),89,93,"/**
 * Iterates through names and calls m1 for each name.
 * @param names Array of names to process.
 */","* Initialize the registry with all rate names passed in.
   * This is an alternative to the above init function since this metric
   * can be used more than just for rpc name.
   * @param names the array of all rate names",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,aggregateLocalStatesToGlobalMetrics,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:aggregateLocalStatesToGlobalMetrics(java.util.concurrent.ConcurrentMap),149,157,"/**
* Updates global metrics with stats from local map.
* @param localStats Map of stat names to ThreadSafeSampleStat objects.
*/
","* Aggregates the thread's local samples into the global metrics. The caller
   * should ensure its thread safety.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean)",310,312,"/**
* Calls m1 with extended set to true.
* @param name Rate name.
* @param desc Rate description.
* @param extended Extended flag.
* @return MutableRate object.
*/
","* Create a mutable rate metric (for throughput measurement).
   * @param name  of the metric
   * @param desc  description
   * @param extended  produce extended stat (stdev/min/max etc.) if true
   * @return a new mutable rate metric object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,init,org.apache.hadoop.metrics2.lib.MutableRates:init(java.lang.Class),58,69,"/**
 * Registers methods of a protocol with the rate metrics registry.
 * @param protocol Class representing the protocol to register.
 */","* Initialize the registry with all the methods in a protocol
   * so they all show up in the first snapshot.
   * Convenient for JMX implementations.
   * @param protocol the protocol class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,<init>,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:<init>(java.lang.String),53,58,"/**
 * Initializes detailed metrics for the DecayRpcScheduler.
 * @param ns Namespace tag for metrics, identifies the RPC port.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,<init>,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:<init>(int),57,62,"/**
 * Initializes RPC detailed metrics with the given port.
 * @param port The RPC port to track metrics for.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addResponseTime,"org.apache.hadoop.ipc.DecayRpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",747,770,"/**
* Records response time metrics for a scheduled task.
* @param callName Call identifier, schedulable task, processing details.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,updateDeferredMetrics,"org.apache.hadoop.ipc.Server:updateDeferredMetrics(java.lang.String,long)",670,673,"/**
 * Records RPC metrics with name and processing time.
 * @param name RPC name.
 * @param processingTime Processing time in milliseconds.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,updateMetrics,"org.apache.hadoop.ipc.Server:updateMetrics(org.apache.hadoop.ipc.Server$Call,long,boolean)",617,668,"/**
* Records call processing details and metrics.
* @param call The call object containing processing info.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,add,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Method)",162,170,"/**
 * Processes methods with Metric annotations, registering them.
 * @param source The source object.
 * @param method The method to process.
 */
",Add {@link MutableMetric} for a method annotated with {@link Metric},,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getMetrics,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",963,969,"/**
 * Delegates m2 call to the delegate's scheduler, if available.
 * @param collector Metrics collector object.
 * @param all Flag to indicate if all metrics should be collected.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKeyLength,org.apache.hadoop.security.KDiag:validateKeyLength(),442,450,"/**
* Checks AES key length and logs a warning if insufficient.
* @throws NoSuchAlgorithmException if AES algorithm is unavailable
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateUGI,"org.apache.hadoop.security.KDiag:validateUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",697,706,"/**
 * Logs Kerberos authentication issues for a user.
 * @param messagePrefix Prefix for log messages.
 * @param user UserGroupInformation object.
 */","* Validate the UGI: verify it is kerberized.
   * @param messagePrefix message in exceptions
   * @param user user to validate",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verifyFileIsValid,"org.apache.hadoop.security.KDiag:verifyFileIsValid(java.io.File,java.lang.String,java.lang.String)",795,805,"/**
 * Checks file existence, type, emptiness, and readability.
 * @param file The file to check.
 * @param category Category for logging.
 * @param text Text to include in log messages.
 */
","* Verify that a file is valid: it is a file, non-empty and readable.
   * @param file file
   * @param category category for exceptions
   * @param text text message
   * @return true if the validation held; false if it did not <i>and</i>
   * {@link #nofail} has disabled raising exceptions.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateShortName,org.apache.hadoop.security.KDiag:validateShortName(),459,476,"/**
* Gets Kerberos short name, logs errors, and throws exceptions.
*/","* Verify whether auth_to_local rules transform a principal name
   * <p>
   * Having a local user name ""bar@foo.com"" may be harmless, so it is noted at
   * info. However if what was intended is a transformation to ""bar""
   * it can be difficult to debug, hence this check.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,getUser,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getUser(),71,87,"/**
* Retrieves UserGroupInformation based on owner and realUser.
* Returns null if conditions are not met; uses m4/m5 internally.
*/","* Get the username encoded in the token identifier
   * 
   * @return the username or owner",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,getUgi,org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto),133,150,"/**
 * Creates UserGroupInformation based on userInfo fields.
 * Returns UGI, null if no effective user is found.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,<init>,org.apache.hadoop.fs.LocalFileSystem:<init>(org.apache.hadoop.fs.FileSystem),70,72,"/**
 * Constructs a LocalFileSystem using a provided FileSystem.
 * @param rawLocalFileSystem The FileSystem to wrap.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,<init>,org.apache.hadoop.fs.shell.find.Find:<init>(),162,164,"/**
 * Initializes the Find object, enabling recursive searching.
 */
",Default constructor for the Find command.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,<init>,org.apache.hadoop.fs.shell.Ls:<init>(),120,120,"/**
 * Protected constructor, prevents external instantiation of Ls.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,<init>,org.apache.hadoop.fs.shell.Count:<init>(),110,110,"/**
 * Default constructor for the Count class. Does nothing.
 */
",Constructor,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createEncoder(),38,41,"/**
 * Creates and returns an ErasureEncoder instance using m1().
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createEncoder(),38,41,"/**
* Creates and returns an ErasureEncoder instance using m1().
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createEncoder(),36,39,"/**
 * Creates and returns a DummyErasureEncoder using m1().
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createEncoder(),39,42,"/**
 * Creates and returns a new XORErasureEncoder using m1().
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createDecoder(),41,44,"/**
 * Creates and returns a new DummyErasureDecoder instance.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createDecoder(),44,47,"/**
* Creates and returns a new XORErasureDecoder using m1().
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createDecoder(),43,46,"/**
* Creates and returns a new RSErasureDecoder using m1().
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createDecoder(),43,46,"/**
 * Creates and returns a new HHXORErasureDecoder instance.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,checkAndUpdateMaps,org.apache.hadoop.security.ShellBasedIdMapping:checkAndUpdateMaps(),166,176,"/**
 * Updates the cache if m1() returns true; logs errors if update fails.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createConnection,org.apache.hadoop.ha.ActiveStandbyElector:createConnection(),894,909,"/**
 * Re-initializes the ZK client connection.
 * Creates a new connection if the old one is null.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,forceReloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:forceReloginFromKeytab(),1262,1266,"/**
* Calls m1 with specific boolean arguments.
*/","* Force re-Login a user in from a keytab file irrespective of the last login
   * time. Loads a user identity from a keytab file and logs them in. They
   * become the currently logged-in user. This method assumes that
   * {@link #loginUserFromKeytab(String, String)} had happened already. The
   * Subject field of this UserGroupInformation object is updated to have the
   * new credentials.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean),1268,1270,"/**
* Calls m1 with the provided checkTGT and a default flag.
* @param checkTGT boolean value to check
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,forceReloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:forceReloginFromTicketCache(),1302,1306,"/**
* Calls m1 with true as an argument.
*/","* Force re-Login a user in from the ticket cache irrespective of the last
   * login time. This method assumes that login had happened already. The
   * Subject field of this UserGroupInformation object is updated to have the
   * new credentials.
   *
   * @throws IOException
   *           raised on errors performing I/O.
   * @throws KerberosAuthException
   *           on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache(),1316,1320,"/**
* Calls m1 with 'false' as the argument.
*/","* Re-Login a user in from the ticket cache.  This
   * method assumes that login had happened already.
   * The Subject field of this UserGroupInformation object is updated to have
   * the new credentials.
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createEncryptor(),58,62,"/**
 * Creates and returns an OpensslCtrCipher for encryption.
 * Uses the result of m1() as the cipher's parameters.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createDecryptor(),64,68,"/**
 * Creates and returns a Decryptor using OpensslCtrCipher.
 * Uses the cipher mode and the result of m1() for setup.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,fillQueueForKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$EncryptedQueueRefiller:fillQueueForKey(java.lang.String,java.util.Queue,int)",145,161,"/**
 * Generates and retrieves a list of encrypted key versions.
 * @param keyName Key name. @param keyQueue Queue to add EKVs.
 * @param numEKVs Number of EKVs to generate.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,generateEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateEncryptedKey(java.lang.String),788,799,"/**
 * Retrieves an EncryptedKeyVersion by name.
 * @param encryptionKeyName Key version name.
 * @return EncryptedKeyVersion or null if not found.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,drain,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:drain(java.lang.String),311,316,"/**
 * Calls m1 on each KMSClientProvider in the providers list.
 * @param keyName The key name to pass to each provider.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,queueCall,org.apache.hadoop.ipc.Server:queueCall(org.apache.hadoop.ipc.Server$Call),3097,3104,"/**
 * Executes a call, wrapping potential RpcServerException as IOException.
 * @param call The call to execute.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddrForHost,"org.apache.hadoop.net.NetUtils:createSocketAddrForHost(java.lang.String,int)",308,325,"/**
* Creates an InetSocketAddress, resolving host if possible.
* @param host The hostname or IP address.
* @param port The port number.
* @return An InetSocketAddress object.
*/
","* Create a socket address with the given host and port.  The hostname
   * might be replaced with another host that was set via
   * {@link #addStaticResolution(String, String)}.  The value of
   * hadoop.security.token.service.use_ip will determine whether the
   * standard java host resolver is used, or if the fully qualified resolver
   * is used.
   * @param host the hostname or IP use to instantiate the object
   * @param port the port number
   * @return InetSocketAddress",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,canonicalizeHost,org.apache.hadoop.net.NetUtils:canonicalizeHost(java.lang.String),362,377,"/**
 * Resolves a host to its fully qualified domain name.
 * Uses cache, resolves if necessary, handles UnknownHostException.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getLocalInetAddress,org.apache.hadoop.net.NetUtils:getLocalInetAddress(java.lang.String),798,811,"/**
 * Resolves a host string to an InetAddress, handling null/UnknownHostException.
 */","* Checks if {@code host} is a local host name and return {@link InetAddress}
   * corresponding to that address.
   * 
   * @param host the specified host
   * @return a valid local {@link InetAddress} or null
   * @throws SocketException if an I/O error occurs",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,sendSaslMessage,"org.apache.hadoop.security.SaslRpcClient:sendSaslMessage(java.io.OutputStream,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)",457,469,"/**
 * Sends a SASL message to the output stream.
 * @param out Output stream to send the message to.
 * @param message SASL message to be sent.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,writeConnectionContext,"org.apache.hadoop.ipc.Client$Connection:writeConnectionContext(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",1007,1028,"/**
* Sends a connection context message over the IPC stream.
* @param remoteId Remote ConnectionId
* @param authMethod Authentication method
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,sendRpcRequest,org.apache.hadoop.ipc.Client$Connection:sendRpcRequest(org.apache.hadoop.ipc.Client$Call),1159,1191,"/**
 * Sends a final packet for a call, handling connection closure.
 * @param call The RPC call to finalize.
 */","Initiates a rpc call by sending the rpc request to the remote server.
     * Note: this is not called from the current thread, but by another
     * thread, so that if the current thread is interrupted that the socket
     * state isn't corrupted with a partially written message.
     * @param call - the rpc request",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),727,730,"/**
* Delegates m1 call to the callQueue.
* @param ugi UserGroupInformation object
* @return int value returned by callQueue.m1()
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,receiveRpcResponse,org.apache.hadoop.ipc.Client$Connection:receiveRpcResponse(),1196,1249,"/**
 * Processes RPC response, handling success or error conditions.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processRpcRequest,"org.apache.hadoop.ipc.Server$Connection:processRpcRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)",2869,2970,"/**
 * Reads and prepares an RPC call from the provided header and buffer.
 * @param header RPC request header
 * @param buffer Buffer containing the RPC request data
 */","* Process an RPC Request 
     *   - the connection headers and context must have been already read.
     *   - Based on the rpcKind, decode the rpcRequest.
     *   - A successfully decoded RpcCall will be deposited in RPC-Q and
     *     its response will be sent later when the request is processed.
     * @param header - RPC request header
     * @param buffer - stream to request payload
     * @throws RpcServerException - generally due to fatal rpc layer issues
     *   such as invalid header or deserialization error.  The call queue
     *   may also throw a fatal or non-fatal exception on overflow.
     * @throws IOException - fatal internal error that should/could not
     *   be sent to client.
     * @throws InterruptedException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponse,"org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)",3507,3547,"/**
 * Sends RPC response, handling success/failure and errors.
 * @param call RPC call context.
 */","* Setup response for the IPC Call.
   * 
   * @param call {@link Call} to which we are setting up the response
   * @param status of the IPC call
   * @param rv return value for the IPC Call, if the call was successful
   * @param errorClass error class, if the the call failed
   * @param error error message, if the call failed
   * @throws IOException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,wrapWithSasl,org.apache.hadoop.ipc.Server:wrapWithSasl(org.apache.hadoop.ipc.Server$RpcCall),3641,3661,"/**
* Wraps a token with SASL server and adds it to the RPC response.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,initializeAuthContext,org.apache.hadoop.ipc.Server$Connection:initializeAuthContext(int),2567,2593,"/**
 * Retrieves AuthProtocol based on authType, throws IOException if invalid.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,reset,org.apache.hadoop.util.CacheableIPList:reset(),42,45,"/**
* Updates the IP list and performs an associated action.
*/",* Reloads the ip list,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,main,org.apache.hadoop.util.SysInfoLinux:main(java.lang.String[]),705,734,"/**
 * Displays system information using SysInfoLinux plugin.
 */","* Test the {@link SysInfoLinux}.
   *
   * @param args - arguments to this calculator test",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.HttpsFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
 * Opens an input stream for a path.
 * @param path Path to open.
 * @param bufferSize Buffer size for stream.
 * @return FSDataInputStream for the path.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.HttpFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
 * Opens an input stream for a path using HTTP.
 * @param path Path to open.
 * @param bufferSize Buffer size for the stream.
 * @return FSDataInputStream for the path.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,remove,org.apache.hadoop.util.LightWeightCache:remove(java.lang.Object),223,232,"/**
 * Removes and returns the value associated with the given key.
 * @param key the key whose value is to be removed
 * @return the removed value, or null if key not found
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,put,org.apache.hadoop.util.LightWeightCache:put(java.lang.Object),201,221,"/**
 * Adds an entry, validates type, updates queue, sets expiration.
 * @param entry Entry to add; must be an Entry.
 * @return Existing entry or null if not found.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean)",394,396,"/**
* Overloads m1 with default values for 'i' and 'data'.
* @param qOption boolean option
* @param hOption boolean option
*/
","Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   * 
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output if to be used
   * @return the string representation of the object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,processPath,org.apache.hadoop.fs.shell.Count:processPath(org.apache.hadoop.fs.shell.PathData),195,217,"/**
* Generates and outputs content summary based on flags.
* @param src PathData object containing file system details.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,org.apache.hadoop.fs.QuotaUsage:toString(boolean),307,309,"/**
* Calls m1 with default values for other parameters.
* @param hOption boolean option, passed to overloaded method.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read,"org.apache.hadoop.fs.FSInputChecker:read(byte[],int,int)",191,209,"/**
* Reads data into byte array.
* @param b buffer to read into
* @param off offset in the buffer
* @param len number of bytes to read
* @return number of bytes read
*/","* Read checksum verified bytes from this byte-input stream into 
   * the specified byte array, starting at the given offset.
   *
   * <p> This method implements the general contract of the corresponding
   * <code>{@link InputStream#read(byte[], int, int) read}</code> method of
   * the <code>{@link InputStream}</code> class.  As an additional
   * convenience, it attempts to read as many bytes as possible by repeatedly
   * invoking the <code>read</code> method of the underlying stream.  This
   * iterated <code>read</code> continues until one of the following
   * conditions becomes true: <ul>
   *
   *   <li> The specified number of bytes have been read,
   *
   *   <li> The <code>read</code> method of the underlying stream returns
   *   <code>-1</code>, indicating end-of-file.
   *
   * </ul> If the first <code>read</code> on the underlying stream returns
   * <code>-1</code> to indicate end-of-file then this method returns
   * <code>-1</code>.  Otherwise this method returns the number of bytes
   * actually read.
   *
   * @param      b     destination buffer.
   * @param      off   offset at which to start storing bytes.
   * @param      len   maximum number of bytes to read.
   * @return     the number of bytes read, or <code>-1</code> if the end of
   *             the stream has been reached.
   * @exception  IOException  if an I/O error occurs.
   *             ChecksumException if any checksum error occurs",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,createExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.Class,org.apache.hadoop.conf.Configuration)",127,134,"/**
 * Creates an Expression instance of the given class.
 * @param expressionClass Expression class to instantiate.
 * @param conf Configuration object used for instantiation.
 * @return Expression instance or null if class is null.
 */
","* Creates an instance of the requested {@link Expression} class.
   *
   * @param expressionClass
   *          {@link Expression} class to be instantiated
   * @param conf
   *          the Hadoop configuration
   * @return a new instance of the requested {@link Expression} class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,getInstance,"org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String,org.apache.hadoop.conf.Configuration)",118,131,"/**
 * Retrieves a Command instance by name, creating if necessary.
 * @param cmdName Command name.
 * @param conf Configuration object.
 * @return Command instance or null if not found/creatable.
 */","* Get an instance of the requested command
   * @param cmdName name of the command to lookup
   * @param conf the hadoop configuration
   * @return the {@link Command} or null if the command is unknown",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,newKey,org.apache.hadoop.io.WritableComparator:newKey(),166,168,"/**
 * Creates a WritableComparable instance using keyClass and conf.
 */","* Construct a new {@link WritableComparable} instance.
   * @return WritableComparable.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,readFields,org.apache.hadoop.io.SortedMapWritable:readFields(java.io.DataInput),156,180,"/**
* Reads data from input, populates key-value pairs, and adds them.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/GenericWritable.java,readFields,org.apache.hadoop.io.GenericWritable:readFields(java.io.DataInput),124,135,"/**
 * Reads data from input, initializes a class instance, and processes it.
 * @param in DataInput to read from.
 * @throws IOException if initialization fails.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,add,"org.apache.hadoop.io.serializer.SerializationFactory:add(org.apache.hadoop.conf.Configuration,java.lang.String)",69,79,"/**
 * Instantiates and registers a serialization class.
 * @param conf Configuration object.
 * @param serializationName Name of the serialization class.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,deserialize,org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:deserialize(org.apache.hadoop.io.Writable),62,73,"/**
 * Processes a Writable object, creating one if null.
 * @param w The Writable object to process; null is allowed.
 * @return The processed Writable object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,readFields,org.apache.hadoop.io.MapWritable:readFields(java.io.DataInput),165,191,"/**
 * Reads data from input, processes it, and populates the instance.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableFactories.java,newInstance,"org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",63,74,"/**
 * Creates a Writable object of the given class, using factory or reflection.
 * @param c Writable class to instantiate.
 * @param conf Configuration object for configurable instances.
 */","* Create a new instance of a class with a defined factory.
   *
   * @param c input c.
   * @param conf input configuration.
   * @return a new instance of a class with a defined factory.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getSocketFactoryFromProperty,"org.apache.hadoop.net.NetUtils:getSocketFactoryFromProperty(org.apache.hadoop.conf.Configuration,java.lang.String)",142,152,"/**
 * Creates a SocketFactory instance using a class name and configuration.
 * @param conf Configuration object.
 * @param propValue Class name of the SocketFactory.
 * @return SocketFactory instance.
 */
","* Get the socket factory corresponding to the given proxy URI. If the
   * given proxy URI corresponds to an absence of configuration parameter,
   * returns null. If the URI is malformed raises an exception.
   *
   * @param conf configuration.
   * @param propValue the property which is the class name of the
   *        SocketFactory to instantiate; assumed non null and non empty.
   * @return a socket factory as defined in the property value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeIdentifier,org.apache.hadoop.security.token.Token:decodeIdentifier(),164,176,"/**
 * Initializes a TokenIdentifier object from input stream.
 * @return TokenIdentifier object or null if initialization fails
 */","* Get the token identifier object, or null if it could not be constructed
   * (because the class could not be loaded, for example).
   * @return the token identifier, or null if there was no class found for it
   * @throws IOException failure to unmarshall the data
   * @throws RuntimeException if the token class could not be instantiated.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskValidatorFactory.java,getInstance,org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.Class),45,62,"/**
 * Retrieves a DiskValidator instance, potentially from a registry.
 * @param clazz DiskValidator class to retrieve.
 * @return A DiskValidator instance.
 */","* Returns a {@link DiskValidator} instance corresponding to the passed clazz.
   * @param clazz a class extends {@link DiskValidator}
   * @return disk validator.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,createFenceMethod,"org.apache.hadoop.ha.NodeFencer:createFenceMethod(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",167,195,"/**
 * Creates a FenceMethodWithArg instance using the given class and argument.
 * @param conf Configuration object.
 * @param clazzName Name of the FenceMethod class.
 * @param arg Argument for the FenceMethod.
 * @return FenceMethodWithArg instance.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>(),144,146,"/**
 * Constructs a DynamicWrappedIO with the default wrapped class name.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>(),224,226,"/**
 * Default constructor, uses WRAPPED_STATISTICS_CLASSNAME.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,"org.apache.hadoop.util.HostsFileReader:refresh(java.lang.String,java.lang.String)",190,193,"/**
* Calls m1 to process includes and excludes files.
* @param includesFile Path to the includes file.
* @param excludesFile Path to the excludes file.
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,lazyRefresh,"org.apache.hadoop.util.HostsFileReader:lazyRefresh(java.lang.String,java.lang.String)",195,198,"/**
 * Calls m1 to process includes and excludes files.
 * @param includesFile Path to the includes file.
 * @param excludesFile Path to the excludes file.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>(int,int,int,int)",126,134,"/**
 * Initializes a DynamicBloomFilter with specified parameters.
 * @param vectorSize size of the filter's vector
 * @param nbHash number of hash functions
 * @param hashType type of hash function
 * @param nr initial number of filters
 */
","* Constructor.
   * <p>
   * Builds an empty Dynamic Bloom filter.
   * @param vectorSize The number of bits in the vector.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).
   * @param nr The threshold for the maximum number of keys to record in a
   * dynamic Bloom filter row.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,addRow,org.apache.hadoop.util.bloom.DynamicBloomFilter:addRow(),275,285,"/**
 * Expands the Bloom filter matrix by adding a new filter.
 */",* Adds a new row to <i>this</i> dynamic Bloom filter.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>(int,int,int)",111,116,"/**
 * Constructs a RetouchedBloomFilter with specified size, hashes, and hash type.
 */
","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,readFields,org.apache.hadoop.util.bloom.DynamicBloomFilter:readFields(java.io.DataInput),259,270,"/**
 * Reads Bloom filters from input stream.
 * Reads nr, currentNbRecord, and initializes BloomFilter matrix.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,readFields,org.apache.hadoop.util.bloom.RetouchedBloomFilter:readFields(java.io.DataInput),429,454,"/**
* Reads data from input, populates fpVector, keyVector, and ratio.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInstanceHelp,"org.apache.hadoop.fs.FsShell:printInstanceHelp(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)",253,288,"/**
 * Prints formatted output to the stream based on the command.
 * @param out PrintStream to write to.
 * @param instance Command object containing data.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadProps,"org.apache.hadoop.conf.Configuration:loadProps(java.util.Properties,int,boolean)",2961,2981,"/**
 * Updates resources from properties, potentially reloading fully.
 * @param props Properties object to read from.
 * @param startIdx Start index for resource processing.
 * @param fullReload Whether to perform a full resource reload.
 */","* Loads the resource at a given index into the properties.
   * @param props the object containing the loaded properties.
   * @param startIdx the index where the new resource has been added.
   * @param fullReload flag whether we do complete reload of the conf instead
   *                   of just loading the new resource.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,becomeActive,org.apache.hadoop.ha.ActiveStandbyElector:becomeActive(),936,956,"/**
 * Attempts to become active; returns true on success, false otherwise.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,quitElection,org.apache.hadoop.ha.ActiveStandbyElector:quitElection(boolean),443,452,"/**
 * Yields from election, potentially triggering a fence.
 * @param needFence whether to trigger a fence if needed
 */
","* Any service instance can drop out of the election by calling quitElection. 
   * <br>
   * This will lose any leader status, if held, and stop monitoring of the lock
   * node. <br>
   * If the instance wants to participate in election again, then it needs to
   * call joinElection(). <br>
   * This allows service instances to take themselves out of rotation for known
   * impending unavailable states (e.g. long GC pause or software upgrade).
   * 
   * @param needFence true if the underlying daemon may need to be fenced
   * if a failover occurs due to dropping out of the election.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PositionedReadable.java,readVectored,"org.apache.hadoop.fs.PositionedReadable:readVectored(java.util.List,java.util.function.IntFunction)",132,135,"/**
* Delegates VectoredReadUtils.m1 to perform vectored read.
* @param ranges List of file ranges to read.
* @param allocate Allocates ByteBuffer for reading.
*/","* Read fully a list of file ranges asynchronously from this file.
   * The default iterates through the ranges to read each synchronously, but
   * the intent is that FSDataInputStream subclasses can make more efficient
   * readers.
   * As a result of the call, each range will have FileRange.setData(CompletableFuture)
   * called with a future that when complete will have a ByteBuffer with the
   * data from the file's range.
   * <p>
   *   The position returned by getPos() after readVectored() is undefined.
   * </p>
   * <p>
   *   If a file is changed while the readVectored() operation is in progress, the output is
   *   undefined. Some ranges may have old data, some may have new and some may have both.
   * </p>
   * <p>
   *   While a readVectored() operation is in progress, normal read api calls may block.
   * </p>
   * @param ranges the byte ranges to read
   * @param allocate the function to allocate ByteBuffer
   * @throws IOException any IOE.
   * @throws IllegalArgumentException if the any of ranges are invalid, or they overlap.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,close,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:close(),228,248,"/**
 * Executes m1 operation, handling closure and performing related tasks.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,read,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:read(org.apache.hadoop.fs.impl.prefetch.BufferData),308,317,"/**
 * Reads a block of data, handling potential IOExceptions.
 * @param data BufferData object to read from.
 * @throws IOException if an error occurs during reading.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,prefetch,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:prefetch(org.apache.hadoop.fs.impl.prefetch.BufferData,java.time.Instant)",319,329,"/**
 * Records prefetch duration and transitions buffer state.
 * @param data BufferData object
 * @param taskQueuedStartTime Start time of the queued task
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long)",109,112,"/**
 * Constructs a BlockLocation with default replication factor.
 * @param names block names, hosts, offset, and length are required.
 */
","* Constructor with host, name, offset and length.
   * @param names names array.
   * @param hosts host array.
   * @param offset offset.
   * @param length length.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,<init>,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:<init>(java.lang.String,boolean,long,long,long,org.apache.hadoop.fs.statistics.MeanStatistic)",71,83,"/**
 * Creates a DurationStatisticSummary.
 * @param key summary key
 * @param success boolean indicating success
 * @param count event count
 * @param max maximum duration
 * @param min minimum duration
 * @param mean statistic mean (cloned)
 */
","* Constructor.
   * @param key Statistic key.
   * @param success Are these success or failure statistics.
   * @param count Count of operation invocations.
   * @param max Max duration; -1 if unknown.
   * @param min Min duration; -1 if unknown.
   * @param mean Mean duration -may be null. (will be cloned)",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,aggregate,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:aggregate(org.apache.hadoop.fs.statistics.IOStatistics),178,199,"/**
* Processes IO statistics from source.
* @param source IOStatistics object to process, null safe.
* @return True if processing was successful.
*/","* Aggregate the current statistics with the
   * source reference passed in.
   *
   * The operation is synchronized.
   * @param source source; may be null
   * @return true if a merge took place.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,<init>,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>(org.apache.hadoop.fs.statistics.IOStatistics),123,129,"/**
 * Initializes the snapshot, either from a source or creates empty maps.
 */","* Construct, taking a snapshot of the source statistics data
   * if the source is non-null.
   * If the source is null, the empty maps are created
   * @param source statistics source. Nullable.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,foreach,"org.apache.hadoop.util.functional.RemoteIterators:foreach(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.ConsumerRaisingIOE)",273,288,"/**
 * Consumes elements from iterator, applying consumer, and returns count.
 * @param source Iterator providing elements.
 * @param consumer Consumer to apply to each element.
 * @return Number of elements consumed.
 */","* Apply an operation to all values of a RemoteIterator.
   *
   * If the iterator is an IOStatisticsSource returning a non-null
   * set of statistics, <i>and</i> this classes log is set to DEBUG,
   * then the statistics of the operation are evaluated and logged at
   * debug.
   * <p>
   * The number of entries processed is returned, as it is useful to
   * know this, especially during tests or when reporting values
   * to users.
   * </p>
   * This does not close the iterator afterwards.
   * @param source iterator source
   * @param consumer consumer of the values.
   * @return the number of elements processed
   * @param <T> type of source
   * @throws IOException if the source RemoteIterator or the consumer raise one.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackInvocation,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackInvocation(org.apache.hadoop.util.functional.InvocationRaisingIOE,java.lang.String,org.apache.hadoop.metrics2.lib.MutableRate)",1014,1024,"/**
 * Records I/O statistics for an invocation and updates a metric.
 * @param invocation Invocation object.
 * @param statistic Statistic name.
 * @param metric Metric to update with elapsed time.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[])",517,520,"/**
* Calls overloaded m1 with full buffer.
* @param position Starting position in the stream.
* @param buffer Byte array to be processed.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataInputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",28,31,"/**
 * Constructs a CryptoFSDataInputStream, wrapping an FSDataInputStream.
 * @param in Input stream, codec, buffer size, key, and IV.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",67,99,"/**
 * Encodes data using Reed-Solomon, validating input/output lengths.
 * @param inputs Data units to encode.
 * @param outputs Parity units to store encoded results.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,writeObject,"org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration)",142,146,"/**
 * Calls the overloaded m1 method with `false` for the `allowSubclasses` parameter.
 */","* Write a {@link Writable}, {@link String}, primitive type, or an array of
   * the preceding.
   *
   * @param out DataOutput.
   * @param instance instance.
   * @param conf Configuration.
   * @param declaredClass declaredClass.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,write,org.apache.hadoop.ipc.WritableRpcEngine$Invocation:write(java.io.DataOutput),166,179,"/**
 * Writes method data to the output stream.
 * Writes RPC version, class name, method name, client version,
 * and parameters to the output.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestPrefetch(int),256,289,"/**
* Processes a block, potentially prefetching data.
* @param blockNumber The block number to process.
*/","* Requests optional prefetching of the given block.
   * The block is prefetched only if we can acquire a free buffer.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,getData,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getData(int),631,633,"/**
* Retrieves buffered data for a given block number.
* @param blockNumber The block number to retrieve.
* @return BufferData object containing the data.
*/
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,acquire,org.apache.hadoop.fs.impl.prefetch.BufferPool:acquire(int),125,150,"/**
* Retrieves BufferData for a block, retrying with exponential backoff.
* @param blockNumber The block number to retrieve.
* @return BufferData object or throws IllegalStateException.
*/","* Acquires a {@code ByteBuffer}; blocking if necessary until one becomes available.
   * @param blockNumber the id of the block to acquire.
   * @return the acquired block's {@code BufferData}.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPathArgument,org.apache.hadoop.fs.shell.Command:processPathArgument(org.apache.hadoop.fs.shell.PathData),315,320,"/**
* Initializes and starts processing of PathData.
* @param item The PathData object to process.
*/","*  This is the last chance to modify an argument before going into the
   *  (possibly) recursive {@link #processPaths(PathData, PathData...)}
   *  {@literal ->} {@link #processPath(PathData)} loop.  Ex.  ls and du use
   *  this to expand out directories.
   *  @param item a {@link PathData} representing a path which exists
   *  @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPaths,"org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)",361,380,"/**
 * Recursively processes PathData items, handling grouping.
 * @param parent Parent PathData.
 * @param itemsIterator Iterator for PathData items.
 */","* Iterates over the given expanded paths and invokes
   * {@link #processPath(PathData)} on each element. If ""recursive"" is true,
   * will do a post-visit DFS on directories.
   * @param parent if called via a recurse, will be the parent dir, else null
   * @param itemsIterator a iterator of {@link PathData} objects to process
   * @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,resolvePartialGroupNames,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:resolvePartialGroupNames(java.lang.String,java.lang.String,java.lang.String)",280,319,"/**
 * Resolves group names for a user, throwing PartialGroupNameException on failure.
 */","* Attempt to partially resolve group names.
   *
   * @param userName the user's name
   * @param errMessage error message from the shell command
   * @param groupNames the incomplete list of group names
   * @return a set of resolved group names
   * @throws PartialGroupNameException if the resolution fails or times out",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File)",1218,1220,"/**
 * Constructs a ShellCommandExecutor with a directory.
 * @param execString Command to execute.
 * @param dir Working directory for the command.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,org.apache.hadoop.util.Shell:execCommand(java.lang.String[]),1358,1360,"/**
 * Delegates to overloaded method with default timeout.
 * @param cmd Command line arguments.
 * @return Result string from the delegated method.
 */
","* Static method to execute a shell command.
   * Covers most of the simple cases without requiring the user to implement
   * the <code>Shell</code> interface.
   * @param cmd shell command to execute.
   * @return the output of the executed command.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,"org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[])",1390,1393,"/**
 * Executes a command with environment variables.
 * @param env Environment variables.
 * @param cmd Command to execute.
 * @return Command output as a string.
 */
","* Static method to execute a shell command.
   * Covers most of the simple cases without requiring the user to implement
   * the <code>Shell</code> interface.
   * @param env the map of environment key=value
   * @param cmd shell command to execute.
   * @return the output of the executed command.
   * @throws IOException on any problem.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addAll,org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials),450,452,"/**
* Calls m1 with the provided credentials and default flag.
* @param other Credentials object to pass to m1.
*/
","* Copy all of the credentials from one credential object into another.
   * Existing secrets and tokens are overwritten.
   * @param other the credentials to copy",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,mergeAll,org.apache.hadoop.security.Credentials:mergeAll(org.apache.hadoop.security.Credentials),459,461,"/**
 * Calls m1 with the given credentials and false as the second argument.
 * @param other The Credentials object to pass to m1.
 */
","* Copy all of the credentials from one credential object into another.
   * Existing secrets and tokens are not overwritten.
   * @param other the credentials to copy",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DelegationTokenIssuer.java,addDelegationTokens,"org.apache.hadoop.security.token.DelegationTokenIssuer:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)",79,87,"/**
 * Retrieves tokens based on renewer and credentials.
 * @param renewer renewer string
 * @param credentials credentials object
 * @return Array of tokens
 */","* Given a renewer, add delegation tokens for issuer and it's child issuers
   * to the <code>Credentials</code> object if it is not already present.
   *<p>
   * Note: This method is not intended to be overridden.  Issuers should
   * implement getCanonicalService and getDelegationToken to ensure
   * consistent token acquisition behavior.
   *
   * @param renewer the user allowed to renew the delegation tokens
   * @param credentials cache in which to add new delegation tokens
   * @return list of new delegation tokens
   * @throws IOException thrown if IOException if an IO error occurs.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addToken,org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.security.token.Token),1701,1703,"/**
* Recursively calls m2 with the token's parent, or returns false.
* @param token The token to process; null terminates recursion.
*/","* Add a token to this UGI
   * 
   * @param token Token to be added
   * @return true on successful add of new token",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.RawLocalFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),850,854,"/**
* Sets the working directory to a new directory.
* @param newDir The new directory to set as working.
*/",* Set the working directory to the given directory.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,exists,org.apache.hadoop.fs.RawLocalFileSystem:exists(org.apache.hadoop.fs.Path),768,771,"/**
* Delegates to the wrapped Path's m2() method.
* @param f The Path object to delegate to.
* @return True if the wrapped Path's m2() returns true.
*/
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getStatus,org.apache.hadoop.fs.RawLocalFileSystem:getStatus(org.apache.hadoop.fs.Path),866,874,"/**
 * Gets filesystem status of a path.
 * @param p Path to get status for; defaults to root if null.
 * @return FsStatus object containing disk usage details.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setTimes,"org.apache.hadoop.fs.RawLocalFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",1129,1140,"/**
 * Sets file modification and access times.
 * @param p Path to the file.
 * @param mtime Modification time in seconds.
 * @param atime Access time in seconds.
 */","* Sets the {@link Path}'s last modified time and last access time to
   * the given valid times.
   *
   * @param mtime the modification time to set (only if no less than zero).
   * @param atime the access time to set (only if no less than zero).
   * @throws IOException if setting the times fails.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,pathToFile,org.apache.hadoop.fs.LocalFileSystem:pathToFile(org.apache.hadoop.fs.Path),79,81,"/**
* Delegates file retrieval to the underlying RawLocalFileSystem.
* @param path Path to the file.
* @return The File object representing the path.
*/
","* Convert a path to a File.
   * @param path the path.
   * @return file.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getUriPath,org.apache.hadoop.fs.viewfs.ViewFileSystem:getUriPath(org.apache.hadoop.fs.Path),264,267,"/**
* Processes a Path object and returns a String result.
* Uses helper methods m1, m2, m3, and m4 for processing.
*/
","* Make the path Absolute and get the path-part of a pathname.
   * Checks that URI matches this file system
   * and that the path-part is a valid name.
   *
   * @param p path
   * @return path-part of the Path p",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),854,859,"/**
 * Calls method m1 on each NflyNode's file system.
 * @param newDir The directory path to pass to m1.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,<init>,"org.apache.hadoop.fs.Stat:<init>(org.apache.hadoop.fs.Path,long,boolean,org.apache.hadoop.fs.FileSystem)",50,68,"/**
 * Constructs a Stat object for a given file path and block size.
 * @param path Path to the file
 * @param blockSize Block size
 * @param deref Dereference symbolic links
 * @param fs FileSystem object
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,makeQualified,org.apache.hadoop.fs.Path:makeQualified(org.apache.hadoop.fs.FileSystem),547,550,"/**
* Calls m3 with arguments obtained from the provided FileSystem.
* @param fs FileSystem object used to get arguments.
* @return Path object returned by m3.
*/
","* Returns a qualified path object for the {@link FileSystem}'s working
   * directory.
   *  
   * @param fs the target FileSystem
   * @return a qualified path object for the FileSystem's working directory
   * @deprecated use {@link #makeQualified(URI, Path)}",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,makeQualified,org.apache.hadoop.fs.FileContext:makeQualified(org.apache.hadoop.fs.Path),629,631,"/**
 * Delegates m3 operation on the given path.
 * @param path The path to operate on.
 * @return The result of the delegated operation.
 */
","* Make the path fully qualified if it is isn't. 
   * A Fully-qualified path has scheme and authority specified and an absolute
   * path.
   * Use the default file system and working dir in this FileContext to qualify.
   * @param path the path.
   * @return qualified path",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,makeQualified,org.apache.hadoop.fs.AbstractFileSystem:makeQualified(org.apache.hadoop.fs.Path),438,441,"/**
 * Calls m1, then returns the result of path.m3().
 * @param path The path object to operate on.
 */","* Make the path fully qualified to this file system
   * @param path the path.
   * @return the qualified path",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path),1926,1937,"/**
 * Resolves a file status by path.
 * @param f The path to resolve.
 * @return An array of FileStatus objects.
 */","* List the statuses of the files/directories in the given path 
     * if the path is a directory.
     * 
     * @param f is the path
     *
     * @return an array that contains statuses of the files/directories 
     *         in the given path
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for <code>f</code> is
     *           not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,fixRelativePart,org.apache.hadoop.fs.Globber:fixRelativePart(org.apache.hadoop.fs.Path),138,144,"/**
 * Delegates m1 operation to either fs or fc based on fs availability.
 * @param path The path to operate on.
 * @return Path object returned by the delegated method.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,delete,"org.apache.hadoop.fs.FileContext:delete(org.apache.hadoop.fs.Path,boolean)",842,853,"/**
 * Resolves links in a file system path recursively.
 * @param f Path to resolve.
 * @param recursive Whether to resolve recursively.
 * @return True if resolved successfully, false otherwise.
 */","* Delete a file.
   * @param f the path to delete.
   * @param recursive if path is a directory and set to 
   * true, the directory is deleted else throws an exception. In
   * case of a file the recursive can be set to either true or false.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is invalid
   *
   * @return if delete success true, not false.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,open,org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path),873,883,"/**
 * Resolves a symbolic link to an FSDataInputStream.
 * @param f Path to the symbolic link
 * @return FSDataInputStream or null if unresolved
 */","* Opens an FSDataInputStream at the indicated Path using
   * default buffersize.
   * @param f the file name to open
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * @return input stream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,open,"org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path,int)",904,915,"/**
 * Opens a data input stream for a path, using the provided buffer size.
 * @param f Path to open
 * @param bufferSize Buffer size for the stream
 * @return FSDataInputStream object
 */","* Opens an FSDataInputStream at the indicated Path.
   * 
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * @return output stream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,truncate,"org.apache.hadoop.fs.FileContext:truncate(org.apache.hadoop.fs.Path,long)",947,958,"/**
 * Resizes a file to the specified length using a file resolver.
 * @param f Path to the file.
 * @param newLength New file length.
 * @return True if successful, false otherwise.
 */","* Truncate the file in the indicated path to the indicated size.
   * <ul>
   * <li>Fails if path is a directory.
   * <li>Fails if path does not exist.
   * <li>Fails if path is not closed.
   * <li>Fails if new size is greater than current size.
   * </ul>
   * @param f The path to the file to be truncated
   * @param newLength The size the file is to be truncated to
   *
   * @return <code>true</code> if the file has been truncated to the desired
   * <code>newLength</code> and is immediately available to be reused for
   * write operations such as <code>append</code>, or
   * <code>false</code> if a background process of adjusting the length of
   * the last block has been started, and clients should wait for it to
   * complete before proceeding with further file updates.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   *
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setReplication,"org.apache.hadoop.fs.FileContext:setReplication(org.apache.hadoop.fs.Path,short)",978,989,"/**
 * Resolves a link and calls FSLinkResolver.m3 with replication factor.
 * @param f Path to resolve.
 * @param replication Replication factor.
 */","* Set replication for an existing file.
   * 
   * @param f file name
   * @param replication new replication
   *
   * @return true if successful
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setPermission,"org.apache.hadoop.fs.FileContext:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1079,1091,"/**
 * Sets file permissions using an AbstractFileSystem.
 * @param f Path to the file.
 * @param permission Permissions to set.
 */","* Set permission of a path.
   * @param f the path.
   * @param permission - the new absolute permission (umask is not applied)
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setOwner,"org.apache.hadoop.fs.FileContext:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1117,1134,"/**
 * Resolves links on a path, setting username/groupname.
 * @param f Path to resolve.
 * @param username Username for access control.
 * @param groupname Groupname for access control.
 */","* Set owner of a path (i.e. a file or a directory). The parameters username
   * and groupname cannot both be null.
   * 
   * @param f The path
   * @param username If it is null, the original username remains unchanged.
   * @param groupname If it is null, the original groupname remains unchanged.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws HadoopIllegalArgumentException If <code>username</code> or
   *           <code>groupname</code> is invalid.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setTimes,"org.apache.hadoop.fs.FileContext:setTimes(org.apache.hadoop.fs.Path,long,long)",1158,1170,"/**
 * Updates the modification and access times of a file.
 * @param f Path to the file.
 * @param mtime Modification time.
 * @param atime Access time.
 */","* Set access time of a file.
   * @param f The path
   * @param mtime Set the modification time of this file.
   *        The number of milliseconds since epoch (Jan 1, 1970). 
   *        A value of -1 means that this call should not set modification time.
   * @param atime Set the access time of this file.
   *        The number of milliseconds since Jan 1, 1970. 
   *        A value of -1 means that this call should not set access time.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileChecksum,org.apache.hadoop.fs.FileContext:getFileChecksum(org.apache.hadoop.fs.Path),1191,1202,"/**
 * Calculates checksum of a file using a file system resolver.
 * @param f Path to the file
 * @return FileChecksum object
 */","* Get the checksum of a file.
   *
   * @param f file path
   *
   * @return The file checksum.  The default return value is null,
   *  which indicates that no checksum algorithm is implemented
   *  in the corresponding FileSystem.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileStatus,org.apache.hadoop.fs.FileContext:getFileStatus(org.apache.hadoop.fs.Path),1248,1258,"/**
 * Resolves a file status by path, using the filesystem's m2 method.
 * @param f The path to resolve.
 * @return FileStatus object.
 */","* Return a file status object that represents the path.
   * @param f The path we want information from
   *
   * @return a FileStatus object
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,access,"org.apache.hadoop.fs.FileContext:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",1305,1318,"/**
 * Executes an FSAction on a path using a LinkResolver.
 * @param path Path to operate on.
 * @param mode The FSAction to perform.
 */","* Checks if the user can access a path.  The mode specifies which access
   * checks to perform.  If the requested permissions are granted, then the
   * method returns normally.  If access is denied, then the method throws an
   * {@link AccessControlException}.
   * <p>
   * The default implementation of this method calls {@link #getFileStatus(Path)}
   * and checks the returned permissions against the requested permissions.
   * Note that the getFileStatus call will be subject to authorization checks.
   * Typically, this requires search (execute) permissions on each directory in
   * the path's prefix, but this is implementation-defined.  Any file system
   * that provides a richer authorization model (such as ACLs) may override the
   * default implementation so that it checks against that model instead.
   * <p>
   * In general, applications should avoid using this method, due to the risk of
   * time-of-check/time-of-use race conditions.  The permissions on a file may
   * change immediately after the access call returns.  Most applications should
   * prefer running specific file system actions as the desired user represented
   * by a {@link UserGroupInformation}.
   *
   * @param path Path to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws FileNotFoundException if the path does not exist
   * @throws UnsupportedFileSystemException if file system for <code>path</code>
   *   is not supported
   * @throws IOException see specific implementation
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileLinkStatus,org.apache.hadoop.fs.FileContext:getFileLinkStatus(org.apache.hadoop.fs.Path),1334,1350,"/**
 * Resolves a file status, potentially following symbolic links.
 * @param f Path to resolve; must be fully qualified.
 * @return FileStatus object or null if not found.
 */","* Return a file status object that represents the path. If the path 
   * refers to a symlink then the FileStatus of the symlink is returned.
   * The behavior is equivalent to #getFileStatus() if the underlying
   * file system does not support symbolic links.
   * @param  f The path we want information from.
   * @return A FileStatus object
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLinkTarget,org.apache.hadoop.fs.FileContext:getLinkTarget(org.apache.hadoop.fs.Path),1367,1378,"/**
 * Resolves symbolic links to their absolute path.
 * @param f Path to resolve
 * @return Absolute path or null if unresolved
 */","* Returns the target of the given symbolic link as it was specified
   * when the link was created.  Links in the path leading up to the
   * final path component are resolved transparently.
   *
   * @param f the path to return the target of
   * @return The un-interpreted target of the symbolic link.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If path <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If the given path does not refer to a symlink
   *           or an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileBlockLocations,"org.apache.hadoop.fs.FileContext:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",1437,1450,"/**
 * Retrieves block locations for a file path within a given range.
 * @param f file path
 * @param start start offset
 * @param len length
 * @return BlockLocation array
 */
","* Return blockLocation of the given file for the given offset and len.
   *  For a nonexistent file or regions, null will be returned.
   *
   * This call is most helpful with DFS, where it returns 
   * hostnames of machines that contain the given file.
   *
   * In HDFS, if file is three-replicated, the returned array contains
   * elements like:
   * <pre>
   * BlockLocation(offset: 0, length: BLOCK_SIZE,
   *   hosts: {""host1:9866"", ""host2:9866, host3:9866""})
   * BlockLocation(offset: BLOCK_SIZE, length: BLOCK_SIZE,
   *   hosts: {""host2:9866"", ""host3:9866, host4:9866""})
   * </pre>
   *
   * And if a file is erasure-coded, the returned BlockLocation are logical
   * block groups.
   *
   * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).
   * 1. If the file size is less than one stripe size, say 2 * CELL_SIZE, then
   * there will be one BlockLocation returned, with 0 offset, actual file size
   * and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks.
   * 3. If the file size is less than one group size but greater than one
   * stripe size, then there will be one BlockLocation returned, with 0 offset,
   * actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting
   * the actual blocks.
   * 4. If the file size is greater than one group size, 3 * BLOCK_SIZE + 123
   * for example, then the result will be like:
   * <pre>
   * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {""host1:9866"",
   *   ""host2:9866"",""host3:9866"",""host4:9866"",""host5:9866""})
   * BlockLocation(offset: 3 * BLOCK_SIZE, length: 123, hosts: {""host1:9866"",
   *   ""host4:9866"", ""host5:9866""})
   * </pre>
   *
   * @param f - get blocklocations of this file
   * @param start position (byte offset)
   * @param len (in bytes)
   *
   * @return block locations for given file at specified offset of len
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is invalid",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFsStatus,org.apache.hadoop.fs.FileContext:getFsStatus(org.apache.hadoop.fs.Path),1476,1489,"/**
 * Resolves a file system status, using default FS if path is null.
 * @param f Path to resolve; null uses default FS.
 * @return FsStatus object.
 */","* Returns a status object describing the use and capacity of the
   * file system denoted by the Parh argument p.
   * If the file system has multiple partitions, the
   * use and capacity of the partition pointed to by the specified
   * path is reflected.
   * 
   * @param f Path for which status should be obtained. null means the
   * root partition of the default file system. 
   *
   * @return a FsStatus object
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSymlink,"org.apache.hadoop.fs.FileContext:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1570,1588,"/**
* Creates a symbolic link from 'link' to 'target', creating parents if needed.
* @param target The path to the target file.
* @param link The path to the symbolic link.
* @param createParent Whether to create parent directories.
*/","* Creates a symbolic link to an existing file. An exception is thrown if 
   * the symlink exits, the user does not have permission to create symlink,
   * or the underlying file system does not support symlinks.
   * 
   * Symlink permissions are ignored, access to a symlink is determined by
   * the permissions of the symlink target.
   * 
   * Symlinks in paths leading up to the final path component are resolved 
   * transparently. If the final path component refers to a symlink some 
   * functions operate on the symlink itself, these are:
   * - delete(f) and deleteOnExit(f) - Deletes the symlink.
   * - rename(src, dst) - If src refers to a symlink, the symlink is 
   *   renamed. If dst refers to a symlink, the symlink is over-written.
   * - getLinkTarget(f) - Returns the target of the symlink. 
   * - getFileLinkStatus(f) - Returns a FileStatus object describing
   *   the symlink.
   * Some functions, create() and mkdir(), expect the final path component
   * does not exist. If they are given a path that refers to a symlink that 
   * does exist they behave as if the path referred to an existing file or 
   * directory. All other functions fully resolve, ie follow, the symlink. 
   * These are: open, setReplication, setOwner, setTimes, setWorkingDirectory,
   * setPermission, getFileChecksum, setVerifyChecksum, getFileBlockLocations,
   * getFsStatus, getFileStatus, exists, and listStatus.
   * 
   * Symlink targets are stored as given to createSymlink, assuming the 
   * underlying file system is capable of storing a fully qualified URI.
   * Dangling symlinks are permitted. FileContext supports four types of 
   * symlink targets, and resolves them as follows
   * <pre>
   * Given a path referring to a symlink of form:
   * 
   *   {@literal <---}X{@literal --->}
   *   fs://host/A/B/link 
   *   {@literal <-----}Y{@literal ----->}
   * 
   * In this path X is the scheme and authority that identify the file system,
   * and Y is the path leading up to the final path component ""link"". If Y is
   * a symlink  itself then let Y' be the target of Y and X' be the scheme and
   * authority of Y'. Symlink targets may:
   * 
   * 1. Fully qualified URIs
   * 
   * fs://hostX/A/B/file  Resolved according to the target file system.
   * 
   * 2. Partially qualified URIs (eg scheme but no host)
   * 
   * fs:///A/B/file  Resolved according to the target file system. Eg resolving
   *                 a symlink to hdfs:///A results in an exception because
   *                 HDFS URIs must be fully qualified, while a symlink to 
   *                 file:///A will not since Hadoop's local file systems 
   *                 require partially qualified URIs.
   * 
   * 3. Relative paths
   * 
   * path  Resolves to [Y'][path]. Eg if Y resolves to hdfs://host/A and path 
   *       is ""../B/file"" then [Y'][path] is hdfs://host/B/file
   * 
   * 4. Absolute paths
   * 
   * path  Resolves to [X'][path]. Eg if Y resolves hdfs://host/A/B and path
   *       is ""/file"" then [X][path] is hdfs://host/file
   * </pre>
   * 
   * @param target the target of the symbolic link
   * @param link the path to be created that points to target
   * @param createParent if true then missing parent dirs are created if 
   *                     false then parent must exist
   *
   *
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If file <code>link</code> already exists
   * @throws FileNotFoundException If <code>target</code> does not exist
   * @throws ParentNotDirectoryException If parent of <code>link</code> is not a
   *           directory.
   * @throws UnsupportedFileSystemException If file system for 
   *           <code>target</code> or <code>link</code> is not supported
   * @throws IOException If an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext:listStatus(org.apache.hadoop.fs.Path),1611,1623,"/**
 * Resolves a path to a RemoteIterator of FileStatus objects.
 * @param f The path to resolve.
 * @return RemoteIterator of FileStatus or null if not found.
 */","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   * 
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories 
   *         in the given path
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listCorruptFileBlocks,org.apache.hadoop.fs.FileContext:listCorruptFileBlocks(org.apache.hadoop.fs.Path),1633,1644,"/**
 * Resolves a path to a remote iterator of paths.
 * @param path The path to resolve.
 * @return RemoteIterator of Path objects.
 */","* List CorruptFile Blocks.
   *
   * @param path the path.
   * @return an iterator over the corrupt files under the given path
   * (may contain duplicates if a file has more than one corrupt block)
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listLocatedStatus,org.apache.hadoop.fs.FileContext:listLocatedStatus(org.apache.hadoop.fs.Path),1673,1686,"/**
 * Resolves a remote iterator of located file statuses for a path.
 * @param f The path to resolve.
 * @return RemoteIterator of LocatedFileStatus.
 */","* List the statuses of the files/directories in the given path if the path is
   * a directory. 
   * Return the file's status and block locations If the path is a file.
   * 
   * If a returned status is a file, it contains the file's block locations.
   *
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories 
   *         in the given path
   * If any IO exception (for example the input directory gets deleted while
   * listing is being executed), next() or hasNext() of the returned iterator
   * may throw a RuntimeException with the io exception as the cause.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolveAbstractFileSystems,org.apache.hadoop.fs.FileContext:resolveAbstractFileSystems(org.apache.hadoop.fs.Path),2372,2386,"/**
 * Resolves file system links and collects AbstractFileSystems.
 * @param f Path to resolve; throws IOException.
 * @return Set of AbstractFileSystems found.
 */
","* Returns the list of AbstractFileSystems accessed in the path. The list may
   * contain more than one AbstractFileSystems objects in case of symlinks.
   * 
   * @param f
   *          Path which needs to be resolved
   * @return List of AbstractFileSystems accessed in the path
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,modifyAclEntries,"org.apache.hadoop.fs.FileContext:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",2456,2467,"/**
 * Resolves and applies ACL specifications to a file system path.
 * @param path The path to resolve.
 * @param aclSpec ACL entries to apply.
 * @throws IOException If an I/O error occurs.
 */","* Modifies ACL entries of files and directories.  This method can add new ACL
   * entries or modify the permissions on existing ACL entries.  All existing
   * ACL entries that are not specified in this call are retained without
   * changes.  (Modifications are merged into the current ACL.)
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing
   * modifications
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeAclEntries,"org.apache.hadoop.fs.FileContext:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",2478,2489,"/**
 * Resolves and applies ACL specifications to a file system path.
 * @param path Path to resolve.
 * @param aclSpec List of ACL entries to apply.
 */","* Removes ACL entries from files and directories.  Other ACL entries are
   * retained.
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing entries
   * to remove
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeDefaultAcl,org.apache.hadoop.fs.FileContext:removeDefaultAcl(org.apache.hadoop.fs.Path),2497,2508,"/**
 * Resolves a file system link at the given path.
 * @param path Path to resolve; throws IOException on failure.
 */","* Removes all default ACL entries from files and directories.
   *
   * @param path Path to modify
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeAcl,org.apache.hadoop.fs.FileContext:removeAcl(org.apache.hadoop.fs.Path),2518,2528,"/**
 * Resolves and executes m2 operation on the absolute file path.
 * @param path The Path object to resolve and operate on.
 * @throws IOException if an I/O error occurs.
 */","* Removes all but the base ACL entries of files and directories.  The entries
   * for user, group, and others are retained for compatibility with permission
   * bits.
   *
   * @param path Path to modify
   * @throws IOException if an ACL could not be removed",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setAcl,"org.apache.hadoop.fs.FileContext:setAcl(org.apache.hadoop.fs.Path,java.util.List)",2540,2551,"/**
 * Resolves and applies ACL specifications to a path.
 * @param path Path to resolve.
 * @param aclSpec ACL entries to apply.
 */","* Fully replaces ACL of files and directories, discarding all existing
   * entries.
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing
   * modifications, must include entries for user, group, and others for
   * compatibility with permission bits.
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAclStatus,org.apache.hadoop.fs.FileContext:getAclStatus(org.apache.hadoop.fs.Path),2561,2570,"/**
 * Resolves ACL status for a given path using FSLinkResolver.
 * @param path Path to resolve ACL status for.
 * @throws IOException if an I/O error occurs.
 */","* Gets the ACLs of files and directories.
   *
   * @param path Path to get
   * @return RemoteIterator{@literal <}AclStatus{@literal >} which returns
   *         each AclStatus
   * @throws IOException if an ACL could not be read",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setXAttr,"org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",2603,2614,"/**
 * Sets extended attributes on a file path.
 * @param path Path to the file.
 * @param name Attribute name.
 * @param value Attribute value.
 * @param flag Attribute flags.
 */","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @param flag xattr set flag
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttr,"org.apache.hadoop.fs.FileContext:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",2628,2637,"/**
 * Resolves a byte array resource by path and name.
 * @param path Path to the resource.
 * @param name Resource name.
 * @return Byte array of the resource.
 */","* Get an xattr for a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attribute
   * @param name xattr name.
   * @return byte[] xattr value.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttrs,org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path),2651,2660,"/**
 * Resolves file content at the given path.
 * @param path Path to the file.
 * @return Map containing file content or empty map.
 * @throws IOException if an I/O error occurs.
 */","* Get all of the xattrs for a file or directory.
   * Only those xattrs for which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @return Map{@literal <}String, byte[]{@literal >} describing the XAttrs
   * of the file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttrs,"org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",2675,2685,"/**
 * Resolves file content by names, using the filesystem.
 * @param path Base path for resolution.
 * @param names List of names to resolve.
 * @return Map of name to byte array content.
 */","* Get all of the xattrs for a file or directory.
   * Only those xattrs for which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @param names XAttr names.
   * @return Map{@literal <}String, byte[]{@literal >} describing the XAttrs
   * of the file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeXAttr,"org.apache.hadoop.fs.FileContext:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",2698,2708,"/**
 * Resolves and links a path with a given name using FSLinkResolver.
 * @param path Path to resolve.
 * @param name Name to associate with the path.
 */","* Remove an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to remove extended attribute
   * @param name xattr name
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listXAttrs,org.apache.hadoop.fs.FileContext:listXAttrs(org.apache.hadoop.fs.Path),2722,2731,"/**
 * Resolves links within a path using the AbstractFileSystem.
 * @param path The path to resolve.
 * @return A list of strings representing resolved links.
 */","* Get all of the xattr names for a file or directory.
   * Only those xattr names which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @return List{@literal <}String{@literal >} of the XAttr names of the
   * file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSnapshot,"org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",2766,2777,"/**
 * Resolves a path to its snapshot using FSLinkResolver.
 * @param path The path to resolve.
 * @param snapshotName Snapshot identifier.
 * @return Resolved Path object.
 */","* Create a snapshot.
   *
   * @param path The directory where snapshots will be taken.
   * @param snapshotName The name of the snapshot
   * @return the snapshot path.
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,renameSnapshot,"org.apache.hadoop.fs.FileContext:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",2794,2805,"/**
 * Renames a snapshot on the file system.
 * @param path Path to the snapshot.
 * @param snapshotOldName Old snapshot name.
 * @param snapshotNewName New snapshot name.
 */","* Rename a snapshot.
   *
   * @param path The directory path where the snapshot was taken
   * @param snapshotOldName Old name of the snapshot
   * @param snapshotNewName New name of the snapshot
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,deleteSnapshot,"org.apache.hadoop.fs.FileContext:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",2821,2832,"/**
 * Resolves and executes a file system operation with a snapshot name.
 * @param path The path to resolve.
 * @param snapshotName The snapshot name to use.
 */","* Delete a snapshot of a directory.
   *
   * @param path The directory that the to-be-deleted snapshot belongs to
   * @param snapshotName The name of the snapshot
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,satisfyStoragePolicy,org.apache.hadoop.fs.FileContext:satisfyStoragePolicy(org.apache.hadoop.fs.Path),2839,2850,"/**
 * Resolves and executes m2 on the absolute file system path.
 * @param path The Path to resolve and execute m2 on.
 */","* Set the source path to satisfy storage policy.
   * @param path The source path referring to either a directory or a file.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setStoragePolicy,"org.apache.hadoop.fs.FileContext:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",2861,2872,"/**
 * Resolves a link using the provided path and policy name.
 * @param path Path to resolve.
 * @param policyName Policy name to apply.
 */","* Set the storage policy for a given file or directory.
   *
   * @param path file or directory path.
   * @param policyName the name of the target storage policy. The list
   *                   of supported Storage policies can be retrieved
   *                   via {@link #getAllStoragePolicies}.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,unsetStoragePolicy,org.apache.hadoop.fs.FileContext:unsetStoragePolicy(org.apache.hadoop.fs.Path),2879,2889,"/**
 * Resolves links in a file using a custom link resolver.
 * @param src The source Path to resolve links from.
 * @throws IOException if an I/O error occurs.
 */
","* Unset the storage policy set for a given file or directory.
   * @param src file or directory path.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getStoragePolicy,org.apache.hadoop.fs.FileContext:getStoragePolicy(org.apache.hadoop.fs.Path),2898,2908,"/**
 * Resolves a BlockStoragePolicySpi for a given path.
 * @param path Path to resolve.
 * @throws IOException if an I/O error occurs.
 */","* Query the effective storage policy ID for the given file or directory.
   *
   * @param path file or directory path.
   * @return storage policy for give file.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,hasPathCapability,"org.apache.hadoop.fs.FileContext:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",3001,3007,"/**
 * Resolves a path with a capability, using a link resolution strategy.
 * @param path The path to resolve.
 * @param capability Capability to use for resolution.
 */","* Return the path capabilities of the bonded {@code AbstractFileSystem}.
   * @param path path to query the capability of.
   * @param capability string to query the stream support for.
   * @return true iff the capability is supported under that FS.
   * @throws IOException path resolution or other IO failure
   * @throws IllegalArgumentException invalid arguments",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getServerDefaults,org.apache.hadoop.fs.FileContext:getServerDefaults(org.apache.hadoop.fs.Path),3015,3020,"/**
 * Resolves FsServerDefaults by linking across file systems.
 * @param path Path to resolve defaults for.
 * @return FsServerDefaults object.
 */","* Return a set of server default configuration values based on path.
   * @param path path to fetch server defaults
   * @return server default configuration values for path
   * @throws IOException an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createMultipartUploader,org.apache.hadoop.fs.FileContext:createMultipartUploader(org.apache.hadoop.fs.Path),3029,3035,"/**
* Resolves and configures multipart upload using a base path.
* @param basePath base path for the upload configuration
* @return MultipartUploaderBuilder configured with resolved path
*/
","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.HarFileSystem:getInitialWorkingDirectory(),278,281,"/**
 * Delegates to m1() and returns its result as a Path.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory(com.jcraft.jsch.ChannelSftp),652,655,"/**
 * Delegates to m1 to obtain a Path object using the provided client.
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,<init>,org.apache.hadoop.fs.RawLocalFileSystem:<init>(),101,103,"/**
 * Initializes the RawLocalFileSystem with the initial working directory.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,refreshStatus,org.apache.hadoop.fs.shell.PathData:refreshStatus(),198,208,"/**
 * Retrieves a file status using helper methods, handles cleanup.
 * @return FileStatus object or null if an error occurs.
 */
","* Updates the paths's file status
   * @return the updated FileStatus
   * @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getUsed,org.apache.hadoop.fs.HarFileSystem:getUsed(),1271,1274,"/**
 * Delegates m1() call to the underlying FileSystem object.
 * @return Long value returned by fs.m1()
 * @throws IOException if an I/O error occurs
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getUsed,org.apache.hadoop.fs.FilterFileSystem:getUsed(),415,418,"/**
 * Delegates m1() call to the underlying FileSystem object.
 * @return long value returned by fs.m1()
 * @throws IOException if an I/O error occurs
 */
",Return the total size of all files in the filesystem.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,resolve,"org.apache.hadoop.fs.viewfs.RegexMountPoint:resolve(java.lang.String,boolean)",168,205,"/**
 * Resolves a path using interceptors and regex matching.
 * @param srcPath Path to resolve.
 * @param resolveLastComponent Whether to resolve last component.
 * @return ResolveResult object or null if unresolved.
 */","* Get resolved path from regex mount points.
   *  E.g. link: ^/user/(?<username>\\w+) => s3://$user.apache.com/_${user}
   *  srcPath: is /user/hadoop/dir1
   *  resolveLastComponent: true
   *  then return value is s3://hadoop.apache.com/_hadoop
   * @param srcPath - the src path to resolve
   * @param resolveLastComponent - whether resolve the path after last `/`
   * @return mapped path of the mount point.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkDest,"org.apache.hadoop.fs.FileUtil:checkDest(java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",607,630,"/**
* Copies or renames a file/directory.
* @param srcName Source name, null for rename.
* @param dstFS Filesystem.
* @param dst Destination path.
* @param overwrite Overwrite if exists.
* @return Destination path.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,advance,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:advance(),561,569,"/**
 * Iterates through root directories, returns on successful m1 check.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,ifExists,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)",615,636,"/**
* Checks if a file exists in local directories based on path.
* @param pathStr File path to check.
* @param conf Hadoop configuration.
* @return True if file exists, false otherwise.
*/","We search through all the configured dirs for the file's existence
     *  and return true when we find one",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,delete,"org.apache.hadoop.io.MapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)",913,921,"/**
 * Processes data, index, and directory paths using the FileSystem.
 * @param fs The FileSystem object to use.
 * @param name The base name of the paths to process.
 */
","* Deletes the named map file.
   * @param fs input fs.
   * @param name input name.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,delete,"org.apache.hadoop.io.BloomMapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)",59,69,"/**
 * Deletes files associated with a map file directory.
 * @param fs FileSystem object
 * @param name Directory path
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,findCurrentDirectory,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:findCurrentDirectory(java.util.Date),551,558,"/**
 * Calculates the file path for data based on the given date.
 * @param now The current date to determine the file path.
 * @return Path object representing the calculated file path.
 */
","* Use the given time to determine the current directory. The current
   * directory will be based on the {@link #rollIntervalMinutes}.
   *
   * @param now the current time
   * @return the current directory",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,createForWrite,"org.apache.hadoop.io.SecureIOUtils:createForWrite(java.io.File,int)",273,280,"/**
 * Opens a file output stream, respecting security or using native IO.
 * @param f The file to open.
 * @param permissions File permissions.
 * @return FileOutputStream object.
 * @throws IOException If an I/O error occurs.
 */
","* Open the specified File for write access, ensuring that it does not exist.
   * @param f the file that we want to create
   * @param permissions we want to have on the file (if security is enabled)
   *
   * @throws AlreadyExistsException if the file already exists
   * @throws IOException if any other error occurred
   * @return createForWrite FileOutputStream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:initFileSystem(java.net.URI),165,171,"/**
 * Initializes the backing JKS path from the provided URI.
 * @param keystoreUri URI pointing to the keystore file.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,extractKMSPath,org.apache.hadoop.crypto.key.kms.KMSClientProvider:extractKMSPath(java.net.URI),443,445,"/**
 * Retrieves a Path object from the given URI using ProviderUtils.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,checkPathsForReservedRaw,"org.apache.hadoop.fs.shell.CommandWithDestination:checkPathsForReservedRaw(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",385,406,"/**
 * Checks if raw xattrs should be preserved during copy.
 * @param src Source path.
 * @param target Destination path.
 * @return True if both paths are RESERVED_RAW, otherwise false.
 */","* Check the source and target paths to ensure that they are either both in
   * /.reserved/raw or neither in /.reserved/raw. If neither src nor target are
   * in /.reserved/raw, then return false, indicating not to preserve raw.*
   * xattrs. If both src/target are in /.reserved/raw, then return true,
   * indicating raw.* xattrs should be preserved. If only one of src/target is
   * in /.reserved/raw then throw an exception.
   *
   * @param src The source path to check. This should be a fully-qualified
   *            path, not relative.
   * @param target The target path to check. This should be a fully-qualified
   *               path, not relative.
   * @return true if raw.* xattrs should be preserved.
   * @throws PathOperationException is only one of src/target are in
   * /.reserved/raw.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",997,1042,"/**
 * Creates a data output stream for a file, using fallback FS.
 * @param f the path of the file to create
 * @return FSDataOutputStream for the created file
 * @throws IOException if creation fails
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",1051,1071,"/**
 * Reads block locations for a file, potentially falling back to a linked FS.
 * @param f Path to file
 * @param start Start offset
 * @param len Length to read
 * @return BlockLocation[] array
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,mkdir,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",1267,1299,"/**
 * Creates a directory at the given path with specified permissions.
 * @param dir Path of the directory to create.
 * @param permission Permissions for the directory.
 * @throws IOException if an I/O error occurs.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1458,1496,"/**
 * Creates a file with specified parameters on a fallback FS.
 * @param f Path of the file to create.
 * @return FSDataOutputStream for the created file.
 * @throws IOException if file creation fails.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatusForFallbackLink,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatusForFallbackLink(),1638,1657,"/**
 * Retrieves FileStatus array, handling fallback FS and URI updates.
 * Returns empty array if no statuses found.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1696,1730,"/**
 * Creates a directory with specified permissions, or throws exception.
 * @param dir Path of the directory to create.
 * @param permission Permissions for the new directory.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,makeTrashRelativePath,"org.apache.hadoop.fs.TrashPolicyDefault:makeTrashRelativePath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",120,122,"/**
* Combines basePath and rmFilePath using Path.m1.
* @param basePath Base path.
* @param rmFilePath File to remove path.
* @return Combined path.
*/
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,innerPutPart,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerPutPart(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)",124,153,"/**
 * Writes a part of a file to a specified path using an InputStream.
 * @param filePath Path of the file
 * @param inputStream Input stream for the part data
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getParent,org.apache.hadoop.fs.Path:getParent(),429,431,"/**
 * Delegates to m1() and returns the resulting Path.
 */","* Returns the parent of a path or null if at root. Better alternative is
   * {@link #getOptionalParentPath()} to handle nullable value for root path.
   *
   * @return the parent of a path or null if at root",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getOptionalParentPath,org.apache.hadoop.fs.Path:getOptionalParentPath(),440,442,"/**
 * Returns an Optional Path derived from m1().
 */","* Returns the parent of a path as {@link Optional} or
   * {@link Optional#empty()} i.e an empty Optional if at root.
   *
   * @return Parent of path wrappen in {@link Optional}.
   * {@link Optional#empty()} i.e an empty Optional if at root.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getDirectoryContentsIterator,org.apache.hadoop.fs.shell.PathData:getDirectoryContentsIterator(),293,299,"/**
 * Returns a RemoteIterator of PathData for directories.
 * Uses fs.m2(path) to get FileStatus and transforms.
 */","* Returns a RemoteIterator for PathData objects of the items contained in the
   * given directory.
   * @return remote iterator of PathData objects for its children
   * @throws IOException if anything else goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,schemeFromPath,org.apache.hadoop.fs.Globber:schemeFromPath(org.apache.hadoop.fs.Path),171,182,"/**
 * Extracts the scheme from a Path, using fallback mechanisms.
 * @param path The Path to extract the scheme from.
 * @return The scheme string.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,authorityFromPath,org.apache.hadoop.fs.Globber:authorityFromPath(org.apache.hadoop.fs.Path),184,195,"/**
 * Extracts the authority from a Path, falling back to fs/fc.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,<init>,"org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",111,122,"/**
 * Creates a new FSDataOutputStreamBuilder with FileContext and Path.
 * @param fc FileContext for file system operations.
 * @param p Path to create the output stream for.
 * @throws IOException if an I/O error occurs.
 */
","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param p path.
   * @throws IOException failure",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setVerifyChecksum,"org.apache.hadoop.fs.FileContext:setVerifyChecksum(boolean,org.apache.hadoop.fs.Path)",1223,1228,"/**
 * Processes a file path.
 * @param verifyChecksum checksum verification flag
 * @param f input file path
 * @throws exceptions during file processing
 */
","* Set the verify checksum flag for the  file system denoted by the path.
   * This is only applicable if the 
   * corresponding FileSystem supports checksum. By default doesn't do anything.
   * @param verifyChecksum verify check sum.
   * @param f set the verifyChecksum for the Filesystem containing this path
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,readFields,org.apache.hadoop.fs.FileStatus:readFields(java.io.DataInput),496,522,"/**
 * Reads a FileStatusProto from the input stream and populates fields.
 */","* Read instance encoded as protobuf from stream.
   * @param in Input stream
   * @see PBHelper#convert(FileStatus)
   * @deprecated Use the {@link PBHelper} and protobuf serialization directly.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",140,149,"/**
 * Constructs a FileStatus with default replication type flags.
 * @param length File length, isdir, block size, etc. See parameters.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean,org.apache.hadoop.fs.BlockLocation[])",113,123,"/**
 * Constructs a LocatedFileStatus with block locations.
 * @param length File length, isdir, replication, blocksize, times,
 *               permission, owner, group, symlink, path, locations
 */
","* Constructor.
   *
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param hasAcl entity has associated ACLs
   * @param isEncrypted entity is encrypted
   * @param isErasureCoded entity is erasure coded
   * @param locations a file's block locations",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,<init>,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:<init>(org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder,org.apache.hadoop.fs.FileSystem)",87,96,"/**
 * Constructs a FileSystemMultipartUploader with provided builder and FileSystem.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,append,"org.apache.hadoop.io.MapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",399,416,"/**
 * Writes key-value pair to data, updates index if needed.
 * @param key The key to write.
 * @param val The value associated with the key.
 */","* Append a key/value pair to the map.  The key must be greater or equal
     * to the previous key added to the map.
     *
     * @param key key.
     * @param val value.
     * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,close,org.apache.hadoop.service.AbstractService:close(),246,249,"/**
 * Calls the m1 method.
 * @throws IOException if an I/O error occurs in m1
 */
","* Relay to {@link #stop()}
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Metadata:equals(java.lang.Object),785,795,"/**
 * Checks equality with another object.
 * @param other The object to compare to.
 * @return True if objects are equal, false otherwise.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,equals,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:equals(java.lang.Object),100,108,"/**
 * Checks if two RenewAction objects are equal by comparing tokens.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,equals,org.apache.hadoop.security.token.Token$PrivateToken:equals(java.lang.Object),284,297,"/**
 * Checks if two PrivateToken objects are equal.
 * @param o The object to compare to.
 * @return True if equal, false otherwise.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,selectDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials),998,1006,"/**
 * Attempts to obtain a token using multiple services.
 * @param creds Credentials to use for token retrieval.
 * @return A token object or null if none is found.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BasicDiskValidator.java,checkStatus,org.apache.hadoop.util.BasicDiskValidator:checkStatus(java.io.File),30,33,"/**
* Checks the given directory using DiskChecker.m1.
* @param dir The directory to check.
* @throws DiskErrorException if an error occurs during checking.
*/
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write,org.apache.hadoop.fs.FSOutputSummer:write(int),76,82,"/**
 * Adds a byte to the internal buffer.
 * @param b The byte to add. Calls m1() when buffer is full.
 */",Write one byte,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write1,"org.apache.hadoop.fs.FSOutputSummer:write1(byte[],int,int)",120,140,"/**
* Copies data from byte array `b` to internal buffer.
* @param b input byte array, @param off offset, @param len length
* @return number of bytes copied
*/","* Write a portion of an array, flushing to the underlying
   * stream at most once if necessary.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",79,121,"/**
 * Decodes data using Reed-Solomon, handling errors based on erasure count.
 * @param inputs Input data units.
 * @param outputs Output buffers for decoded data.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingValidator.java,validate,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])",138,143,"/**
 * Processes EC chunks, converting to ByteBuffers and calling m2.
 * @param inputs ECChunk array, outputs array
 * @param erasedIndexes Indexes of erased chunks
 * @param outputs ECChunk array
 */
","*  Validate outputs decoded from inputs, by decoding an input back from
   *  those outputs and comparing it with the original one.
   * @param inputs input buffers used for decoding
   * @param erasedIndexes indexes of erased units used for decoding
   * @param outputs decoded output buffers
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",54,58,"/**
* Decodes input chunks and writes to output chunks using rawDecoder.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartA,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartA(),1077,1104,"/**
* Advances the decompression state based on current conditions.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartA,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartA(),1106,1126,"/**
 * Advances the state based on comparison and reads data.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,finish,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finish(),718,732,"/**
* Releases resources associated with the current processing.
* Clears currentChar, calls m2/m3, and sets out/data to null.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write0,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write0(int),881,900,"/**
 * Processes a byte, potentially extending a run-length sequence.
 * @param b The byte to process.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,finishDataBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:finishDataBlock(boolean),653,671,"/**
 * Finishes the block appender if forced or block is full.
 * @param bForceFinish flag to force finishing the appender
 */","* Close the current data block if necessary.
     * 
     * @param bForceFinish
     *          Force the closure regardless of the block size.
     * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:close(),3549,3556,"/**
 * Processes segment descriptors until m2() returns null.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,parkCursorAtEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:parkCursorAtEnd(),1561,1571,"/**
 * Initializes state, moves location, and releases block reader.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageStream,org.apache.hadoop.security.Credentials:readTokenStorageStream(java.io.DataInputStream),273,295,"/**
 * Reads and processes token storage data from input stream.
 * @param in Input stream containing token storage data.
 */","* Convenience method for reading a token from a DataInputStream.
   *
   * @param in DataInputStream.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getCandidateTokensForCleanup,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getCandidateTokensForCleanup(),176,197,"/**
* Retrieves delegation tokens for cleanup from the SQL secret manager.
* @return Map of TokenIdent to DelegationTokenInformation.
*/","* Obtain a list of tokens that will be considered for cleanup, based on the last
   * time the token was updated in SQL. This list may include tokens that are not
   * expired and should not be deleted (e.g. if the token was last renewed using a
   * higher renewal interval).
   * The number of results is limited to reduce performance impact. Some level of
   * contention is expected when multiple routers run cleanup simultaneously.
   * @return Map of tokens that have not been updated in SQL after the token renewal
   *         period.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)",644,650,"/**
 * Retrieves delegation token information.
 * @param ident TokenIdent object; @param quiet quiet flag
 * @return DelegationTokenInformation object
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawValue,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes),3866,3869,"/**
 * Reads a value length from ValueBytes and returns it.
 * @param rawValue ValueBytes object to read from
 * @return Length of the value read.
 */
","* Fills up the passed rawValue with the value corresponding to the key
       * read earlier.
       * @param rawValue input ValueBytes rawValue.
       * @return the length of the value
       * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeExpiredStoredToken,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),213,229,"/**
 * Removes a token, handling deletion by other routers.
 * @param ident TokenIdent to remove.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,readFields,org.apache.hadoop.fs.FsServerDefaults:readFields(java.io.DataInput),175,185,"/**
 * Reads block metadata from input stream.
 * @param in Input stream containing block metadata.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeProto,org.apache.hadoop.security.Credentials:writeProto(java.io.DataOutput),378,397,"/**
* Writes credentials data to the provided DataOutput stream.
*/","* Write contents of this instance as CredentialsProto message to DataOutput.
   * @param out
   * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,protoFromToken,org.apache.hadoop.ipc.ProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token),128,130,"/**
* Delegates Token processing to ShadedProtobufHelper.
* @param tok Token object to process
* @return TokenProto object
*/
","* Create a {@code TokenProto} instance
   * from a hadoop token.
   * This builds and caches the fields
   * (identifier, password, kind, service) but not
   * renewer or any payload.
   * @param tok token
   * @return a marshallable protobuf class.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenIdentifier.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text),37,39,"/**
 * Constructs a DelegationTokenIdentifier with the given kind.
 * @param kind The kind of delegation token.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invoke,"org.apache.hadoop.io.retry.RetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",358,374,"/**
 * Executes a method call, handling retry logic and async invocation.
 * Returns the result or null for async calls.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,entry,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:entry(),1619,1622,"/**
 * Creates and returns a new Entry object after calling m1().
 */","* Get an entry to access the key and value.
       * 
       * @return The Entry object to access the key and value.
       * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareCursorKeyTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:compareCursorKeyTo(org.apache.hadoop.io.file.tfile.RawComparable),1642,1646,"/**
* Reads data using a reader, passing derived values from 'other'.
*/","* Internal API. Comparing the key at cursor to user-specified key.
       * 
       * @param other
       *          user-specified key.
       * @return negative if key at cursor is smaller than user key; 0 if equal;
       *         and positive if key at cursor greater than user key.
       * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,get,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:get(org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable)",1675,1679,"/**
* Executes m1 with the key and m2 with the value.
* @param key The key to pass to m1.
* @param value The value to pass to m2.
*/
","* Copy the key and value in one shot into BytesWritables. This is
         * equivalent to getKey(key); getValue(value);
         * 
         * @param key
         *          BytesWritable to hold key.
         * @param value
         *          BytesWritable to hold value
         * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,inBlockAdvance,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(long),1986,1995,"/**
 * Executes a loop of operations using valueBufferInputStream.
 * @param n iteration count
 */","* Advance cursor by n positions within the block.
       * 
       * @param n
       *          Number of key-value pairs to skip in block.
       * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String)",168,172,"/**
 * Delegates token renewal.
 * @param url URL to delegate to.
 * @param token Existing token.
 * @param renewer Renewal identifier.
 */
","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return abstract delegation token identifier.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)",217,222,"/**
 * Delegates a URL request.
 * @param url URL to request.
 * @param token Authentication token.
 * @param dToken Delegation token.
 * @return Result code.
 */
","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)",257,262,"/**
 * Calls m1 with null as the fourth argument.
 * @param url The URL to process.
 * @param token Authentication token.
 * @param dToken Delegation token.
 */
","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getInputStream,org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket),470,473,"/**
* Creates a SocketInputWrapper using the socket and its timeout.
* @param socket The socket to wrap.
* @return A SocketInputWrapper.
*/
","* Same as <code>getInputStream(socket, socket.getSoTimeout()).</code>
   *
   * @param socket socket.
   * @throws IOException raised on errors performing I/O.
   * @return SocketInputWrapper for reading from the socket.
   * @see #getInputStream(Socket, long)",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getOutputStream,org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket),526,529,"/**
* Returns an OutputStream for the given socket, using default flags.
* @param socket Socket to create OutputStream for.
* @return OutputStream for the socket.
*/
","* Same as getOutputStream(socket, 0). Timeout of zero implies write will
   * wait until data is available.<br><br>
   * 
   * From documentation for {@link #getOutputStream(Socket, long)} : <br>
   * Returns OutputStream for the socket. If the socket has an associated
   * SocketChannel then it returns a 
   * {@link SocketOutputStream} with the given timeout. If the socket does not
   * have a channel, {@link Socket#getOutputStream()} is returned. In the later
   * case, the timeout argument is ignored and the write will wait until 
   * data is available.<br><br>
   * 
   * Any socket created using socket factories returned by {@link NetUtils},
   * must use this interface instead of {@link Socket#getOutputStream()}.
   * 
   * @see #getOutputStream(Socket, long)
   * 
   * @param socket socket.
   * @return OutputStream for writing to the socket.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,putMetrics,org.apache.hadoop.metrics2.sink.StatsDSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),97,148,"/**
 * Processes a metrics record, extracting and formatting data.
 * @param record The metrics record to process.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,connect,"org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,int)",574,578,"/**
* Delegates to overloaded method with null bindAddress.
* @param socket Socket to use
* @param address SocketAddress to connect to
* @param timeout Connection timeout in milliseconds
*/","* This is a drop-in replacement for 
   * {@link Socket#connect(SocketAddress, int)}.
   * In the case of normal sockets that don't have associated channels, this 
   * just invokes <code>socket.connect(endpoint, timeout)</code>. If 
   * <code>socket.getChannel()</code> returns a non-null channel,
   * connect is implemented using Hadoop's selectors. This is done mainly
   * to avoid Sun's connect implementation from creating thread-local 
   * selectors, since Hadoop does not have control on when these are closed
   * and could end up taking all the available file descriptors.
   * 
   * @see java.net.Socket#connect(java.net.SocketAddress, int)
   * 
   * @param socket socket.
   * @param address the remote address
   * @param timeout timeout in milliseconds
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,sampleMetrics,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:sampleMetrics(),403,418,"/**
 * Creates a MetricsBuffer by collecting data from configured sources.
 * Returns the populated MetricsBuffer.
 */","* Sample all the sources for a snapshot of metrics/tags
   * @return  the metrics buffer containing the snapshot",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getAttribute,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttribute(java.lang.String),104,118,"/**
 * Retrieves an attribute value by name.
 * @param attribute Attribute name to retrieve.
 * @return Attribute value or throws exception if not found.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getAttributes,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttributes(java.lang.String[]),127,141,"/**
 * Creates an AttributeList from the provided attribute keys.
 * @param attributes Array of attribute keys to retrieve.
 * @return An AttributeList containing the retrieved attributes.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getMBeanInfo,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMBeanInfo(),154,158,"/**
 * Retrieves MBeanInfo from the cache after calling m1().
 * @return MBeanInfo object from the cache.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,register,"org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.lang.Object)",71,74,"/**
* Delegates to overloaded method with empty collection.
* @param serviceName MBean service name.
* @param nameName MBean name.
* @param theMbean The MBean object.
* @return ObjectName for the MBean.
*/
","* Register the MBean using our standard MBeanName format
   * ""hadoop:service={@literal <serviceName>,name=<nameName>}""
   * Where the {@literal <serviceName> and <nameName>} are the supplied
   * parameters.
   *
   * @param serviceName serviceName.
   * @param nameName nameName.
   * @param theMbean - the MBean to register
   * @return the named used to register the MBean",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stop,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stop(),196,219,"/**
* Stops the metrics system, executes callbacks, and logs status.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,getMetric,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:getMetric(java.lang.String),86,103,"/**
 * Retrieves or creates metrics for the given directory.
 * @param dirName The directory name for which metrics are needed.
 * @return ReadWriteDiskValidatorMetrics object.
 */","* Get a metric by given directory name.
   *
   * @param dirName directory name
   * @return the metric",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class,java.lang.String)",190,193,"/**
* Sets type prefix and calls m1 with the given protocol.
* @param protocol Class object representing the protocol.
* @param prefix String prefix for the type.
*/
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,init,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:init(int),70,80,"/**
 * Initializes RPC stats for a given number of priority levels.
 * @param numLevels The number of priority levels to initialize.
 */","* Initialize the metrics for JMX with priority levels.
   * @param numLevels input numLevels.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",116,132,"/**
 * Processes weak references and global metrics.
 * Iterates through references, handles nulls, and updates records.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,collectThreadLocalStates,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:collectThreadLocalStates(),137,143,"/**
 * Updates thread-local statistics if available.
 */","* Collects states maintained in {@link ThreadLocal}, if any.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:<init>(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,java.lang.String,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,int,int,int,float,int)",62,94,"/**
 * Creates a MetricsSinkAdapter with specified configuration parameters.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String),289,291,"/**
* Creates a MutableRate with name and default values.
* @param name The name of the rate.
* @return A new MutableRate object.
*/
","* Create a mutable rate metric
   * @param name  of the metric
   * @return a new mutable metric object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String)",299,301,"/**
* Creates a MutableRate with the given name and description.
* @param name Rate name.
* @param description Rate description.
*/
","* Create a mutable rate metric
   * @param name  of the metric
   * @param description of the metric
   * @return a new mutable rate metric object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,create,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:create(java.lang.String),60,64,"/**
 * Creates and registers a DecayRpcSchedulerDetailedMetrics instance.
 * @param ns Namespace for the metrics.
 * @return Registered DecayRpcSchedulerDetailedMetrics object.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,create,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:create(int),66,69,"/**
 * Creates and registers RPC metrics for the given port.
 * @param port The port number for RPC metrics.
 * @return RpcDetailedMetrics object.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Handler:run(),3151,3230,"/**
 * Processes calls from the call queue until the 'running' flag is false.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpKeytab,org.apache.hadoop.security.KDiag:dumpKeytab(java.io.File),596,620,"/**
 * Loads and processes a keytab file, extracting and displaying entries.
 */","* Dump a keytab: list all principals.
   *
   * @param keytabFile the keytab file
   * @throws IOException IO problems",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateJAAS,org.apache.hadoop.security.KDiag:validateJAAS(boolean),755,771,"/**
 * Checks and initializes JAAS configuration if required.
 * @param jaasRequired flag to determine if JAAS is needed.
 */","* Validate any JAAS entry referenced in the {@link #SUN_SECURITY_JAAS_FILE}
   * property.
   * @param jaasRequired is JAAS required",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateNTPConf,org.apache.hadoop.security.KDiag:validateNTPConf(),773,784,"/**
 * Configures NTP settings if not on Windows.
 * Reads & validates NTP file, updates config if valid.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTokenRealOwner,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenRealOwner(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),907,917,"/**
 * Extracts a user identifier based on TokenIdent properties.
 * Returns the identifier from m1().m2() or m4().m5().
 */","* Return the real owner for a token. If this is a token from a proxy user,
   * the real/effective user will be returned.
   *
   * @param id
   * @return real owner",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,getUgi,org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto),124,131,"/**
 * Retrieves UserGroupInformation from IpcConnectionContext.
 * @param context IpcConnectionContext containing user info.
 * @return UserGroupInformation or null if context is invalid.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUid,org.apache.hadoop.security.ShellBasedIdMapping:getUid(java.lang.String),632,644,"/**
 * Retrieves user ID by name, retries, and throws on failure.
 * @param user User's name.
 * @return User's ID as an integer.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGid,org.apache.hadoop.security.ShellBasedIdMapping:getGid(java.lang.String),646,658,"/**
 * Retrieves group ID by name, creating if necessary.
 * @param group Group name. Throws IOException if not found.
 * @return Group ID.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUserName,"org.apache.hadoop.security.ShellBasedIdMapping:getUserName(int,java.lang.String)",660,676,"/**
 * Retrieves user name by UID, using default if not found.
 * @param uid User identifier.
 * @param unknown Default user name to use if UID not found.
 * @return User's name or default name.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGroupName,"org.apache.hadoop.security.ShellBasedIdMapping:getGroupName(int,java.lang.String)",678,694,"/**
 * Retrieves group name by ID, using default if not found.
 * @param gid Group ID.
 * @param unknown Default group name to use if not found.
 * @return Group name.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,ensureParentZNode,org.apache.hadoop.ha.ActiveStandbyElector:ensureParentZNode(),360,396,"/**
 * Creates parent ZNodes in ZK, ensuring existence and ACLs.
 * Throws IOException/KeeperException if creation fails.
 */","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.
   * @throws KeeperException other zookeeper operation errors.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,getActiveData,org.apache.hadoop.ha.ActiveStandbyElector:getActiveData(),474,491,"/**
 * Retrieves data from ZooKeeper lock file path.
 * @return Byte array containing data or null if not found.
 * @throws ActiveNotFoundException, KeeperException, InterruptedException, IOException
 */","* get data set by the active leader
   * 
   * @return data set by the active instance
   * @throws ActiveNotFoundException
   *           when there is no active leader
   * @throws KeeperException
   *           other zookeeper operation errors
   * @throws InterruptedException
   *           interrupted exception.
   * @throws IOException
   *           when ZooKeeper connection could not be established",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reEstablishSession,org.apache.hadoop.ha.ActiveStandbyElector:reEstablishSession(),872,892,"/**
* Attempts to establish a Zookeeper connection with retries.
* Returns true on success, false if max retries are exhausted.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,checkTGTAndReloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:checkTGTAndReloginFromKeytab(),1197,1199,"/**
 * Calls m1 with true as an argument.
 */","* Re-login a user from keytab if TGT is expired or is close to expiry.
   * 
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if it's a kerberos login exception.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(),1245,1249,"/**
* Calls m1 with 'false' as the parameter.
*/","* Re-Login a user in from a keytab file. Loads a user identity from a keytab
   * file and logs them in. They become the currently logged-in user. This
   * method assumes that {@link #loginUserFromKeytab(String, String)} had
   * happened already.
   * The Subject field of this UserGroupInformation object is updated to have
   * the new credentials.
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean,boolean)",229,261,"/**
 * Resolves a target address to an InetSocketAddress, using config if available.
 * @param target Address string to resolve.
 * @param defaultPort Default port if not specified.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getConnectAddress,org.apache.hadoop.net.NetUtils:getConnectAddress(java.net.InetSocketAddress),450,460,"/**
 * Modifies address if condition is met, using fallback if needed.
 * @param addr The InetSocketAddress to potentially modify.
 * @return The modified or original InetSocketAddress.
 */","* Returns an InetSocketAddress that a client can use to connect to the
   * given listening address.
   * 
   * @param addr of a listener
   * @return socket address that a client can use to connect to the server.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,updateAddress,org.apache.hadoop.ipc.Client$Connection:updateAddress(),588,606,"/**
 * Updates server address and user info if a change is detected.
 * @return True if address changed, false otherwise.
 */","* Update the server address if the address corresponding to the host
     * name has changed.
     *
     * @return true if an addr change was detected.
     * @throws IOException when the hostname cannot be resolved.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getCanonicalUri,"org.apache.hadoop.net.NetUtils:getCanonicalUri(java.net.URI,int)",333,354,"/**
 * Modifies URI, updating host to fqHost, using defaultPort if needed.
 */","* Resolve the uri's hostname and add the default port if not in the uri
   * @param uri to resolve
   * @param defaultPort if none is given
   * @return URI",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",1467,1531,"/**
* Sends an RPC request and returns a Writable response or null.
* @param rpcKind RPC kind, request, remoteId, serviceClass, auth flags, context
* @return Writable response or null if async RPC is enabled.
*/","* Make a call, passing <code>rpcRequest</code>, to the IPC server defined by
   * <code>remoteId</code>, returning the rpc response.
   *
   * @param rpcKind
   * @param rpcRequest -  contains serialized method and method parameters
   * @param remoteId - the target rpc server
   * @param serviceClass - service class for RPC
   * @param fallbackToSimpleAuth - set to true or false during this method to
   *   indicate if a secure client falls back to simple auth
   * @param alignmentContext - state alignment context
   * @return the rpc response
   * Throws exceptions if there are network problems or if the remote code
   * threw an exception.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection:run(),1082,1109,"/**
* Processes RPC requests, handles errors, and logs activity.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,isIn,org.apache.hadoop.util.CacheableIPList:isIn(java.lang.String),62,75,"/**
 * Checks if IP is in the list and refreshes cache if expired.
 * @param ipAddress IP address to check.
 * @return True if IP is in the list, false otherwise.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache$CacheEntry),259,300,"/**
* Adds a CacheEntry to the set, handles retry logic, and waits.
* @param newEntry The CacheEntry to add.
* @return The CacheEntry, after potential waiting.
*/","* This method handles the following conditions:
   * <ul>
   * <li>If retry is not to be processed, return null</li>
   * <li>If there is no cache entry, add a new entry {@code newEntry} and return
   * it.</li>
   * <li>If there is an existing entry, wait for its completion. If the
   * completion state is {@link CacheEntry#FAILED}, the expectation is that the
   * thread that waited for completion, retries the request. the
   * {@link CacheEntry} state is set to {@link CacheEntry#INPROGRESS} again.
   * <li>If the completion state is {@link CacheEntry#SUCCESS}, the entry is
   * returned so that the thread that waits for it can can return previous
   * response.</li>
   * <ul>
   * 
   * @return {@link CacheEntry}.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,addCacheEntry,"org.apache.hadoop.ipc.RetryCache:addCacheEntry(byte[],int)",309,319,"/**
 * Adds a new cache entry to the set.
 * @param clientId Client identifier.
 * @param callId Call identifier.
 */
","* Add a new cache entry into the retry cache. The cache entry consists of 
   * clientId and callId extracted from editlog.
   *
   * @param clientId input clientId.
   * @param callId input callId.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,addCacheEntryWithPayload,"org.apache.hadoop.ipc.RetryCache:addCacheEntryWithPayload(byte[],int,java.lang.Object)",321,333,"/**
 * Adds a new cache entry with payload to the cache set.
 * @param clientId Client identifier.
 * @param callId Call identifier.
 * @param payload The data to be cached.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,org.apache.hadoop.fs.ContentSummary:toString(boolean),381,384,"/**
* Overload of m1, calls the other m1 with default false value.
*/","Return the string representation of the object in the output format.
   * if qOption is false, output directory count, file count, and content size;
   * if qOption is true, output quota and remaining quota as well.
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @return the string representation of the object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,org.apache.hadoop.fs.QuotaUsage:toString(),302,305,"/**
* Calls m1 with default value for the boolean parameter.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,getExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:getExpression(java.lang.String,org.apache.hadoop.conf.Configuration)",108,116,"/**
 * Creates and returns an Expression instance by name and config.
 * @param expressionName Name of the expression to create.
 * @param conf Configuration object for the expression.
 */
","* Get an instance of the requested expression
   *
   * @param expressionName
   *          name of the command to lookup
   * @param conf
   *          the Hadoop configuration
   * @return the {@link Expression} or null if the expression is unknown",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,createExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.String,org.apache.hadoop.conf.Configuration)",145,155,"/**
 * Creates an Expression instance by class name.
 * @param expressionClassname Class name of the Expression.
 * @param conf Configuration object.
 * @throws IllegalArgumentException if class not found.
 */
","* Creates an instance of the requested {@link Expression} class.
   *
   * @param expressionClassname
   *          name of the {@link Expression} class to be instantiated
   * @param conf
   *          the Hadoop configuration
   * @return a new instance of the requested {@link Expression} class",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,buildDescription,org.apache.hadoop.fs.shell.find.Find:buildDescription(org.apache.hadoop.fs.shell.find.ExpressionFactory),109,159,"/**
 * Generates a help string describing recognised expressions.
 * @param factory Expression factory to create expressions.
 * @return Help string describing expressions.
 */",Build the description used by the help command.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getExpression,org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.Class),438,442,"/**
* Creates an expression of the given class.
* @param expressionClass Class of the expression to create.
* @return An Expression object.
*/
",Gets an instance of an expression from the factory.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,getInstance,org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String),108,110,"/**
 * Executes command 'cmd' with default value from m1().
 * @param cmd The command to execute.
 * @return A Command object.
 */
","* Returns an instance of the class implementing the given command.  The
   * class must have been registered via
   * {@link #addClass(Class, String...)}
   * @param cmd name of the command
   * @return instance of the requested command",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,"org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)",141,154,"/**
 * Constructs a WritableComparator.
 * @param keyClass Class of WritableComparable keys.
 * @param conf Configuration object.
 * @param createInstances Whether to create internal objects.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readObject,"org.apache.hadoop.io.ObjectWritable:readObject(java.io.DataInput,org.apache.hadoop.io.ObjectWritable,org.apache.hadoop.conf.Configuration)",261,340,"/**
 * Reads an object from DataInput, handling primitives, arrays, and custom types.
 * @param in DataInput to read from, objectWritable to store result, conf Configuration
 * @return Object read from input
 */","* Read a {@link Writable}, {@link String}, primitive type, or an array of
   * the preceding.
   *
   * @param in DataInput.
   * @param objectWritable objectWritable.
   * @param conf configuration.
   * @return Object.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableFactories.java,newInstance,org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class),81,83,"/**
 * Creates a new Writable object of the specified class.
 * @param c Class of the Writable object to create.
 * @return A new Writable object.
 */
","* Create a new instance of a class with a defined factory.
   * @param c input c.
   * @return a new instance of a class with a defined factory.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,decodeTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token),870,872,"/**
 * Delegates to token's m1() method.
 * @param token The token to delegate to.
 * @return The result of token.m1()
 */
","* Decode the token identifier. The subclass can customize the way to decode
   * the token identifier.
   * 
   * @param token the token where to extract the identifier
   * @return the delegation token identifier
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,identifierToString,org.apache.hadoop.security.token.Token:identifierToString(java.lang.StringBuilder),422,435,"/**
 * Appends ID to buffer or uses identifier if ID retrieval fails.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,printCredentials,"org.apache.hadoop.security.token.DtFileOperations:printCredentials(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text,java.io.PrintStream)",137,163,"/**
 * Prints delegation tokens from credentials to output stream.
 * @param creds Credentials object containing tokens.
 * @param alias Text alias.
 * @param out PrintStream to write output to.
 */","Print out a Credentials object.
   *  @param creds the Credentials object to be printed out.
   *  @param alias print only tokens matching alias (null matches all).
   *  @param out print to this stream.
   *  @throws IOException failure to unmarshall a token identifier.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskValidatorFactory.java,getInstance,org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.String),72,92,"/**
 * Creates a DiskValidator instance based on the diskValidator string.
 * @param diskValidator String representing the validator class name.
 * @return DiskValidator instance.
 */","* Returns {@link DiskValidator} instance corresponding to its name.
   * The diskValidator parameter can be ""basic"" for {@link BasicDiskValidator}
   * or ""read-write"" for {@link ReadWriteDiskValidator}.
   * @param diskValidator canonical class name, for example, ""basic""
   * @throws DiskErrorException if the class cannot be located
   * @return disk validator.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,parseMethod,"org.apache.hadoop.ha.NodeFencer:parseMethod(org.apache.hadoop.conf.Configuration,java.lang.String)",151,165,"/**
* Parses a line to create a FenceMethodWithArg.
* @param conf Configuration object.
* @param line Line to parse.
* @return FenceMethodWithArg object.
* @throws BadFencingConfigurationException if parsing fails.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,<init>,"org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.lang.String)",58,65,"/**
 * Initializes the HostsFileReader with input and exception files.
 * @param inFile Input file path.
 * @param exFile Exception file path.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,org.apache.hadoop.util.HostsFileReader:refresh(),118,121,"/**
* Calls m2 with files from HostDetails obtained via current.m1().
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,add,org.apache.hadoop.util.bloom.DynamicBloomFilter:add(org.apache.hadoop.util.bloom.Key),136,153,"/**
 * Adds a key to the Bloom filter.
 * @param key The key to add. Throws NullPointerException if null.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResourceObject,org.apache.hadoop.conf.Configuration:addResourceObject(org.apache.hadoop.conf.Configuration$Resource),1034,1038,"/**
 * Processes a resource, updates restrictions, and adjusts properties.
 * @param resource The resource to process.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getProps,org.apache.hadoop.conf.Configuration:getProps(),2946,2952,"/**
 * Returns the Properties object, initializing it if null.
 * Calls m1 to populate the properties.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readVectored,"org.apache.hadoop.fs.BufferedFSInputStream:readVectored(java.util.List,java.util.function.IntFunction)",179,183,"/**
* Delegates m1 call to the underlying PositionedReadable.
* @param ranges List of file ranges.
* @param allocate Allocates ByteBuffer.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,readVectored,"org.apache.hadoop.fs.FSDataInputStream:readVectored(java.util.List,java.util.function.IntFunction)",304,308,"/**
 * Delegates m1 to the underlying PositionedReadable.
 * @param ranges List of FileRange objects.
 * @param allocate IntFunction for ByteBuffer allocation.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,getInternal,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getInternal(org.apache.hadoop.fs.impl.prefetch.BufferData),177,208,"/**
* Processes BufferData, transitioning states and performing operations.
* @param data BufferData object to process.
* @throws IOException if an I/O error occurs.
*/
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,org.apache.hadoop.fs.BlockLocation:<init>(),82,84,"/**
 * Default constructor, initializes with empty arrays and zero timestamps.
 */",* Default Constructor.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",883,901,"/**
 * Returns an array containing a single BlockLocation.
 * @param file FileStatus object
 * @param start Start offset (unused)
 * @param len Length (unused)
 * @return BlockLocation array
 */","* Return an array containing hostnames, offset and size of
   * portions of the given file.  For nonexistent
   * file or regions, {@code null} is returned.
   *
   * <pre>
   *   if f == null :
   *     result = null
   *   elif f.getLen() {@literal <=} start:
   *     result = []
   *   else result = [ locations(FS, b) for b in blocks(FS, p, s, s+l)]
   * </pre>
   * This call is most helpful with and distributed filesystem
   * where the hostnames of machines that contain blocks of the given file
   * can be determined.
   *
   * The default implementation returns an array containing one element:
   * <pre>
   * BlockLocation( { ""localhost:9866"" },  { ""localhost"" }, 0, file.getLen())
   * </pre>
   *
   * In HDFS, if file is three-replicated, the returned array contains
   * elements like:
   * <pre>
   * BlockLocation(offset: 0, length: BLOCK_SIZE,
   *   hosts: {""host1:9866"", ""host2:9866, host3:9866""})
   * BlockLocation(offset: BLOCK_SIZE, length: BLOCK_SIZE,
   *   hosts: {""host2:9866"", ""host3:9866, host4:9866""})
   * </pre>
   *
   * And if a file is erasure-coded, the returned BlockLocation are logical
   * block groups.
   *
   * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).
   * 1. If the file size is less than one stripe size, say 2 * CELL_SIZE, then
   * there will be one BlockLocation returned, with 0 offset, actual file size
   * and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks.
   * 3. If the file size is less than one group size but greater than one
   * stripe size, then there will be one BlockLocation returned, with 0 offset,
   * actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting
   * the actual blocks.
   * 4. If the file size is greater than one group size, 3 * BLOCK_SIZE + 123
   * for example, then the result will be like:
   * <pre>
   * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {""host1:9866"",
   *   ""host2:9866"",""host3:9866"",""host4:9866"",""host5:9866""})
   * BlockLocation(offset: 3 * BLOCK_SIZE, length: 123, hosts: {""host1:9866"",
   *   ""host4:9866"", ""host5:9866""})
   * </pre>
   *
   * @param file FilesStatus to get data from
   * @param start offset into the given file
   * @param len length for which to get locations for
   * @throws IOException IO failure
   * @return block location array.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,fetchDurationSummary,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchDurationSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String,boolean)",129,140,"/**
 * Calculates duration statistics for a key, considering success/failure.
 * @param source IOStatistics source.
 * @param key Statistic key.
 * @param success Success indicator.
 * @return DurationStatisticSummary object.
 */
","* Fetch the duration timing summary of success or failure operations
   * from an IO Statistics source.
   * If the duration key is unknown, the summary will be incomplete.
   * @param source source of data
   * @param key duration statistic key
   * @param success fetch success statistics, or if false, failure stats.
   * @return a summary of the statistics.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,snapshot,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:snapshot(),92,96,"/**
 * Creates an IOStatisticsSnapshot from the current context.
 * @return A new IOStatisticsSnapshot object.
 */","* Returns a snapshot of the current thread's IOStatistics.
   *
   * @return IOStatisticsSnapshot of the context.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,snapshotIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics(org.apache.hadoop.fs.statistics.IOStatistics),46,50,"/**
 * Creates an IOStatisticsSnapshot from the given statistics.
 * @param statistics The IOStatistics object to snapshot.
 * @return A new IOStatisticsSnapshot object.
 */
","* Take a snapshot of the current statistics state.
   * <p>
   * This is not an atomic option.
   * <p>
   * The instance can be serialized, and its
   * {@code toString()} method lists all the values.
   * @param statistics statistics
   * @return a snapshot of the current values.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create(java.lang.Object),123,125,"/**
 * Creates an IOStatisticsSnapshot from an IOStatistics object.
 * @param source The IOStatistics object to snapshot.
 * @return An IOStatisticsSnapshot.
 */
","* Create a new {@link IOStatisticsSnapshot} instance.
   * @param source optional source statistics
   * @return an IOStatisticsSnapshot.
   * @throws ClassCastException if the {@code source} is not null and not an IOStatistics instance",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,toList,org.apache.hadoop.util.functional.RemoteIterators:toList(org.apache.hadoop.fs.RemoteIterator),232,237,"/**
 * Collects elements from a RemoteIterator into a List.
 * @param source Iterator to collect from.
 * @return List containing elements from the iterator.
 */","* Build a list from a RemoteIterator.
   * @param source source iterator
   * @param <T> type
   * @return a list of the values.
   * @throws IOException if the source RemoteIterator raises it.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackStoreToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackStoreToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1002,1004,"/**
* Calls m1 to store a token, using provided invocation.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackUpdateToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackUpdateToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1006,1008,"/**
* Logs an invocation with associated token update.
* @param invocation The invocation to log.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackRemoveToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackRemoveToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1010,1012,"/**
* Calls m1 to process an invocation, passing specific parameters.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",59,65,"/**
* Processes ECChunks by converting to ByteBuffers and calling m2.
* @param inputChunks Input ECChunk array.
* @param outputChunks Output ECChunk array.
*/
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,write,org.apache.hadoop.io.EnumSetWritable:write(java.io.DataOutput),135,154,"/**
* Serializes the value to the DataOutput.
* Writes -1 if value is null, otherwise serializes elements.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,write,org.apache.hadoop.io.ObjectWritable:write(java.io.DataOutput),89,92,"/**
* Writes data to the output stream using m1.
* @param out DataOutput stream to write to.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processArgument,org.apache.hadoop.fs.shell.Command:processArgument(org.apache.hadoop.fs.shell.PathData),299,305,"/**
 * Processes a PathData item: calls m2 if exists, else m1.
 * @param item The PathData object to process.
 * @throws IOException if an I/O error occurs.
 */
","* Processes a {@link PathData} item, calling
   * {@link #processPathArgument(PathData)} or
   * {@link #processNonexistentPath(PathData)} on each item.
   * @param item {@link PathData} item to process
   * @throws IOException if anything goes wrong...",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getUnixGroups,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getUnixGroups(java.lang.String),203,231,"/**
 * Retrieves a set of group names for a given user.
 * Returns empty set on failure or if retry is not needed.
 */","* Get the current user's group list from Unix by running the command 'groups'
   * NOTE. For non-existing user it will return EMPTY list.
   *
   * @param user get groups for this user
   * @return the groups list that the <code>user</code> belongs to. The primary
   *         group is returned first.
   * @throws IOException if encounter any error when running the command",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,runResolveCommand,"org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:runResolveCommand(java.util.List,java.lang.String)",222,262,"/**
 * Executes commands from a script with arguments, returning combined output.
 * @param args Arguments to pass to the script.
 * @param commandScriptName Script name to execute.
 * @return Combined output string or null on error.
 */","* Build and execute the resolution command. The command is
     * executed in the directory specified by the system property
     * ""user.dir"" if set; otherwise the current working directory is used.
     * @param args a list of arguments
     * @param commandScriptName input commandScriptName.
     * @return null if the number of arguments is out of range,
     * or the output of the command.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[]),1214,1216,"/**
 * Constructs a ShellCommandExecutor with command strings.
 * @param execString Command strings to execute.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,readLink,org.apache.hadoop.fs.FileUtil:readLink(java.io.File),213,230,"/**
 * Reads and masks the content of a file.
 * @param f the file to read
 * @return masked content or """" if error occurs
 */
","* Returns the target of the given symlink. Returns the empty string if
   * the given path does not refer to a symlink or there is an error
   * accessing the symlink.
   * @param f File representing the symbolic link.
   * @return The target of the symbolic link, empty string on error or if not
   *         a symlink.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,execCommand,"org.apache.hadoop.fs.FileUtil:execCommand(java.io.File,java.lang.String[])",1531,1537,"/**
 * Executes a command with a file path and returns the output.
 * @param f The file to pass to the command.
 * @param cmd Command arguments.
 * @return String output from the command.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setPermission,"org.apache.hadoop.fs.RawLocalFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1108,1119,"/**
 * Sets file permissions.
 * @param p Path to the file.
 * @param permission FsPermission object defining permissions.
 */",* Use the command chmod to set permission.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,execShellGetUserForNetgroup,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:execShellGetUserForNetgroup(java.lang.String),134,146,"/**
 * Masks a netgroup string using m4 transformations.
 * @param netgroup The netgroup string to mask.
 * @return The masked string, or an empty string on error.
 */","* Calls shell to get users for a netgroup by calling getent
   * netgroup, this is a low level function that just returns string
   * that 
   *
   * @param netgroup get users for this netgroup
   * @return string of users for a given netgroup in getent netgroups format
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,org.apache.hadoop.security.UserGroupInformation$TicketCacheRenewalRunnable:relogin(),1073,1078,"/**
 * Renews a Kerberos ticket and logs the output.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,stashOriginalFilePermissions,org.apache.hadoop.security.alias.LocalKeyStoreProvider:stashOriginalFilePermissions(),92,114,"/**
* Retrieves file permissions based on OS.
* Uses Files.getPosixFilePermissions() on Unix, 
* or external command on Windows.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,<init>,org.apache.hadoop.security.Credentials:<init>(org.apache.hadoop.security.Credentials),103,105,"/**
 * Creates a new Credentials object, copying from an existing one.
 */","* Create a copy of the given credentials.
   * @param credentials to copy",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addCredentials,org.apache.hadoop.security.UserGroupInformation:addCredentials(org.apache.hadoop.security.Credentials),1753,1757,"/**
 * Executes m1().m2(credentials) under synchronization.
 * @param credentials Authentication credentials.
 */","* Add the given Credentials to this user.
   * @param credentials of tokens and secrets",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",580,590,"/**
 * Creates a data output stream for a file.
 * @param f Path to the file.
 * @param permission File permissions.
 * @param flags Create flags.
 * @return FSDataOutputStream
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,toFile,org.apache.hadoop.fs.shell.PathData:toFile(),494,499,"/**
 * Delegates path resolution to LocalFileSystem.
 * @param path Path to resolve.
 * @return Resolved file object.
 */","* Get the path to a local file
   * @return File representing the local path
   * @throws IllegalArgumentException if this.fs is not the LocalFileSystem",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,mkdirsWithExistsAndPermissionCheck,"org.apache.hadoop.util.DiskChecker:mkdirsWithExistsAndPermissionCheck(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",224,235,"/**
 * Creates a directory and sets its permissions if needed.
 * @param localFS filesystem object
 * @param dir path of the directory
 * @param expected desired permissions
 */
","* Create the directory or check permissions if it already exists.
   *
   * The semantics of mkdirsWithExistsAndPermissionCheck method is different
   * from the mkdirs method provided in the Sun's java.io.File class in the
   * following way:
   * While creating the non-existent parent directories, this method checks for
   * the existence of those directories if the mkdir fails at any point (since
   * that directory might have just been created by some other process).
   * If both mkdir() and the exists() check fails for any seemingly
   * non-existent directory, then we signal an error; Sun's mkdir would signal
   * an error (return false) if a directory it is attempting to create already
   * exists or the mkdir fails.
   *
   * @param localFS local filesystem
   * @param dir directory to be created or checked
   * @param expected expected permission
   * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.ViewFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),441,445,"/**
* Sets the working directory.
* @param new_dir The new directory to set as working.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getNativeFileLinkStatus,"org.apache.hadoop.fs.RawLocalFileSystem:getNativeFileLinkStatus(org.apache.hadoop.fs.Path,boolean)",1300,1306,"/**
 * Retrieves a FileStatus object for the given path.
 * @param f The path to retrieve the status for.
 * @param dereference Whether to dereference symbolic links.
 * @return FileStatus object representing the file or directory.
 */","* Calls out to platform's native stat(1) implementation to get file metadata
   * (permissions, user, group, atime, mtime, etc). This works around the lack
   * of lstat(2) in Java 6.
   * 
   *  Currently, the {@link Stat} class used to do this only supports Linux
   *  and FreeBSD, so the old {@link #deprecatedGetFileLinkStatusInternal(Path)}
   *  implementation (deprecated) remains further OS support is added.
   *
   * @param f File to stat
   * @param dereference whether to dereference symlinks
   * @return FileStatus of f
   * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getResolvedQualifiedPath,org.apache.hadoop.fs.viewfs.ChRootedFs:getResolvedQualifiedPath(org.apache.hadoop.fs.Path),167,171,"/**
 * Retrieves a Path object from the file system using a combined path.
 * @param f The input Path object.
 * @return A Path object representing the retrieved file.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.AbstractFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1614,1625,"/**
 * Checks if a path has a specific capability.
 * @param path The path to check.
 * @param capability Capability to check for.
 * @return True if capability exists, false otherwise.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.AbstractFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),1652,1657,"/**
 * Processes a path and returns a new Path object.
 * @param path Input path to be processed.
 * @return A new Path object.
 */","* Return path of the enclosing root for a given path
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   *
   * Call makeQualified on the param path to ensure its part of the correct filesystem
   *
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,makeQualified,org.apache.hadoop.fs.FilterFs:makeQualified(org.apache.hadoop.fs.Path),73,76,"/**
 * Delegates Path retrieval to the underlying FileSystem.
 * @param path The Path to retrieve.
 * @return The requested Path object.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",1892,1903,"/**
 * Filters file statuses based on a filter and adds to results.
 * @param results List to add matching FileStatus objects.
 * @param f Path to check.
 * @param filter Filter to apply.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,listStatus,org.apache.hadoop.fs.Globber:listStatus(org.apache.hadoop.fs.Path),125,136,"/**
 * Lists file statuses for a given path.
 * Uses fs or fc if fs is null. Returns empty array on failure.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,processDeleteOnExit,org.apache.hadoop.fs.FileContext:processDeleteOnExit(),296,312,"/**
* Deletes files associated with FileContexts, then clears the set.
*/",* Delete all the paths that were marked as delete-on-exit.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,exists,org.apache.hadoop.fs.FileContext$Util:exists(org.apache.hadoop.fs.Path),1757,1766,"/**
 * Checks if a file exists. Returns true if found, false otherwise.
 */","* Does the file exist?
     * Note: Avoid using this method if you already have FileStatus in hand.
     * Instead reuse the FileStatus 
     * @param f the  file or dir to be checked
     *
     * @throws AccessControlException If access is denied
     * @throws IOException If an I/O error occurred
     * @throws UnsupportedFileSystemException If file system for <code>f</code> is
     *           not supported
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server
     * @return if f exists true, not false.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,getFileStatus,org.apache.hadoop.fs.Globber:getFileStatus(org.apache.hadoop.fs.Path),112,123,"/**
 * Gets FileStatus, using either fs or fc.
 * @param path Path to the file.
 * @return FileStatus object or null if not found.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setWorkingDirectory,org.apache.hadoop.fs.FileContext:setWorkingDirectory(org.apache.hadoop.fs.Path),542,554,"/**
 * Sets the working directory to the given path.
 * @param newWDir The new working directory path.
 * @throws IOException if an I/O error occurs.
 */
","* Set the working directory for wd-relative names (such a ""foo/bar""). Working
   * directory feature is provided by simply prefixing relative names with the
   * working dir. Note this is different from Unix where the wd is actually set
   * to the inode. Hence setWorkingDir does not follow symlinks etc. This works
   * better in a distributed environment that has multiple independent roots.
   * {@link #getWorkingDirectory()} should return what setWorkingDir() set.
   * 
   * @param newWDir new working directory
   * @throws IOException 
   * <br>
   *           NewWdir can be one of:
   *           <ul>
   *           <li>relative path: ""foo/bar"";</li>
   *           <li>absolute without scheme: ""/foo/bar""</li>
   *           <li>fully qualified with scheme: ""xx://auth/foo/bar""</li>
   *           </ul>
   * <br>
   *           Illegal WDs:
   *           <ul>
   *           <li>relative with scheme: ""xx:foo/bar""</li>
   *           <li>non existent directory</li>
   *           </ul>",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,checkDest,"org.apache.hadoop.fs.FileContext:checkDest(java.lang.String,org.apache.hadoop.fs.Path,boolean)",2261,2278,"/**
* Copies a file or recursively copies a directory.
* @param srcName Source file/directory name.
* @param dst Destination Path.
* @param overwrite Overwrite if destination exists.
*/","* Check if copying srcName to dst would overwrite an existing 
   * file or directory.
   * @param srcName File or directory to be copied.
   * @param dst Destination to copy srcName to.
   * @param overwrite Whether it's ok to overwrite an existing file. 
   * @throws AccessControlException If access is denied.
   * @throws IOException If dst is an existing directory, or dst is an 
   * existing file and the overwrite option is not passed.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getContentSummary,org.apache.hadoop.fs.FileContext$Util:getContentSummary(org.apache.hadoop.fs.Path),1786,1812,"/**
 * Calculates content summary for a file or directory.
 * @param f Path to file/directory. Returns ContentSummary object.
 */","* Return the {@link ContentSummary} of path f.
     * @param f path
     *
     * @return the {@link ContentSummary} of path f.
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>f</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getDelegationTokens,"org.apache.hadoop.fs.FileContext:getDelegationTokens(org.apache.hadoop.fs.Path,java.lang.String)",2432,2443,"/**
 * Retrieves tokens from file systems associated with a path.
 * @param p Path to check.
 * @param renewer Renewal identifier.
 * @return List of tokens.
 */
","* Get delegation tokens for the file systems accessed for a given
   * path.
   * @param p Path for which delegations tokens are requested.
   * @param renewer the account name that is allowed to renew the token.
   * @return List of delegation tokens.
   * @throws IOException If an I/O error occurred.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setXAttr,"org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",2584,2588,"/**
* Sets an XAttribute with CREATE/REPLACE flags.
* @param path Path to the file.
* @param name Attribute name.
* @param value Attribute value.
*/
","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException If an I/O error occurred.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSnapshot,org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path),2747,2749,"/**
 * Delegates to the overloaded method with a default resolver.
 * @param path The path to process.
 * @return The processed path.
 */
","* Create a snapshot with a default name.
   *
   * @param path The directory where snapshots will be taken.
   * @return the snapshot path.
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",77,87,"/**
 * Initializes the MultipartUploaderBuilder with FileContext and Path.
 * @param fc FileContext for server defaults.
 * @param p Path to upload to.
 */
","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param p path.
   * @throws IOException failure",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,<init>,org.apache.hadoop.fs.LocalFileSystem:<init>(),40,42,"/**
 * Constructs a LocalFileSystem using a RawLocalFileSystem.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,hasMoreThanOneSourcePaths,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:hasMoreThanOneSourcePaths(java.util.LinkedList),105,118,"/**
 * Processes PathData list. Returns true if m1() > 1,
 * otherwise checks PathData and calls m4().
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,waitForRecovery,org.apache.hadoop.fs.shell.Truncate:waitForRecovery(),102,116,"/**
 * Truncates PathData items in waitList to newLength.
 * Iterates and waits for each item to reach the specified length.
 */",* Wait for all files in waitList to have length equal to newLength.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,tryResolveInRegexMountpoint,"org.apache.hadoop.fs.viewfs.InodeTree:tryResolveInRegexMountpoint(java.lang.String,boolean)",1022,1032,"/**
 * Attempts to resolve a path using regex mount points.
 * @param srcPath Path to resolve.
 * @param resolveLastComponent Whether to resolve the last component.
 * @return ResolveResult object or null if no match found.
 */
","* Walk through all regex mount points to see
   * whether the path match any regex expressions.
   *  E.g. link: ^/user/(?&lt;username&gt;\\w+) =&gt; s3://$user.apache.com/_${user}
   *  srcPath: is /user/hadoop/dir1
   *  resolveLastComponent: true
   *  then return value is s3://hadoop.apache.com/_hadoop
   *
   * @param srcPath srcPath.
   * @param resolveLastComponent resolveLastComponent.
   * @return ResolveResult.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,<init>,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.Path[])",548,554,"/**
 * Initializes a PathIterator with a file system, path string, and root directories.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,next,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:next(),571,583,"/**
 * Returns the next path element, checking its existence.
 * @return Path object, or throws NoSuchElementException if null.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,ifExists,"org.apache.hadoop.fs.LocalDirAllocator:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)",248,251,"/**
 * Delegates path processing to AllocatorPerContext.
 * @param pathStr Path to process.
 * @param conf Configuration object.
 * @return Result of the delegated processing.
 */","*  We search through all the configured dirs for the file's existence
   *  and return true when we find.
   *  @param pathStr the requested file (this will be searched)
   *  @param conf the Configuration object
   *  @return true if files exist. false otherwise",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.LocalKeyStoreProvider:initFileSystem(java.net.URI),116,141,"/**
 * Initializes a local file based on the provided URI.
 * @param uri URI to construct the local file from.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,archivePath,org.apache.hadoop.fs.HarFileSystem:archivePath(org.apache.hadoop.fs.Path),200,211,"/**
 * Finds the first subpath ending with "".har"".
 * @param p The path to search within.
 * @return The matching Path, or null if not found.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getPathInHar,org.apache.hadoop.fs.HarFileSystem:getPathInHar(org.apache.hadoop.fs.Path),356,373,"/**
* Calculates a modified path based on archive path comparisons.
* Returns a new Path object or Path.SEPARATOR if no match.
*/","* this method returns the path 
   * inside the har filesystem.
   * this is relative path inside 
   * the har filesystem.
   * @param path the fully qualified path in the har filesystem.
   * @return relative path in the filesystem.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,makeRelative,"org.apache.hadoop.fs.HarFileSystem:makeRelative(java.lang.String,org.apache.hadoop.fs.Path)",379,393,"/**
 * Constructs a Path object based on initial, URI components, and path traversal.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getChecksumFile,org.apache.hadoop.fs.ChecksumFs:getChecksumFile(org.apache.hadoop.fs.Path),88,90,"/**
 * Creates a new Path with a modified extension (.crc).
 * @param file The input Path object.
 * @return A new Path object with the modified extension.
 */
","* Return the name of the checksum file associated with a file.
   *
   * @param file the file path.
   * @return the checksum file associated with a file.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,createCollectorPath,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:createCollectorPath(org.apache.hadoop.fs.Path),155,161,"/**
 * Generates a masked path by combining several path manipulations.
 * @param filePath input path to be masked
 * @return Masked Path object
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,parentExists,org.apache.hadoop.fs.shell.PathData:parentExists(),250,253,"/**
* Reads a file; returns true on success, false otherwise.
*/","* Test if the parent directory exists
   * @return boolean indicating parent exists
   * @throws IOException upon unexpected error",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processNonexistentPath,org.apache.hadoop.fs.shell.Mkdir:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),69,90,"/**
* Checks if the path exists, creating parents if needed.
* @param item PathData object containing path and filesystem.
* @throws IOException if path or parent doesn't exist.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,primitiveMkdir,"org.apache.hadoop.fs.FileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",1392,1415,"/**
 * Creates a directory.
 * @param f the path to create
 * @param absolutePermission permissions for the directory
 * @param createParent whether to create parent directories
 */","* This version of the mkdirs method assumes that the permission is absolute.
   * It has been added to support the FileContext that processes the permission
   * with umask before calling this method.
   * This a temporary method added to support the transition from FileSystem
   * to FileContext for user applications.
   *
   * @param f the path.
   * @param absolutePermission permission.
   * @param createParent create parent.
   * @throws IOException IO failure.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,rename,"org.apache.hadoop.fs.FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",1669,1726,"/**
 * Renames a file or directory from src to dst, handling overwrite.
 * @param src Source path
 * @param dst Destination path
 * @param options Rename options (e.g., OVERWRITE)
 */","* Renames Path src to Path dst
   * <ul>
   *   <li>Fails if src is a file and dst is a directory.</li>
   *   <li>Fails if src is a directory and dst is a file.</li>
   *   <li>Fails if the parent of dst does not exist or is a file.</li>
   * </ul>
   * <p>
   * If OVERWRITE option is not passed as an argument, rename fails
   * if the dst already exists.
   * </p>
   * <p>
   * If OVERWRITE option is passed as an argument, rename overwrites
   * the dst if it is a file or an empty directory. Rename fails if dst is
   * a non-empty directory.
   * </p>
   * Note that atomicity of rename is dependent on the file system
   * implementation. Please refer to the file system documentation for
   * details. This default implementation is non atomic.
   * <p>
   * This method is deprecated since it is a temporary method added to
   * support the transition from FileSystem to FileContext for user
   * applications.
   * </p>
   *
   * @param src path to be renamed
   * @param dst new path after rename
   * @param options rename options.
   * @throws FileNotFoundException src path does not exist, or the parent
   * path of dst does not exist.
   * @throws FileAlreadyExistsException dest path exists and is a file
   * @throws ParentNotDirectoryException if the parent path of dest is not
   * a directory
   * @throws IOException on failure",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getNflyTmpPath,org.apache.hadoop.fs.viewfs.NflyFSystem:getNflyTmpPath(org.apache.hadoop.fs.Path),447,449,"/**
* Creates a new Path object with modified components.
* @param f The input Path object.
* @return A new Path object.
*/
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getChecksumFile,org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFile(org.apache.hadoop.fs.Path),120,122,"/**
 * Creates a new Path with a CRC extension based on the input Path.
 * @param file The input Path object.
 * @return A new Path object with the modified extension.
 */
","* Return the name of the checksum file associated with a file.
   *
   * @param file the file path.
   * @return name of the checksum file associated with a file.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BulkDeleteUtils.java,validatePathIsUnderParent,"org.apache.hadoop.fs.BulkDeleteUtils:validatePathIsUnderParent(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",56,64,"/**
 * Checks if any parent directory of p matches basePath.
 * @param p Path to check.
 * @param basePath Base path to compare against.
 * @return True if a match is found, false otherwise.
 */
","* Check if a given path is the base path or under the base path.
   * @param p path to check.
   * @param basePath base path.
   * @return true if the given path is the base path or under the base path.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isRoot,org.apache.hadoop.fs.Path:isRoot(),408,410,"/**
* Checks if m1() returns null.
* @return True if m1() is null, false otherwise.
*/
","* Returns true if and only if this path represents the root of a file system.
   *
   * @return true if and only if this path represents the root of a file system",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,suffix,org.apache.hadoop.fs.Path:suffix(java.lang.String),467,474,"/**
 * Constructs a Path object with a suffix, using m2() as a prefix.
 */","* Adds a suffix to the final name in the path.
   *
   * @param suffix the suffix to add
   * @return a new path with the suffix added",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSLinkResolver.java,qualifySymlinkTarget,"org.apache.hadoop.fs.FSLinkResolver:qualifySymlinkTarget(java.net.URI,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",46,55,"/**
 * Returns target path or modified target based on URI scheme/auth.
 * @param pathURI URI path
 * @param pathWithLink Path with link
 * @param target Target path
 * @return Path object
 */","* Return a fully-qualified version of the given symlink target if it
   * has no scheme and authority. Partially and fully-qualified paths
   * are returned unmodified.
   * @param pathURI URI of the filesystem of pathWithLink
   * @param pathWithLink Path that contains the symlink
   * @param target The symlink's absolute target
   * @return Fully qualified version of the target.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,renameInternal,"org.apache.hadoop.fs.AbstractFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",848,897,"/**
 * Renames a file or directory from src to dst, handling exceptions.
 * @param src Source path
 * @param dst Destination path
 * @param overwrite Overwrite existing destination
 */","* The specification of this method matches that of
   * {@link FileContext#rename(Path, Path, Options.Rename...)} except that Path
   * f must be for this file system.
   *
   * @param src src.
   * @param dst dst.
   * @param overwrite overwrite flag.
   * @throws AccessControlException access control exception.
   * @throws FileAlreadyExistsException file already exists exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not directory exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createPath,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createPath(org.apache.hadoop.fs.Path,java.lang.String,boolean)",362,377,"/**
 * Returns a Path object, validating write access if checkWrite is true.
 * @param dir Path to directory
 * @param path Path within directory
 * @param checkWrite Flag to check write access
 * @return Path object or null if write access fails
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,createInternal,"org.apache.hadoop.fs.DelegateToFileSystem:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",79,109,"/**
 * Creates a data output stream for writing to a file.
 * @param f path of the file, flag, permission, buffer size, etc.
 * @return FSDataOutputStream object
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,<init>,"org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",716,721,"/**
 * Creates a FCDataOutputStreamBuilder with FileContext and Path.
 * @param fc The FileContext to use.
 * @param p The Path to write to.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)",131,138,"/**
 * Constructs a FileStatus with a null name.
 * @param length File size, isdir, replication, etc. See other params.
 */
","* Constructor for file systems on which symbolic links are not supported
   *
   * @param length length.
   * @param isdir isdir.
   * @param block_replication block replication.
   * @param blocksize block size.
   * @param modification_time modification time.
   * @param access_time access_time.
   * @param permission permission.
   * @param owner owner.
   * @param group group.
   * @param path the path.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,org.apache.hadoop.fs.FileStatus:<init>(org.apache.hadoop.fs.FileStatus),198,206,"/**
 * Copies FileStatus from another object.
 * Uses getters to allow subclasses to override values.
 */
","* Copy constructor.
   *
   * @param other FileStatus to copy
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,parseExecResult,org.apache.hadoop.fs.Stat:parseExecResult(java.io.BufferedReader),110,170,"/**
 * Parses stat output from BufferedReader to populate FileStatus.
 * @param lines BufferedReader containing stat output.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,cloneStatus,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:cloneStatus(),172,182,"/**
 * Creates a FileStatus object using values from the existing status.
 * @return A new FileStatus object.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.BlockLocation[])",49,62,"/**
 * Constructs a LocatedFileStatus with FileStatus and block locations.
 * @param stat The FileStatus object.
 * @param locations Block locations for the file.
 */
","* Constructor 
   * @param stat a file status
   * @param locations a file's block locations",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.BlockLocation[])",80,92,"/**
 * Constructs a LocatedFileStatus with basic permission flags.
 * @param length File length, isdir, replication, blocksize, times,
 *                permission, owner, group, symlink, path, locations
 */
","* Constructor
   * 
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param locations a file's block locations",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,build,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:build(),48,52,"/**
* Creates and returns a FileSystemMultipartUploader instance.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,append,org.apache.hadoop.io.ArrayFile$Writer:append(org.apache.hadoop.io.Writable),84,87,"/**
* Increments a counter and writes to a Writable.
* @param value The Writable object to write to.
* @throws IOException if an I/O error occurs.
*/
","* Append a value to the file.
     * @param value value.
     * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,selectDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials),147,165,"/**
 * Retrieves a token using credentials, trying multiple providers.
 * @param creds Credentials to use for token retrieval.
 * @return Token object or null if no provider can provide one.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write,"org.apache.hadoop.fs.FSOutputSummer:write(byte[],int,int)",102,114,"/**
 * Processes a byte array segment, performing operations in a loop.
 * @param b byte array
 * @param off start offset
 * @param len length of segment
 * @throws IOException if I/O error occurs
 * @throws ArrayIndexOutOfBoundsException if arguments are invalid
 */
","* Writes <code>len</code> bytes from the specified byte array 
   * starting at offset <code>off</code> and generate a checksum for
   * each data chunk.
   *
   * <p> This method stores bytes from the given array into this
   * stream's buffer before it gets checksumed. The buffer gets checksumed 
   * and flushed to the underlying output stream when all data 
   * in a checksum chunk are in the buffer.  If the buffer is empty and
   * requested length is at least as large as the size of next checksum chunk
   * size, this method will checksum and write the chunk directly 
   * to the underlying output stream.  Thus it avoids unnecessary data copy.
   *
   * @param      b     the data.
   * @param      off   the start offset in the data.
   * @param      len   the number of bytes to write.
   * @exception  IOException  if an I/O error occurs.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",67,77,"/**
 * Processes input/output chunks after checking for erased indexes.
 * @param inputChunks Input ECChunk array.
 * @param outputChunks Output ECChunk array.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartC,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartC(),1156,1167,"/**
 * Advances state based on su_j2 and su_z, updates currentChar/crc.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupBlock(),1039,1075,"/**
 * Initializes data structures for decompression.
 * Populates arrays and sets initial state variables.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartC,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartC(),1183,1195,"/**
 * Processes data based on su_j2 and su_z values.
 * Advances state and calls m1() if conditions are met.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,finalize,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finalize(),711,715,"/**
* Calls m1() and super.m2().
*/",* Overriden to close the stream.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,close,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:close(),734,746,"/**
 * Executes m2 and calls outShadow.m3, closing outShadow.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,finish,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:finish(),263,272,"/**
 * Executes m2 on the output stream and sets needsReset to true.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(int),645,652,"/**
 * Writes an integer to the output stream.
 * @param b the integer to write
 * @throws IOException if the stream is closed
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(byte[],int,int)",859,879,"/**
* Processes a byte array segment.
* @param buf byte array
* @param offs start index
* @param len number of bytes to process
* @throws IOException if stream is closed
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer$ValueRegister:close(),492,510,"/**
 * Executes method m1, handles errors, and transitions state.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekToEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekToEnd(),1447,1449,"/**
 * Calls method m1, potentially throwing an IOException.
 */","* Seek to the end of the scanner. The entry returned by the previous
       * entry() call will be invalid.
       * 
       * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:close(),1578,1581,"/**
 * Calls the m1 method.
 * @throws IOException if an I/O error occurs during m1 execution
 */
","* Close the scanner. Release all resources. The behavior of using the
       * scanner after calling close is not defined. The entry returned by the
       * previous entry() call will be invalid.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageFile,"org.apache.hadoop.security.Credentials:readTokenStorageFile(java.io.File,org.apache.hadoop.conf.Configuration)",250,265,"/**
 * Reads credentials from a file.
 * @param filename File containing credentials.
 * @param conf Configuration object.
 * @return Credentials object.
 */","* Convenience method for reading a token storage file and loading its Tokens.
   * @param filename filename.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return Token.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),639,642,"/**
 * Retrieves delegation token information for the given token identifier.
 * @param ident TokenIdent object; identifies the token.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredToken,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)",791,831,"/**
 * Removes a ZKDTSM delegation token from Zookeeper.
 * @param ident TokenIdent object
 * @param checkAgainstZkBeforeDeletion Flag to check ZK before deletion
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeProtobufOutputStream,org.apache.hadoop.security.Credentials:writeProtobufOutputStream(java.io.DataOutputStream),328,333,"/**
 * Writes header data to the output stream.
 * @param os DataOutputStream to write to.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,decodeToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:decodeToken(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",223,232,"/**
 * Reads a DelegationTokenIdentifier from a token stream.
 * @param token Token to read from.
 * @param tokenKind Token kind identifier.
 * @return DelegationTokenIdentifier object.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createIdentifier,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:createIdentifier(),80,83,"/**
 * Creates a DelegationTokenIdentifier with the token kind.
 * @return A DelegationTokenIdentifier object.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createIdentifier,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:createIdentifier(),103,106,"/**
 * Creates a new DelegationTokenIdentifier using the tokenKind.
 * @return A DelegationTokenIdentifier object.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSDelegationToken.java,<init>,org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier:<init>(),43,45,"/**
 * Constructs a KMSDelegationTokenIdentifier with the TOKEN_KIND.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,invoke,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",41,46,"/**
 * Invokes the superclass method after resetting the retry count.
 * @param proxy The proxy object.
 * @param method The invoked method.
 * @param args Arguments passed to the method.
 * @return The result of the superclass method.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,inBlockAdvance,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",2008,2028,"/**
* Searches for a key within a block, returning true if found.
* @param key The key to search for.
* @param greater True for greater than, false otherwise.
*/","* Advance cursor in block until we find a key that is greater than or
       * equal to the input key.
       * 
       * @param key
       *          Key to compare.
       * @param greater
       *          advance until we find a key greater than the input key.
       * @return true if we find a equal key.
       * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$IpcStreams:<init>(java.net.Socket,int)",1897,1903,"/**
 * Initializes IpcStreams with a socket and max response length.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,onTimerEvent,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:onTimerEvent(),382,387,"/**
* Updates logical time and processes sinks if count > 0.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,publishMetricsNow,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetricsNow(),392,397,"/**
* Executes m3 if sinks.m1() is positive, using m2 as input.
*/",* Requests an immediate publish of all metrics from sources to sinks.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,initSystemMBean,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initSystemMBean(),583,588,"/**
 * Initializes mbeanName using prefix and MS_CONTROL_NAME.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,startMBeans,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:startMBeans(),216,224,"/**
 * Registers an MBean for the source, logging if already initialized.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:<init>(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)",859,867,"/**
 * Initializes the MetricsProxy with namespace, levels, and scheduler.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:<init>(java.lang.String),403,408,"/**
 * Initializes the MetricsProxy with a namespace and registers it.
 * @param namespace The namespace for metrics registration.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,shutdown,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown(),590,614,"/**
 * Decrements refCount and shuts down metrics system if refCount <= 0.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidator.java,checkStatus,org.apache.hadoop.util.ReadWriteDiskValidator:checkStatus(java.io.File),42,94,"/**
 * Validates disk integrity by writing and reading test data.
 * @param dir Directory to validate; throws DiskErrorException on failure.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,init,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:init(java.lang.Class),75,79,"/**
 * Updates rate metrics for the given protocol.
 * @param protocol The protocol class to update rates for.
 */
","* Initialize the metrics for JMX with protocol methods
   * @param protocol the protocol class",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,run,org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:run(),223,240,"/**
 * Updates metrics snapshot and performs related actions.
 * Synchronized on 'parent' to ensure thread safety.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,collectThreadLocalStates,org.apache.hadoop.metrics2.lib.MutableRollingAverages:collectThreadLocalStates(),202,204,"/**
* Delegates the m1 call to the innerMetrics object.
*/","* Collects states maintained in {@link ThreadLocal}, if any.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,newSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,org.apache.hadoop.metrics2.impl.MetricsConfig)",521,532,"/**
 * Creates a MetricsSinkAdapter with configured parameters.
 * @param name Sink name, desc description, sink the sink, conf config
 * @return A new MetricsSinkAdapter instance.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,add,"org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,long)",358,373,"/**
 * Adds a value to a metric, creating it if it doesn't exist.
 * @param name metric name
 * @param value value to add
 */
","* Add sample to a stat metric by name.
   * @param name  of the metric
   * @param value of the snapshot to add",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addTokenForOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),924,928,"/**
 * Updates token owner stats based on the provided token identifier.
 * @param id The token identifier.
 */
","* Add token stats to the owner to token count mapping.
   *
   * @param id token id.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeTokenForOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),935,945,"/**
 * Updates token owner stats based on the provided token ID.
 */","* Remove token stats to the owner to token count mapping.
   *
   * @param id",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUidAllowingUnknown,org.apache.hadoop.security.ShellBasedIdMapping:getUidAllowingUnknown(java.lang.String),697,707,"/**
 * Extracts a user ID, falling back to string hashcode on error.
 * @param user The user string to map to an ID.
 * @return The user ID.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGidAllowingUnknown,org.apache.hadoop.security.ShellBasedIdMapping:getGidAllowingUnknown(java.lang.String),710,720,"/**
 * Maps a group to an integer ID. Uses m4, falls back to hashcode.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,getCurrentActive,org.apache.hadoop.ha.ZKFailoverController:getCurrentActive(),784,802,"/**
 * Retrieves the HAServiceTarget from active data.
 * Returns null if ActiveNotFoundException occurs.
 */","* @return an {@link HAServiceTarget} for the current active node
   * in the cluster, or null if no node is active.
   * @throws IOException if a ZK-related issue occurs
   * @throws InterruptedException if thread is interrupted",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,<init>,"org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,boolean,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",273,299,"/**
 * Constructs an ActiveStandbyElector with provided configuration.
 * @param app Callback for election events.
 */
","* Create a new ActiveStandbyElector object <br>
   * The elector is created by providing to it the Zookeeper configuration, the
   * parent znode under which to create the znode and a reference to the
   * callback interface. <br>
   * The parent znode name must be the same for all service instances and
   * different across services. <br>
   * After the leader has been lost, a new leader will be elected after the
   * session timeout expires. Hence, the app must set this parameter based on
   * its needs for failure response time. The session timeout must be greater
   * than the Zookeeper disconnect timeout and is recommended to be 3X that
   * value to enable Zookeeper to retry transient disconnections. Setting a very
   * short session timeout may result in frequent transitions between active and
   * standby states during issues like network outages/GS pauses.
   * 
   * @param zookeeperHostPorts
   *          ZooKeeper hostPort for all ZooKeeper servers
   * @param zookeeperSessionTimeout
   *          ZooKeeper session timeout
   * @param parentZnodeName
   *          znode under which to create the lock
   * @param acl
   *          ZooKeeper ACL's
   * @param authInfo a list of authentication credentials to add to the
   *                 ZK connection
   * @param app
   *          reference to callback interface object
   * @param failFast
   *          whether need to add the retry when establishing ZK connection.
   * @param maxRetryNum max Retry Num
   * @param truststoreKeystore truststore keystore, that we will use for ZK if SSL/TLS is enabled
   * @throws IOException
   *          raised on errors performing I/O.
   * @throws HadoopIllegalArgumentException
   *          if valid data is not supplied.
   * @throws KeeperException
   *          other zookeeper operation errors.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,joinElectionInternal,org.apache.hadoop.ha.ActiveStandbyElector:joinElectionInternal(),783,796,"/**
 * Attempts to join an election, handling app data and ZK connection.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,org.apache.hadoop.security.UserGroupInformation$KeytabRenewalRunnable:relogin(),1094,1097,"/**
 * Calls the m1 method.
 * @throws IOException if an I/O error occurs during m1 execution
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddrUnresolved,org.apache.hadoop.net.NetUtils:createSocketAddrUnresolved(java.lang.String),166,168,"/**
* Resolves a target address to an InetSocketAddress.
* @param target The address string to resolve.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean)",222,227,"/**
 * Resolves a target address, using cached value if available.
 * @param target Target address string.
 * @param defaultPort Default port if resolution fails.
 * @param configName Configuration name.
 * @param useCacheIfPresent Whether to use cached value.
 * @return Resolved InetSocketAddress.
 */","* Create an InetSocketAddress from the given target string and
   * default port. If the string cannot be parsed correctly, the
   * <code>configName</code> parameter is used as part of the
   * exception message, allowing the user to better diagnose
   * the misconfiguration.
   *
   * @param target a string of either ""host"" or ""host:port""
   * @param defaultPort the default port if <code>target</code> does not
   *                    include a port number
   * @param configName the name of the configuration from which
   *                   <code>target</code> was loaded. This is used in the
   *                   exception message in the case that parsing fails.
   * @param useCacheIfPresent Whether use cache when create URI
   * @return  socket addr",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getConnectAddress,org.apache.hadoop.net.NetUtils:getConnectAddress(org.apache.hadoop.ipc.Server),439,441,"/**
* Returns an InetSocketAddress from the server's address.
* @param server The server object to extract address from.
* @return An InetSocketAddress representing the server's address.
*/
","* Returns InetSocketAddress that a client can use to 
   * connect to the server. Server.getListenerAddress() is not correct when
   * the server binds to ""0.0.0.0"". This returns ""hostname:port"" of the server,
   * or ""127.0.0.1:port"" when the getListenerAddress() returns ""0.0.0.0:port"".
   * 
   * @param server server.
   * @return socket address that a client can use to connect to the server.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupConnection,org.apache.hadoop.ipc.Client$Connection:setupConnection(org.apache.hadoop.security.UserGroupInformation),608,695,"/**
* Establishes a connection to the server, handling timeouts/failures.
* @param ticket UserGroupInformation ticket for authentication.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean)",1415,1420,"/**
 * Calls m1 with a default RPC service class.
 * @param rpcKind RPC kind, request, remoteId, fallback flag.
 * @return Writable result.
 */
","* Make a call, passing <code>rpcRequest</code>, to the IPC server defined by
   * <code>remoteId</code>, returning the rpc respond.
   *
   * @param rpcKind - input rpcKind.
   * @param rpcRequest -  contains serialized method and method parameters
   * @param remoteId - the target rpc server
   * @param fallbackToSimpleAuth - set to true or false during this method to
   *   indicate if a secure client falls back to simple auth
   * @return the rpc response
   * Throws exceptions if there are network problems or if the remote code
   * threw an exception.
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",1422,1428,"/**
 * Calls m1 with a default RPC service class.
 * @param rpcKind RPC kind, request, remoteId, auth flag, context
 * @return Writable result
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean)",1444,1450,"/**
 * Calls m1 with null as the last argument.
 * @param rpcKind RPC kind, request, remoteId, serviceClass, auth flag
 * @return Writable result or null if an error occurs.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,"org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,byte[],int)",354,362,"/**
 * Retrieves a CacheEntry, returning null if m1 returns true.
 * @param cache RetryCache instance
 * @param clientId Client identifier
 * @param callId Call identifier
 * @return CacheEntry or null
 */
","* Static method that provides null check for retryCache.
   * @param cache input Cache.
   * @param clientId client id of this request
   * @param callId client call id of this request
   * @return CacheEntry.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,"org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,java.lang.Object,byte[],int)",372,380,"/**
 * Retrieves a CacheEntryWithPayload from the cache.
 * Returns null if m1 returns true.
 */","* Static method that provides null check for retryCache.
   * @param cache input cache.
   * @param payload input payload.
   * @param clientId client id of this request
   * @param callId client call id of this request
   * @return CacheEntryWithPayload.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,org.apache.hadoop.fs.ContentSummary:toString(),369,372,"/**
* Calls m1 with the default value of true.
* Returns a String value.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getExpression,org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.String),432,435,"/**
* Creates an expression.
* @param expressionName Name of the expression to create.
* @return The created expression.
*/",Gets a named expression from the factory.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInfo,"org.apache.hadoop.fs.FsShell:printInfo(java.io.PrintStream,java.lang.String,boolean)",212,247,"/**
 * Executes a command or lists available commands based on input.
 * @param out Output stream for messages.
 * @param cmd Command to execute (or null to list).
 * @param showHelp Show help for the command if available.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,displayError,"org.apache.hadoop.fs.FsShell:displayError(java.lang.String,java.lang.String)",353,365,"/**
 * Prints a message to the error stream, checks for missing dash.
 * @param cmd Command identifier.
 * @param message Message to print, split by newlines.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,get,"org.apache.hadoop.io.WritableComparator:get(java.lang.Class,org.apache.hadoop.conf.Configuration)",65,81,"/**
 * Retrieves or creates a WritableComparator for the given class.
 * @param c Class of WritableComparable
 * @param conf Hadoop configuration
 * @return WritableComparator instance
 */
","* Get a comparator for a {@link WritableComparable} implementation.
   * @param c class.
   * @param conf configuration.
   * @return WritableComparator.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class),132,134,"/**
 * Constructs a WritableComparator for the given key class.
 * @param keyClass Class of WritableComparable to compare.
 */
","* Construct for a {@link WritableComparable} implementation.
   * @param keyClass WritableComparable Class.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,"org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,boolean)",136,139,"/**
 * Constructor for WritableComparator.
 * @param keyClass Class of WritableComparable key.
 * @param createInstances Whether to create instances.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readFields,org.apache.hadoop.io.ObjectWritable:readFields(java.io.DataInput),84,87,"/**
* Calls m1 with input stream, current object, and configuration.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,readFields,org.apache.hadoop.ipc.WritableRpcEngine$Invocation:readFields(java.io.DataInput),148,164,"/**
* Reads RPC arguments from input.
* @param in DataInput to read arguments from.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,readFields,org.apache.hadoop.io.ArrayWritable:readFields(java.io.DataInput),93,101,"/**
 * Reads a sequence of Writable objects from the input stream.
 * @param in DataInput to read from.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,verifyToken,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:verifyToken(org.apache.hadoop.security.token.Token),208,215,"/**
 * Retrieves UserGroupInformation from a delegation token.
 * @param token Delegation token to extract information from.
 * @return UserGroupInformation object.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,toString,org.apache.hadoop.security.token.Token:toString(),437,447,"/**
 * Constructs a string representation of the object's details.
 * Returns the formatted string.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,<init>,org.apache.hadoop.fs.LocalDirAllocator:<init>(java.lang.String),85,93,"/**
 * Creates a LocalDirAllocator with the given context configuration item name.
 */
","* Create an allocator object.
   * @param contextCfgItemName contextCfgItemName.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,parseMethods,"org.apache.hadoop.ha.NodeFencer:parseMethods(org.apache.hadoop.conf.Configuration,java.lang.String)",134,149,"/**
 * Parses fencing specifications and returns a list of methods.
 * @param conf Configuration object
 * @param spec Fencing specification string
 * @return List of FenceMethodWithArg objects
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,append,"org.apache.hadoop.io.BloomMapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",182,190,"/**
 * Processes key-value pair, writes to buffer, and updates bloom filter.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.lang.String,boolean)",927,929,"/**
* Calls m1 with a new Resource object.
* @param name Resource name.
* @param restrictedParser Parser restriction flag.
*/
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.net.URL,boolean)",945,947,"/**
* Calls m1 with a new Resource object created from the URL.
* @param url The URL to use for creating the Resource.
* @param restrictedParser Parser restriction flag.
*/
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path,boolean)",963,965,"/**
 * Processes a file using a Resource object and calls method m1.
 * @param file Path to the file to be processed.
 * @param restrictedParser Flag to control parsing restrictions.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,boolean)",984,986,"/**
* Processes an input stream using a Resource object.
* @param in Input stream to process.
* @param restrictedParser Parser restriction flag.
*/
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String,boolean)",1002,1005,"/**
* Processes an input stream using a Resource object.
* @param in Input stream to process.
* @param name Resource name.
* @param restrictedParser Parser restriction flag.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setDeprecatedProperties,org.apache.hadoop.conf.Configuration:setDeprecatedProperties(),688,706,"/**
* Updates properties based on deprecation context and overlay.
*/","* Sets all deprecated properties that are not currently set but have a
   * corresponding new property that is set. Useful for iterating the
   * properties when all deprecated properties for currently set properties
   * need to be present.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updatePropertiesWithDeprecatedKeys,"org.apache.hadoop.conf.Configuration:updatePropertiesWithDeprecatedKeys(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String[])",764,775,"/**
* Processes deprecations, updating values for new names.
* @param deprecations Deprecation context object.
* @param newNames Array of new names to process.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(org.apache.hadoop.conf.Configuration),843,875,"/**
 * Creates a new configuration by cloning another configuration object.
 */","* A new configuration with the same settings cloned from another.
   * 
   * @param other the configuration from which to clone settings.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.conf.Configuration),1015,1017,"/**
* Creates a Resource object using configuration and restricts props.
*/","* Add a configuration resource.
   *
   * The properties of this resource will override properties of previously
   * added resources, unless they were marked <a href=""#Final"">final</a>.
   *
   * @param conf Configuration object from which to load properties",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getAlternativeNames,org.apache.hadoop.conf.Configuration:getAlternativeNames(java.lang.String),1372,1393,"/**
* Retrieves alternative names based on the provided name.
* @param name The name to search for alternative names.
* @return Array of alternative names or null if not found.
*/","* Returns alternative names (non-deprecated keys or previously-set deprecated keys)
   * for a given non-deprecated key.
   * If the given key is deprecated, return null.
   *
   * @param name property name.
   * @return alternative names.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPropertySources,org.apache.hadoop.conf.Configuration:getPropertySources(java.lang.String),2109,2129,"/**
 * Retrieves string array based on name, returns null if fails.
 */","* Gets information about why a property was set.  Typically this is the 
   * path to the resource objects (file, URL, etc.) the property came from, but
   * it can also indicate that it was set programmatically, or because of the
   * command line.
   *
   * @param name - The property name to get the source of.
   * @return null - If the property or its source wasn't found. Otherwise, 
   * returns a list of the sources of the resource.  The older sources are
   * the first ones in the list.  So for example if a configuration is set from
   * the command line, and then written out to a file that is read back in the
   * first entry would indicate that it was set from the command line, while
   * the second one would indicate the file that the new configuration was read
   * in from.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,size,org.apache.hadoop.conf.Configuration:size(),2988,2990,"/**
* Delegates to the result of m1() and returns its m2() value.
*/","* Return the number of keys in the configuration.
   *
   * @return number of keys in the configuration.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,clear,org.apache.hadoop.conf.Configuration:clear(),2995,2998,"/**
 * Calls m2() on instances of m1 and m3. No return value.
 */",* Clears all keys from the configuration.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,iterator,org.apache.hadoop.conf.Configuration:iterator(),3006,3022,"/**
 * Converts Properties to a Map<String, String> iterator.
 * Extracts String key-value pairs from Properties.
 */","* Get an {@link Iterator} to go through the list of <code>String</code> 
   * key-value pairs in the configuration.
   * 
   * @return an iterator over the entries.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,write,org.apache.hadoop.conf.Configuration:write(java.io.DataOutput),3965,3975,"/**
 * Writes properties and their associated data to the output stream.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getValByRegex,org.apache.hadoop.conf.Configuration:getValByRegex(java.lang.String),3982,4001,"/**
* Filters entries from m7().m8() matching regex, populating a map.
*/","* get keys matching the the regex.
   * @param regex the regex to match against.
   * @return {@literal Map<String,String>} with matching keys",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,readVectored,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readVectored(java.util.List,java.util.function.IntFunction)",438,482,"/**
 * Reads file ranges, calculates checksums, and allocates buffers.
 */","* Vectored read.
     * If the file has no checksums: delegate to the underlying stream.
     * If the file is checksummed: calculate the checksum ranges as
     * well as the data ranges, read both, and validate the checksums
     * as well as returning the data.
     * @param ranges the byte ranges to read
     * @param allocate the function to allocate ByteBuffer
     * @throws IOException",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:get(int),144,175,"/**
 * Retrieves buffer data for a given block number, with retries.
 * @param blockNumber The block number to retrieve.
 * @return BufferData object containing the retrieved data.
 */","* Gets the block having the given {@code blockNumber}.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,waitForReplication,org.apache.hadoop.fs.shell.SetReplication:waitForReplication(),108,141,"/**
 * Processes wait list items, checking for replication inconsistencies.
 */",* Wait for all files in waitList to have replication number equal to rep.,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",924,931,"/**
 * Gets block locations for a file range.
 * @param p Path to the file; start & len define the range.
 * @return BlockLocation[] array of block locations.
 */
","* Return an array containing hostnames, offset and size of
   * portions of the given file.  For a nonexistent
   * file or regions, {@code null} is returned.
   *
   * This call is most helpful with location-aware distributed
   * filesystems, where it returns hostnames of machines that
   * contain the given file.
   *
   * A FileSystem will normally return the equivalent result
   * of passing the {@code FileStatus} of the path to
   * {@link #getFileBlockLocations(FileStatus, long, long)}
   *
   * @param p path is used to identify an FS since an FS could have
   *          another FS that it could be delegating the call to
   * @param start offset into the given file
   * @param len length for which to get locations for
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException IO failure
   * @return block location array.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FilterFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",151,155,"/**
* Delegates block location retrieval to the file system.
* @param file FileStatus object
* @param start Start offset
* @param len Length of data
* @return BlockLocation[] array
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,fetchSuccessSummary,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchSuccessSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String)",149,153,"/**
 * Returns duration statistic summary for a key from the source.
 * @param source IOStatistics object
 * @param key Key to retrieve the statistic for
 * @return DurationStatisticSummary object
 */
","* Fetch the duration timing summary from an IOStatistics source.
   * If the duration key is unknown, the summary will be incomplete.
   * @param source source of data
   * @param key duration statistic key
   * @return a summary of the statistics.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create(),113,115,"/**
 * Calls m1 with a null argument.
 * @return Serializable object returned by m1(null)
 */","* Create a new {@link IOStatisticsSnapshot} instance.
   * @return an empty IOStatisticsSnapshot.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_retrieve,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object),146,152,"/**
 * Extracts a serializable object from a source, using m1 & m2.
 * @param source The source object. Returns null if m1 returns null.
 */","* Extract the IOStatistics from an object in a serializable form.
   * @param source source object, may be null/not a statistics source/instance
   * @return {@link IOStatisticsSnapshot} or null if the object is null/doesn't have statistics",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,toArray,"org.apache.hadoop.util.functional.RemoteIterators:toArray(org.apache.hadoop.fs.RemoteIterator,java.lang.Object[])",248,252,"/**
 * Converts RemoteIterator to array.
 * @param source Iterator to convert.
 * @param a Array to populate.
 * @return Populated array.
 */
","* Build an array from a RemoteIterator.
   * @param source source iterator
   * @param a destination array; if too small a new array
   * of the same type is created
   * @param <T> type
   * @return an array of the values.
   * @throws IOException if the source RemoteIterator raises it.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,createPassword,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),493,515,"/**
 * Creates a password token for a given identifier.
 * @param identifier Token identifier.
 * @return Byte array representing the generated password.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,renewToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)",592,643,"/**
 * Renews a token with provided renewer, verifies access, and updates expiry.
 */","* Renew a delegation token.
   * @param token the token to renew
   * @param renewer the full principal name of the user doing the renewal
   * @return the new expiration time
   * @throws InvalidToken if the token is invalid
   * @throws AccessControlException if the user can't renew token",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",654,685,"/**
 * Cancels a token, verifying authorization and updating metrics.
 * @param token The token to cancel.
 * @param canceler The canceler principal.
 * @return TokenIdent object.
 */","* Cancel a token by removing it from cache.
   *
   * @param token token.
   * @param canceller canceller.
   * @return Identifier of the canceled token
   * @throws InvalidToken for invalid token
   * @throws AccessControlException if the user isn't allowed to cancel",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processArguments,org.apache.hadoop.fs.shell.Command:processArguments(java.util.LinkedList),281,290,"/**
 * Processes a list of PathData objects, handling potential IOExceptions.
 */","*  Processes the command's list of expanded arguments.
   *  {@link #processArgument(PathData)} will be invoked with each item
   *  in the list.  The loop catches IOExceptions, increments the error
   *  count, and displays the exception.
   *  @param args a list of {@link PathData} to process
   *  @throws IOException if anything goes wrong...",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroups,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroups(java.lang.String),98,101,"/**
* Returns a list of strings from m1, filtered by userName.
*/","* Returns list of groups for a user
   *
   * @param userName get groups for this user
   * @return list of groups for a given user",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsSet(java.lang.String),121,124,"/**
 * Delegates the user mask retrieval to the m1 method.
 * @param userName The username to fetch the mask for.
 * @return A set of strings representing the user's mask.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,resolve,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:resolve(java.util.List),174,211,"/**
 * Processes a list of names, potentially using a script.
 * @param names List of names to process; returns a List<String>.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,getLinkCount,org.apache.hadoop.fs.HardLink:getLinkCount(java.io.File),214,255,"/**
 * Gets the hard link count of a file.
 * @param fileName The file to check.
 * @throws IOException if file is invalid or an error occurs.
 */","* Retrieves the number of links to the specified file.
    *
    * @param fileName file name.
    * @throws IOException raised on errors performing I/O.
    * @return link count.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingTar,"org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.File,java.io.File,boolean)",1053,1084,"/**
 * Untars a file to a directory, handling gzipped archives.
 * @param inFile input tar file
 * @param untarDir directory to extract to
 * @param gzipped true if input is gzipped
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,symLink,"org.apache.hadoop.fs.FileUtil:symLink(java.lang.String,java.lang.String)",1226,1279,"/**
* Creates a symbolic link from linkname to target.
* @param target The path to the target file.
* @param linkname The path to the symbolic link.
* @return Exit code of the symlink creation process.
*/","* Create a soft link between a src and destination
   * only on a local disk. HDFS does not support this.
   * On Windows, when symlink creation fails due to security
   * setting, we will log a warning. The return code in this
   * case is 2.
   *
   * @param target the target for symlink
   * @param linkname the symlink
   * @return 0 on success
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,chmod,"org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String,boolean)",1303,1319,"/**
* Changes file permissions.
* @param filename File to modify.
* @param perm Permission string.
* @param recursive Recursive flag.
* @return Exit code of the shell command.
*/","* Change the permissions on a file / directory, recursively, if
   * needed.
   * @param filename name of the file whose permissions are to change
   * @param perm permission string
   * @param recursive true, if permissions should be changed recursively
   * @return the exit code from the command.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getConf,org.apache.hadoop.util.SysInfoLinux:getConf(java.lang.String),158,170,"/**
 * Retrieves a system configuration value (long) via shell command.
 * @param attr Configuration attribute to retrieve. Returns -1 on failure.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getSystemInfoInfoFromShell,org.apache.hadoop.util.SysInfoWindows:getSystemInfoInfoFromShell(),81,92,"/**
 * Executes systeminfo command and returns the output.
 * Returns null if an IOException occurs.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkIsBashSupported,org.apache.hadoop.util.Shell:checkIsBashSupported(),810,834,"/**
 * Checks if bash is supported on the system. Throws InterruptedIOException if interrupted.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,isSetsidSupported,org.apache.hadoop.util.Shell:isSetsidSupported(),845,879,"/**
* Checks if setsid is supported on the system.
* Returns true if supported, false otherwise.
*/","* Look for <code>setsid</code>.
   * @return true if <code>setsid</code> was present",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfoByNonNativeIO,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNonNativeIO(),998,1045,"/**
* Retrieves file permissions, owner, and group, handling exceptions.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setOwner,"org.apache.hadoop.fs.FileUtil:setOwner(java.io.File,java.lang.String,java.lang.String)",1329,1338,"/**
 * Executes a shell command with username/group and writes to file.
 * @param file Output file.
 * @param username Username (optional).
 * @param groupname Groupname (optional).
 */","* Set the ownership on a file / directory. User name and group name
   * cannot both be null.
   * @param file the file to change
   * @param username the new user owner name
   * @param groupname the new group owner name
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,execSetPermission,"org.apache.hadoop.fs.FileUtil:execSetPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1520,1529,"/**
 * Sets file permissions based on the provided FsPermission.
 * @param f The file to modify.
 * @param permission The new file permissions.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,getUsersForNetgroup,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getUsersForNetgroup(java.lang.String),97,123,"/**
 * Extracts usernames from netgroup string.
 * @param netgroup The netgroup string to parse.
 * @return List of usernames found in the netgroup.
 */","* Gets users for a netgroup
   *
   * @param netgroup return users for this netgroup
   * @return list of users for a given netgroup
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCredentials,org.apache.hadoop.security.UserGroupInformation:getCredentials(),1736,1747,"/**
 * Retrieves credentials, iterating and removing invalidated tokens.
 * @return Credentials object containing tokens.
 */","* Obtain the tokens in credentials form associated with this user.
   * 
   * @return Credentials of tokens associated with this user",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,flush,org.apache.hadoop.security.alias.UserProvider:flush(),94,97,"/**
 * Calls the user's m1 method with provided credentials.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,flush,org.apache.hadoop.crypto.key.UserProvider:flush(),138,141,"/**
 * Calls user's m1 method with provided credentials.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirInternal,"org.apache.hadoop.util.DiskChecker:checkDirInternal(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",138,143,"/**
* Sets permissions on a directory and its parent.
* @param localFS Filesystem object.
* @param dir Path to directory.
* @param expected Expected permissions.
*/
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,hasPathCapability,"org.apache.hadoop.fs.FilterFs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",451,455,"/**
* Delegates to FileSystem's m1 method.
* @param path The path to operate on.
* @param capability Capability string.
* @throws IOException If an I/O error occurs.
*/
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getEnclosingRoot,org.apache.hadoop.fs.FilterFs:getEnclosingRoot(org.apache.hadoop.fs.Path),463,466,"/**
* Delegates m1 operation to the underlying file system.
* @param path The path to operate on.
* @return Path object returned by the file system.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",1850,1856,"/**
 * Lists status of files/directories under a path, filtered by a PathFilter.
 * @param f path to list
 * @param filter filter to apply
 * @return Array of FileStatus objects
 */
","* Filter files/directories in the given path using the user-supplied path
     * filter.
     * 
     * @param f is the path name
     * @param filter is the user-supplied path filter
     *
     * @return an array of FileStatus objects for the files under the given path
     *         after applying the filter
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)",1879,1886,"/**
 * Retrieves FileStatus objects for paths matching the filter.
 * @param files array of paths to check
 * @param filter filter to apply
 * @return array of FileStatus objects
 */","* Filter files/directories in the given list of paths using user-supplied
     * path filter.
     * 
     * @param files is a list of paths
     * @param filter is the filter
     *
     * @return a list of statuses for the files under the given paths after
     *         applying the filter
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If a file in <code>files</code> does not 
     *           exist
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,run,org.apache.hadoop.fs.FileContext$FileContextFinalizer:run(),2318,2321,"/**
* Calls the m1 method. Synchronized to prevent race conditions.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,isMultiThreadNecessary,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:isMultiThreadNecessary(java.util.LinkedList),98,102,"/**
 * Checks if threadCount > 1 and m1(args) returns true.
 * @param args List of PathData objects.
 * @throws IOException if an I/O error occurs during m1 execution.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processArguments,org.apache.hadoop.fs.shell.Truncate:processArguments(java.util.LinkedList),68,73,"/**
 * Calls m2() if waitOpt is true, after calling super.m1(args).
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,resolve,"org.apache.hadoop.fs.viewfs.InodeTree:resolve(java.lang.String,boolean)",893,988,"/**
* Resolves a path and returns a ResolveResult.
* @param p The path to resolve.
* @param resolveLastComponent Whether to resolve the last component.
* @return ResolveResult object containing resolution details.
*/","* Resolve the pathname p relative to root InodeDir.
   * @param p - input path
   * @param resolveLastComponent resolveLastComponent.
   * @return ResolveResult which allows further resolution of the remaining path
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileHarStatus,org.apache.hadoop.fs.HarFileSystem:getFileHarStatus(org.apache.hadoop.fs.Path),646,659,"/**
 * Retrieves HarStatus for a given file path.
 * @param f Path to the file.
 * @throws IOException if file name is invalid or not found.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,int)",153,178,"/**
 * Initializes a ChecksumFSInputChecker with provided file and buffer sizes.
 * @param fs FileSystem object
 * @param file Path to the file
 * @param bufferSize Buffer size for reading data
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,setReplication,"org.apache.hadoop.fs.ChecksumFs:setReplication(org.apache.hadoop.fs.Path,short)",463,475,"/**
 * Performs replication of a path with a given replication factor.
 * @param src The source path to replicate.
 * @param replication The desired replication factor.
 * @return True if replication was successful, false otherwise.
 */
","* Set replication for an existing file.
   * Implement the abstract <tt>setReplication</tt> of <tt>FileSystem</tt>
   * @param src file name
   * @param replication new replication
   * @throws IOException if an I/O error occurs.
   * @return true if successful;
   *         false if file does not exist or is a directory",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,delete,"org.apache.hadoop.fs.ChecksumFs:delete(org.apache.hadoop.fs.Path,boolean)",529,550,"/**
 * Processes a file path, recursively if needed.
 * @param f the file path
 * @param recursive whether to process recursively
 * @return true if processed, false otherwise
 */","* Implement the delete(Path, boolean) in checksum
   * file system.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",363,389,"/**
 * Creates a ChecksumFSOutputSummer for summing checksums.
 * @param fs filesystem, file, flags, permissions, buffer size,
 *        replication, block size, progress, checksum options,
 *        createParent boolean.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processArguments,org.apache.hadoop.fs.shell.CommandWithDestination:processArguments(java.util.LinkedList),223,245,"/**
* Handles path validation and exceptions based on arguments.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,mkdir,"org.apache.hadoop.fs.DelegateToFileSystem:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",185,192,"/**
 * Creates a directory.
 * @param dir The directory to create.
 * @param permission Permissions for the directory.
 * @param createParent Create parent directories if they don't exist.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,rename,"org.apache.hadoop.fs.FileUtil:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",2068,2082,"/**
 * Renames a file or directory from src to dst on the given FS.
 * @param srcFs Filesystem to rename on.
 * @param src Source path.
 * @param dst Destination path.
 * @param options Rename options.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,renameInternal,"org.apache.hadoop.fs.DelegateToFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",206,212,"/**
* Copies a Path to another, invoking m1 and fsImpl.m2.
* @param src Source Path
* @param dst Destination Path
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,rename,"org.apache.hadoop.fs.FilterFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",254,258,"/**
* Delegates file rename operation to the underlying file system.
* @param src Source path.
* @param dst Destination path.
* @param options Rename options.
*/
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InternalOperations.java,rename,"org.apache.hadoop.fs.InternalOperations:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",35,39,"/**
* Renames a file or directory.
* @param fs FileSystem object
* @param src Source path
* @param dst Destination path
* @param options Rename options
*/
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,run,org.apache.hadoop.fs.ChecksumFileSystem$FsOperation:run(org.apache.hadoop.fs.Path),771,780,"/**
 * Checks a path, performs actions based on m1 and m2 results.
 * @param p the path to check
 * @return true if initial check succeeds, false otherwise.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,int)",187,214,"/**
 * Initializes a ChecksumFSInputChecker with a file and checksum file.
 * @param fs ChecksumFileSystem instance
 * @param file Path to the file
 * @param bufferSize Buffer size for file I/O
 * @throws IOException if checksum file cannot be opened
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,rename,"org.apache.hadoop.fs.ChecksumFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",890,915,"/**
* Copies a Path to another Path, handling various file system operations.
* @param src Source Path
* @param dst Destination Path
* @return True if copy successful, false otherwise.
*/",* Rename files/dirs,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,delete,"org.apache.hadoop.fs.ChecksumFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",921,941,"/**
 * Processes a Path, recursively if needed, handling file status.
 * @param f Path to process
 * @param recursive Recursive flag
 * @return True if successful, false otherwise.
 */","* Implement the delete(Path, boolean) in checksum
   * file system.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)",621,642,"/**
 * Creates a ChecksumFSOutputSummer for summing checksum data.
 * @param fs ChecksumFileSystem instance
 * @param file Output file
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isAncestor,"org.apache.hadoop.fs.shell.find.Find:isAncestor(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",335,343,"/**
 * Checks if a path from source is an ancestor of target's path.
 */",Returns true if the target is an ancestor of the source.,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,fullPath,org.apache.hadoop.fs.viewfs.ChRootedFs:fullPath(org.apache.hadoop.fs.Path),91,95,"/**
 * Constructs a Path by prepending a root path part.
 * @param path The Path to modify.
 * @return A new Path object.
 */
","* 
   * @param path
   * @return return full path including the chroot",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,stripOutRoot,org.apache.hadoop.fs.viewfs.ChRootedFs:stripOutRoot(org.apache.hadoop.fs.Path),137,148,"/**
 * Extracts a path component, handling URI validation and root path.
 * @param p Path object to process.
 * @return String path component, potentially modified.
 */","*  
   * Strip out the root from the path.
   * 
   * @param p - fully qualified path p
   * @return -  the remaining path  without the beginning /",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,stripOutRoot,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:stripOutRoot(org.apache.hadoop.fs.Path),153,163,"/**
 * Extracts and processes a path component.
 * @param p The Path object to process.
 * @return A string representing the processed path.
 */","* Strip out the root from the path.
   * @param p - fully qualified path p
   * @return -  the remaining path  without the beginning /
   * @throws IOException if the p is not prefixed with root",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,"org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(org.apache.hadoop.fs.Path,java.util.Date)",335,359,"/**
 * Creates a trash checkpoint directory, retrying if necessary.
 * @param trashRoot Root directory for trash.
 * @param date Date for the checkpoint.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSLinkResolver.java,resolve,"org.apache.hadoop.fs.FSLinkResolver:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",79,112,"/**
 * Resolves a path, following symlinks until resolved or error.
 * @param fc FileContext to operate on.
 * @param path Path to resolve.
 * @return Resolved object of type T.
 */","* Performs the operation specified by the next function, calling it
   * repeatedly until all symlinks in the given path are resolved.
   * @param fc FileContext used to access file systems.
   * @param path The path to resolve symlinks on.
   * @return Generic type determined by the implementation of next.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,rename,"org.apache.hadoop.fs.AbstractFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",795,808,"/**
 * Moves a file from src to dst, optionally overwriting existing files.
 * @param src Source path
 * @param dst Destination path
 * @param options Rename options
 */
","* The specification of this method matches that of
   * {@link FileContext#rename(Path, Path, Options.Rename...)} except that Path
   * f must be for this file system.
   *
   * @param src src.
   * @param dst dst.
   * @param options options.
   * @throws AccessControlException access control exception.
   * @throws FileAlreadyExistsException file already exists exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not directory exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameInternal,"org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",253,259,"/**
 * Delegates file copy operation to the underlying file system.
 * @param src Source path.
 * @param dst Destination path.
 * @param overwrite Overwrite if destination exists.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,toFileStatus,org.apache.hadoop.fs.HarFileSystem:toFileStatus(org.apache.hadoop.fs.HarFileSystem$HarStatus),530,553,"/**
* Returns a FileStatus object, adjusting modification time based on metadata version.
*/","* Combine the status stored in the index and the underlying status. 
   * @param h status stored in the index
   * @return the combined file status
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,<init>,"org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:<init>(java.io.File,long,org.apache.hadoop.fs.FileSystem)",942,949,"/**
 * Constructs a RawLocalFileStatus using the provided file and details.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,org.apache.hadoop.fs.FileStatus:<init>(),107,107,"/**
* Default constructor for FileStatus, initializing all fields to default values.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,org.apache.hadoop.fs.Path)",110,115,"/**
 * Constructs a FileStatus with default access control and other nulls.
 * @param length File size in bytes.
 * @param isdir True if this is a directory.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPFile,org.apache.hadoop.fs.Path)",557,572,"/**
 * Creates a FileStatus object from an FTPFile.
 * @param ftpFile Source FTPFile object.
 * @param parentPath Parent path of the file.
 * @return A FileStatus representing the FTPFile.
 */","* Convert the file information in FTPFile to a {@link FileStatus} object. *
   * 
   * @param ftpFile
   * @param parentPath
   * @return FileStatus",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartB,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartB(),1128,1154,"/**
 * Updates state based on su_ch2 and su_count; calls m1/m2.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,changeStateToProcessABlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:changeStateToProcessABlock(),355,362,"/**
 * Skips processing if skipResult is true, otherwise sets state to EOF.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,init,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:init(),492,509,"/**
 * Checks BZip2 stream format and initializes block size.
 * Throws IOException if format is invalid.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartB,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartB(),1169,1181,"/**
 * Processes data based on su_ch2 and su_count values.
 * Updates state and calls m1() or m2() accordingly.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,close,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:close(),302,308,"/**
 * Calls super.m1() and ensures output.m1() is always executed.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,write,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(int),288,293,"/**
 * Calls m1 if needed, then writes 'b' to the output stream.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,write,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(byte[],int,int)",295,300,"/**
 * Writes a portion of a byte array to the output stream.
 * @param b The byte array to write.
 * @param off Offset into the array.
 * @param len Number of bytes to write.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verify,"org.apache.hadoop.security.KDiag:verify(java.io.File,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",990,1003,"/**
 * Attempts token file acquisition, logs failure if nofail is true.
 * @param tokenFile Token file. @param conf Configuration.
 * @return True if successful, false otherwise.
 */","* Verify that tokenFile contains valid Credentials.
   *
   * If not, an exception is raised, or, if {@link #nofail} is set,
   * an error will be logged and the method return false.
   *",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,printTokenFile,"org.apache.hadoop.security.token.DtFileOperations:printTokenFile(java.io.File,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration,java.io.PrintStream)",123,129,"/**
 * Processes a token file, creates credentials, and prints output.
 * @param tokenFile File containing the token.
 * @param alias Text alias.
 * @param conf Configuration object.
 * @param out PrintStream for output.
 */
","Print out a Credentials file from the local filesystem.
   *  @param tokenFile a local File object.
   *  @param alias print only tokens matching alias (null matches all).
   *  @param conf Configuration object passed along.
   *  @param out print to this stream.
   *  @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfo,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),600,617,"/**
 * Retrieves DelegationTokenInformation by token identifier.
 * @param ident TokenIdent object to identify the token.
 * @return DelegationTokenInformation or null if not found.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,syncLocalCacheWithZk,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:syncLocalCacheWithZk(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),625,637,"/**
 * Updates token information for a given ident, handling errors.
 * @param ident TokenIdent object representing the token.
 */","* This method synchronizes the state of a delegation token information in
   * local cache with its actual value in Zookeeper.
   *
   * @param ident Identifier of the token",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredToken,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),785,789,"/**
 * Calls m1 with the given TokenIdent and false for the flag.
 * @param ident The TokenIdent to process.
 * @throws IOException if an I/O error occurs.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageToStream,"org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream,org.apache.hadoop.security.Credentials$SerializedFormat)",306,319,"/**
 * Writes data to the output stream based on the specified format.
 * @param os DataOutputStream to write to.
 * @param format SerializedFormat enum indicating the format.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>(java.lang.String),118,134,"/**
 * Constructs a MetricsSystemImpl with a given prefix.
 * @param prefix Prefix for metrics names; null allows later init.
 */
","* Construct the metrics system
   * @param prefix  for the system",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,startMetricsMBeans,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:startMetricsMBeans(),334,339,"/**
 * Calls the m1 method on each MetricsSourceAdapter in sources.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,start,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:start(),100,102,"/**
 * Conditionally calls m1() if startMBeans is true.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getInstance,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getInstance(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)",869,881,"/**
 * Retrieves or creates a MetricsProxy, updating the delegate if needed.
 * @param namespace MetricsProxy namespace
 * @param numLevels Number of levels for metrics
 * @param drs DecayRpcScheduler delegate
 * @return MetricsProxy instance
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getInstance,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getInstance(java.lang.String),410,418,"/**
 * Retrieves a MetricsProxy for the given namespace.
 * Creates one if it doesn't exist.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,call,"org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)",546,642,"/**
 * Handles RPC calls, version checking, and method invocation.
 * @param server RPC server, request, timestamp, version
 * @return Writable result or throws RPC.VersionMismatch.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,processCall,"org.apache.hadoop.ipc.ProtobufRpcEngine$Server:processCall(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)",463,504,"/**
 * Executes a RPC method call on the server.
 * @param server RPC server instance
 * @return RPC response or null if async
 * @throws Exception if an error occurs during execution
 */","* This implementation is same as
     * ProtobufRpcEngine2.Server.ProtobufInvoker#call(..)
     * except this implementation uses non-shaded protobuf classes from legacy
     * protobuf version (default 2.5.0).",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)",599,641,"/**
 * Executes RPC method, handles exceptions, and returns result.
 * @param server RPC server, protocol name, request, method name, protocol impl
 * @return RpcWritable object containing the result
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,<init>,org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:<init>(org.apache.hadoop.metrics2.lib.MutableRollingAverages),219,221,"/**
 * Constructor for RatesRoller, sets the parent rolling average.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)",296,306,"/**
 * Registers a metrics sink with the given name and description.
 * @param name Sink name
 * @param desc Sink description
 * @param sink The metrics sink to register
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,newSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.impl.MetricsConfig)",534,537,"/**
* Creates a MetricsSinkAdapter using provided name, desc, and config.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,add,"org.apache.hadoop.metrics2.lib.MutableRates:add(java.lang.String,long)",76,78,"/**
* Delegates m1 call to the registry with provided name and elapsed time.
*/","* Add a rate sample for a rate metric
   * @param name of the rate metric
   * @param elapsed time",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addPersistedDelegationToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)",404,433,"/**
 * Adds a persisted delegation token, updating renew date and password.
 * @param identifier Token identifier.
 * @param renewDate Token renewal date.
 */","* This method is intended to be used for recovering persisted delegation
   * tokens. Tokens that have an unknown <code>DelegationKey</code> are
   * marked as expired and automatically cleaned up.
   * This method must be called before this secret manager is activated (before
   * startThreads() is called)
   * @param identifier identifier read from persistent storage
   * @param renewDate token renew time
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,syncTokenOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:syncTokenOwnerStats(),952,957,"/**
 * Processes current tokens, updating owner stats.
 */","* This method syncs token information from currentTokens to tokenOwnerStats.
   * It is used when the currentTokens is initialized or refreshed. This is
   * called from a single thread thus no synchronization is needed.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredToken(),762,780,"/**
* Removes expired delegation tokens.
* Iterates through tokens, removes expired ones, and notifies.
*/",Remove expired delegation tokens from cache,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,<init>,"org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",226,233,"/**
 * Creates an ActiveStandbyElector with default enableSecure flag.
 * @param zookeeperHostPorts Zookeeper host ports, session timeout, parent Znode, ACL, auth info, callback, retry num, truststoreKeystore.
 */","* Create a new ActiveStandbyElector object <br>
   * The elector is created by providing to it the Zookeeper configuration, the
   * parent znode under which to create the znode and a reference to the
   * callback interface. <br>
   * The parent znode name must be the same for all service instances and
   * different across services. <br>
   * After the leader has been lost, a new leader will be elected after the
   * session timeout expires. Hence, the app must set this parameter based on
   * its needs for failure response time. The session timeout must be greater
   * than the Zookeeper disconnect timeout and is recommended to be 3X that
   * value to enable Zookeeper to retry transient disconnections. Setting a very
   * short session timeout may result in frequent transitions between active and
   * standby states during issues like network outages/GS pauses.
   * 
   * @param zookeeperHostPorts
   *          ZooKeeper hostPort for all ZooKeeper servers
   * @param zookeeperSessionTimeout
   *          ZooKeeper session timeout
   * @param parentZnodeName
   *          znode under which to create the lock
   * @param acl
   *          ZooKeeper ACL's
   * @param authInfo a list of authentication credentials to add to the
   *                 ZK connection
   * @param app
   *          reference to callback interface object
   * @param maxRetryNum maxRetryNum.
   * @param truststoreKeystore truststore keystore, that we will use for ZK if SSL/TLS is enabled
   * @throws IOException raised on errors performing I/O.
   * @throws HadoopIllegalArgumentException
   *         if valid data is not supplied.
   * @throws KeeperException
   *         other zookeeper operation errors.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,joinElection,org.apache.hadoop.ha.ActiveStandbyElector:joinElection(byte[]),315,334,"/**
 * Copies data and initiates an active election if not already in one.
 * @param data The byte array to copy. Throws IllegalArgumentException if null.
 */","* To participate in election, the app will call joinElection. The result will
   * be notified by a callback on either the becomeActive or becomeStandby app
   * interfaces. <br>
   * After this the elector will automatically monitor the leader status and
   * perform re-election if necessary<br>
   * The app could potentially start off in standby mode and ignore the
   * becomeStandby call.
   * 
   * @param data
   *          to be set by the app. non-null data must be set.
   * @throws HadoopIllegalArgumentException
   *           if valid data is not supplied",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reJoinElection,org.apache.hadoop.ha.ActiveStandbyElector:reJoinElection(int),798,823,"/**
 * Re-establishes ZK session, waits, and joins election if healthy.
 * @param sleepTime Time to wait before attempting election join.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String)",200,204,"/**
 * Resolves a target address with a default port and config name.
 */","* Create an InetSocketAddress from the given target string and
   * default port. If the string cannot be parsed correctly, the
   * <code>configName</code> parameter is used as part of the
   * exception message, allowing the user to better diagnose
   * the misconfiguration.
   *
   * @param target a string of either ""host"" or ""host:port""
   * @param defaultPort the default port if <code>target</code> does not
   *                    include a port number
   * @param configName the name of the configuration from which
   *                   <code>target</code> was loaded. This is used in the
   *                   exception message in the case that parsing fails.
   * @return socket addr.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,invoke,"org.apache.hadoop.ipc.WritableRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",232,261,"/**
 * Executes a remote method call and returns the result.
 * @param proxy Proxy object.
 * @param method Method to invoke.
 * @param args Method arguments.
 * @return Result of the remote method call.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,invoke,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",212,294,"/**
 * Executes a method on a remote service, handling parameters and tracing.
 * @param proxy The proxy object.
 * @param method The method to execute.
 * @param args Method arguments.
 * @return RpcWritable.Buffer or null depending on configuration.
 */","* This is the client side invoker of RPC method. It only throws
     * ServiceException, since the invocation proxy expects only
     * ServiceException to be thrown by the method in case protobuf service.
     * 
     * ServiceException has the following causes:
     * <ol>
     * <li>Exceptions encountered on the client side in this method are 
     * set as cause in ServiceException as is.</li>
     * <li>Exceptions from the server are wrapped in RemoteException and are
     * set as cause in ServiceException</li>
     * </ol>
     * 
     * Note that the client calling protobuf RPC methods, must handle
     * ServiceException by getting the cause from the ServiceException. If the
     * cause is RemoteException, then unwrap it to get the exception thrown by
     * the server.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,invoke,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",220,304,"/**
 * Invokes a remote method on a service proxy.
 * @param proxy The proxy object.
 * @param method The method to invoke.
 * @param args Arguments for the method.
 * @return RpcWritable.Buffer or null.
 */","* This is the client side invoker of RPC method. It only throws
     * ServiceException, since the invocation proxy expects only
     * ServiceException to be thrown by the method in case protobuf service.
     *
     * ServiceException has the following causes:
     * <ol>
     * <li>Exceptions encountered on the client side in this method are
     * set as cause in ServiceException as is.</li>
     * <li>Exceptions from the server are wrapped in RemoteException and are
     * set as cause in ServiceException</li>
     * </ol>
     *
     * Note that the client calling protobuf RPC methods, must handle
     * ServiceException by getting the cause from the ServiceException. If the
     * cause is RemoteException, then unwrap it to get the exception thrown by
     * the server.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,parseExpression,org.apache.hadoop.fs.shell.find.Find:parseExpression(java.util.Deque),272,332,"/**
 * Parses expressions from a deque of arguments.
 * @param args deque of string arguments
 * @return Expression object representing the parsed expression
 */","* Parse a list of arguments to to extract the {@link Expression} elements.
   * The input Deque will be modified to remove the used elements.
   * 
   * @param args arguments to be parsed
   * @return list of {@link Expression} elements applicable to this command
   * @throws IOException if list can not be parsed",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printUsage,org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream),193,195,"/**
 * Calls m1 with null second argument and false for the third.
 * @param out PrintStream to write to.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printUsage,"org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream,java.lang.String)",198,200,"/**
* Calls m1 with `out` and `cmd`, setting the third argument to false.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printHelp,org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream),203,205,"/**
 * Calls m1 with provided PrintStream, null, and true.
 * @param out PrintStream to use for output.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printHelp,"org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream,java.lang.String)",208,210,"/**
* Calls m1 with true as the third argument, using provided out and cmd.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,get,org.apache.hadoop.io.WritableComparator:get(java.lang.Class),55,57,"/**
 * Creates a WritableComparator for the given class.
 * @param c Class of WritableComparable to compare.
 * @return A WritableComparator instance.
 */
","* For backwards compatibility.
   *
   * @param c WritableComparable Type.
   * @return WritableComparator.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ByteWritable.java,<init>,org.apache.hadoop.io.ByteWritable$Comparator:<init>(),88,90,"/**
 * Default constructor. Calls super with ByteWritable.class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IntWritable.java,<init>,org.apache.hadoop.io.IntWritable$Comparator:<init>(),90,92,"/**
 * Default constructor. Calls super with IntWritable class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,org.apache.hadoop.io.WritableComparator:<init>(),124,126,"/**
 * Default constructor. Calls the single-arg constructor.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text$Comparator:<init>(),429,431,"/**
 * Constructs a Comparator based on the Text class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/NullWritable.java,<init>,org.apache.hadoop.io.NullWritable$Comparator:<init>(),62,64,"/**
 * Default constructor. Calls superclass constructor with NullWritable.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,<init>,org.apache.hadoop.io.LongWritable$Comparator:<init>(),90,92,"/**
 * Constructs a Comparator, using LongWritable for comparison.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DoubleWritable.java,<init>,org.apache.hadoop.io.DoubleWritable$Comparator:<init>(),88,90,"/**
 * Constructs a comparator using DoubleWritable.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,<init>,org.apache.hadoop.io.MD5Hash$Comparator:<init>(),249,251,"/**
 * Constructs a Comparator using MD5Hash class as the comparison key.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ShortWritable.java,<init>,org.apache.hadoop.io.ShortWritable$Comparator:<init>(),98,100,"/**
 * Constructs a Comparator using ShortWritable for comparison.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FloatWritable.java,<init>,org.apache.hadoop.io.FloatWritable$Comparator:<init>(),85,87,"/**
 * Default constructor. Calls super with FloatWritable.class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,<init>,org.apache.hadoop.io.BytesWritable$Comparator:<init>(),224,226,"/**
 * Default constructor. Calls super with BytesWritable.class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,<init>,org.apache.hadoop.io.BooleanWritable$Comparator:<init>(),111,113,"/**
 * Default constructor. Calls super with BooleanWritable class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8$Comparator:<init>(),212,214,"/**
 * Constructs a Comparator, initializing it with the UTF8 class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,authenticate,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",380,410,"/**
 * Authenticates request, using delegation token or fallback handler.
 * @param request HTTP request
 * @param response HTTP response
 * @return AuthenticationToken or null on failure
 */","* Authenticates a request looking for the <code>delegation</code>
   * query-string parameter and verifying it is a valid token. If there is not
   * <code>delegation</code> query-string parameter, it delegates the
   * authentication to the {@link KerberosAuthenticationHandler} unless it is
   * disabled.
   *
   * @param request the HTTP client request.
   * @param response the HTTP client response.
   * @return the authentication token for the authenticated request.
   * @throws IOException thrown if an IO error occurred.
   * @throws AuthenticationException thrown if the authentication failed.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,<init>,"org.apache.hadoop.ha.NodeFencer:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)",78,81,"/**
 * Parses fencing methods from config and spec.
 * @param conf Configuration object.
 * @param spec Fencing specification string.
 * @throws BadFencingConfigurationException on parsing error.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleDeprecation,"org.apache.hadoop.conf.Configuration:handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String)",724,762,"/**
 * Processes deprecation keys, updates properties, and returns names.
 */","* Checks for the presence of the property <code>name</code> in the
   * deprecation map. Returns the first of the list of new keys if present
   * in the deprecation map or the <code>name</code> itself. If the property
   * is not presently set but the property map contains an entry for the
   * deprecated key, the value of the deprecated key is set as the value for
   * the provided property name.
   *
   * Also updates properties and overlays with deprecated keys, if the new
   * key does not already exist.
   *
   * @param deprecations deprecation context
   * @param name the property name
   * @return the first property in the list of properties mapping
   *         the <code>name</code> or the <code>name</code> itself.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,setConfAsEnvVars,org.apache.hadoop.ha.ShellCommandFencer:setConfAsEnvVars(java.util.Map),207,211,"/**
* Populates the environment map with keys and values from m5().
* @param env The environment map to populate.
*/","* Set the environment of the subprocess to be the Configuration,
   * with '.'s replaced by '_'s.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,setHeaders,org.apache.hadoop.http.HttpServer2:setHeaders(org.apache.hadoop.conf.Configuration),1958,1970,"/**
 * Creates a map of X-Frame-Options parameters from configuration.
 * @param conf Configuration object to extract parameters from.
 * @return Map containing X-Frame-Options parameters.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,getFilterParameters,"org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getFilterParameters(org.apache.hadoop.conf.Configuration,java.lang.String)",54,65,"/**
 * Extracts key-value pairs from configuration with a prefix.
 * @param conf Configuration object.
 * @param prefix Prefix for configuration keys.
 * @return Map of extracted key-value pairs.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processArguments,org.apache.hadoop.fs.shell.SetReplication:processArguments(java.util.LinkedList),74,79,"/**
* Calls m2() if waitOpt is true, after calling super.m1(args).
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.DelegateToFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",117,122,"/**
 * Delegates block location retrieval to the file system implementation.
 * @param f Path to the file.
 * @param start Start offset.
 * @param len Length of data.
 * @return Array of BlockLocation objects.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",1512,1535,"/**
* Reads block locations from a fallback FS if applicable, otherwise throws exception.
* @param fs FileStatus object
* @param start Starting offset
* @param len Length of data to read
* @return BlockLocation[] array
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,renewToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)",190,196,"/**
 * Renews a token with a renewer.
 * @param token The token to renew.
 * @param renewer The renewer string.
 * @return The renewal result.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",198,206,"/**
 * Cancels a token using a canceler, defaulting to one from m2().
 * @param token The token to cancel.
 * @param canceler The canceler to use, or null to use a default.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",154,164,"/**
 * Processes a token, retrieves its ID, registers it, then delegates.
 */","* Cancels a token by removing it from the SQL database. This will
   * call the corresponding method in {@link AbstractDelegationTokenSecretManager}
   * to perform validation and remove the token from the cache.
   * @return Identifier of the canceled token",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,getGroups,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String),52,58,"/**
 * Retrieves user groups, caches them, and returns the list.
 */","* Get unix groups (parent) and netgroups for given user
   *
   * @param user get groups and netgroups for this user
   * @return groups and netgroups for user",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTar,"org.apache.hadoop.fs.FileUtil:unTar(java.io.File,java.io.File)",1015,1033,"/**
 * Extracts a file to a directory, handling gzip and OS differences.
 * @param inFile Input file to extract.
 * @param untarDir Directory to extract to.
 */
","* Given a Tar File as input it will untar the file in a the untar directory
   * passed as the second parameter
   *
   * This utility will untar "".tar"" files and "".tar.gz"",""tgz"" files.
   *
   * @param inFile The tar file as input.
   * @param untarDir The untar directory where to untar the tar file.
   * @throws IOException an exception occurred.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,chmod,"org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String)",1289,1292,"/**
 * Calls m1 with default 'copy' flag.
 * @param filename File to process.
 * @param perm Permissions to apply.
 * @return Result code from m1.
 */","* Change the permissions on a filename.
   * @param filename the name of the file to change
   * @param perm the permission string
   * @return the exit code from the command
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException command interrupted.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setReadable,"org.apache.hadoop.fs.FileUtil:setReadable(java.io.File,boolean)",1347,1359,"/**
 * Sets file read permission on Windows, otherwise uses File.setReadable.
 * @param f The file to modify.
 * @param readable True to grant read permission, false to revoke.
 * @return True on success, false on failure.
 */","* Platform independent implementation for {@link File#setReadable(boolean)}
   * File#setReadable does not work as expected on Windows.
   * @param f input file
   * @param readable readable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setWritable,"org.apache.hadoop.fs.FileUtil:setWritable(java.io.File,boolean)",1368,1380,"/**
 * Sets file write permission on Windows, otherwise delegates.
 * @param f The file to modify.
 * @param writable True to add write permission, false to remove.
 * @return True if successful, false otherwise.
 */","* Platform independent implementation for {@link File#setWritable(boolean)}
   * File#setWritable does not work as expected on Windows.
   * @param f input file
   * @param writable writable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setExecutable,"org.apache.hadoop.fs.FileUtil:setExecutable(java.io.File,boolean)",1392,1404,"/**
 * Sets file executable permission on Windows, otherwise uses File.setExecutable().
 * @param f The file to modify.
 * @param executable True to set executable, false to remove it.
 * @return True if successful, false otherwise.
 */","* Platform independent implementation for {@link File#setExecutable(boolean)}
   * File#setExecutable does not work as expected on Windows.
   * Note: revoking execute permission on folders does not have the same
   * behavior on Windows as on Unix platforms. Creating, deleting or renaming
   * a file within that folder will still succeed on Windows.
   * @param f input file
   * @param executable executable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,refreshIfNeeded,org.apache.hadoop.util.SysInfoWindows:refreshIfNeeded(),94,141,"/**
 * Refreshes system information if the interval has elapsed.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfo,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfo(),983,995,"/**
 * Performs actions based on conditions involving m1, m2, and IO.
 */","* Load file permission information (UNIX symbol rwxrwxrwx, sticky bit info).
     *
     * To improve peformance, give priority to native stat() call. First try get
     * permission information by using native JNI call then fall back to use non
     * native (ProcessBuilder) call in case native lib is not loaded or native
     * call is not successful",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setOwner,"org.apache.hadoop.fs.RawLocalFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1099,1103,"/**
* Delegates file processing to FileUtil.
* @param p Path to the file.
* @param username Username.
* @param groupname Group name.
*/",* Use the command chown to set owner.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setPermission,"org.apache.hadoop.fs.FileUtil:setPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1470,1508,"/**
* Sets file permissions based on group permissions, handling edge cases.
*/","* Set permissions to the required value. Uses the java primitives instead
   * of forking if group == other.
   * @param f the file to change
   * @param permission the new permissions
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,cacheGroupsAdd,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List),75,88,"/**
 * Processes a list of groups, caching those meeting criteria.
 */","* Add a group to cache, only netgroups are cached
   *
   * @param groups list of group names to add to cache",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpTokens,org.apache.hadoop.security.KDiag:dumpTokens(org.apache.hadoop.security.UserGroupInformation),811,819,"/**
 * Logs user group tokens.
 * @param ugi UserGroupInformation object containing tokens.
 */","* Dump all tokens of a UGI.
   * @param ugi UGI to examine",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logUserInfo,"org.apache.hadoop.security.UserGroupInformation:logUserInfo(org.slf4j.Logger,java.lang.String,org.apache.hadoop.security.UserGroupInformation)",1980,1991,"/**
 * Logs user group information with tokens, if logging is enabled.
 * @param log Logger instance for logging.
 * @param caption Log message caption.
 * @param ugi UserGroupInformation object.
 */","* Log current UGI and token information into specified log.
   * @param ugi - UGI
   * @param log log.
   * @param caption caption.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,containsKmsDt,org.apache.hadoop.crypto.key.kms.KMSClientProvider:containsKmsDt(org.apache.hadoop.security.UserGroupInformation),1155,1165,"/**
 * Checks if KMS delegation token exists in user credentials.
 * @param ugi UserGroupInformation object
 * @return True if token exists, false otherwise.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDir,"org.apache.hadoop.util.DiskChecker:checkDir(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",113,117,"/**
 * Sets file system permissions on a directory.
 * @param localFS File system object.
 * @param dir Directory path.
 * @param expected Expected permissions.
 */","* Create the local directory if necessary, check permissions and also ensure
   * it can be read from and written into.
   *
   * @param localFS local filesystem
   * @param dir directory
   * @param expected permission
   * @throws DiskErrorException disk problem.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirWithDiskIo,"org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",131,136,"/**
* Sets file permissions recursively.
* @param localFS File system.
* @param dir Directory path.
* @param expected Expected permissions.
*/
","* Create the local directory if necessary, also ensure permissions
   * allow it to be read from and written into. Perform some diskIO
   * to ensure that the disk is usable for writes. 
   *
   * @param localFS local filesystem
   * @param dir directory
   * @param expected permission
   * @throws DiskErrorException disk problem.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[]),1823,1826,"/**
 * Lists status of files in the given paths.
 * @param files array of paths to check
 * @return FileStatus array for each path
 */","* See {@link #listStatus(Path[], PathFilter)}
     *
     * @param files files.
     * @throws AccessControlException If access is denied.
     * @throws FileNotFoundException If <code>files</code> does not exist.
     * @throws IOException If an I/O error occurred.
     * @return file status array.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path),1481,1496,"/**
 * Resolves a path, handling exceptions and finding the enclosing path.
 * @param path The path to resolve.
 * @return The resolved path.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ViewFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",447,453,"/**
 * Opens a data output stream to a file.
 * @param f path to the file
 * @param bufferSize buffer size
 * @param progress progressable object
 * @return FSDataOutputStream
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.viewfs.ViewFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",455,468,"/**
 * Creates a data output stream at the specified path.
 * @param f path to create stream on
 * @return FSDataOutputStream object
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ViewFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",470,483,"/**
 * Creates a data output stream for writing to a file.
 * @param f path to the file, permission, overwrite, bufferSize, etc.
 * @return FSDataOutputStream object
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",486,496,"/**
 * Delegates delete operation to target file system.
 * @param f Path to delete. @param recursive If recursive delete.
 * @return true if successful, false otherwise.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),514,521,"/**
 * Calculates checksum of a file using resolved file system.
 * @param f Path to the file.
 * @return FileChecksum object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",523,530,"/**
 * Calculates checksum for a file.
 * @param f Path to the file. @param length File length.
 * @return FileChecksum object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listLocatedStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",630,655,"/**
 * Lists files under a path, filtered by the given PathFilter.
 * @param f the path to list
 * @param filter the filter to apply
 * @return RemoteIterator of LocatedFileStatus objects
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path),670,675,"/**
 * Delegates to the target file system's m3 method.
 * @param dir Path to resolve; delegates to target FS.
 * @return Result of the target file system's m3 method.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",677,683,"/**
 * Sets file permissions on a path.
 * @param dir Path to set permissions on.
 * @param permission The desired file system permissions.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ViewFileSystem:open(org.apache.hadoop.fs.Path,int)",685,691,"/**
 * Opens a file for input.
 * @param f Path to the file.
 * @param bufferSize Buffer size for input stream.
 * @return FSDataInputStream for reading the file.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFileSystem:truncate(org.apache.hadoop.fs.Path,long)",800,806,"/**
 * Updates file length using the target file system.
 * @param f Path to the file. @param newLength New file length.
 * @return True on success, false otherwise.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",808,815,"/**
 * Delegates file operation to target file system.
 * @param f Path to the file.
 * @param username Username.
 * @param groupname Groupname.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",817,823,"/**
 * Sets file permissions.
 * @param f Path to the file.
 * @param permission FileSystem permission to set.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",825,831,"/**
 * Delegates file replication to the target file system.
 * @param f Path to the file. @param replication Replication factor.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",833,839,"/**
* Updates file modification times on the target file system.
* @param f Path to the file.
* @param mtime Modification time.
* @param atime Access time.
*/
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",841,847,"/**
 * Delegates path access and ACL specification to the target FS.
 * @param path The path to access.
 * @param aclSpec ACL entries to apply.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",849,855,"/**
 * Delegates path access and ACL specification to target FS.
 * @param path The path to access.
 * @param aclSpec ACL entries to apply.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),857,863,"/**
 * Delegates path resolution and processing to the target FS.
 * @param path The path to resolve and process.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAcl(org.apache.hadoop.fs.Path),865,871,"/**
 * Delegates path resolution and operation to the target file system.
 * @param path The path to resolve and operate on.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",873,878,"/**
 * Delegates path access and ACL specification to the target FS.
 * @param path The path to access.
 * @param aclSpec ACL entries to apply.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getAclStatus(org.apache.hadoop.fs.Path),880,885,"/**
 * Resolves ACL status by traversing the file system.
 * @param path Path to resolve.
 * @return AclStatus object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",887,893,"/**
 * Sets an Xattr on a file.
 * @param path File path.
 * @param name Xattr name.
 * @param value Xattr value.
 * @param flag Xattr set flags.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",895,900,"/**
 * Delegates file read operation to the target file system.
 * @param path Path to the file.
 * @param name File name.
 * @return Byte array containing file content.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path),902,907,"/**
 * Delegates file metadata retrieval to the target file system.
 * @param path Path to the file.
 * @return Map of metadata (name -> bytes).
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",909,915,"/**
 * Resolves and retrieves file contents by names.
 * @param path Path to resolve.
 * @param names List of file names to retrieve.
 * @return Map of file names to byte array contents.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem:listXAttrs(org.apache.hadoop.fs.Path),917,922,"/**
 * Delegates to the target file system to list directory entries.
 * @param path The path to the directory.
 * @return List of strings representing directory entries.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",924,929,"/**
 * Delegates a file operation to the target file system.
 * @param path The initial path.
 * @param name The file name.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),990,1002,"/**
 * Delegates m3 call to the target file system.
 * @param f Path to delegate the call to.
 * @return short value returned by the target fs's m3.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ViewFileSystem:getContentSummary(org.apache.hadoop.fs.Path),1015,1020,"/**
 * Retrieves a content summary by resolving a file path.
 * @param f Path to the file; returns ContentSummary.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ViewFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),1022,1027,"/**
 * Calculates quota usage for a file path.
 * @param f Path to the file; returns QuotaUsage object.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1072,1078,"/**
 * Delegates path operation to the target file system.
 * @param path Input path.
 * @param snapshotName Snapshot name.
 * @return Path object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1080,1087,"/**
 * Delegates snapshot creation to the target file system.
 * @param path Path to snapshot.
 * @param snapshotOldName Old snapshot name.
 * @param snapshotNewName New snapshot name.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1089,1095,"/**
 * Delegates file system operation to target FS.
 * @param path The path to operate on.
 * @param snapshotName Snapshot name for the operation.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1097,1102,"/**
 * Resolves and executes m3 on the target file system.
 * @param src Path to resolve and process.
 * @throws IOException if an I/O error occurs.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1104,1109,"/**
 * Delegates path resolution and policy application to FS.
 * @param src Path to resolve.
 * @param policyName Policy name to apply.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),1111,1116,"/**
 * Resolves and executes m3 on the target file system.
 * @param src Path to the source file.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),1118,1123,"/**
 * Resolves a BlockStoragePolicySpi from a Path.
 * @param src Path to resolve; throws IOException on failure.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus(org.apache.hadoop.fs.Path),1304,1312,"/**
 * Resolves a path, handling null paths.
 * @param p The path to resolve.
 * @return FsStatus object indicating resolution status.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getUsed,org.apache.hadoop.fs.viewfs.ViewFileSystem:getUsed(),1321,1330,"/**
 * Gets used space for the root filesystem.
 * Throws exception if not a mount point.
 */","* Return the total size of all files under ""/"", if {@link
   * Constants#CONFIG_VIEWFS_LINK_MERGE_SLASH} is supported and is a valid
   * mount point. Else, throw NotInMountpointException.
   *
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),1332,1341,"/**
 * Resolves a path within a file system.
 * @param path The path to resolve.
 * @return Resolved Path object.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.HarFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",468,481,"/**
 * Retrieves block locations for a file segment.
 * @param file FileStatus object
 * @param start Start offset of the segment
 * @param len Length of the segment
 * @return Array of BlockLocation objects
 */","* Get block locations from the underlying fs and fix their
   * offsets and lengths.
   * @param file the input file status to get block locations
   * @param start the start of the desired range in the contained file
   * @param len the length of the desired range
   * @return block locations for this segment of file
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,open,"org.apache.hadoop.fs.HarFileSystem:open(org.apache.hadoop.fs.Path,int)",679,690,"/**
 * Opens an FSDataInputStream for a file.
 * @param f Path to the file.
 * @param bufferSize Input stream buffer size.
 * @return FSDataInputStream for the file.
 */","* Returns a har input stream which fakes end of 
   * file. It reads the index files to get the part 
   * file name and the size and start of the file.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path)",148,151,"/**
 * Constructs a ChecksumFSInputChecker with default buffer size.
 * @param fs ChecksumFs instance
 * @param file Path to the file
 * @throws IOException, UnresolvedLinkException
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,open,"org.apache.hadoop.fs.ChecksumFs:open(org.apache.hadoop.fs.Path,int)",334,339,"/**
 * Opens an input stream for the given path.
 * @param f Path to open. @param bufferSize I/O buffer size.
 * @return FSDataInputStream for reading data.
 */","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,createInternal,"org.apache.hadoop.fs.ChecksumFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",418,428,"/**
 * Creates a data output stream with checksumming.
 * @param f Path to output.
 * @return FSDataOutputStream object.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,processArguments,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:processArguments(java.util.LinkedList),81,94,"/**
 * Processes path data: calls m1, m2, calls super.m3, and m4.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,open,"org.apache.hadoop.fs.ChecksumFileSystem:open(org.apache.hadoop.fs.Path,int)",566,578,"/**
 * Opens an input stream for a file, handling checksum verification.
 * @param f Path to the file.
 * @param bufferSize Buffer size for input stream.
 * @return FSDataInputStream for reading the file.
 */","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * @throws IOException if an I/O error occurs.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable)",704,734,"/**
 * Opens a data output stream to a file, creating it if necessary.
 * @param f Path to the file.
 * @return FSDataOutputStream object.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,isValidName,org.apache.hadoop.fs.viewfs.ChRootedFs:isValidName(java.lang.String),97,100,"/**
* Delegates m4 operation to FileSystem after processing the path.
* @param src The input file path string.
* @return True if the operation is successful, false otherwise.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",173,182,"/**
 * Creates a data output stream for the given path.
 * @param f Path to create stream for.
 * @return FSDataOutputStream
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,delete,"org.apache.hadoop.fs.viewfs.ChRootedFs:delete(org.apache.hadoop.fs.Path,boolean)",184,188,"/**
 * Delegates file deletion to the underlying FileSystem.
 * @param f Path to delete. @param recursive Delete recursively.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ChRootedFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",190,194,"/**
 * Retrieves block locations for a file range.
 * @param f file path
 * @param start start offset
 * @param len length of data
 * @return BlockLocation array
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileChecksum(org.apache.hadoop.fs.Path),196,200,"/**
* Calculates checksum for a file using the file system's method.
* @param f Path to the file.
* @return FileChecksum object.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileStatus(org.apache.hadoop.fs.Path),202,206,"/**
 * Delegates FileStatus retrieval to the underlying FileSystem.
 * @param f The Path to get the FileStatus for.
 * @return The FileStatus object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileLinkStatus(org.apache.hadoop.fs.Path),213,217,"/**
 * Gets the FileStatus for the given path.
 * @param f Path for which to retrieve FileStatus.
 * @return FileStatus object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFs:getServerDefaults(org.apache.hadoop.fs.Path),230,233,"/**
 * Retrieves FsServerDefaults using a modified path.
 * @param f The input Path object.
 * @return FsServerDefaults object.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:listStatus(org.apache.hadoop.fs.Path),240,244,"/**
 * Retrieves FileStatus for a path.
 * @param f The path to retrieve status for.
 * @return FileStatus array.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listStatusIterator,org.apache.hadoop.fs.viewfs.ChRootedFs:listStatusIterator(org.apache.hadoop.fs.Path),246,250,"/**
 * Returns a RemoteIterator for the file at the given path.
 * @param f the path to the file
 * @return RemoteIterator for the file
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:listLocatedStatus(org.apache.hadoop.fs.Path),252,256,"/**
 * Gets a remote iterator for a file status.
 * @param f the path to the file
 * @return A RemoteIterator object
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,mkdir,"org.apache.hadoop.fs.viewfs.ChRootedFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",258,263,"/**
* Calls the underlying filesystem's m2 method with adjusted Path.
* @param dir The directory path.
* @param permission FsPermission for the directory.
* @param createParent Whether to create parent directories.
*/
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,open,"org.apache.hadoop.fs.viewfs.ChRootedFs:open(org.apache.hadoop.fs.Path,int)",265,269,"/**
 * Opens an input stream for a file.
 * @param f Path to the file.
 * @param bufferSize Buffer size for the stream.
 * @return FSDataInputStream object.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,truncate,"org.apache.hadoop.fs.viewfs.ChRootedFs:truncate(org.apache.hadoop.fs.Path,long)",271,275,"/**
* Delegates file length update to the underlying file system.
* @param f Path to the file. @param newLength New length of the file.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",277,283,"/**
 * Copies a file or directory from src to dst using the filesystem.
 * @param src Source path.
 * @param dst Destination path.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",285,292,"/**
* Copies a Path to another Path using the filesystem.
* @param src Source Path
* @param dst Destination Path
* @param overwrite Overwrites destination if true
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setOwner,"org.apache.hadoop.fs.viewfs.ChRootedFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",294,300,"/**
 * Calls myFs.m2 with processed path, username, and groupname.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setPermission,"org.apache.hadoop.fs.viewfs.ChRootedFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",302,306,"/**
 * Sets file access permissions using the file system.
 * @param f Path to the file. @param permission Permissions to set.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setReplication,"org.apache.hadoop.fs.viewfs.ChRootedFs:setReplication(org.apache.hadoop.fs.Path,short)",308,312,"/**
 * Delegates m2 operation to the underlying FileSystem.
 * @param f Path object
 * @param replication replication factor
 * @return Result of the delegated operation
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setTimes,"org.apache.hadoop.fs.viewfs.ChRootedFs:setTimes(org.apache.hadoop.fs.Path,long,long)",314,318,"/**
 * Delegates m2 operation to the underlying file system.
 * @param f Path to the file. mtime, atime: modification/access times.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",320,324,"/**
* Calls myFs.m2 with a modified path and the given ACL specifications.
* @param path The path to operate on.
* @param aclSpec ACL entries to apply.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",326,330,"/**
* Calls myFs.m2 with a modified path and the provided ACL entries.
* @param path The path to operate on.
* @param aclSpec ACL entries to apply.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ChRootedFs:removeDefaultAcl(org.apache.hadoop.fs.Path),332,335,"/**
* Calls myFs.m2 with the result of m1(path).
* @param path The path to process.
* @throws IOException if an I/O error occurs.
*/
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ChRootedFs:removeAcl(org.apache.hadoop.fs.Path),337,340,"/**
* Calls myFs.m2 with the result of m1(path).
* @param path The path to process.
* @throws IOException if an I/O error occurs.
*/
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ChRootedFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",342,345,"/**
* Calls myFs.m2 with transformed path and ACL specifications.
* @param path The path to operate on.
* @param aclSpec ACL entries to apply.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getAclStatus(org.apache.hadoop.fs.Path),347,350,"/**
 * Delegates AclStatus retrieval to the underlying file system.
 * @param path The path to check ACL status for.
 * @return AclStatus object from the file system.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",352,356,"/**
 * Delegates m2 operation to the underlying filesystem.
 * @param path Path to operate on.
 * @param name Attribute name.
 * @param value Attribute value.
 * @param flag Attribute set flags.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",358,361,"/**
* Calls m2 on the file system with a modified path and name.
* @param path The path object
* @param name The file name
* @return Byte array returned by the file system's m2 method
*/
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path),363,366,"/**
 * Delegates to the underlying FileSystem's m2 method.
 * @param path The path to process.
 * @return A map of string keys to byte array values.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",368,372,"/**
* Delegates file data retrieval to the underlying file system.
* @param path Path to the file.
* @param names List of names to retrieve.
* @return Map of names to byte arrays.
*/
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFs:listXAttrs(org.apache.hadoop.fs.Path),374,377,"/**
* Delegates to the underlying filesystem, using a transformed path.
* @param path The input path.
* @return A list of strings.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",379,382,"/**
* Calls myFs.m2 with a modified path and given name.
* @param path The input path.
* @param name The name to pass to myFs.m2.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",384,387,"/**
* Calls m2 on a modified path.
* @param path The input path.
* @param name The name to pass to m2.
* @return The result of the m2 call.
*/
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",389,393,"/**
 * Creates a snapshot of a path.
 * @param path Path to snapshot.
 * @param snapshotOldName Old snapshot name.
 * @param snapshotNewName New snapshot name.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",395,399,"/**
 * Delegates snapshot creation to the file system.
 * @param snapshotDir Snapshot directory.
 * @param snapshotName Snapshot name.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ChRootedFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",406,410,"/**
* Calls myFs.m2 with processed path and policyName.
* @param path The path to operate on.
* @param policyName The policy name to apply.
*/
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),412,416,"/**
* Calls myFs.m2 with the result of m1(src).
* @param src Path to process; throws IOException.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ChRootedFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",441,451,"/**
 * Delegates m2 operation to the underlying filesystem.
 * @param target Target path.
 * @param link Link path.
 * @param createParent Whether to create parent directories.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ChRootedFs:getLinkTarget(org.apache.hadoop.fs.Path),453,456,"/**
* Delegates file operation to underlying FileSystem.
* @param f Input Path object
* @return Path object returned by FileSystem's m2
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",568,636,"/**
 * Renames a file or directory from src to dst, optionally overwriting.
 * @param src Source path
 * @param dst Destination path
 * @param overwrite Whether to overwrite existing files
 * @throws IOException, UnresolvedLinkException
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,next,org.apache.hadoop.fs.viewfs.ViewFs$WrappingRemoteIterator:next(),945,952,"/**
 * Processes an inner iteration status, constructs a path, and returns a result.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getChrootedPath,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getChrootedPath(org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",657,668,"/**
 * Determines the path based on file system type and status.
 * @param res ResolveResult containing file system info.
 * @param status FileStatus object.
 * @param f Initial path.
 * @return Resolved Path object.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,renameInternal,"org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",480,497,"/**
 * Copies a file from src to dst, handling various conditions.
 * @param src Source file path
 * @param dst Destination file path
 */
",* Rename files/dirs.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,renameInternal,"org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",499,523,"/**
* Copies a file from src to dst, overwriting if specified.
* @param src source file path
* @param dst destination file path
* @param overwrite whether to overwrite existing files
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,rename,"org.apache.hadoop.fs.FileContext:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",1031,1060,"/**
 * Renames a file or directory from src to dst, handling unresolved links.
 * @param src Source path.
 * @param dst Destination path.
 * @param options Rename options.
 */","* Renames Path src to Path dst
   * <ul>
   * <li>Fails if src is a file and dst is a directory.
   * <li>Fails if src is a directory and dst is a file.
   * <li>Fails if the parent of dst does not exist or is a file.
   * </ul>
   * <p>
   * If OVERWRITE option is not passed as an argument, rename fails if the dst
   * already exists.
   * <p>
   * If OVERWRITE option is passed as an argument, rename overwrites the dst if
   * it is a file or an empty directory. Rename fails if dst is a non-empty
   * directory.
   * <p>
   * Note that atomicity of rename is dependent on the file system
   * implementation. Please refer to the file system documentation for details
   * <p>
   * 
   * @param src path to be renamed
   * @param dst new path after rename
   * @param options rename options.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If <code>dst</code> already exists and
   *           <code>options</code> has {@link Options.Rename#OVERWRITE}
   *           option false.
   * @throws FileNotFoundException If <code>src</code> does not exist
   * @throws ParentNotDirectoryException If parent of <code>dst</code> is not a
   *           directory
   * @throws UnsupportedFileSystemException If file system for <code>src</code>
   *           and <code>dst</code> is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameInternal,"org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",245,251,"/**
* Moves a file or directory from src to dst.
* @param src Source path.
* @param dst Destination path.
*/
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,fileStatusesInIndex,"org.apache.hadoop.fs.HarFileSystem:fileStatusesInIndex(org.apache.hadoop.fs.HarFileSystem$HarStatus,java.util.List)",511,522,"/**
 * Adds file statuses of children to the provided list.
 * @param parent  Parent HarStatus object.
 * @param statuses List to store FileStatus objects.
 */","* Get filestatuses of all the children of a given directory. This just reads
   * through index file and reads line by line to get all statuses for children
   * of a directory. Its a brute force way of getting all such filestatuses
   * 
   * @param parent
   *          the parent path directory
   * @param statuses
   *          the list to add the children filestatuses to",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileStatus,org.apache.hadoop.fs.HarFileSystem:getFileStatus(org.apache.hadoop.fs.Path),640,644,"/**
 * Retrieves a FileStatus for the given path.
 * @param f Path to retrieve status for.
 * @return FileStatus object.
 */
","* return the filestatus of files in har archive.
   * The permission returned are that of the archive
   * index files. The permissions are not persisted 
   * while creating a hadoop archive.
   * @param f the path in har filesystem
   * @return filestatus.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,deprecatedGetFileStatus,org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileStatus(org.apache.hadoop.fs.Path),910,919,"/**
 * Retrieves FileStatus for a Path. Returns status or throws exception.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,org.apache.hadoop.fs.LocatedFileStatus:<init>(),40,42,"/**
 * Default constructor for LocatedFileStatus.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:<init>(org.apache.hadoop.fs.viewfs.ChRootedFileSystem,org.apache.hadoop.fs.FileStatus)",464,468,"/**
 * Constructs a NflyStatus with a FileStatus and ChRootedFileSystem.
 * @param realFs The ChRootedFileSystem.
 * @param realStatus The FileStatus.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFsFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",35,38,"/**
 * Initializes a ViewFsFileStatus with a FileStatus and new path.
 * @param fs The FileStatus to use.
 * @param newPath The new Path for the ViewFsFileStatus.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",205,250,"/**
 * Retrieves FileStatus for a file on the SFTP server.
 * @param client SFTP client
 * @param file Path of the file
 * @return FileStatus object or throws FileNotFoundException
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.AbstractHttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
 * Creates a FileStatus object for a given path.
 * @param path The path to create a FileStatus for.
 * @return A FileStatus object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,doGlob,org.apache.hadoop.fs.Globber:doGlob(),208,396,"/**
 * Finds FileStatus objects matching a glob pattern.
 * @return Array of FileStatus objects or null if no match.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,notFoundStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:notFoundStatus(org.apache.hadoop.fs.Path),623,625,"/**
 * Creates a FileStatus object with default values for a given path.
 * @param f The Path for which to create the FileStatus.
 * @return A FileStatus object.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",516,548,"/**
 * Retrieves FileStatus for a file, recursively searching FTP directories.
 * @param client FTP client
 * @param file Path to the file
 * @return FileStatus object or throws FileNotFoundException
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE,boolean)",299,321,"/**
 * Initializes the CBZip2InputStream with an input stream and mode.
 * @param in Input stream to read from.
 * @param readMode Read mode (CONTINUOUS or BYBLOCK).
 * @param skipDecompression Whether to skip decompression.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read0,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read0(),450,490,"/**
 * Processes current state and returns a value or throws exception.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Print:execute(),193,198,"/**
* Processes each token file using DtFileOperations.
* Iterates through tokenFiles and calls m3 with provided params.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",833,843,"/**
 * Processes a token, reads its data, and calls super.m5.
 * @param token The token to process.
 * @param canceller Cancellation token.
 * @return TokenIdent object.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageToStream,org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream),300,304,"/**
* Calls m1 with SerializedFormat.WRITABLE.
* @param os Output stream to write to.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>(),139,141,"/**
 * Default constructor. Calls the parameterized constructor with null.
 */",* Construct the system but not initializing (read config etc.) it.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSource,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)",260,270,"/**
 * Registers a new metrics source with the given name and details.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,long,java.lang.String,java.lang.String,long)",577,597,"/**
* Handles RPC calls, either directly or via ProtobufRpcEngine.
* @param server RPC server instance
* @return Writable result of the RPC call
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,<init>,org.apache.hadoop.metrics2.lib.MutableRollingAverages:<init>(java.lang.String),144,155,"/**
 * Initializes a MutableRollingAverages object.
 * @param metricValueName Name of the metric being averaged.
 */
","* Constructor for {@link MutableRollingAverages}.
   * @param metricValueName input metricValueName.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,replaceScheduledTask,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:replaceScheduledTask(int,long,java.util.concurrent.TimeUnit)",160,167,"/**
* Configures windowing, starts task, and schedules rate rolling.
*/",* This method is for testing only to replace the scheduledTask.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)",272,294,"/**
 * Adds a metrics sink with name. Creates if not exists.
 * @param name Sink name.
 * @param description Sink description.
 * @param sink The sink to add.
 * @return The added sink.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSinks,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSinks(),492,519,"/**
 * Initializes and registers metrics sinks based on configuration.
 * Calculates the overall sink period based on sink configurations.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,recheckElectability,org.apache.hadoop.ha.ZKFailoverController:recheckElectability(),808,860,"/**
* Handles master election based on health state & delay.
*/","* Check the current state of the service, and join the election
   * if it should be in the election.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reJoinElectionAfterFailureToBecomeActive,org.apache.hadoop.ha.ActiveStandbyElector:reJoinElectionAfterFailureToBecomeActive(),627,629,"/**
* Calls m1 with SLEEP_AFTER_FAILURE_TO_BECOME_ACTIVE.
*/","* We failed to become active. Re-join the election, but
   * sleep for a few seconds after terminating our existing
   * session, so that other nodes have a chance to become active.
   * The failure to become active is already logged inside
   * becomeActive().",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processWatchEvent,"org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)",635,713,"/**
 * Handles ZooKeeper watch events, updating state and triggering actions.
 * @param zk ZooKeeper instance
 * @param event WatchedEvent received from ZooKeeper
 */","* interface implementation of Zookeeper watch events (connection and node),
   * proxied by {@link WatcherWithClientRef}.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int)",180,183,"/**
 * Resolves a target host to an InetSocketAddress.
 * @param target Hostname or IP address.
 * @param defaultPort Default port if resolution fails.
 * @return InetSocketAddress object.
 */
","* Util method to build socket addr from either.
   *   {@literal <host>}
   *   {@literal <host>:<port>}
   *   {@literal <fs>://<host>:<port>/<path>}
   *
   * @param target target.
   * @param defaultPort default port.
   * @return socket addr.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processOptions,org.apache.hadoop.fs.shell.find.Find:processOptions(java.util.LinkedList),166,212,"/**
 * Processes command arguments, handles follow links, and evaluates expressions.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,authenticate,"org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",154,182,"/**
* Handles authentication based on delegation schemes.
* Returns AuthenticationToken or returns null if unauthorized.
*/","* This method is overridden to restrict HTTP authentication schemes
   * available for delegation token management functionality. The
   * authentication schemes to be used for delegation token management are
   * configured using {@link DELEGATION_TOKEN_SCHEMES_PROPERTY}
   *
   * The basic logic here is to check if the current request is for delegation
   * token management. If yes then check if the request contains an
   * ""Authorization"" header. If it is missing, then return the HTTP 401
   * response with WWW-Authenticate header for each scheme configured for
   * delegation token management.
   *
   * It is also possible for a client to preemptively send Authorization header
   * for a scheme not configured for delegation token management. We detect
   * this case and return the HTTP 401 response with WWW-Authenticate header
   * for each scheme configured for delegation token management.
   *
   * If a client has sent a request with ""Authorization"" header for a scheme
   * configured for delegation token management, then it is forwarded to
   * underlying {@link MultiSchemeAuthenticationHandler} for actual
   * authentication.
   *
   * Finally all other requests (excluding delegation token management) are
   * forwarded to underlying {@link MultiSchemeAuthenticationHandler} for
   * actual authentication.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleDeprecation,org.apache.hadoop.conf.Configuration:handleDeprecation(),777,786,"/**
 * Handles deprecation for all properties in the configuration.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,onlyKeyExists,org.apache.hadoop.conf.Configuration:onlyKeyExists(java.lang.String),1295,1305,"/**
 * Checks if a name exists in the deprecation context.
 * @param name The name to search for.
 * @return True if the name is found, false otherwise.
 */
","* Return existence of the <code>name</code> property, but only for
   * names which have no valid value, usually non-existent or commented
   * out in XML.
   *
   * @param name the property name
   * @return true if the property <code>name</code> exists without value",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRaw,org.apache.hadoop.conf.Configuration:getRaw(java.lang.String),1355,1362,"/**
 * Processes a name using helper methods.
 * @param name The name to process.
 * @return The processed name or null if processing fails.
 */
","* Get the value of the <code>name</code> property, without doing
   * <a href=""#VariableExpansion"">variable expansion</a>.If the key is 
   * deprecated, it returns the value of the first key which replaces 
   * the deprecated key and is not null.
   * 
   * @param name the property name.
   * @return the value of the <code>name</code> property or 
   *         its replacing property and null if no such property exists.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,set,"org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String,java.lang.String)",1420,1458,"/**
 * Sets a property value, handling deprecation and aliases.
 * @param name Property name.
 * @param value Property value.
 * @param source Source of the property.
 */","* Set the <code>value</code> of the <code>name</code> property. If 
   * <code>name</code> is deprecated, it also sets the <code>value</code> to
   * the keys that replace the deprecated key. Name will be trimmed before put
   * into configuration.
   *
   * @param name property name.
   * @param value property value.
   * @param source the place that this configuration value came from 
   * (For debugging).
   * @throws IllegalArgumentException when the value or name is null.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,unset,org.apache.hadoop.conf.Configuration:unset(java.lang.String),1476,1492,"/**
 * Processes a name, potentially retrieving or creating it.
 * Updates associated objects with the provided name.
 */","* Unset a previously set property.
   * @param name the property name",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,tryFence,"org.apache.hadoop.ha.ShellCommandFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",81,135,"/**
 * Executes a fencing command using ProcessBuilder, handles streams.
 * @param target HAServiceTarget object
 * @param args Command arguments
 * @return True if command executed successfully, false otherwise.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,grantPermissions,org.apache.hadoop.fs.FileUtil:grantPermissions(java.io.File),235,239,"/**
 * Applies masking operations to the specified file.
 * @param f The file to be masked.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getVirtualMemorySize,org.apache.hadoop.util.SysInfoWindows:getVirtualMemorySize(),144,148,"/**
 * Calculates and returns the size of the virtual memory.
 * Calls m1() and returns the vmemSize value.
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getPhysicalMemorySize,org.apache.hadoop.util.SysInfoWindows:getPhysicalMemorySize(),151,155,"/**
 * Calculates and returns the memory size after calling m1().
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getAvailableVirtualMemorySize,org.apache.hadoop.util.SysInfoWindows:getAvailableVirtualMemorySize(),158,162,"/**
 * Returns the available virtual memory. Calls m1() first.
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getAvailablePhysicalMemorySize,org.apache.hadoop.util.SysInfoWindows:getAvailablePhysicalMemorySize(),165,169,"/**
 * Returns the amount of available memory. Calls m1() first.
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumProcessors,org.apache.hadoop.util.SysInfoWindows:getNumProcessors(),172,176,"/**
 * Returns the number of processors available.
 * Calls m1() first, then returns numProcessors.
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCpuFrequency,org.apache.hadoop.util.SysInfoWindows:getCpuFrequency(),185,189,"/**
* Returns the CPU frequency in kHz after executing m1().
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCumulativeCpuTime,org.apache.hadoop.util.SysInfoWindows:getCumulativeCpuTime(),192,196,"/**
* Returns the cumulative CPU time in milliseconds.
* Includes time spent in m1().
*/
",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCpuUsagePercentage,org.apache.hadoop.util.SysInfoWindows:getCpuUsagePercentage(),199,207,"/**
 * Calculates CPU usage per processor.
 * Returns CPU usage, divided by numProcessors if > -1.
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumVCoresUsed,org.apache.hadoop.util.SysInfoWindows:getNumVCoresUsed(),210,218,"/**
* Returns CPU usage as a float, normalized to a percentage.
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNetworkBytesRead,org.apache.hadoop.util.SysInfoWindows:getNetworkBytesRead(),221,225,"/**
* Reads network bytes and returns the count.
* Calls m1() and returns netBytesRead.
*/
",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNetworkBytesWritten,org.apache.hadoop.util.SysInfoWindows:getNetworkBytesWritten(),228,232,"/**
* Calculates a masked value, calls m1(), and returns netBytesWritten.
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getStorageBytesRead,org.apache.hadoop.util.SysInfoWindows:getStorageBytesRead(),234,238,"/**
 * Returns the number of bytes read from storage.
 * Increments storageBytesRead via m1().
 * @return The total bytes read.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getStorageBytesWritten,org.apache.hadoop.util.SysInfoWindows:getStorageBytesWritten(),240,244,"/**
 * Returns the number of bytes written to storage.
 * Increments m1() before returning the value.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getPermission,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getPermission(),951,957,"/**
* Calls super.m3() after potentially calling m2() based on m1().
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getOwner,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getOwner(),959,965,"/**
 * Calls m2 if m1 fails, then calls super.m3().
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getGroup,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getGroup(),967,973,"/**
* Calls m2 if m1 fails, then returns the superclass's m3 result.
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:write(java.io.DataOutput),1087,1093,"/**
* Writes data to output. Calls m2 if m1 fails, then calls super.m3.
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,flush,org.apache.hadoop.security.alias.LocalKeyStoreProvider:flush(),143,160,"/**
 * Resets file permissions based on the 'permissions' variable.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,cacheGroupsRefresh,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh(),63,68,"/**
* Retrieves netgroup data, updates cache, and processes groups.
*/",* Refresh the netgroup cache,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path),498,503,"/**
 * Calls m1 with the provided Path and default 'copy' flag.
 * @param f Path object
 * @return boolean result of m1 call
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus(),1299,1302,"/**
 * Calls m1 with a null argument and returns the FsStatus.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,updateMountPointFsStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:updateMountPointFsStatus(org.apache.hadoop.fs.viewfs.ViewFileSystem,java.util.Map,org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint,org.apache.hadoop.fs.Path)",169,175,"/**
 * Updates mount point status in the map.
 * @param viewFileSystem View file system.
 * @param mountPointMap Map of mount points and statuses.
 * @param mountPoint The mount point.
 * @param path The path to check status.
 */
","* Update FsStatus for the given the mount point.
   *
   * @param viewFileSystem
   * @param mountPointMap
   * @param mountPoint
   * @param path",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,read,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:read(long,byte[],int,int)",194,210,"/**
 * Reads data from the input stream, verifying checksum.
 * @param position file position, b buffer, off offset, len length
 * @return Number of bytes read.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$Put:processArguments(java.util.LinkedList),306,315,"/**
* Processes path data; calls m6 if conditions are met.
* @param args LinkedList of PathData objects
* @throws IOException if an I/O error occurs
*/
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",696,702,"/**
* Calls overloaded method with default create flag.
* @param f Path to create file.
* @param permission FsPermission for the file.
*/
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",736,742,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to the file, permission, overwrite, etc.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",757,768,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path of the file.
 * @return FSDataOutputStream object.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",638,644,"/**
* Copies a file from src to dst.
* @param src Source file path.
* @param dst Destination file path.
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,listStatus,org.apache.hadoop.fs.HarFileSystem:listStatus(org.apache.hadoop.fs.Path),784,804,"/**
* Retrieves FileStatus objects for a given path.
* @param f Path to check; throws IOException if not found.
* @return Array of FileStatus objects.
*/","* liststatus returns the children of a directory 
   * after looking up the index files.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileLinkStatusInternal,"org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatusInternal(org.apache.hadoop.fs.Path,boolean)",1233,1242,"/**
 * Gets a FileStatus for a path, using different methods based on flags.
 */","* Public {@link FileStatus} methods delegate to this function, which in turn
   * either call the new {@link Stat} based implementation or the deprecated
   * methods based on platform support.
   * 
   * @param f Path to stat
   * @param dereference whether to dereference the final path component if a
   *          symlink
   * @return FileStatus of f
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:<init>(org.apache.hadoop.fs.LocatedFileStatus,org.apache.hadoop.fs.Path)",32,35,"/**
 * Initializes LocatedFileStatus with provided status and path.
 * @param locatedFileStatus The LocatedFileStatus object.
 * @param path The Path object.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",505,512,"/**
 * Retrieves BlockLocations for a file, using the underlying FS.
 * @param fs FileStatus representing the file.
 * @param start Start offset for block locations.
 * @param len Length of the requested block locations.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFileStatus(org.apache.hadoop.fs.Path),407,426,"/**
 * Gets file status for a given path.
 * @param f Path to the file.
 * @return FileStatus object.
 */","* {@inheritDoc}
   *
   * If the given path is a symlink(mount link), the path will be resolved to a
   * target path and it will get the resolved path's FileStatus object. It will
   * not be represented as a symlink and isDirectory API returns true if the
   * resolved path is a directory, false otherwise.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFs:listStatus(org.apache.hadoop.fs.Path),518,538,"/**
 * Retrieves file statuses for a given path.
 * @param f the path to retrieve statuses for
 * @return An array of FileStatus objects.
 */","* {@inheritDoc}
   *
   * Note: listStatus considers listing from fallbackLink if available. If the
   * same directory path is present in configured mount path as well as in
   * fallback fs, then only the fallback path will be listed in the returned
   * result except for link.
   *
   * If any of the the immediate children of the given path f is a symlink(mount
   * link), the returned FileStatus object of that children would be represented
   * as a symlink. It will not be resolved to the target path and will not get
   * the target path FileStatus object. The target path will be available via
   * getSymlink on that children's FileStatus object. Since it represents as
   * symlink, isDirectory on that children's FileStatus will return false.
   * This behavior can be changed by setting an advanced configuration
   * fs.viewfs.mount.links.as.symlinks to false. In this case, mount points will
   * be represented as non-symlinks and all the file/directory attributes like
   * permissions, isDirectory etc will be assigned from it's resolved target
   * directory/file.
   *
   * If you want to get the FileStatus of target path for that children, you may
   * want to use GetFileStatus API with that children's symlink path. Please see
   * {@link ViewFs#getFileStatus(Path f)}
   *
   * Note: In ViewFs, by default the mount links are represented as symlinks.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,exists,"org.apache.hadoop.fs.sftp.SFTPFileSystem:exists(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",189,198,"/**
 * Executes a file operation and handles exceptions.
 * @param channel SFTP channel; @param file Path to file
 * @throws IOException if an I/O error occurs
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,com.jcraft.jsch.ChannelSftp$LsEntry,org.apache.hadoop.fs.Path)",260,297,"/**
 * Creates a FileStatus object from an LsEntry, resolving symlinks.
 * @param channel SFTP channel, parent path, and file entry.
 * @return FileStatus object representing the file or directory.
 */","* Convert the file information in LsEntry to a {@link FileStatus} object. *
   *
   * @param sftpFile
   * @param parentPath
   * @return file status
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,isFile,"org.apache.hadoop.fs.sftp.SFTPFileSystem:isFile(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",355,363,"/**
 * Checks file status via SFTP. Returns true if failed, false otherwise.
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.HttpsFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
 * Creates a FileStatus object for a given path.
 * @param path The path for which to create the status.
 * @return A FileStatus object.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.HttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
 * Creates a FileStatus object for the given path.
 * @param path The path for which to create the status.
 * @return A FileStatus object.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,glob,org.apache.hadoop.fs.Globber:glob(),197,206,"/**
 * Retrieves file statuses based on a defined pattern.
 * Uses tracing for performance analysis.
 * @return Array of FileStatus objects.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,exists,"org.apache.hadoop.fs.ftp.FTPFileSystem:exists(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",391,398,"/**
 * Attempts file operation; returns true on success, false if not found.
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException on IO problems other than FileNotFoundException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,listStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",484,498,"/**
 * Retrieves FileStatus objects for files within a given path.
 * @param client FTPClient object
 * @param file Path to search within
 * @return Array of FileStatus objects
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,isFile,"org.apache.hadoop.fs.ftp.FTPFileSystem:isFile(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",617,625,"/**
 * Checks a file's status via m1, handling exceptions.
 * @param client FTPClient to use, file Path to check
 * @return True if successful, false if not found, throws FTPException
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",294,297,"/**
 * Creates a CBZip2InputStream with specified input stream and mode.
 * @param in Input stream to read from.
 * @param readMode Read mode for decompression.
 */
","* Constructs a new CBZip2InputStream which decompresses bytes read from the
  * specified stream.
  *
  * <p>
  * Although BZip2 headers are marked with the magic <tt>""Bz""</tt> this
  * constructor expects the next byte in the stream to be the first one after
  * the magic. Thus callers have to skip the first two bytes. Otherwise this
  * constructor will throw an exception.
  * </p>
  * @param in in.
  * @param readMode READ_MODE.
  * @throws IOException
  *             if the stream content is malformed or an I/O error occurs.
  * @throws NullPointerException
  *             if <tt>in == null</tt>",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,numberOfBytesTillNextMarker,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:numberOfBytesTillNextMarker(java.io.InputStream),346,349,"/**
 * Decompresses data from an InputStream using CBZip2.
 * @param in Input stream containing compressed data.
 * @return Long value returned by the CBZip2InputStream.
 */","* Returns the number of bytes between the current stream position
   * and the immediate next BZip2 block marker.
   *
   * @param in
   *             The InputStream
   *
   * @return long Number of bytes between current stream position and the
   * next BZip2 block start marker.
 * @throws IOException raised on errors performing I/O.
   *",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read(byte[],int,int)",400,448,"/**
* Reads data from the input stream into the destination array.
* @param dest Destination array
* @param offs Offset in the destination array
* @param len Number of bytes to read
* @return Number of bytes read
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)",529,575,"/**
 * Delegates to overloaded m5 with additional request details.
 * @param server RPC server, request, time, method, class, version
 * @return Writable object
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newMutableRollingAverages,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newMutableRollingAverages(java.lang.String,java.lang.String)",339,346,"/**
 * Creates and registers a MutableRollingAverages object.
 * @param name Identifier for the averages.
 * @param valueName Name of the value being averaged.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,verifyChangedServiceState,org.apache.hadoop.ha.ZKFailoverController:verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState),884,922,"/**
 * Handles service state changes, potentially restarting election.
 * @param changedState The new service state observed.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processResult,"org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,java.lang.String)",496,551,"/**
* Handles Zookeeper node creation, retries on failure, logs errors.
*/",* interface implementation of Zookeeper callback for create,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processResult,"org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)",556,613,"/**
 * Processes StatNode result, handles errors, and retries if needed.
 * @param rc Zookeeper result code
 * @param path Znode path
 * @param ctx Context object
 * @param stat Stat object
 */",* interface implementation of Zookeeper callback for monitor (exists),,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,process,org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:process(org.apache.zookeeper.WatchedEvent),1233,1247,"/**
 * Processes a ZooKeeper event, updating state and triggering actions.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String),162,164,"/**
 * Resolves a target string to an InetSocketAddress.
 * @param target The target host/port string.
 */
","* Util method to build socket addr from either.
   *   {@literal <host>:<port>}
   *   {@literal <fs>://<host>:<port>/<path>}
   *
   * @param target target.
   * @return socket addr.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Servers.java,parse,"org.apache.hadoop.metrics2.util.Servers:parse(java.lang.String,int)",50,62,"/**
 * Parses host/port specs, returns a list of InetSocketAddresses.
 * @param specs Comma-separated host/port specifications.
 * @param defaultPort Default port to use if not specified.
 */","* Parses a space and/or comma separated sequence of server specifications
   * of the form <i>hostname</i> or <i>hostname:port</i>.  If
   * the specs string is null, defaults to localhost:defaultPort.
   *
   * @param specs   server specs (see description)
   * @param defaultPort the default port if not specified
   * @return a list of InetSocketAddress objects.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildDTServiceName,"org.apache.hadoop.security.SecurityUtil:buildDTServiceName(java.net.URI,int)",338,345,"/**
 * Extracts the host from a URI, using a default port if needed.
 * @param uri The URI to process.
 * @param defPort Default port to use if host is missing.
 * @return Host string, or null if URI is invalid.
 */
","* create the service name for a Delegation token
   * @param uri of the service
   * @param defPort is used if the uri lacks a port
   * @return the token service, or null if no authority
   * @see #buildTokenService(InetSocketAddress)",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,asXmlDocument,"org.apache.hadoop.conf.Configuration:asXmlDocument(java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3650,3685,"/**
 * Creates a Document with configuration properties, or throws exceptions.
 */",* Return the XML DOM corresponding to this Configuration.,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,substituteVars,org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String),1150,1218,"/**
* Substitutes variables in an expression.
* @param expr expression string to substitute variables in
* @return expression with variables substituted, or original if failed
*/","* Attempts to repeatedly expand the value {@code expr} by replacing the
   * left-most substring of the form ""${var}"" in the following precedence order
   * <ol>
   *   <li>by the value of the environment variable ""var"" if defined</li>
   *   <li>by the value of the Java system property ""var"" if defined</li>
   *   <li>by the value of the configuration key ""var"" if defined</li>
   * </ol>
   *
   * If var is unbounded the current state of expansion ""prefix${var}suffix"" is
   * returned.
   * <p>
   * This function also detects self-referential substitutions, i.e.
   * <pre>
   *   {@code
   *   foo.bar = ${foo.bar}
   *   }
   * </pre>
   * If a cycle is detected then the original expr is returned. Loops
   * involving multiple substitutions are not detected.
   *
   * In order not to introduce breaking changes (as Oozie for example contains a method with the
   * same name and same signature) do not make this method public, use substituteCommonVariables
   * in this case.
   *
   * @param expr the literal value of a config key
   * @return null if expr is null, otherwise the value resulting from expanding
   * expr using the algorithm above.
   * @throws IllegalArgumentException when more than
   * {@link Configuration#MAX_SUBST} replacements are required",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,applyChanges,"org.apache.hadoop.conf.ReconfigurationServlet:applyChanges(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable,javax.servlet.http.HttpServletRequest)",140,197,"/**
* Applies request parameters to reconfigure, logging changes.
* @param out Output writer for change messages.
* @param reconf Reconfigurable object to update.
* @param req HTTP request containing configuration parameters.
*/",* Apply configuratio changes after admin has approved them.,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,set,"org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String)",1404,1406,"/**
 * Overloads m1 with a null context.
 * @param name Name of the parameter.
 * @param value Value of the parameter.
 */","* Set the <code>value</code> of the <code>name</code> property. If 
   * <code>name</code> is deprecated or there is a deprecated name associated to it,
   * it sets the value to both names. Name will be trimmed before put into
   * configuration.
   * 
   * @param name property name.
   * @param value property value.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,set,"org.apache.hadoop.conf.ConfigurationWithLogging:set(java.lang.String,java.lang.String,java.lang.String)",107,112,"/**
* Logs a setting change and calls the superclass's m3 method.
* @param name Setting name.
* @param value Setting value.
* @param source Source of the setting.
*/
","* See {@link Configuration#set(String, String, String)}.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDelete,"org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File,boolean)",187,203,"/**
 * Checks directory access, optionally granting permissions.
 * @param dir The directory to check.
 * @param tryGrantPermissions Whether to try granting permissions.
 * @return True if access is granted, false otherwise.
 */","* Delete a directory and all its contents.  If
   * we return false, the directory may be partially-deleted.
   * (1) If dir is symlink to a file, the symlink is deleted. The file pointed
   *     to by the symlink is not deleted.
   * (2) If dir is symlink to a directory, symlink is deleted. The directory
   *     pointed to by symlink is not deleted.
   * (3) If dir is a normal file, it is deleted.
   * (4) If dir is a normal directory, then dir and all its contents recursively
   *     are deleted.
   * @param dir the file or directory to be deleted
   * @param tryGrantPermissions true if permissions should be modified to delete a file.
   * @return true on success false on failure.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumCores,org.apache.hadoop.util.SysInfoWindows:getNumCores(),179,182,"/**
* Delegates to m1() and returns its result.
*/",{@inheritDoc},,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,getStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:getStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",106,159,"/**
 * Retrieves status of mount points for a given path.
 * @param fileSystem The ViewFileSystem to check.
 * @param path The path to check against mount points.
 * @return Map of MountPoint to FsStatus.
 */","* Get FsStatus for all ViewFsMountPoints matching path for the given
   * ViewFileSystem.
   *
   * Say ViewFileSystem has following mount points configured
   *  (1) hdfs://NN0_host:port/sales mounted on /dept/sales
   *  (2) hdfs://NN1_host:port/marketing mounted on /dept/marketing
   *  (3) hdfs://NN2_host:port/eng_usa mounted on /dept/eng/usa
   *  (4) hdfs://NN3_host:port/eng_asia mounted on /dept/eng/asia
   *
   * For the above config, here is a sample list of paths and their matching
   * mount points while getting FsStatus
   *
   *  Path                  Description                      Matching MountPoint
   *
   *  ""/""                   Root ViewFileSystem lists all    (1), (2), (3), (4)
   *                         mount points.
   *
   *  ""/dept""               Not a mount point, but a valid   (1), (2), (3), (4)
   *                         internal dir in the mount tree
   *                         and resolved down to ""/"" path.
   *
   *  ""/dept/sales""         Matches a mount point            (1)
   *
   *  ""/dept/sales/india""   Path is over a valid mount point (1)
   *                         and resolved down to
   *                         ""/dept/sales""
   *
   *  ""/dept/eng""           Not a mount point, but a valid   (1), (2), (3), (4)
   *                         internal dir in the mount tree
   *                         and resolved down to ""/"" path.
   *
   *  ""/erp""                Doesn't match or leads to or
   *                         over any valid mount points     None
   *
   *
   * @param fileSystem - ViewFileSystem on which mount point exists
   * @param path - URI for which FsStatus is requested
   * @return Map of ViewFsMountPoint and FsStatus
   * @throws IOException raised on errors performing I/O.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",744,755,"/**
 * Calls m2 with CreateFlag.OVERWRITE added to the flags.
 * @param f Path to create the output stream on.
 * @return FSDataOutputStream
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileStatus,org.apache.hadoop.fs.RawLocalFileSystem:getFileStatus(org.apache.hadoop.fs.Path),905,908,"/**
 * Retrieves file status for a given path.
 * @param f the path to check
 * @return FileStatus object
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),1209,1220,"/**
 * Resolves symbolic links in a file status.
 * @param f Path to the file.
 * @return FileStatus object with resolved link, if applicable.
 */
","* Return a FileStatus representing the given path. If the path refers
   * to a symlink return a FileStatus representing the link rather than
   * the object the link refers to.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getLinkTarget,org.apache.hadoop.fs.RawLocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),1308,1313,"/**
* Returns a Path object based on the input Path.
* @param f input Path
* @return Path object derived from the input
*/
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,wrapLocalFileStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:wrapLocalFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",552,557,"/**
 * Adapts FileStatus to ViewFsFileStatus or ViewFsLocatedFileStatus.
 * @param orig The original FileStatus.
 * @param qualified The qualified Path.
 * @return Adapted FileStatus.
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,rename,"org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",464,491,"/**
 * Renames a file on the SFTP channel, checking for existence.
 * @param channel SFTP channel
 * @param src Source path
 * @param dst Destination path
 * @return True if rename was successful, false otherwise.
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   *
   * @param channel
   * @param src
   * @param dst
   * @return rename successful?
   * @throws IOException",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,listStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",421,451,"/**
 * Lists file statuses within a directory on the SFTP server.
 * @param client SFTP client
 * @param file Path to directory
 * @return Array of FileStatus objects
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",314,347,"/**
 * Creates a file on the SFTP server, creating parent dirs if needed.
 * @param client SFTP client
 * @param file Path of the file to create
 * @param permission File permissions
 * @return True if file creation was successful
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,globStatus,org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path),2122,2126,"/**
* Lists status of files matching a path pattern.
* @param pathPattern glob-style path pattern to match
* @return FileStatus[] array of matching FileStatus objects
*/
","* <p>Return all the files that match filePattern and are not checksum
     * files. Results are sorted by their names.
     * 
     * <p>
     * A filename pattern is composed of <i>regular</i> characters and
     * <i>special pattern matching</i> characters, which are:
     *
     * <dl>
     *  <dd>
     *   <dl>
     *    <dt> <tt> ? </tt>
     *    <dd> Matches any single character.
     *
     *    <dt> <tt> * </tt>
     *    <dd> Matches zero or more characters.
     *
     *    <dt> <tt> [<i>abc</i>] </tt>
     *    <dd> Matches a single character from character set
     *     <tt>{<i>a,b,c</i>}</tt>.
     *
     *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>
     *    <dd> Matches a single character from the character range
     *     <tt>{<i>a...b</i>}</tt>. Note: character <tt><i>a</i></tt> must be
     *     lexicographically less than or equal to character <tt><i>b</i></tt>.
     *
     *    <dt> <tt> [^<i>a</i>] </tt>
     *    <dd> Matches a single char that is not from character set or range
     *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur
     *     immediately to the right of the opening bracket.
     *
     *    <dt> <tt> \<i>c</i> </tt>
     *    <dd> Removes (escapes) any special meaning of character <i>c</i>.
     *
     *    <dt> <tt> {ab,cd} </tt>
     *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>
     *
     *    <dt> <tt> {ab,c{de,fh}} </tt>
     *    <dd> Matches a string from string set <tt>{<i>ab, cde, cfh</i>}</tt>
     *
     *   </dl>
     *  </dd>
     * </dl>
     *
     * @param pathPattern a glob specifying a path pattern
     *
     * @return an array of paths that match the path pattern
     *
     * @throws AccessControlException If access is denied
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,globStatus,"org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2151,2155,"/**
 * Lists status of files matching a path pattern, filtered by a PathFilter.
 */","* Return an array of FileStatus objects whose path names match pathPattern
     * and is accepted by the user-supplied path filter. Results are sorted by
     * their path names.
     * Return null if pathPattern has no glob and the path does not exist.
     * Return an empty array if pathPattern has a glob and no path matches it. 
     * 
     * @param pathPattern glob specifying the path pattern
     * @param filter user-supplied path filter
     *
     * @return an array of FileStatus objects
     *
     * @throws AccessControlException If access is denied
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,globStatus,org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path),2219,2226,"/**
 * Lists status for files matching a path pattern.
 * @param pathPattern Path pattern to match (e.g., ""*/.txt"")
 * @return Array of FileStatus objects matching the pattern.
 */
","* <p>Return all the files that match filePattern and are not checksum
   * files. Results are sorted by their names.
   *
   * <p>
   * A filename pattern is composed of <i>regular</i> characters and
   * <i>special pattern matching</i> characters, which are:
   *
   * <dl>
   *  <dd>
   *   <dl>
   *    <dt> <tt> ? </tt>
   *    <dd> Matches any single character.
   *
   *    <dt> <tt> * </tt>
   *    <dd> Matches zero or more characters.
   *
   *    <dt> <tt> [<i>abc</i>] </tt>
   *    <dd> Matches a single character from character set
   *     <tt>{<i>a,b,c</i>}</tt>.
   *
   *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>
   *    <dd> Matches a single character from the character range
   *     <tt>{<i>a...b</i>}</tt>.  Note that character <tt><i>a</i></tt> must be
   *     lexicographically less than or equal to character <tt><i>b</i></tt>.
   *
   *    <dt> <tt> [^<i>a</i>] </tt>
   *    <dd> Matches a single character that is not from character set or range
   *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur
   *     immediately to the right of the opening bracket.
   *
   *    <dt> <tt> \<i>c</i> </tt>
   *    <dd> Removes (escapes) any special meaning of character <i>c</i>.
   *
   *    <dt> <tt> {ab,cd} </tt>
   *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>
   *
   *    <dt> <tt> {ab,c{de,fh}} </tt>
   *    <dd> Matches a string from the string set <tt>{<i>ab, cde, cfh</i>}</tt>
   *
   *   </dl>
   *  </dd>
   * </dl>
   *
   * @param pathPattern a glob specifying a path pattern

   * @return an array of paths that match the path pattern
   * @throws IOException IO failure",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,globStatus,"org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2241,2244,"/**
 * Lists status of files matching a path pattern, filtered by PathFilter.
 * @param pathPattern Path pattern to match.
 * @param filter Filter to apply.
 * @return FileStatus[] of matching files.
 */
","* Return an array of {@link FileStatus} objects whose path names match
   * {@code pathPattern} and is accepted by the user-supplied path filter.
   * Results are sorted by their path names.
   *
   * @param pathPattern a glob specifying the path pattern
   * @param filter a user-supplied path filter
   * @return null if {@code pathPattern} has no glob and the path does not exist
   *         an empty array if {@code pathPattern} has a glob and no path
   *         matches it else an array of {@link FileStatus} objects matching the
   *         pattern
   * @throws IOException if any I/O error occurs when fetching file status",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,rename,"org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",670,705,"/**
* Renames a file on an FTP server.
* @param client FTP client, src source path, dst destination path
* @return True if rename was successful, false otherwise.
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * 
   * @param client
   * @param src
   * @param dst
   * @return
   * @throws IOException",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,delete,"org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,boolean)",416,438,"/**
 * Deletes a file or directory recursively.
 * @param client FTPClient instance
 * @param file Path to delete
 * @param recursive Whether to delete recursively
 * @throws IOException if directory is not empty and recursive is false
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",590,610,"/**
 * Creates a directory path.
 * @param client FTPClient instance
 * @param file Path to create
 * @param permission FsPermission for the directory
 * @return True if directory was created, false otherwise.
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",358,407,"/**
 * Creates a BZip2CompressionInputStream with specified parameters.
 * @param in Input stream.
 * @param start Start position.
 * @param end End position.
 * @param readMode Read mode.
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,internalReset,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:internalReset(),530,537,"/**
 * Resets the input stream if needed, prepares for reading.
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream),351,353,"/**
* Creates a CBZip2InputStream with continuous read mode.
* @param in The input stream to read from.
*/
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read(),365,376,"/**
 * Processes data; returns result or value from array.
 * Throws IOException if stream is closed.
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,newForField,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)",40,92,"/**
 * Creates and registers a MutableMetric based on field type.
 * @param field Field containing the metric annotation.
 * @param annotation Metric annotation.
 * @param registry Metrics registry.
 * @return MutableMetric instance or null if creation fails.
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,reportServiceStatus,org.apache.hadoop.ha.ZKFailoverController$ServiceStateCallBacks:reportServiceStatus(org.apache.hadoop.ha.HAServiceStatus),999,1002,"/**
* Updates internal state based on the provided service status.
* @param status The HAServiceStatus object containing update info.
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,normalizeIP2HostName,org.apache.hadoop.net.NetUtils:normalizeIP2HostName(java.lang.String),731,738,"/**
 * Masks an IP port string if valid. Returns original if invalid.
 */","* Attempt to normalize the given string to ""host:port""
   * if it like ""ip:port"".
   *
   * @param ipPort maybe lik ip:port or host:port.
   * @return host:port",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getTokenServiceAddr,org.apache.hadoop.security.SecurityUtil:getTokenServiceAddr(org.apache.hadoop.security.token.Token),447,449,"/**
* Resolves an InetSocketAddress from a Token.
* @param token Token containing address information.
* @return Resolved InetSocketAddress.
*/","* Decode the given token's service field into an InetAddress
   * @param token from which to obtain the service
   * @return InetAddress for the service",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildTokenService,org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.URI),495,497,"/**
 * Parses a Text object from a URI.
 * @param uri The URI to parse.
 * @return A Text object parsed from the URI.
 */","* Construct the service key for a token
   * @param uri of remote connection with a token
   * @return ""ip:port"" or ""host:port"" depending on the value of
   *          hadoop.security.token.service.use_ip",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,init,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:init(org.apache.commons.configuration2.SubsetConfiguration),120,172,"/**
 * Initializes the GangliaSink with configuration from the provided subset.
 * @param conf SubsetConfiguration containing Ganglia configuration.
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.FileSystem:getCanonicalServiceName(),453,460,"/**
 * Returns a masked string if m1() is null; otherwise, returns null.
 */","* Get a canonical service name for this FileSystem.
   * The token cache is the only user of the canonical service name,
   * and uses it to lookup this FileSystem's service tokens.
   * If the file system provides a token of its own then it must have a
   * canonical name, otherwise the canonical name can be null.
   *
   * Default implementation: If the FileSystem has child file systems
   * (such as an embedded file system) then it is assumed that the FS has no
   * tokens of its own and hence returns a null name; otherwise a service
   * name is built using Uri and port.
   *
   * @return a service string that uniquely identifies this file system, null
   *         if the filesystem does not implement tokens
   * @see SecurityUtil#buildDTServiceName(URI, int)",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.AbstractFileSystem:getCanonicalServiceName(),1243,1245,"/**
* Masks data using SecurityUtil with values from m1() and m2().
*/","* Get a canonical name for this file system.
   * @return a URI string that uniquely identifies this file system",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,substituteCommonVariables,org.apache.hadoop.conf.Configuration:substituteCommonVariables(java.lang.String),1115,1117,"/**
* Delegates expression processing to m1.
* @param expr The input expression string.
*/","* Provides a public wrapper over substituteVars in order to avoid compatibility issues.
   * See HADOOP-18021 for further details.
   *
   * @param expr the literal value of a config key
   * @return null if expr is null, otherwise the value resulting from expanding
   * expr using the algorithm above.
   * @throws IllegalArgumentException when more than
   * {@link Configuration#MAX_SUBST} replacements are required",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,get,org.apache.hadoop.conf.Configuration:get(java.lang.String),1263,1270,"/**
 * Processes a name using helper methods and returns the result.
 * @param name The name to process.
 * @return The processed result string.
 */
","* Get the value of the <code>name</code> property, <code>null</code> if
   * no such property exists. If the key is deprecated, it returns the value of
   * the first key which replaces the deprecated key and is not null.
   * 
   * Values are processed for <a href=""#VariableExpansion"">variable expansion</a> 
   * before being returned.
   *
   * As a side effect get loads the properties from the sources if called for
   * the first time as a lazy init.
   * 
   * @param name the property name, will be trimmed before get value.
   * @return the value of the <code>name</code> or its replacing property, 
   *         or null if no such property exists.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,get,"org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)",1524,1531,"/**
 * Retrieves a value based on a name, using a default if needed.
 * @param name Name to search for.
 * @param defaultValue Value if name is not found.
 */
","* Get the value of the <code>name</code>. If the key is deprecated,
   * it returns the value of the first key which replaces the deprecated key
   * and is not null.
   * If no such property exists,
   * then <code>defaultValue</code> is returned.
   * 
   * @param name property name, will be trimmed before get value.
   * @param defaultValue default value.
   * @return property value, or <code>defaultValue</code> if the property 
   *         doesn't exist.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,doPost,"org.apache.hadoop.conf.ReconfigurationServlet:doPost(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",214,236,"/**
* Handles POST requests, processes reconfiguration, and writes HTML.
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String)",169,175,"/**
* Updates key-value pair, validates keys, and returns a B object.
*/",* Set optional Builder parameter.,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String)",256,261,"/**
 * Updates an option with the given key and value.
 * @param key option key
 * @param value option value
 * @return Current builder instance
 */
","* Set mandatory option to the Builder.
   *
   * If the option is not supported or unavailable on the {@link FileSystem},
   * the client should expect {@link #build()} throws IllegalArgumentException.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setDefaultUri,"org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.net.URI)",311,313,"/**
 * Sets the filesystem name in the configuration to the URI's scheme.
 * @param conf Configuration object
 * @param uri URI object
 */
","* Set the default FileSystem URI in a configuration.
   * @param conf the configuration to alter
   * @param uri the new default filesystem uri",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLink,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.net.URI)",55,59,"/**
 * Configures a viewfs link for a mount table.
 * @param conf Configuration object.
 * @param mountTableName Mount table name.
 * @param src Source path.
 * @param target Target URI.
 */
","* Add a link to the config for the specified mount table
   * @param conf - add the link to this conf
   * @param mountTableName mountTable.
   * @param src - the src path name
   * @param target - the target URI link",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMergeSlash,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",79,83,"/**
* Sets the viewfs link merge path in the configuration.
* @param conf Configuration object.
* @param mountTableName Mount table name.
* @param target URI containing the merge path.
*/
","* Add a LinkMergeSlash to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param target target.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkFallback,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",102,106,"/**
 * Configures viewfs link fallback for a table.
 * @param conf Configuration object.
 * @param mountTableName Table name.
 * @param target URI for the fallback link.
 */
","* Add a LinkFallback to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param target targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMerge,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])",125,129,"/**
* Configures viewfs link merge for a mount table.
* @param conf Configuration object.
* @param mountTableName Mount table name.
* @param targets Array of target URIs.
*/
","* Add a LinkMerge to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param targets targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",150,156,"/**
* Sets a configuration value based on provided parameters.
*/","* Add nfly link to configuration for the given mount table.
   *
   * @param conf configuration.
   * @param mountTableName mount table.
   * @param src src.
   * @param settings settings.
   * @param targets targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkRegex,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkRegex(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",191,202,"/**
* Sets a configuration value based on provided parameters.
* @param conf Configuration object.
* @param mountTableName Mount table name.
* @param srcRegex Source regex pattern.
* @param targetStr Target string value.
*/
","* Add a LinkRegex to the config for the specified mount table.
   * @param conf - get mountable config from this conf
   * @param mountTableName - the mountable name of the regex config item
   * @param srcRegex - the src path regex expression that applies to this config
   * @param targetStr - the string of target path
   * @param interceptorSettings - the serialized interceptor string to be
   *                            applied while resolving the mapping",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setHomeDirConf,"org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",220,228,"/**
 * Sets the viewfs.homedir configuration with a derived key.
 * @param conf Hadoop configuration object
 * @param mountTableName Mount table name
 * @param homedir Home directory path
 */
","* Add config variable for homedir the specified mount table
   * @param conf - add to this conf
   * @param homedir - the home dir path starting with slash
   * @param mountTableName - the mount table.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,setUMask,"org.apache.hadoop.fs.permission.FsPermission:setUMask(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.permission.FsPermission)",400,402,"/**
* Sets the UMASK configuration property using the provided permission.
* @param conf Configuration object.
* @param umask FsPermission object representing the UMASK.
*/
","* Set the user file creation mask (umask)
   * @param conf configuration.
   * @param umask umask.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,setCodecClasses,"org.apache.hadoop.io.compress.CompressionCodecFactory:setCodecClasses(org.apache.hadoop.conf.Configuration,java.util.List)",156,169,"/**
 * Sets compression codecs in configuration from a list of classes.
 * @param conf Configuration object.
 * @param classes List of codec classes.
 */
","* Sets a list of codec classes in the configuration. In addition to any
   * classes specified using this method, {@link CompressionCodec} classes on
   * the classpath are discovered using a Java ServiceLoader.
   * @param conf the configuration to modify
   * @param classes the list of classes to set",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,setDefaultCompressionType,"org.apache.hadoop.io.SequenceFile:setDefaultCompressionType(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$CompressionType)",262,265,"/**
* Sets the compression type for the job configuration.
* @param job The job configuration object.
* @param val Compression type to set.
*/
","* Set the default compression type for sequence files.
   * @param job the configuration to modify
   * @param val the new compression type (none, block, record)",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,getProxyuserConfiguration,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig),107,119,"/**
 * Extracts proxy user configurations from FilterConfig.
 * @param filterConfig Filter configuration object
 * @return Configuration object with proxy user settings
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,getProxyuserConfiguration,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig),158,174,"/**
* Extracts proxy user configurations from FilterConfig.
* @param filterConfig filter configuration object
* @return Configuration object with proxy user settings
*/","* Returns the proxyuser configuration. All returned properties must start
   * with <code>proxyuser.</code>'
   * <p>
   * Subclasses may override this method if the proxyuser configuration is 
   * read from other place than the filter init parameters.
   *
   * @param filterConfig filter configuration object
   * @return the proxyuser configuration properties.
   * @throws ServletException thrown if the configuration could not be created.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,prepareConf,org.apache.hadoop.security.CompositeGroupsMapping:prepareConf(java.lang.String),175,191,"/**
* Creates a Configuration object for a given provider.
* @param providerName The name of the provider.
* @return Configuration object containing provider-specific settings.
*/
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,init,org.apache.hadoop.security.alias.CredentialShell:init(java.lang.String[]),78,126,"/**
* Parses command-line arguments and executes corresponding actions.
*/","* Parse the command line arguments and initialize the data.
   * <pre>
   * % hadoop credential create alias [-provider providerPath]
   * % hadoop credential list [-provider providerPath]
   * % hadoop credential check alias [-provider providerPath]
   * % hadoop credential delete alias [-provider providerPath] [-f]
   * </pre>
   * @param args args.
   * @return 0 if the argument(s) were recognized, 1 otherwise
   * @throws IOException raised on errors performing I/O.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setAuthenticationMethod,"org.apache.hadoop.security.SecurityUtil:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,org.apache.hadoop.conf.Configuration)",741,748,"/**
 * Sets Hadoop security authentication in config based on method.
 * @param authenticationMethod Authentication method to use.
 * @param conf Configuration object to update.
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setInt,"org.apache.hadoop.conf.Configuration:setInt(java.lang.String,int)",1582,1584,"/**
* Calls m2 with the name and the integer value converted by m1.
*/","* Set the value of the <code>name</code> property to an <code>int</code>.
   * 
   * @param name property name.
   * @param value <code>int</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setLong,"org.apache.hadoop.conf.Configuration:setLong(java.lang.String,long)",1655,1657,"/**
 * Calls m2 with the provided name and a modified value.
 * @param name The name to pass to m2.
 * @param value The value to modify and pass to m2.
 */
","* Set the value of the <code>name</code> property to a <code>long</code>.
   * 
   * @param name property name.
   * @param value <code>long</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setFloat,"org.apache.hadoop.conf.Configuration:setFloat(java.lang.String,float)",1684,1686,"/**
* Calls m2 with the given name and a modified value.
* @param name The name to pass to m2.
* @param value The value to modify and pass to m2.
*/
","* Set the value of the <code>name</code> property to a <code>float</code>.
   * 
   * @param name property name.
   * @param value property value.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setDouble,"org.apache.hadoop.conf.Configuration:setDouble(java.lang.String,double)",1713,1715,"/**
* Calls m2 with the given name and a modified value.
* @param name The name to pass to m2.
* @param value The value to modify and pass to m2.
*/
","* Set the value of the <code>name</code> property to a <code>double</code>.
   * 
   * @param name property name.
   * @param value property value.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setBoolean,"org.apache.hadoop.conf.Configuration:setBoolean(java.lang.String,boolean)",1750,1752,"/**
* Calls m2 with the provided name and a boolean value.
* @param name The name to pass to m2.
* @param value Boolean value to be converted and passed.
*/
","* Set the value of the <code>name</code> property to a <code>boolean</code>.
   * 
   * @param name property name.
   * @param value <code>boolean</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setTimeDuration,"org.apache.hadoop.conf.Configuration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)",1868,1870,"/**
* Logs a named value, adjusted by a unit-based duration.
* @param name The name of the value to log.
* @param value The value to log, adjusted by the unit.
* @param unit Time unit used for duration calculation.
*/
","* Set the value of <code>name</code> to the given time duration. This
   * is equivalent to <code>set(&lt;name&gt;, value + &lt;time suffix&gt;)</code>.
   * @param name Property name
   * @param value Time duration
   * @param unit Unit of time",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setStorageSize,"org.apache.hadoop.conf.Configuration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)",2039,2041,"/**
* Calls m2 with the name and a value adjusted by the unit's m1.
*/","* Sets Storage Size for the specified key.
   *
   * @param name - Key to set.
   * @param value - The numeric value to set.
   * @param unit - Storage Unit to be used.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setPattern,"org.apache.hadoop.conf.Configuration:setPattern(java.lang.String,java.util.regex.Pattern)",2089,2092,"/**
* Calls m2 with the given name and a result from pattern.m1().
*/","* Set the given property to <code>Pattern</code>.
   * If the pattern is passed as null, sets the empty pattern which results in
   * further calls to getPattern(...) returning the default value.
   *
   * @param name property name
   * @param pattern new value",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setStrings,"org.apache.hadoop.conf.Configuration:setStrings(java.lang.String,java.lang.String[])",2405,2407,"/**
* Calls m2 with a name and a concatenated string of values.
* @param name The name to pass to m2.
* @param values Variable arguments to be concatenated.
*/
","* Set the array of string values for the <code>name</code> property as 
   * as comma delimited values.  
   * 
   * @param name property name.
   * @param values The values",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setSocketAddr,"org.apache.hadoop.conf.Configuration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)",2578,2580,"/**
 * Calls m2 with the provided name and address converted to a string.
 * @param name The name to pass to m2.
 * @param addr The address to convert and pass to m2.
 */","* Set the socket address for the <code>name</code> property as
   * a <code>host:port</code>.
   * @param name property name.
   * @param addr inetSocketAddress addr.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setClass,"org.apache.hadoop.conf.Configuration:setClass(java.lang.String,java.lang.Class,java.lang.Class)",2811,2815,"/**
 * Validates class against interface and logs a message.
 * @param name Name to log.
 * @param theClass Class to validate.
 * @param xface Interface to check against.
 */
","* Set the value of the <code>name</code> property to the name of a 
   * <code>theClass</code> implementing the given interface <code>xface</code>.
   * 
   * An exception is thrown if <code>theClass</code> does not implement the 
   * interface <code>xface</code>. 
   * 
   * @param name property name.
   * @param theClass property value.
   * @param xface the interface implemented by the named class.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,readFields,org.apache.hadoop.conf.Configuration:readFields(java.io.DataInput),3949,3962,"/**
* Processes data from input, iterating through key-value pairs.
* @param in Input stream containing data to be processed.
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDelete,org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File),169,171,"/**
 * Delegates to the overloaded method with default recursive flag.
 * @param dir The directory to process.
 */
","* Delete a directory and all its contents.  If
   * we return false, the directory may be partially-deleted.
   * (1) If dir is symlink to a file, the symlink is deleted. The file pointed
   *     to by the symlink is not deleted.
   * (2) If dir is symlink to a directory, symlink is deleted. The directory
   *     pointed to by symlink is not deleted.
   * (3) If dir is a normal file, it is deleted.
   * (4) If dir is a normal directory, then dir and all its contents recursively
   *     are deleted.
   * @param dir dir.
   * @return fully delete status.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDeleteContents,"org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File,boolean)",282,316,"/**
* Deletes files in a directory, attempting permission grants if specified.
* @param dir Directory to delete files from.
* @param tryGrantPermissions Whether to try granting permissions.
* @return True if all files were deleted successfully.
*/","* Delete the contents of a directory, not the directory itself.  If
   * we return false, the directory may be partially-deleted.
   * If dir is a symlink to a directory, all the contents of the actual
   * directory pointed to by dir will be deleted.
   *
   * @param dir dir.
   * @param tryGrantPermissions if 'true', try grant +rwx permissions to this
   * and all the underlying directories before trying to delete their contents.
   * @return fully delete contents status.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPath,org.apache.hadoop.fs.shell.FsUsage$Df:processPath(org.apache.hadoop.fs.shell.PathData),129,157,"/**
 * Processes file system status, handling mounts or default status.
 * @param item Contains file system and path information.
 * @throws IOException if an I/O error occurs.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,open,"org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.Path,int)",392,397,"/**
 * Opens an input stream for the given path.
 * @param f Path to open.
 * @param bufferSize Initial buffer size.
 * @return FSDataInputStream for reading data.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,open,"org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.PathHandle,int)",399,409,"/**
 * Opens an input stream for the path, handling path conversion.
 * @param fd PathHandle to open.
 * @param bufferSize Buffer size for the stream.
 * @return FSDataInputStream for the path.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,append,"org.apache.hadoop.fs.RawLocalFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",535,545,"/**
 * Opens a data output stream for appending.
 * @param f path to append to, bufferSize buffer size, progress progressable
 * @return FSDataOutputStream for appending data.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,truncate,"org.apache.hadoop.fs.RawLocalFileSystem:truncate(org.apache.hadoop.fs.Path,long)",675,698,"/**
* Truncates a file to the specified length.
* @param f the file to truncate
* @param newLength the new length of the file
* @return true if successful
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,listStatus,org.apache.hadoop.fs.RawLocalFileSystem:listStatus(org.apache.hadoop.fs.Path),729,766,"/**
* Retrieves FileStatus array for files within a directory.
* @param f Path to the directory.
* @return FileStatus[] or empty array if no files found.
*/","* {@inheritDoc}
   *
   * (<b>Note</b>: Returned list is not sorted in any given order,
   * due to reliance on Java's {@link File#list()} API.)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,deprecatedGetFileLinkStatusInternal,org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileLinkStatusInternal(org.apache.hadoop.fs.Path),1248,1285,"/**
* Returns FileStatus, masking path with target if applicable.
* @param f Path to mask
* @return FileStatus object
*/","* Deprecated. Remains for legacy support. Should be removed when {@link Stat}
   * gains support for Windows and other operating systems.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,fixFileStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:fixFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",532,550,"/**
 * Updates FileStatus with qualified path if it's a file.
 * @param orig The original FileStatus.
 * @param qualified Qualified Path.
 * @return Updated FileStatus.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,delete,"org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,boolean)",370,414,"/**
 * Checks file/dir existence and recursively deletes if needed.
 * @param channel SFTP channel
 * @param file Path to file/directory
 * @param recursive Recursive deletion flag
 * @throws IOException on failure
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream),354,356,"/**
* Creates a BZip2CompressionInputStream using the provided input stream.
* @param in The input stream to be decompressed.
*/
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,"org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",200,211,"/**
 * Creates a compression input stream from a seekable stream.
 * @param seekableIn Seekable input stream.
 * @param decompressor Decompressor object.
 * @param start Start position.
 * @param end End position.
 * @param readMode Read mode.
 * @return Compression input stream.
 */","* Creates CompressionInputStream to be used to read off uncompressed data
   * in one of the two reading modes. i.e. Continuous or Blocked reading modes
   *
   * @param seekableIn The InputStream
   * @param start The start offset into the compressed stream
   * @param end The end offset into the compressed stream
   * @param readMode Controls whether progress is reported continuously or
   *                 only at block boundaries.
   *
   * @return CompressionInputStream for BZip2 aligned at block boundaries",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,read,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(byte[],int,int)",483,522,"/**
* Reads bytes from input, handling end-of-block and advertisement.
* @param b buffer, off offset, len length to read.
* @return Number of bytes read, or BZip2Constants.END_OF_BLOCK.
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,add,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Field)",133,159,"/**
* Processes a field annotated with Metric, setting its value.
* Handles exceptions and logs errors during the process.
*/","* Change the declared field {@code field} in {@code source} Object to
   * {@link MutableMetric}",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,init,org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:init(org.apache.commons.configuration2.SubsetConfiguration),57,84,"/**
 * Processes subset configuration, extracts tags, and stores them.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.DelegateToFileSystem:getCanonicalServiceName(),262,265,"/**
* Delegates m1() call to the fsImpl.
* Returns the result of the delegated call.
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getCanonicalServiceName,org.apache.hadoop.fs.FilterFs:getCanonicalServiceName(),312,315,"/**
* Delegates the call to the underlying FileSystem object's m1() method.
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getConf,"org.apache.hadoop.fs.StorageType:getConf(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.StorageType,java.lang.String)",120,123,"/**
 * Retrieves a masked value from configuration based on type and name.
 */","* Get the configured values for different StorageType.
   * @param conf - absolute or fully qualified path
   * @param t - the StorageType
   * @param name - the sub-name of key
   * @return the file system of the path",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getTransferMode,org.apache.hadoop.fs.ftp.FTPFileSystem:getTransferMode(org.apache.hadoop.conf.Configuration),188,209,"/**
* Determines FTP transfer mode based on configuration.
* @param conf Configuration object to read transfer mode from.
* @return FTP transfer mode constant.
*/","* Set FTP's transfer mode based on configuration. Valid values are
   * STREAM_TRANSFER_MODE, BLOCK_TRANSFER_MODE and COMPRESSED_TRANSFER_MODE.
   * <p>
   * Defaults to BLOCK_TRANSFER_MODE.
   *
   * @param conf
   * @return",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,setDataConnectionMode,"org.apache.hadoop.fs.ftp.FTPFileSystem:setDataConnectionMode(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)",222,240,"/**
 * Configures FTP data connection mode based on configuration.
 * @param client FTP client to configure.
 * @param conf Configuration object.
 */","* Set the FTPClient's data connection mode based on configuration. Valid
   * values are ACTIVE_LOCAL_DATA_CONNECTION_MODE,
   * PASSIVE_LOCAL_DATA_CONNECTION_MODE and PASSIVE_REMOTE_DATA_CONNECTION_MODE.
   * <p>
   * Defaults to ACTIVE_LOCAL_DATA_CONNECTION_MODE.
   *
   * @param client
   * @param conf
   * @throws IOException",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getHomeDirValue,"org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration,java.lang.String)",245,249,"/**
 * Retrieves a configuration value based on mount table name.
 * @param conf Configuration object.
 * @param mountTableName Mount table name.
 * @return Configuration string value.
 */
","* Get the value of the home dir conf value for specified mount table
   * @param conf - from this conf
   * @param mountTableName - the mount table
   * @return home dir value, null if variable is not in conf",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getUMask,org.apache.hadoop.fs.permission.FsPermission:getUMask(org.apache.hadoop.conf.Configuration),325,349,"/**
 * Parses umask from configuration, defaults to DEFAULT_UMASK.
 * @param conf Hadoop configuration object.
 * @return FsPermission object representing the umask.
 */","* Get the user file creation mask (umask)
   * 
   * {@code UMASK_LABEL} config param has umask value that is either symbolic 
   * or octal.
   * 
   * Symbolic umask is applied relative to file mode creation mask; 
   * the permission op characters '+' clears the corresponding bit in the mask, 
   * '-' sets bits in the mask.
   * 
   * Octal umask, the specified bits are set in the file mode creation mask.
   *
   * @param conf configuration.
   * @return FsPermission UMask.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",791,796,"/**
 * Initializes the DiskBlockFactory with a buffer directory.
 * @param keyToBufferDir Key to find buffer directory in config.
 * @param conf Hadoop configuration object.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecClasses,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClasses(org.apache.hadoop.conf.Configuration),111,147,"/**
 * Retrieves a list of available compression codecs based on config.
 * @param conf Hadoop configuration object.
 * @return List of CompressionCodec classes.
 */","* Get the list of codecs discovered via a Java ServiceLoader, or
   * listed in the configuration. Codecs specified in configuration come
   * later in the returned list, and are considered to override those
   * from the ServiceLoader.
   * @param conf the configuration to look in
   * @return a list of the {@link CompressionCodec} classes",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getDefaultCompressionType,org.apache.hadoop.io.SequenceFile:getDefaultCompressionType(org.apache.hadoop.conf.Configuration),251,255,"/**
* Gets the compression type from the job configuration.
* @param job The job configuration object.
* @return CompressionType based on the configuration.
*/
","* Get the compression type for the reduce outputs
   * @param job the job config to look in
   * @return the kind of compression to use",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,setConf,org.apache.hadoop.net.SocksSocketFactory:setConf(org.apache.hadoop.conf.Configuration),135,142,"/**
 * Configures proxy settings based on the provided configuration.
 * @param conf Configuration object containing proxy settings.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,isSingleSwitchByScriptPolicy,org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitchByScriptPolicy(),135,138,"/**
* Checks if the net topology script file name is null in config.
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createWebAppContext,"org.apache.hadoop.http.HttpServer2:createWebAppContext(org.apache.hadoop.http.HttpServer2$Builder,org.apache.hadoop.security.authorize.AccessControlList,java.lang.String)",834,860,"/**
 * Creates a WebAppContext with configurations, ACL, and app directory.
 * @param b Builder object, adminsAcl AccessControlList, appDir app directory
 * @return WebAppContext object
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,stringifySecurityProperty,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:stringifySecurityProperty(java.lang.String),314,333,"/**
* Masks a property string based on defined rules.
* @param property The property to mask.
* @return Masked property string.
*/
","* Turn a security property into a nicely formatted set of <i>name=value</i>
   * strings, allowing for either the property or the configuration not to be
   * set.
   *
   * @param property the property to stringify
   * @return the stringified property",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateHadoopTokenFiles,org.apache.hadoop.security.KDiag:validateHadoopTokenFiles(org.apache.hadoop.conf.Configuration),521,553,"/**
 * Locates Hadoop token files from system properties & config.
 * @param conf Hadoop configuration object.
 */","* Validate that hadoop.token.files (if specified) exist and are valid.
   * @throws ClassNotFoundException
   * @throws SecurityException
   * @throws NoSuchMethodException
   * @throws KerberosDiagsFailure",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,locateKeystore,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:locateKeystore(),314,339,"/**
 * Loads the keystore using credentials from env/file/default.
 */","* Open up and initialize the keyStore.
   *
   * @throws IOException If there is a problem reading the password file
   * or a problem reading the keystore.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,needsPassword,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:needsPassword(),341,346,"/**
* Checks if the credential password is null or empty.
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getLocalHostName,org.apache.hadoop.security.SecurityUtil:getLocalHostName(org.apache.hadoop.conf.Configuration),255,272,"/**
* Resolves DNS address, using config or default.
* @param conf Configuration object, can be null.
* @return Resolved DNS address as String.
*/","* Retrieve the name of the current host. Multihomed hosts may restrict the
   * hostname lookup to a specific interface and nameserver with {@link
   * org.apache.hadoop.fs.CommonConfigurationKeysPublic#HADOOP_SECURITY_DNS_INTERFACE_KEY}
   * and {@link org.apache.hadoop.fs.CommonConfigurationKeysPublic#HADOOP_SECURITY_DNS_NAMESERVER_KEY}
   *
   * @param conf Configuration object. May be null.
   * @return
   * @throws UnknownHostException",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getClientPrincipal,"org.apache.hadoop.security.SecurityUtil:getClientPrincipal(java.lang.Class,org.apache.hadoop.conf.Configuration)",404,413,"/**
 * Retrieves the username from Kerberos info, using config.
 * @param protocol Protocol class.
 * @param conf Configuration object.
 * @return Username string or null if not found.
 */
","* Look up the client principal for a given protocol. It searches all known
   * SecurityInfo providers.
   * @param protocol the protocol class to get the information for
   * @param conf configuration object
   * @return client principal or null if it has no client principal defined.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,needsPassword,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:needsPassword(),308,313,"/**
 * Checks if keystore password is null.
 * @throws IOException if an I/O error occurs.
 * @return True if null, false otherwise.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getMetricsTimeUnit,org.apache.hadoop.ipc.metrics.RpcMetrics:getMetricsTimeUnit(org.apache.hadoop.conf.Configuration),189,204,"/**
* Retrieves the RPC metrics time unit from configuration; uses default if invalid.
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,validateSslConfiguration,org.apache.hadoop.util.curator.ZKCuratorManager:validateSslConfiguration(org.apache.hadoop.conf.Configuration),196,221,"/**
 * Validates ZooKeeper SSL configuration parameters in the config.
 * Throws IOException if any SSL parameter is missing.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmed,org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String),1320,1328,"/**
 * Calls m1 and returns the result of m2 on its result, or null.
 */","* Get the value of the <code>name</code> property as a trimmed <code>String</code>, 
   * <code>null</code> if no such property exists. 
   * If the key is deprecated, it returns the value of
   * the first key which replaces the deprecated key and is not null
   * 
   * Values are processed for <a href=""#VariableExpansion"">variable expansion</a> 
   * before being returned. 
   * 
   * @param name the property name.
   * @return the value of the <code>name</code> or its replacing property, 
   *         or null if no such property exists.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setIfUnset,"org.apache.hadoop.conf.Configuration:setIfUnset(java.lang.String,java.lang.String)",1499,1503,"/**
 * Adds a value for the given name if it doesn't exist.
 * @param name Key for the value.
 * @param value Value to be associated with the key.
 */
","* Sets a property if it is currently unset.
   * @param name the property name
   * @param value the new value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1906,1914,"/**
 * Retrieves a value by name, using default if not found.
 * @param name Value name.
 * @param defaultValue Default value.
 * @return Value as long.
 */
","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d). If no unit is
   * provided, the default unit is applied.
   *
   * @param name Property name
   * @param defaultValue Value returned if no mapping exists.
   * @param defaultUnit Default time unit if no valid suffix is provided.
   * @param returnUnit The unit used for the returned value.
   * @throws NumberFormatException If the property stripped of its unit is not
   *         a number
   * @return time duration in given time unit",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1916,1924,"/**
 * Retrieves a value, using m2 with either a provided or retrieved value.
 * @param name Value name.
 * @param defaultValue Default value if not found.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStorageSize,"org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)",1988,2005,"/**
 * Retrieves a storage size, using default if empty.
 * @param name Key for storage size.
 * @param defaultValue Default size if key is empty.
 * @param targetUnit Unit to convert the size to.
 * @return Storage size as a double.
 */
","* Gets the Storage Size from the config, or returns the defaultValue. The
   * unit of return value is specified in target unit.
   *
   * @param name - Key Name
   * @param defaultValue - Default Value -- e.g. 100MB
   * @param targetUnit - The units that we want result to be in.
   * @return double -- formatted in target Units",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStorageSize,"org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)",2017,2030,"/**
* Converts a storage size string to a value in target unit.
* @param name Storage size string name.
* @param defaultValue Default value if conversion fails.
* @param targetUnit Unit to convert to.
*/","* Gets storage size from a config file.
   *
   * @param name - Key to read.
   * @param defaultValue - The default value to return in case the key is
   * not present.
   * @param targetUnit - The Storage unit that should be used
   * for the return value.
   * @return - double value in the Storage Unit specified.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPattern,"org.apache.hadoop.conf.Configuration:getPattern(java.lang.String,java.util.regex.Pattern)",2067,2079,"/**
 * Parses a pattern from a name, using default if parsing fails.
 * @param name Property name.
 * @param defaultValue Default pattern to use if parsing fails.
 */","* Get the value of the <code>name</code> property as a <code>Pattern</code>.
   * If no such property is specified, or if the specified value is not a valid
   * <code>Pattern</code>, then <code>DefaultValue</code> is returned.
   * Note that the returned value is NOT trimmed by this method.
   *
   * @param name property name
   * @param defaultValue default value
   * @return property value as a compiled Pattern, or defaultValue",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStringCollection,org.apache.hadoop.conf.Configuration:getStringCollection(java.lang.String),2310,2313,"/**
 * Retrieves a collection of strings based on the input name.
 * @param name Input string used to generate the collection.
 * @return A collection of strings.
 */
","* Get the comma delimited values of the <code>name</code> property as 
   * a collection of <code>String</code>s.  
   * If no such property is specified then empty collection is returned.
   * <p>
   * This is an optimized version of {@link #getStrings(String)}
   * 
   * @param name property name.
   * @return property value as a collection of <code>String</code>s.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStrings,org.apache.hadoop.conf.Configuration:getStrings(java.lang.String),2324,2327,"/**
 * Splits a string using m1's result.
 * @param name Input string; used in m1.
 * @return String array from m1's output.
 */
","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s.  
   * If no such property is specified then <code>null</code> is returned.
   * 
   * @param name property name.
   * @return property value as an array of <code>String</code>s, 
   *         or <code>null</code>.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStrings,"org.apache.hadoop.conf.Configuration:getStrings(java.lang.String,java.lang.String[])",2339,2346,"/**
 * Retrieves string array from m1 or defaultValue if m1 returns null.
 * @param name Parameter for m1.
 * @param defaultValue Default string array.
 * @return String array.
 */
","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s.  
   * If no such property is specified then default value is returned.
   * 
   * @param name property name.
   * @param defaultValue The default value
   * @return property value as an array of <code>String</code>s, 
   *         or default value.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStringCollection,org.apache.hadoop.conf.Configuration:getTrimmedStringCollection(java.lang.String),2356,2363,"/**
 * Retrieves a collection of strings based on the input name.
 * @param name The name to use for retrieval.
 * @return A collection of strings or an empty collection.
 */
","* Get the comma delimited values of the <code>name</code> property as 
   * a collection of <code>String</code>s, trimmed of the leading and trailing whitespace.  
   * If no such property is specified then empty <code>Collection</code> is returned.
   *
   * @param name property name.
   * @return property value as a collection of <code>String</code>s, or empty <code>Collection</code>",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStrings,org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String),2374,2377,"/**
 * Processes a name using m1 and StringUtils.m2.
 * @param name The input name string.
 * @return String array result of processing.
 */
","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s, trimmed of the leading and trailing whitespace.
   * If no such property is specified then an empty array is returned.
   * 
   * @param name property name.
   * @return property value as an array of trimmed <code>String</code>s, 
   *         or empty array.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStrings,"org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String,java.lang.String[])",2389,2396,"/**
 * Retrieves string array from m1 or defaultValue if m1 is null.
 * @param name Parameter for m1.
 * @param defaultValue Default array if m1 returns null.
 * @return String array.
 */
","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s, trimmed of the leading and trailing whitespace.
   * If no such property is specified then default value is returned.
   * 
   * @param name property name.
   * @param defaultValue The default value
   * @return property value as an array of trimmed <code>String</code>s, 
   *         or default value.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPropsWithPrefix,org.apache.hadoop.conf.Configuration:getPropsWithPrefix(java.lang.String),3032,3043,"/**
 * Extracts configuration values from properties with a prefix.
 * @param confPrefix Prefix to filter configuration keys.
 * @return Map of configuration key-value pairs.
 */","* Constructs a mapping of configuration and includes all properties that
   * start with the specified configuration prefix.  Property names in the
   * mapping are trimmed to remove the configuration prefix.
   *
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties with prefix stripped",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,appendJSONProperty,"org.apache.hadoop.conf.Configuration:appendJSONProperty(com.fasterxml.jackson.core.JsonGenerator,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3861,3881,"/**
* Writes configuration data to a JSON generator.
* @param jsonGen Generator to write to, name, config, redactor.
*/","* Write property and its attributes as json format to given
   * {@link JsonGenerator}.
   *
   * @param jsonGen json writer
   * @param config configuration
   * @param name property name
   * @throws IOException raised on errors performing I/O.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationUtil.java,getChangedProperties,"org.apache.hadoop.conf.ReconfigurationUtil:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",39,65,"/**
 * Identifies property changes between two configurations.
 * @param newConf The new configuration.
 * @param oldConf The old configuration.
 * @return Collection of PropertyChange objects.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,reconfigureProperty,"org.apache.hadoop.conf.ReconfigurableBase:reconfigureProperty(java.lang.String,java.lang.String)",223,241,"/**
* Updates a property value.
* @param property Property name to update.
* @param newVal New value for the property.
* @throws ReconfigurationException if property is not configurable.
*/","* {@inheritDoc}
   *
   * This method makes the change to this objects {@link Configuration}
   * and calls reconfigurePropertyImpl to update internal data structures.
   * This method cannot be overridden, subclasses should instead override
   * reconfigurePropertyImpl.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,get,org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String),46,51,"/**
 * Calls super.m1, logs a redacted value, and returns the value.
 * @param name Input string parameter
 * @return The value returned by super.m1
 */
",* See {@link Configuration#get(String)}.,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,create,"org.apache.hadoop.ha.NodeFencer:create(org.apache.hadoop.conf.Configuration,java.lang.String)",83,90,"/**
 * Creates a NodeFencer instance from config.
 * @param conf Configuration object.
 * @param confKey Key to retrieve config string.
 * @return NodeFencer instance or null if config is missing.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getDefaultMountTableName,org.apache.hadoop.fs.viewfs.ConfigUtil:getDefaultMountTableName(org.apache.hadoop.conf.Configuration),260,263,"/**
 * Retrieves the default ViewFS mount table name from configuration.
 */","* Get the name of the default mount table to use. If
   * {@link Constants#CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE_NAME_KEY} is specified,
   * it's value is returned. Otherwise,
   * {@link Constants#CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE} is returned.
   *
   * @param conf Configuration to use.
   * @return the name of the default mount table to use.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,getCodecClassName,"org.apache.hadoop.io.erasurecode.CodecUtil:getCodecClassName(org.apache.hadoop.conf.Configuration,java.lang.String)",256,285,"/**
 * Retrieves codec class name based on codec type from configuration.
 * @param conf Configuration object
 * @param codec Codec name
 * @return Codec class name or null if not found
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,isNativeBzip2Loaded,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration),47,70,"/**
 * Attempts to load the native Bzip2 library.
 * @param conf Hadoop configuration object.
 * @return True if native library loaded, false otherwise.
 */","* Check if native-bzip2 code is loaded &amp; initialized correctly and
   * can be loaded for this job.
   * 
   * @param conf configuration
   * @return <code>true</code> if native-bzip2 is loaded &amp; initialized
   *         and can be loaded for this job, else <code>false</code>",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getDefaultSocketFactory,org.apache.hadoop.net.NetUtils:getDefaultSocketFactory(org.apache.hadoop.conf.Configuration),121,130,"/**
 * Gets the SocketFactory based on configuration.
 * Returns default if not configured.
 */","* Get the default socket factory as specified by the configuration
   * parameter <tt>hadoop.rpc.socket.factory.default</tt>
   * 
   * @param conf the configuration
   * @return the default socket factory as specified in the configuration or
   *         the JVM default socket factory if the configuration does not
   *         contain a default socket factory property.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,load,org.apache.hadoop.net.TableMapping$RawTableMapping:load(),93,125,"/**
 * Loads table mapping from file. Returns map or null on error.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,getUsernameFromConf,org.apache.hadoop.http.lib.StaticUserWebFilter:getUsernameFromConf(org.apache.hadoop.conf.Configuration),133,146,"/**
* Gets the static user from config, handles deprecated format.
* @param conf Configuration object to retrieve the user from.
* @return Static user string.
*/
",* Retrieve the static username from the configuration.,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,setEnabledProtocols,org.apache.hadoop.http.HttpServer2$Builder:setEnabledProtocols(org.eclipse.jetty.util.ssl.SslContextFactory),665,696,"/**
 * Resets excluded SSL protocols based on enabled protocols.
 * @param sslContextFactory SSL context factory to update.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,parseStaticMapping,org.apache.hadoop.security.Groups:parseStaticMapping(org.apache.hadoop.conf.Configuration),164,191,"/**
 * Parses static user-to-group mappings from configuration.
 * @param conf Hadoop configuration object
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printConfOpt,org.apache.hadoop.security.KDiag:printConfOpt(java.lang.String),907,909,"/**
* Sets the value of an option to the result of processing it.
* @param option The option to set.
*/
","* Print a configuration option, or {@link #UNSET} if unset.
   *
   * @param option option to print",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,<init>,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore:<init>(org.apache.hadoop.conf.Configuration),868,873,"/**
 * Initializes TruststoreKeystore with configuration parameters.
 * @param conf Configuration object containing keystore/truststore details.
 */
","* Configuration for the ZooKeeper connection when SSL/TLS is enabled.
     * When a value is not configured, ensure that empty string is set instead of null.
     *
     * @param conf ZooKeeper Client configuration",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getDirContext,org.apache.hadoop.security.LdapGroupsMapping:getDirContext(),653,706,"/**
 * Retrieves a DirContext for LDAP access. Creates if not present.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,spawnAutoRenewalThreadForUserCreds,org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForUserCreds(boolean),882,899,"/**
 * Renews a Kerberos ticket if forced or if conditions are met.
 * @param force Whether to force renewal, regardless of conditions.
 */","* Spawn a thread to do periodic renewals of kerberos credentials. NEVER
   * directly call this method. This method should only be used for ticket cache
   * based kerberos credentials.
   *
   * @param force - used by tests to forcibly spawn thread",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,getHostnameVerifier,org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier(org.apache.hadoop.conf.Configuration),206,210,"/**
* Retrieves a HostnameVerifier from configuration, using ""DEFAULT"" if absent.
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getAuthenticationMethod,org.apache.hadoop.security.SecurityUtil:getAuthenticationMethod(org.apache.hadoop.conf.Configuration),730,739,"/**
 * Retrieves the AuthenticationMethod from configuration.
 * @param conf Configuration object to read from
 * @throws IllegalArgumentException if value is invalid
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getCodecClasses,"org.apache.hadoop.crypto.CryptoCodec:getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)",105,140,"/**
 * Retrieves a list of CryptoCodec classes configured for a cipher suite.
 * @param conf Configuration object.
 * @param cipherSuite Cipher suite to configure codecs for.
 * @return List of CryptoCodec classes or null if not configured.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.JceCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),83,102,"/**
* Initializes secure random number generator using provided configuration.
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,setConf,org.apache.hadoop.crypto.random.OsSecureRandom:setConf(org.apache.hadoop.conf.Configuration),83,90,"/**
 * Initializes the configuration and random device path.
 * @param conf Configuration object to set the configuration.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,<init>,org.apache.hadoop.crypto.key.KeyProvider:<init>(org.apache.hadoop.conf.Configuration),403,417,"/**
 * Initializes the KeyProvider with a Configuration, sets JCEKS filter,
 * and adds BouncyCastle provider if configured.
 */","* Constructor.
   * 
   * @param conf configuration for the provider",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,"org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",143,146,"/**
 * Constructs a Builder with a context and separator.
 * @param context The context string.
 * @param conf Configuration object.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getZKAcls,org.apache.hadoop.util.curator.ZKCuratorManager:getZKAcls(org.apache.hadoop.conf.Configuration),97,109,"/**
 * Retrieves ACLs from ZooKeeper configuration.
 * @param conf Configuration object; contains ZK ACL setting.
 * @throws IOException if there's an error reading ACLs.
 */","* Utility method to fetch the ZK ACLs from the configuration.
   *
   * @param conf configuration.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @return acl list.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getHashType,org.apache.hadoop.util.hash.Hash:getHashType(org.apache.hadoop.conf.Configuration),64,68,"/**
 * Retrieves a hash type from configuration.
 * @param conf Configuration object to read from.
 * @return The hash type as an integer.
 */
","* This utility method converts the name of the configured
   * hash type to a symbolic constant.
   * @param conf configuration
   * @return one of the predefined constants",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getEnumSet,"org.apache.hadoop.conf.Configuration:getEnumSet(java.lang.String,java.lang.Class,boolean)",1803,1809,"/**
 * Retrieves an EnumSet from configuration.
 * @param key Config key, enumClass, ignoreUnknown flag.
 * @return EnumSet or throws IllegalArgumentException.
 */","* Build an enumset from a comma separated list of values.
   * Case independent.
   * Special handling of ""*"" meaning: all values.
   * @param key key to look for
   * @param enumClass class of enum
   * @param ignoreUnknown should unknown values raise an exception?
   * @return a mutable set of the identified enum values declared in the configuration
   * @param <E> enumeration type
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   *           or there are two entries in the enum which differ only by case.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRange,"org.apache.hadoop.conf.Configuration:getRange(java.lang.String,java.lang.String)",2296,2298,"/**
 * Creates an IntegerRanges object using the result of m1.
 * @param name Parameter for m1. @param defaultValue Default value for m1.
 */
","* Parse the given attribute as a set of integer ranges.
   * @param name the attribute name
   * @param defaultValue the default value if it is not set
   * @return a new set of ranges from the configured value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,<init>,org.apache.hadoop.conf.ConfigRedactor:<init>(org.apache.hadoop.conf.Configuration),44,55,"/**
 * Initializes the ConfigRedactor with a configuration.
 * @param conf Configuration object containing sensitive keys.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,get,"org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String,java.lang.String)",56,62,"/**
 * Retrieves a value by name, logs it, and returns the value.
 */","* See {@link Configuration#get(String, String)}.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,getParentZnode,org.apache.hadoop.ha.ZKFailoverController:getParentZnode(),384,391,"/**
 * Constructs a masked string by combining ZK parent node and m3().
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,boolean)",182,185,"/**
* Delegates to another m2 method with a boxed boolean value.
* @param key The key to use.
* @param value The boolean value.
* @return The result of the delegated method.
*/","* Set optional boolean parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,optLong,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optLong(java.lang.String,long)",202,205,"/**
* Delegates to m2, converting value to a Long using m1.
* @param key Key for the operation.
* @param value Value to be processed.
* @return Result of the delegated operation.
*/
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,optDouble,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optDouble(java.lang.String,double)",232,235,"/**
* Calls m2 with the key and a modified value.
* @param key The key to use.
* @param value The value to modify and pass to m2.
* @return The result of calling m2.
*/
","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,boolean)",268,271,"/**
* Delegates to another m2 method with a boxed boolean value.
* @param key The key for the method.
* @param value The boolean value.
* @return The result of the delegated method.
*/","* Set mandatory boolean option.
   *
   * @see #must(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,mustLong,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustLong(java.lang.String,long)",273,276,"/**
 * Calls m2 with the given key and value (converted to Long).
 * @param key The key to use.
 * @param value The value to use.
 * @return The result of m2.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,mustDouble,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustDouble(java.lang.String,double)",283,286,"/**
* Delegates to m2 with the key and a modified value.
*/","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setDefaultUri,"org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.lang.String)",319,321,"/**
 * Calls m3 with a modified URI, using the provided configuration.
 * @param conf Configuration object.
 * @param uri URI string to process.
 */
","Set the default FileSystem URI in a configuration.
   * @param conf the configuration to alter
   * @param uri the new default filesystem uri",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.net.URI[])",167,175,"/**
 * Overloads m2 with default settings if settings is null.
 * @param conf Configuration object
 * @param mountTableName Mount table name
 * @param src Source path
 * @param settings Settings string, defaults if null
 * @param targets Target URIs
 */","* Add nfly link to configuration for the given mount table.
   *
   * @param conf configuration.
   * @param mountTableName mount table.
   * @param src src.
   * @param settings settings.
   * @param targets targets.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,addMappingProvider,"org.apache.hadoop.security.CompositeGroupsMapping:addMappingProvider(java.lang.String,java.lang.Class)",162,168,"/**
 * Registers a group mapping provider.
 * @param providerName Provider name.
 * @param providerClass Provider class.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,setBlockSize,"org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setBlockSize(org.apache.hadoop.conf.Configuration,int)",126,128,"/**
* Sets the Bzip2 compression block size in the configuration.
* @param conf Configuration object to update.
* @param blockSize The block size for Bzip2 compression.
*/
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,setWorkFactor,"org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setWorkFactor(org.apache.hadoop.conf.Configuration,int)",135,137,"/**
* Sets the Bzip2 compression work factor in the configuration.
* @param conf Configuration object.
* @param workFactor Work factor for Bzip2 compression.
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,setIndexInterval,"org.apache.hadoop.io.MapFile$Writer:setIndexInterval(org.apache.hadoop.conf.Configuration,int)",380,382,"/**
* Sets the index interval in the configuration.
* @param conf Configuration object.
* @param interval The interval value to set.
*/
","* Sets the index interval and stores it in conf.
     * @see #getIndexInterval()
     *
     * @param conf configuration.
     * @param interval interval.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setPingInterval,"org.apache.hadoop.ipc.Client:setPingInterval(org.apache.hadoop.conf.Configuration,int)",175,178,"/**
* Sets the IPC ping interval in the configuration.
* @param conf Configuration object to modify.
* @param pingInterval Ping interval in milliseconds.
*/
","* set the ping interval value in configuration
   * 
   * @param conf Configuration
   * @param pingInterval the ping interval",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setConnectTimeout,"org.apache.hadoop.ipc.Client:setConnectTimeout(org.apache.hadoop.conf.Configuration,int)",233,235,"/**
* Sets the IPC client connect timeout in the configuration.
* @param conf Configuration object to modify.
* @param timeout Timeout value in milliseconds.
*/
","* set the connection timeout value in configuration
   * 
   * @param conf Configuration
   * @param timeout the socket connect timeout value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setIsNestedMountPointSupported,"org.apache.hadoop.fs.viewfs.ConfigUtil:setIsNestedMountPointSupported(org.apache.hadoop.conf.Configuration,boolean)",279,281,"/**
 * Sets the nested mount point support configuration.
 * @param conf Configuration object to update.
 * @param isNestedMountPointSupported Flag indicating support.
 */
","* Set the bool value isNestedMountPointSupported in config.
   * @param conf - from this conf
   * @param isNestedMountPointSupported - whether nested mount point is supported",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String[])",242,248,"/**
 * Processes keys and values, then returns a B object.
 * @param key The key to process.
 * @param values Optional values associated with the key.
 * @return A B object.
 */
","* Set an array of string values as optional parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String[])",318,324,"/**
 * Processes keys and values, then returns a B object.
 * @param key The key to process.
 * @param values Optional values associated with the key.
 * @return A B object.
 */
","* Set a string array as mandatory option.
   *
   * @see #must(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updateConnectAddr,"org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)",2624,2629,"/**
 * Resolves and registers a socket address.
 * @param name identifier; @param addr address to resolve/register
 * @return Resolved InetSocketAddress
 */
","* Set the socket address a client can use to connect for the
   * <code>name</code> property as a <code>host:port</code>.  The wildcard
   * address is replaced with the local host's address.
   * @param name property name.
   * @param addr InetSocketAddress of a listener to store in the given property
   * @return InetSocketAddress for clients to connect",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,setProtocolEngine,"org.apache.hadoop.ipc.RPC:setProtocolEngine(org.apache.hadoop.conf.Configuration,java.lang.Class,java.lang.Class)",211,217,"/**
* Sets the engine class if not already configured.
* @param conf Configuration object.
* @param protocol Protocol class.
* @param engine Engine class to register.
*/
","* Set a protocol to use a non-default RpcEngine if one
   * is not specified in the configuration.
   * @param conf configuration to use
   * @param protocol the protocol interface
   * @param engine the RpcEngine impl",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,delete,"org.apache.hadoop.fs.RawLocalFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",707,721,"/**
 * Checks if a file/directory can be deleted.
 * @param p Path to delete.
 * @param recursive Recursive deletion for directories.
 * @return True if deletion is possible.
 */","* Delete the given path to a file or directory.
   * @param p the path to delete
   * @param recursive to delete sub-directories
   * @return true if the file or directory and all its contents were deleted
   * @throws IOException if p is non-empty and recursive is false",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDeleteContents,org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File),267,269,"/**
 * Delegates to overloaded method with default delete flag.
 * @param dir The directory to process.
 */
","* Delete the contents of a directory, not the directory itself.  If
   * we return false, the directory may be partially-deleted.
   * If dir is a symlink to a directory, all the contents of the actual
   * directory pointed to by dir will be deleted.
   *
   * @param dir dir.
   * @return fullyDeleteContents Status.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileStatus(org.apache.hadoop.fs.Path),567,574,"/**
 * Retrieves FileStatus for a path, resolving intermediate filesystems.
 * @param f Path to retrieve status for.
 * @return FileStatus object.
 */","* {@inheritDoc}
   *
   * If the given path is a symlink(mount link), the path will be resolved to a
   * target path and it will get the resolved path's FileStatus object. It will
   * not be represented as a symlink and isDirectory API returns true if the
   * resolved path is a directory, false otherwise.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:listStatus(org.apache.hadoop.fs.Path),611,628,"/**
 * Retrieves FileStatus array for a given path.
 * @param f the path to retrieve status for
 * @return FileStatus array or null if not found
 */","* {@inheritDoc}
   *
   * Note: listStatus considers listing from fallbackLink if available. If the
   * same directory path is present in configured mount path as well as in
   * fallback fs, then only the fallback path will be listed in the returned
   * result except for link.
   *
   * If any of the the immediate children of the given path f is a symlink(mount
   * link), the returned FileStatus object of that children would be represented
   * as a symlink. It will not be resolved to the target path and will not get
   * the target path FileStatus object. The target path will be available via
   * getSymlink on that children's FileStatus object. Since it represents as
   * symlink, isDirectory on that children's FileStatus will return false.
   * This behavior can be changed by setting an advanced configuration
   * fs.viewfs.mount.links.as.symlinks to false. In this case, mount points will
   * be represented as non-symlinks and all the file/directory attributes like
   * permissions, isDirectory etc will be assigned from it's resolved target
   * directory/file.
   *
   * If you want to get the FileStatus of target path for that children, you may
   * want to use GetFileStatus API with that children's symlink path. Please see
   * {@link ViewFileSystem#getFileStatus(Path f)}
   *
   * Note: In ViewFileSystem, by default the mount links are represented as
   * symlinks.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,read,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(),524,528,"/**
* Reads a single byte from the input stream.
* Returns the byte value or -1 if an error occurs.
*/
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,<init>,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:<init>(java.lang.Object,org.apache.hadoop.metrics2.lib.MutableMetricsFactory)",62,74,"/**
 * Creates a MetricsSourceBuilder for the given source object.
 * @param source object to create metrics for
 * @param factory mutable metrics factory
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkOneDirWithMode,"org.apache.hadoop.fs.RawLocalFileSystem:mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission)",777,802,"/**
* Sets file permissions. Uses native calls on Windows, otherwise uses chmod.
* @param p Path to the directory.
* @param p2f File object.
* @param permission File permissions to set.
* @return True if successful, false otherwise.
*/
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1226,1236,"/**
* Opens a data output stream, using default FsCreateModes.
* @param f Path to open. Overwrite and progress are also params.
* @return FSDataOutputStream
*/","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize the size of the buffer to be used.
   * @param progress to report progress.
   * @throws IOException IO failure
   * @return output stream.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getUMask,org.apache.hadoop.fs.FileContext:getUMask(),585,587,"/**
* Returns the FsPermission. Uses umask if available, otherwise uses conf.
*/","* 
   * @return the umask of this FileContext",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,createFactory,"org.apache.hadoop.fs.store.DataBlocks:createFactory(java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String)",129,144,"/**
 * Creates a BlockFactory based on the provided name.
 * @param keyToBufferDir Buffer directory key.
 * @param configuration Configuration object.
 * @param name Factory name (DATA_BLOCKS_BUFFER_ARRAY, etc.)
 * @return BlockFactory instance.
 */","* Create a factory.
   *
   * @param keyToBufferDir Key to buffer directory config for a FS.
   * @param configuration  factory configurations.
   * @param name           factory name -the option from {@link CommonConfigurationKeys}.
   * @return the factory, ready to be initialized.
   * @throws IllegalArgumentException if the name is unknown.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,<init>,org.apache.hadoop.io.compress.CompressionCodecFactory:<init>(org.apache.hadoop.conf.Configuration),177,191,"/**
 * Initializes the CompressionCodecFactory with codecs from config.
 * @param conf Hadoop configuration object.
 */
","* Find the codecs specified in the config value io.compression.codecs 
   * and register them. Defaults to gzip and deflate.
   *
   * @param conf configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",81,90,"/**
 * Initializes the keystore provider with URI and configuration.
 * @param uri keystore URI
 * @param conf configuration object
 * @throws IOException if an I/O error occurs during initialization
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,replacePattern,"org.apache.hadoop.security.SecurityUtil:replacePattern(java.lang.String[],java.lang.String)",235,243,"/**
 * Constructs a masked string using components and hostname.
 * @param components String array for constructing the string.
 * @param hostname Hostname to use, resolves if invalid.
 * @return Masked string.
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,registerProtocolAndImpl,"org.apache.hadoop.ipc.RPC$Server:registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)",1101,1135,"/**
 * Registers a protocol with the RPC framework.
 * @param rpcKind RPC kind, protocol class, and protocol implementation.
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,getKeyProviderUri,"org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration,java.lang.String)",71,79,"/**
 * Retrieves a URI from configuration.
 * @param conf Configuration object.
 * @param configKeyName Key to fetch URI from config.
 * @return URI object or null if not found/invalid.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmed,"org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String,java.lang.String)",1340,1343,"/**
 * Returns the value of 'name' or 'defaultValue' if 'name' is null.
 */","* Get the value of the <code>name</code> property as a trimmed <code>String</code>, 
   * <code>defaultValue</code> if no such property exists. 
   * See @{Configuration#getTrimmed} for more details.
   * 
   * @param name          the property name.
   * @param defaultValue  the property default value.
   * @return              the value of the <code>name</code> or defaultValue
   *                      if it is not set.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInt,"org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)",1546,1555,"/**
 * Parses an integer value from a string, or returns a default.
 * @param name String to parse, or default if parsing fails.
 * @param defaultValue Default value if parsing fails.
 */
","* Get the value of the <code>name</code> property as an <code>int</code>.
   *   
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>int</code>,
   * then an error is thrown.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as an <code>int</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLong,"org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)",1599,1608,"/**
 * Retrieves a long value from a name, using default if unavailable.
 * @param name Name to lookup.
 * @param defaultValue Default value if not found.
 */
","* Get the value of the <code>name</code> property as a <code>long</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>long</code>,
   * then an error is thrown.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>long</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLongBytes,"org.apache.hadoop.conf.Configuration:getLongBytes(java.lang.String,long)",1624,1629,"/**
 * Retrieves a value by name, returns defaultValue if null.
 * @param name Value name.
 * @param defaultValue Default value if not found.
 * @return Value as long.
 */
","* Get the value of the <code>name</code> property as a <code>long</code> or
   * human readable format. If no such property exists, the provided default
   * value is returned, or if the specified value is not a valid
   * <code>long</code> or human readable format, then an error is thrown. You
   * can use the following suffix (case insensitive): k(kilo), m(mega), g(giga),
   * t(tera), p(peta), e(exa)
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>long</code>,
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getFloat,"org.apache.hadoop.conf.Configuration:getFloat(java.lang.String,float)",1671,1676,"/**
 * Retrieves a float value by name, returns default if not found.
 * @param name Name of the value to retrieve.
 * @param defaultValue Default value if not found.
 */
","* Get the value of the <code>name</code> property as a <code>float</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>float</code>,
   * then an error is thrown.
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>float</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDouble,"org.apache.hadoop.conf.Configuration:getDouble(java.lang.String,double)",1700,1705,"/**
 * Retrieves a double value by name, returns defaultValue if not found.
 * @param name The name of the value to retrieve.
 * @param defaultValue The default value to return if not found.
 */
","* Get the value of the <code>name</code> property as a <code>double</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>double</code>,
   * then an error is thrown.
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>double</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getBoolean,"org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)",1727,1742,"/**
 * Retrieves a boolean value by name, using default if invalid.
 * @param name Property name.
 * @param defaultValue Default boolean value.
 * @return Boolean value or default if invalid.
 */","* Get the value of the <code>name</code> property as a <code>boolean</code>.  
   * If no such property is specified, or if the specified value is not a valid
   * <code>boolean</code>, then <code>defaultValue</code> is returned.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @return property value as a <code>boolean</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClass,"org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class)",2730,2739,"/**
 * Resolves a class by name, returns default if not found.
 * @param name Class name to resolve.
 * @param defaultValue Default class if name is not found.
 */
","* Get the value of the <code>name</code> property as a <code>Class</code>.  
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * @param name the conf key name.
   * @param defaultValue default value.
   * @return property value as a <code>Class</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setBooleanIfUnset,"org.apache.hadoop.conf.Configuration:setBooleanIfUnset(java.lang.String,boolean)",1759,1761,"/**
 * Calls m2 with the given name and a boolean value converted to Boolean.
 */","* Set the given property, if it is currently unset.
   * @param name property name
   * @param value new value",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)",1884,1886,"/**
* Overloads m1 with a default timing unit.
* @param name Metric name, @param defaultValue Default value, @param unit Timing unit
*/","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param defaultValue Value returned if no mapping exists.
   * @param unit Unit to convert the stored property, if it exists.
   * @throws NumberFormatException If the property stripped of its unit is not
   *         a number
   * @return time duration in given time unit",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)",1888,1890,"/**
* Overloads m1 with a default TimeUnit for the second unit.
* @param name Metric name
* @param defaultValue Default value if metric is absent
* @param unit Time unit for the metric
*/
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialProviderFactory.java,getProviders,org.apache.hadoop.security.alias.CredentialProviderFactory:getProviders(org.apache.hadoop.conf.Configuration),73,112,"/**
 * Retrieves CredentialProvider instances from configured paths.
 * @param conf Configuration object; used for provider loading.
 * @return List of CredentialProvider objects.
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderFactory.java,getProviders,org.apache.hadoop.crypto.key.KeyProviderFactory:getProviders(org.apache.hadoop.conf.Configuration),62,81,"/**
* Retrieves KeyProvider instances from configuration paths.
* @param conf Configuration object containing provider paths.
* @return List of KeyProvider objects.
*/
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseServiceUserNames,"org.apache.hadoop.ipc.DecayRpcScheduler:parseServiceUserNames(java.lang.String,org.apache.hadoop.conf.Configuration)",409,413,"/**
 * Retrieves a set of users from configuration for namespace.
 * @param ns Namespace to fetch users from.
 * @param conf Configuration object.
 * @return Set of user strings.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java,getPackages,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getPackages(),65,73,"/**
* Populates the 'packages' set with package names from m1().m2().
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,getRawCoderNames,"org.apache.hadoop.io.erasurecode.CodecUtil:getRawCoderNames(org.apache.hadoop.conf.Configuration,java.lang.String)",168,174,"/**
 * Retrieves raw codec names from configuration.
 * @param conf Configuration object.
 * @param codecName Codec name to search for.
 * @return Array of raw codec names.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getSaslProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getSaslProperties(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.SaslRpcServer$QualityOfProtection)",136,150,"/**
* Creates SASL properties with QOP based on configuration.
* @param conf Configuration object.
* @param configKey Key to fetch QOP from config.
* @param defaultQOP Default QOP to use.
* @return Map of SASL properties.
*/","* A util function to retrieve specific additional sasl property from config.
   * Used by subclasses to read sasl properties used by themselves.
   * @param conf the configuration
   * @param configKey the config key to look for
   * @param defaultQOP the default QOP if the key is missing
   * @return sasl property associated with the given key",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,getKeyFiles,org.apache.hadoop.ha.SshFenceByTcpPort:getKeyFiles(),220,222,"/**
* Retrieves a collection of identities from configuration.
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,refresh,org.apache.hadoop.security.authorize.ProxyServers:refresh(org.apache.hadoop.conf.Configuration),35,45,"/**
 * Extracts proxy servers from configuration and stores them.
 * @param conf Configuration object containing proxy server list.
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInts,org.apache.hadoop.conf.Configuration:getInts(java.lang.String),1567,1574,"/**
 * Converts string array to integer array using m2.
 * @param name Input string, used to generate the array.
 * @return Integer array converted from the string array.
 */
","* Get the value of the <code>name</code> property as a set of comma-delimited
   * <code>int</code> values.
   * 
   * If no such property exists, an empty array is returned.
   * 
   * @param name property name
   * @return property value interpreted as an array of comma-delimited
   *         <code>int</code> values",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurations,"org.apache.hadoop.conf.Configuration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)",1971,1978,"/**
 * Calculates durations for given name and unit.
 * @param name Name to process.
 * @param unit Time unit for duration calculation.
 * @return Array of durations.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClasses,"org.apache.hadoop.conf.Configuration:getClasses(java.lang.String,java.lang.Class[])",2703,2718,"/**
 * Resolves class array from name, uses default if not found.
 * @param name Class name to resolve.
 * @param defaultValue Default class array if resolution fails.
 * @return Array of resolved classes.
 */","* Get the value of the <code>name</code> property
   * as an array of <code>Class</code>.
   * The value of the property specifies a list of comma separated class names.  
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * @param name the property name.
   * @param defaultValue default value.
   * @return property value as a <code>Class[]</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getFile,"org.apache.hadoop.conf.Configuration:getFile(java.lang.String,java.lang.String)",2861,2874,"/**
 * Finds a file within specified directories based on a hash.
 * @param dirsProp Property string containing directory paths.
 * @param path File path to search for.
 * @throws IOException If no valid directory is found.
 */
","* Get a local file name under a directory named in <i>dirsProp</i> with
   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,
   * then one is chosen based on <i>path</i>'s hash code.  If the selected
   * directory does not exist, an attempt is made to create it.
   *
   * @param dirsProp directory in which to locate the file.
   * @param path file-path.
   * @return local file under the directory with the given path.
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,<init>,org.apache.hadoop.io.serializer.SerializationFactory:<init>(org.apache.hadoop.conf.Configuration),58,67,"/**
 * Constructs a SerializationFactory with provided Configuration.
 * Initializes with default serializers from configuration.
 */
","* <p>
   * Serializations are found by reading the <code>io.serializations</code>
   * property from <code>conf</code>, which is a comma-delimited list of
   * classnames.
   * </p>
   *
   * @param conf configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,setConf,org.apache.hadoop.security.SaslPropertiesResolver:setConf(org.apache.hadoop.conf.Configuration),60,73,"/**
* Initializes the authentication properties using the given configuration.
* @param conf Hadoop configuration object.
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,getFilterParams,"org.apache.hadoop.security.http.RestCsrfPreventionFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)",229,232,"/**
* Retrieves configuration parameters with a given prefix.
* @param conf Configuration object.
* @param confPrefix Prefix for configuration keys.
* @return Map of configuration key-value pairs.
*/
","* Constructs a mapping of configuration properties to be used for filter
   * initialization.  The mapping includes all properties that start with the
   * specified configuration prefix.  Property names in the mapping are trimmed
   * to remove the configuration prefix.
   *
   * @param conf configuration to read
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties to be used for filter
   *     initialization",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/XFrameOptionsFilter.java,getFilterParams,"org.apache.hadoop.security.http.XFrameOptionsFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)",80,83,"/**
 * Retrieves configuration map using prefix.
 * @param conf Configuration object.
 * @param confPrefix Prefix for configuration keys.
 * @return Map of configuration string values.
 */
","* Constructs a mapping of configuration properties to be used for filter
   * initialization.  The mapping includes all properties that start with the
   * specified configuration prefix.  Property names in the mapping are trimmed
   * to remove the configuration prefix.
   *
   * @param conf configuration to read
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties to be used for filter
   *     initialization",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,propagateOptions,"org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",356,374,"/**
 * Adds/sets properties to the builder with a given prefix.
 * @param builder FSBuilder instance.
 * @param conf Configuration object.
 * @param prefix Property prefix.
 * @param mandatory Whether properties are mandatory.
 */","* Propagate options to any builder, converting everything with the
   * prefix to an option where, if there were 2+ dot-separated elements,
   * it is converted to a schema.
   * <pre>
   *   fs.example.s3a.option becomes ""s3a.option""
   *   fs.example.fs.io.policy becomes ""fs.io.policy""
   *   fs.example.something becomes ""something""
   * </pre>
   * @param builder builder to modify
   * @param conf configuration to read
   * @param prefix prefix to scan/strip
   * @param mandatory are the options to be mandatory or optional?",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationUtil.java,parseChangedProperties,"org.apache.hadoop.conf.ReconfigurationUtil:parseChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",67,70,"/**
 * Delegates to m1 to retrieve property changes between configs.
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,printConf,"org.apache.hadoop.conf.ReconfigurationServlet:printConf(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable)",88,130,"/**
 * Generates HTML form for reconfiguration changes.
 * @param out PrintWriter to write HTML output.
 * @param reconf Reconfigurable object.
 */",* Print configuration options that can be changed.,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLink,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",67,70,"/**
* Calls m2 with a default value for the intermediate parameter.
* @param conf Configuration object.
* @param src Source URI.
* @param target Target URI.
*/
","* Add a link to the config for the default mount table
   * @param conf - add the link to this conf
   * @param src - the src path name
   * @param target - the target URI link",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMergeSlash,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.net.URI)",91,93,"/**
* Calls m2 with a derived URI.
* @param conf Configuration object.
* @param target Target URI to be used in the call.
*/","* Add a LinkMergeSlash to the config for the default mount table.
   *
   * @param conf configuration.
   * @param target targets.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkFallback,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.net.URI)",114,116,"/**
 * Calls m2 with a derived URI and provided configuration.
 * @param conf Configuration object.
 * @param target Target URI.
 */
","* Add a LinkFallback to the config for the default mount table.
   *
   * @param conf configuration.
   * @param target targets.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMerge,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.net.URI[])",137,139,"/**
 * Calls m2 with configuration and results of m1, given targets.
 */","* Add a LinkMerge to the config for the default mount table.
   *
   * @param conf configuration.
   * @param targets targets array.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setHomeDirConf,"org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String)",209,212,"/**
* Delegates to another m2 method with a derived value.
* @param conf Configuration object; @param homedir home directory
*/","* Add config variable for homedir for default mount table
   * @param conf - add to this conf
   * @param homedir - the home dir path starting with slash",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getHomeDirValue,org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration),235,237,"/**
 * Calls m2 with the configuration and result of m1.
 * @param conf Configuration object used by m1 and m2.
 */
","* Get the value of the home dir conf value for default mount table
   * @param conf - from this conf
   * @return home dir value, null if variable is not in conf",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createEncoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createEncoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",94,104,"/**
 * Creates an ErasureEncoder using the given configuration and options.
 * @param conf Configuration object.
 * @param options ErasureCodecOptions object.
 * @return An ErasureEncoder instance.
 */
","* Create encoder corresponding to given codec.
   * @param options Erasure codec options
   * @param conf configuration.
   * @return erasure encoder",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createDecoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createDecoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",112,122,"/**
 * Creates an ErasureDecoder using provided config and options.
 * @param conf Configuration object.
 * @param options ErasureCodecOptions object.
 * @return An ErasureDecoder instance.
 */
","* Create decoder corresponding to given codec.
   * @param options Erasure codec options
   * @param conf configuration.
   * @return erasure decoder",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getLibraryName,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getLibraryName(org.apache.hadoop.conf.Configuration),72,78,"/**
 * Returns Bzip2 compressor name based on configuration.
 * Returns Bzip2Compressor.m2() if m1(conf) is true.
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2CompressorType,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2CompressorType(org.apache.hadoop.conf.Configuration),86,90,"/**
 * Returns the Compressor class based on configuration.
 * @param conf Configuration object to determine compressor.
 */","* Return the appropriate type of the bzip2 compressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the bzip2 compressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2DecompressorType,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2DecompressorType(org.apache.hadoop.conf.Configuration),109,113,"/**
 * Returns the appropriate Decompressor class based on config.
 */","* Return the appropriate type of the bzip2 decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the bzip2 decompressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2Decompressor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Decompressor(org.apache.hadoop.conf.Configuration),121,124,"/**
 * Creates a Decompressor based on configuration.
 * @param conf Configuration object to determine Decompressor type.
 */","* Return the appropriate implementation of the bzip2 decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the bzip2 decompressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getSocketFactory,"org.apache.hadoop.net.NetUtils:getSocketFactory(org.apache.hadoop.conf.Configuration,java.lang.Class)",96,110,"/**
 * Returns a SocketFactory, using config or default if not found.
 */","* Get the socket factory for the given class according to its
   * configuration parameter
   * <tt>hadoop.rpc.socket.factory.class.&lt;ClassName&gt;</tt>. When no
   * such parameter exists then fall back on the default socket factory as
   * configured by <tt>hadoop.rpc.socket.factory.class.default</tt>. If
   * this default socket factory is not configured, then fall back on the JVM
   * default socket factory.
   * 
   * @param conf the configuration
   * @param clazz the class (usually a {@link VersionedProtocol})
   * @return a socket factory",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,resolve,org.apache.hadoop.net.TableMapping$RawTableMapping:resolve(java.util.List),127,147,"/**
* Maps a list of names to rack names using a topology table.
* @param names List of names to map.
* @return List of corresponding rack names.
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings(),149,160,"/**
 * Reloads the topology table, updating the internal map if successful.
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,initFilter,"org.apache.hadoop.http.lib.StaticUserWebFilter:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",122,128,"/**
 * Adds a static user filter to the container using provided config.
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,<init>,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String,java.lang.String,java.lang.String)",520,524,"/**
 * Constructs a HadoopZookeeperFactory with default truststore config.
 * @param zkPrincipal Zookeeper principal.
 * @param kerberosPrincipal Kerberos principal.
 * @param kerberosKeytab Kerberos keytab file path.
 */
","* Constructor for the helper class to configure the ZooKeeper client connection.
     * @param zkPrincipal Optional.
     * @param kerberosPrincipal Optional. Use along with kerberosKeytab.
     * @param kerberosKeytab Optional. Use along with kerberosPrincipal.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,goUpGroupHierarchy,"org.apache.hadoop.security.LdapGroupsMapping:goUpGroupHierarchy(java.util.Set,int,java.util.Set)",592,618,"/**
 * Recursively finds groups based on provided DNs and hierarchy.
 * @param groupDNs Set of group DNs to search from.
 * @param goUpHierarchy Hierarchy level to traverse.
 * @param groups Set to store found groups.
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,init,org.apache.hadoop.security.ssl.SSLFactory:init(),194,204,"/**
 * Initializes SSL context with provided mode and configuration.
 */","* Initializes the factory.
   *
   * @throws  GeneralSecurityException thrown if an SSL initialization error
   * happened.
   * @throws IOException thrown if an IO error happened while reading the SSL
   * configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,isSimpleAuthentication,org.apache.hadoop.security.KDiag:isSimpleAuthentication(org.apache.hadoop.conf.Configuration),427,430,"/**
* Checks configuration for authentication using SecurityUtil.
* @param conf Configuration object to check.
* @return True if authentication is enabled, false otherwise.
*/
","* Is the authentication method of this configuration ""simple""?
   * @param conf configuration to check
   * @return true if auth is simple (i.e. not kerberos)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HadoopKerberosName.java,setConfiguration,org.apache.hadoop.security.HadoopKerberosName:setConfiguration(org.apache.hadoop.conf.Configuration),63,85,"/**
* Configures security rules based on the provided configuration.
*/","* Set the static configuration to get and evaluate the rules.
   * <p>
   * IMPORTANT: This method does a NOP if the rules have been set already.
   * If there is a need to reset the rules, the {@link KerberosName#setRules(String)}
   * method should be invoked directly.
   * 
   * @param conf the new configuration
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuthMethods,"org.apache.hadoop.ipc.Server:getAuthMethods(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.conf.Configuration)",3472,3491,"/**
 * Retrieves authentication methods based on config and secret manager.
 * @param secretManager Secret manager instance.
 * @param conf Configuration object.
 * @return List of authentication methods.
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getInstance,"org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)",59,88,"/**
 * Finds a suitable CryptoCodec based on configuration and cipher suite.
 * @param conf Configuration object.
 * @param cipherSuite Cipher suite to match.
 * @return CryptoCodec instance or null if none found.
 */","* Get crypto codec for specified algorithm/mode/padding.
   * 
   * @param conf
   *          the configuration
   * @param cipherSuite
   *          algorithm/mode/padding
   * @return CryptoCodec the codec object. Null value will be returned if no
   *         crypto codec classes with cipher suite configured.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,<init>,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(org.apache.hadoop.crypto.key.JavaKeyStoreProvider),115,127,"/**
 * Copies another JavaKeyStoreProvider instance, duplicating its state.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getInstance,org.apache.hadoop.util.hash.Hash:getInstance(org.apache.hadoop.conf.Configuration),92,95,"/**
 * Creates a Hash object based on the configuration.
 * @param conf Configuration object used to determine hash type.
 * @return Hash object created based on configuration.
 */
","* Get a singleton instance of hash function of a type
   * defined in the configuration.
   * @param conf current configuration
   * @return defined hash type, or null if type is invalid",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,buildFlagSet,"org.apache.hadoop.fs.impl.FlagSet:buildFlagSet(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",318,325,"/**
 * Creates a FlagSet from a configuration, ignoring unknown flags.
 * @param enumClass Enum class for flags.
 * @param conf Configuration object.
 * @param key Key for configuration.
 */
","* Build a FlagSet from a comma separated list of values.
   * Case independent.
   * Special handling of ""*"" meaning: all values.
   * @param enumClass class of enum
   * @param conf configuration
   * @param key key to look for
   * @param ignoreUnknown should unknown values raise an exception?
   * @param <E> enumeration type
   * @return a mutable FlagSet
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   * or there are two entries in the enum which differ only by case.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,bind,"org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int,org.apache.hadoop.conf.Configuration,java.lang.String)",690,720,"/**
 * Binds a ServerSocket to an address, optionally within a port range.
 * @param socket ServerSocket to bind
 * @param address Address to bind to
 * @param backlog Backlog size
 * @param conf Configuration object
 * @param rangeConf Range configuration string
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,"org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer,org.apache.hadoop.conf.Configuration)",3622,3640,"/**
 * Writes configuration to a Writer, redacting property if provided.
 * @param propertyName Property to redact, or null.
 * @param out Output Writer.
 * @param config Configuration object.
 */","* Write out the non-default properties in this configuration to the
   * given {@link Writer}.
   * <ul>
   * <li>
   * When property name is not empty and the property exists in the
   * configuration, this method writes the property and its attributes
   * to the {@link Writer}.
   * </li>
   *
   * <li>
   * When property name is null or empty, this method writes all the
   * configuration properties and their attributes to the {@link Writer}.
   * </li>
   *
   * <li>
   * When property name is not empty but the property doesn't exist in
   * the configuration, this method throws an {@link IllegalArgumentException}.
   * </li>
   * </ul>
   * @param propertyName xml property name.
   * @param out the writer to write to.
   * @param config configuration.
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpConfiguration,"org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.io.Writer)",3832,3850,"/**
* Dumps configuration properties to a JSON writer.
* @param config Configuration object to dump.
* @param out JSON writer to write to.
*/","*  Writes out all properties and their attributes (final and resource) to
   *  the given {@link Writer}, the format of the output would be,
   *
   *  <pre>
   *  { ""properties"" :
   *      [ { key : ""key1"",
   *          value : ""value1"",
   *          isFinal : ""key1.isFinal"",
   *          resource : ""key1.resource"" },
   *        { key : ""key2"",
   *          value : ""value2"",
   *          isFinal : ""ke2.isFinal"",
   *          resource : ""key2.resource"" }
   *       ]
   *   }
   *  </pre>
   *
   *  It does not output the properties of the configuration object which
   *  is loaded from an input stream.
   *  <p>
   *
   * @param config the configuration
   * @param out the Writer to write to
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,<init>,org.apache.hadoop.conf.ConfigurationWithLogging:<init>(org.apache.hadoop.conf.Configuration),37,41,"/**
 * Initializes a ConfigurationWithLogging object.
 * @param conf The configuration object.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,confirmFormat,org.apache.hadoop.ha.ZKFailoverController:confirmFormat(),301,317,"/**
 * Confirms formatting of a parent znode, prompting user confirmation.
 * Returns true on confirmation, false on failure or cancellation.
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,int)",192,195,"/**
* Delegates to m1 with the provided key and value.
*/","* Set optional int parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,long)",197,200,"/**
* Delegates to m1 with the provided key and value.
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,float)",212,215,"/**
 * Calls m1 with the given key and value (casted to long).
 * @param key The key to use.
 * @param value The value to pass (as a long).
 * @return The result of calling m1.
 */
","* Set optional float parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,double)",222,225,"/**
 * Calls m1 with the given key and value (casted to long).
 * @param key The key to use.
 * @param value The value to cast to a long.
 * @return The result of calling m1.
 */
","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,int)",293,296,"/**
* Delegates to m1 with the provided key and value.
*/","* Set mandatory int option.
   *
   * @see #must(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,long)",298,301,"/**
* Delegates to m1 with the provided key and value.
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,float)",303,306,"/**
 * Calls m1 with the given key and value (casted to long).
 * @param key The key to use.
 * @param value The value to use, cast to a long.
 * @return The result of calling m1.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,double)",308,311,"/**
 * Calls m1 with the given key and value (casted to long).
 * @param key The key to use.
 * @param value The value to cast to a long.
 * @return The result of calling m1.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])",177,180,"/**
 * Calls m2 with default partitioner, src, null key, and targets.
 * @param conf Hadoop configuration.
 * @param src Source path.
 * @param targets Target URIs.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,handleEmptyDstDirectoryOnWindows,"org.apache.hadoop.fs.RawLocalFileSystem:handleEmptyDstDirectoryOnWindows(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.Path,java.io.File)",646,673,"/**
* Moves a file. Returns true on success, false otherwise.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsAnnotations.java,makeSource,org.apache.hadoop.metrics2.lib.MetricsAnnotations:makeSource(java.lang.Object),36,39,"/**
 * Creates a MetricsSource from the given source object.
 * @param source The object to track metrics for.
 * @return A MetricsSource instance.
 */
","* Make an metrics source from an annotated object.
   * @param source  the annotated object.
   * @return a metrics source",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsAnnotations.java,newSourceBuilder,org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder(java.lang.Object),41,44,"/**
 * Creates a MetricsSourceBuilder with the given source object.
 * @param source The source object for metrics.
 * @return A MetricsSourceBuilder instance.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkOneDir,org.apache.hadoop.fs.RawLocalFileSystem:mkOneDir(java.io.File),773,775,"/**
* Masks a file using an internal helper function.
* @param p2f The file to mask.
* @throws IOException if an I/O error occurs.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirsWithOptionalPermission,"org.apache.hadoop.fs.RawLocalFileSystem:mkdirsWithOptionalPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",818,839,"/**
* Checks if a file can be created with given permissions.
* @param f The path to the file.
* @param permission The file permissions.
* @return True if the file can be created, false otherwise.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)",1205,1211,"/**
 * Opens a data output stream at the given path.
 * @param f Path to open; overwrite, bufferSize, etc. are config.
 * @return FSDataOutputStream
 */
","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize the size of the buffer to be used.
   * @throws IOException IO failure
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,create,"org.apache.hadoop.fs.FileContext:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",681,706,"/**
* Creates a data output stream at the specified path.
* @param f Path to create stream.
* @param createFlag Flags for stream creation.
* @param opts Creation options.
* @return FSDataOutputStream object.
*/","* Create or overwrite file on indicated path and returns an output stream for
   * writing into the file.
   * 
   * @param f the file name to open
   * @param createFlag gives the semantics of create; see {@link CreateFlag}
   * @param opts file creation options; see {@link Options.CreateOpts}.
   *          <ul>
   *          <li>Progress - to report progress on the operation - default null
   *          <li>Permission - umask is applied against permission: default is
   *          FsPermissions:getDefault()
   * 
   *          <li>CreateParent - create missing parent path; default is to not
   *          to create parents
   *          <li>The defaults for the following are SS defaults of the file
   *          server implementing the target path. Not all parameters make sense
   *          for all kinds of file system - eg. localFS ignores Blocksize,
   *          replication, checksum
   *          <ul>
   *          <li>BufferSize - buffersize used in FSDataOutputStream
   *          <li>Blocksize - block size for file blocks
   *          <li>ReplicationFactor - replication for blocks
   *          <li>ChecksumParam - Checksum parameters. server default is used
   *          if not specified.
   *          </ul>
   *          </ul>
   * 
   * @return {@link FSDataOutputStream} for created file
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If file <code>f</code> already exists
   * @throws FileNotFoundException If parent of <code>f</code> does not exist
   *           and <code>createParent</code> is false
   * @throws ParentNotDirectoryException If parent of <code>f</code> is not a
   *           directory.
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is not valid",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,mkdir,"org.apache.hadoop.fs.FileContext:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",799,816,"/**
* Creates a directory at the given path with specified permissions.
* @param dir The directory path.
* @param permission FsPermission for the directory.
* @param createParent Whether to create parent directories.
*/","* Make(create) a directory and all the non-existent parents.
   * 
   * @param dir - the dir to make
   * @param permission - permissions is set permission{@literal &~}umask
   * @param createParent - if true then missing parent dirs are created if false
   *          then parent must exist
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If directory <code>dir</code> already
   *           exists
   * @throws FileNotFoundException If parent of <code>dir</code> does not exist
   *           and <code>createParent</code> is false
   * @throws ParentNotDirectoryException If parent of <code>dir</code> is not a
   *           directory
   * @throws UnsupportedFileSystemException If file system for <code>dir</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>dir</code> is not valid",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,main,org.apache.hadoop.io.compress.CompressionCodecFactory:main(java.lang.String[]),301,358,"/**
 * Processes compression/decompression based on command-line args.
 * Uses codecs from factory, encodes/decodes files accordingly.
 */","* A little test program.
   * @param args arguments.
   * @throws Exception exception.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.KeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",47,50,"/**
 * Initializes a KeyStoreProvider with a URI and configuration.
 * @param uri The URI of the keystore.
 * @param conf The configuration for the keystore.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",54,57,"/**
 * Initializes a LocalKeyStoreProvider with a URI and configuration.
 * @param uri URI of the keystore.
 * @param conf Configuration object.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getServerPrincipal,"org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.lang.String)",185,196,"/**
 * Masks a principal configuration based on hostname validation.
 * @param principalConfig The principal configuration string.
 * @param hostname The hostname to validate against.
 * @return The masked configuration or original if invalid.
 */
","* Convert Kerberos principal name pattern to valid Kerberos principal
   * names. It replaces hostname pattern with hostname, which should be
   * fully-qualified domain name. If hostname is null or ""0.0.0.0"", it uses
   * dynamically looked-up fqdn of the current host instead.
   * 
   * @param principalConfig
   *          the Kerberos principal name conf value to convert
   * @param hostname
   *          the fully-qualified domain name used for substitution
   * @return converted Kerberos principal name
   * @throws IOException if the client address cannot be determined",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getServerPrincipal,"org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.net.InetAddress)",212,227,"/**
* Masks a principal config string with an IP address.
* @param principalConfig Config string to mask.
* @param addr IP address to use for masking.
* @throws IOException if addr is null.
*/
","* Convert Kerberos principal name pattern to valid Kerberos principal names.
   * This method is similar to {@link #getServerPrincipal(String, String)},
   * except 1) the reverse DNS lookup from addr to hostname is done only when
   * necessary, 2) param addr can't be null (no default behavior of using local
   * hostname when addr is null).
   * 
   * @param principalConfig
   *          Kerberos principal name pattern to convert
   * @param addr
   *          InetAddress of the host used for substitution
   * @return converted Kerberos principal name
   * @throws IOException if the client address cannot be determined",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,addProtocol,"org.apache.hadoop.ipc.RPC$Server:addProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)",1218,1222,"/**
 * Creates a server with the given RPC kind and protocol.
 * @param rpcKind RPC kind, protocol class, and implementation.
 * @return The created server instance.
 */
","* Add a protocol to the existing server.
     * @param rpcKind - input rpcKind
     * @param protocolClass - the protocol class
     * @param protocolImpl - the impl of the protocol that will be called
     * @return the server (for convenience)",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,createKeyProvider,"org.apache.hadoop.util.KMSUtil:createKeyProvider(org.apache.hadoop.conf.Configuration,java.lang.String)",59,64,"/**
 * Creates a KeyProvider using the given configuration and key name.
 * @param conf Configuration object.
 * @param configKeyName Key name for the provider.
 * @return KeyProvider or null if URI is null.
 */
","* Creates a new KeyProvider from the given Configuration
   * and configuration key name.
   *
   * @param conf Configuration
   * @param configKeyName The configuration key name
   * @return new KeyProvider, or null if no provider was found.
   * @throws IOException if the KeyProvider is improperly specified in
   *                             the Configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,getKeyProviderUri,org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration),66,69,"/**
* Retrieves a URI using the provided configuration.
* @param conf Configuration object to use.
* @return URI object.
*/
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultUri,org.apache.hadoop.fs.FileSystem:getDefaultUri(org.apache.hadoop.conf.Configuration),297,304,"/**
 * Retrieves the URI for the default file system.
 * @param conf Hadoop configuration object.
 * @return URI of the default file system.
 */
","* Get the default FileSystem URI from a configuration.
   * @param conf the configuration to use
   * @return the uri of the default filesystem",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,setConf,org.apache.hadoop.io.compress.PassthroughCodec:setConf(org.apache.hadoop.conf.Configuration),95,102,"/**
 * Initializes configuration and sets the file extension.
 * @param conf Configuration object to be used.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKinitExecutable,org.apache.hadoop.security.KDiag:validateKinitExecutable(),715,727,"/**
 * Executes kinit command, handling PATH issues if necessary.
 */","* A cursory look at the {@code kinit} executable.
   *
   * If it is an absolute path: it must exist with a size > 0.
   * If it is just a command, it has to be on the path. There's no check
   * for that -but the PATH is printed out.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getSocketAddr,"org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,int)",2566,2570,"/**
 * Resolves an address and port from a name, using defaults if needed.
 * @param name Name to resolve, defaults used if not found.
 * @param defaultAddress Default address if name resolution fails.
 * @param defaultPort Default port to use.
 * @return Resolved InetSocketAddress.
 */
","* Get the socket address for <code>name</code> property as a
   * <code>InetSocketAddress</code>.
   * @param name property name.
   * @param defaultAddress the default value
   * @param defaultPort the default port
   * @return InetSocketAddress",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updateConnectAddr,"org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)",2596,2614,"/**
 * Resolves an InetSocketAddress, prioritizing provided properties.
 * @param hostProperty Host property key.
 * @param addressProperty Address property key.
 * @param defaultAddressValue Default address value.
 * @param addr Fallback InetSocketAddress.
 * @return Resolved InetSocketAddress.
 */","* Set the socket address a client can use to connect for the
   * <code>name</code> property as a <code>host:port</code>.  The wildcard
   * address is replaced with the local host's address. If the host and address
   * properties are configured the host component of the address will be combined
   * with the port component of the addr to generate the address.  This is to allow
   * optional control over which host name is used in multi-home bind-host
   * cases where a host can have multiple names
   * @param hostProperty the bind-host configuration name
   * @param addressProperty the service address configuration name
   * @param defaultAddressValue the service default address configuration value
   * @param addr InetSocketAddress of the service listener
   * @return InetSocketAddress for clients to connect",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,initializeMetadataCache,org.apache.hadoop.fs.HarFileSystem:initializeMetadataCache(org.apache.hadoop.conf.Configuration),108,113,"/**
 * Initializes the harMetaCache with a size from configuration.
 * @param conf Configuration object to retrieve cache size from.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,build,org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:build(),2971,2990,"/**
* Opens a file stream using provided parameters.
* @return CompletableFuture containing FSDataInputStream
*/","* Perform the open operation.
     *
     * @return a future to the input stream.
     * @throws IOException early failure to open
     * @throws UnsupportedOperationException if the specific operation
     * is not supported.
     * @throws IllegalArgumentException if the parameters are not valid.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,build,org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:build(),4941,4958,"/**
* Opens a file stream, returning a CompletableFuture. Uses m14/m17.
*/","* Perform the open operation.
     * Returns a future which, when get() or a chained completion
     * operation is invoked, will supply the input stream of the file
     * referenced by the path/path handle.
     * @return a future to the input stream.
     * @throws IOException early failure to open
     * @throws UnsupportedOperationException if the specific operation
     * is not supported.
     * @throws IllegalArgumentException if the parameters are not valid.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,setConfigurationFromURI,"org.apache.hadoop.fs.sftp.SFTPFileSystem:setConfigurationFromURI(java.net.URI,org.apache.hadoop.conf.Configuration)",97,135,"/**
 * Configures SFTP connection details from URI info and configuration.
 */","* Set configuration from UI.
   *
   * @param uri
   * @param conf
   * @throws IOException",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,connect,org.apache.hadoop.fs.sftp.SFTPFileSystem:connect(),143,157,"/**
* Establishes an SFTP channel using configuration.
* @return ChannelSftp object or null if connection fails.
*/
","* Connecting by using configuration parameters.
   *
   * @return An FTPClient instance
   * @throws IOException",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path)",180,185,"/**
 * Constructs a ChecksumFSInputChecker with default buffer size.
 * @param fs ChecksumFileSystem instance
 * @param file Path to check checksums for
 * @throws IOException if an I/O error occurs
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,initFromFS,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:initFromFS(),113,116,"/**
* Retrieves buffer size from file system configuration.
*/",* Initialize from a filesystem.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,create,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.shell.PathData,boolean)",515,544,"/**
* Creates a FSDataOutputStream, optionally with lazy persist.
* @param item PathData object
* @param lazyPersist Flag to enable lazy persist
* @return FSDataOutputStream object
*/
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,open,org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.Path),996,999,"/**
 * Opens an FSDataInputStream for the given path.
 * @param f Path to open; returns an FSDataInputStream.
 */","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file to open
   * @throws IOException IO failure
   * @return input stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,open,org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.PathHandle),1014,1017,"/**
 * Opens an FSDataInputStream for the given path.
 * @param fd PathHandle object representing the file path.
 */","* Open an FSDataInputStream matching the PathHandle instance. The
   * implementation may encode metadata in PathHandle to address the
   * resource directly and verify that the resource referenced
   * satisfies constraints specified at its construciton.
   * @param fd PathHandle object returned by the FS authority.
   * @throws InvalidPathHandleException If {@link PathHandle} constraints are
   *                                    not satisfied
   * @throws IOException IO failure
   * @throws UnsupportedOperationException If {@link #open(PathHandle, int)}
   *                                       not overridden by subclass
   * @return input stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,append,org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path),1516,1519,"/**
 * Opens a data output stream for the given path.
 * @param f The path to open.
 * @return FSDataOutputStream for writing data.
 */","* Append to an existing file (optional operation).
   * Same as
   * {@code append(f, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,
   *     IO_FILE_BUFFER_SIZE_DEFAULT), null)}
   * @param f the existing file to be appended.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default).
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,append,"org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,boolean)",1558,1561,"/**
 * Opens a data output stream to write to a file.
 * @param f the path to the file
 * @param appendToNewBlock if true, append to new block
 * @return FSDataOutputStream object
 */","* Append to an existing file (optional operation).
   * @param f the existing file to be appended.
   * @param appendToNewBlock whether to append data to a new block
   * instead of the end of the last partial block
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default).
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,setConf,org.apache.hadoop.fs.ChecksumFileSystem:setConf(org.apache.hadoop.conf.Configuration),83,93,"/**
 * Calls super.m1 and initializes bytesPerChecksum from config.
 * @param conf Configuration object; used to get bytesPerChecksum.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getSumBufferSize,"org.apache.hadoop.fs.ChecksumFileSystem:getSumBufferSize(int,int)",156,163,"/**
 * Calculates buffer size based on bytesPerSum and bufferSize.
 * Uses proportional size, default size, or bytesPerSum.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem$Cache:<init>(org.apache.hadoop.conf.Configuration),3657,3663,"/**
 * Initializes the cache with a given configuration.
 * @param conf Configuration object for cache settings.
 */
","* Instantiate. The configuration is used to read the
     * count of permits issued for concurrent creation
     * of filesystem instances.
     * @param conf configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Metadata)",2948,2974,"/**
 * Constructs a Sorter with provided configuration and metadata.
 */","* Sort and merge using an arbitrary {@link RawComparator}.
     * @param fs input FileSystem.
     * @param comparator input RawComparator.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.
     * @param metadata input metadata.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBlockSize,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBlockSize(org.apache.hadoop.conf.Configuration),130,133,"/**
* Gets the Bzip2 compression block size from configuration.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getWorkFactor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getWorkFactor(org.apache.hadoop.conf.Configuration),139,142,"/**
 * Gets the bzip2 compression work factor from configuration.
 * @param conf Configuration object to retrieve the value from.
 * @return The bzip2 compression work factor as an integer.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createOutputStream,"org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",60,67,"/**
 * Creates a CompressionOutputStream using given stream and compressor.
 * @param out The output stream.
 * @param compressor The compressor to use.
 * @return A CompressionOutputStream.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createInputStream,"org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",86,93,"/**
 * Creates a DecompressorStream using the provided input and decompressor.
 * @param in Input stream to decompress.
 * @param decompressor Decompressor implementation.
 * @return DecompressorStream instance.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createOutputStream,"org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",82,94,"/**
 * Creates a BlockCompressorStream with configured buffer size.
 * @param out Output stream to compress to.
 * @param compressor Compressor to use for compression.
 * @return BlockCompressorStream instance.
 */","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createInputStream,"org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",146,153,"/**
 * Creates a BlockDecompressorStream for LZ4 decompression.
 * @param in Input stream to decompress.
 * @param decompressor Decompressor instance.
 * @return BlockDecompressorStream object.
 */","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createDecompressor,org.apache.hadoop.io.compress.Lz4Codec:createDecompressor(),170,176,"/**
 * Creates and returns an Lz4Decompressor with a buffer size.
 */","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createOutputStream,"org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",122,130,"/**
 * Creates a compression output stream based on configuration.
 * @param out The output stream.
 * @param compressor Compressor object.
 * @return CompressionOutputStream instance.
 */","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to, to have it 
   *         compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,"org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",177,186,"/**
 * Creates a CompressionInputStream based on configuration.
 * @param in Input stream to compress.
 * @param decompressor Decompressor object.
 * @return CompressionInputStream instance.
 */","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}, and return a 
   * stream for uncompressed data.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressionLevel,org.apache.hadoop.io.compress.ZStandardCodec:getCompressionLevel(org.apache.hadoop.conf.Configuration),88,92,"/**
 * Gets the Zstd compression level from the configuration.
 * @param conf Configuration object.
 * @return Zstd compression level.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getBufferSize(org.apache.hadoop.conf.Configuration),108,111,"/**
 * Retrieves the Zstd buffer size from the configuration.
 * @param conf Configuration object to read from.
 * @return The Zstd buffer size.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createOutputStream,"org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",51,60,"/**
 * Creates a compression output stream. Uses compressor if provided.
 * @param out The underlying output stream.
 * @param compressor Compressor to use, or null for no compression.
 * @return CompressionOutputStream or a regular OutputStream.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createInputStream,"org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",83,93,"/**
 * Creates a compression input stream using provided or default decompressor.
 * @param in Input stream to decompress.
 * @param decompressor Decompressor to use, or null to create one.
 * @return CompressionInputStream object.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createOutputStream,"org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",82,94,"/**
 * Creates a BlockCompressorStream with configured buffer size.
 * @param out Output stream to compress to.
 * @param compressor Compressor to use.
 * @return BlockCompressorStream instance.
 */","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createCompressor,org.apache.hadoop.io.compress.SnappyCodec:createCompressor(),111,117,"/**
 * Creates a SnappyCompressor with buffer size from configuration.
 * @return SnappyCompressor instance
 */
","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createInputStream,"org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",143,150,"/**
 * Creates a BlockDecompressorStream for decompressing input.
 * @param in Input stream to decompress.
 * @param decompressor Decompressor instance.
 * @return BlockDecompressorStream instance.
 */","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createDecompressor,org.apache.hadoop.io.compress.SnappyCodec:createDecompressor(),167,173,"/**
 * Creates a SnappyDecompressor with buffer size from config.
 * @return SnappyDecompressor instance
 */
","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration)",114,118,"/**
* Calls m2 with default buffer size and copy mode true.
* @param in input stream
* @param out output stream
* @param conf configuration object
*/
","* Copies from one stream to another. <strong>closes the input and output streams 
   * at the end</strong>.
   *
   * @param in InputStrem to read from
   * @param out OutputStream to write to
   * @param conf the Configuration object.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration,boolean)",130,134,"/**
* Delegates to overloaded method with default buffer size.
* @param in Input stream. @param out Output stream.
* @param conf Configuration object. @param close Close streams.
*/","* Copies from one stream to another.
   *
   * @param in InputStream to read from
   * @param out OutputStream to write to
   * @param conf the Configuration object
   * @param close whether or not close the InputStream and 
   * OutputStream at the end. The streams are closed in the finally clause.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getChunkBufferSize,org.apache.hadoop.io.file.tfile.TFile:getChunkBufferSize(org.apache.hadoop.conf.Configuration),142,145,"/**
 * Retrieves chunk buffer size from configuration, defaults to 1024KB.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFSInputBufferSize,org.apache.hadoop.io.file.tfile.TFile:getFSInputBufferSize(org.apache.hadoop.conf.Configuration),147,149,"/**
* Retrieves a configuration attribute value.
* @param conf Configuration object to read from.
* @return Integer value of the attribute.
*/
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFSOutputBufferSize,org.apache.hadoop.io.file.tfile.TFile:getFSOutputBufferSize(org.apache.hadoop.conf.Configuration),151,153,"/**
* Retrieves the FS output buffer size from the configuration.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getBufferSize,org.apache.hadoop.io.SequenceFile:getBufferSize(org.apache.hadoop.conf.Configuration),1738,1740,"/**
 * Retrieves the IO file buffer size from configuration.
 * @param conf Configuration object to read from.
 * @return The IO file buffer size as an integer.
 */",Get the configured buffer size,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,setConf,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration),156,166,"/**
 * Initializes script name and maxArgs from configuration.
 * @param conf Configuration object; null if not provided.
 */","* Set the configuration and extract the configuration parameters of interest
     * @param conf the new configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createHttpChannelConnector,"org.apache.hadoop.http.HttpServer2$Builder:createHttpChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)",566,581,"/**
 * Creates a ServerConnector with default acceptor/selector counts.
 * @param server The server to connect to.
 * @param httpConfig Http configuration for the connector.
 * @return The created ServerConnector.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,doOp,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:doOp(org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$ProviderCallable,int,boolean)",167,234,"/**
 * Executes a provider operation with failover and retry logic.
 * @param op Provider operation to execute.
 * @param currPos Current provider position.
 * @param isIdempotent Whether the operation is idempotent.
 * @return Result of the operation.
 * @throws IOException If operation fails after retries.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,<init>,org.apache.hadoop.crypto.key.KeyProvider$Options:<init>(org.apache.hadoop.conf.Configuration),341,344,"/**
 * Constructs an Options object from a Configuration.
 * @param conf Configuration object containing options.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,getBufferSize,org.apache.hadoop.crypto.CryptoStreamUtils:getBufferSize(org.apache.hadoop.conf.Configuration),65,68,"/**
 * Retrieves the Hadoop security crypto buffer size from configuration.
 */","* Read crypto buffer size.
   *
   * @param conf configuration.
   * @return hadoop.security.crypto.buffer.size.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,parseNumLevels,"org.apache.hadoop.ipc.CallQueueManager:parseNumLevels(java.lang.String,org.apache.hadoop.conf.Configuration)",391,411,"/**
* Gets the priority levels from configuration, logs deprecation warning.
* @param ns namespace
* @param conf Configuration object
* @return Number of priority levels.
*/","* Read the number of levels from the configuration.
   * This will affect the FairCallQueue's overall capacity.
   * @throws IllegalArgumentException on invalid queue count",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getRpcTimeout,org.apache.hadoop.ipc.RPC:getRpcTimeout(org.apache.hadoop.conf.Configuration),829,832,"/**
 * Gets the IPC client RPC timeout from configuration.
 * @param conf Configuration object to retrieve timeout from.
 * @return Timeout value in milliseconds.
 */
","* Get the RPC time from configuration;
   * If not set in the configuration, return the default value.
   *
   * @param conf Configuration
   * @return the RPC timeout (ms)",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getPingInterval,org.apache.hadoop.ipc.Client:getPingInterval(org.apache.hadoop.conf.Configuration),187,190,"/**
* Gets the IPC ping interval from the configuration.
* @param conf Configuration object to retrieve the value from.
* @return The IPC ping interval in milliseconds.
*/
","* Get the ping interval from configuration;
   * If not set in the configuration, return the default value.
   * 
   * @param conf Configuration
   * @return the ping interval",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getRpcTimeout,org.apache.hadoop.ipc.Client:getRpcTimeout(org.apache.hadoop.conf.Configuration),221,226,"/**
 * Returns the RPC timeout from configuration, defaulting to 0 if negative.
 */","* The time after which a RPC will timeout.
   *
   * @param conf Configuration
   * @return the timeout period in milliseconds.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedTimeCostProvider.java,init,"org.apache.hadoop.ipc.WeightedTimeCostProvider:init(java.lang.String,org.apache.hadoop.conf.Configuration)",65,90,"/**
* Initializes weights based on configuration for given namespace.
* @param namespace Namespace to configure weights for.
* @param conf Configuration object to read weights from.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,"org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration)",94,96,"/**
 * Constructs a LineReader with default buffer size.
 * @param in InputStream to read from
 * @param conf Configuration object
 */
","* Create a line reader that reads from the given stream using the
   * <code>io.file.buffer.size</code> specified in the given
   * <code>Configuration</code>.
   * @param in input stream
   * @param conf configuration
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,"org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration,byte[])",138,144,"/**
 * Constructs a LineReader with an InputStream, Configuration, and delimiter.
 */","* Create a line reader that reads from the given stream using the
   * <code>io.file.buffer.size</code> specified in the given
   * <code>Configuration</code>, and using a custom delimiter of array of
   * bytes.
   * @param in input stream
   * @param conf configuration
   * @param recordDelimiterBytes The delimiter
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getInt,"org.apache.hadoop.conf.ConfigurationWithLogging:getInt(java.lang.String,int)",87,92,"/**
 * Calls super.m1, logs the result, and returns the value.
 * @param name Parameter name.
 * @param defaultValue Default value if not found.
 */
","* See {@link Configuration#getInt(String, int)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,setConf,org.apache.hadoop.ha.HAAdmin:setConf(org.apache.hadoop.conf.Configuration),337,345,"/**
 * Calls super.m1 and sets rpcTimeoutForChecks from config.
 * @param conf Hadoop configuration object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,getSshConnectTimeout,org.apache.hadoop.ha.SshFenceByTcpPort:getSshConnectTimeout(),215,218,"/**
* Retrieves a value using m1().m2(), with timeout key and default.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,getGracefulFenceTimeout,org.apache.hadoop.ha.FailoverController:getGracefulFenceTimeout(org.apache.hadoop.conf.Configuration),82,86,"/**
* Retrieves graceful fence timeout from configuration.
* @param conf Configuration object to retrieve from.
* @return Graceful fence timeout in milliseconds.
*/
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,getRpcTimeoutToNewActive,org.apache.hadoop.ha.FailoverController:getRpcTimeoutToNewActive(org.apache.hadoop.conf.Configuration),88,92,"/**
* Retrieves the HA failover new active timeout value from config.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,setTimeout,"org.apache.hadoop.fs.ftp.FTPFileSystem:setTimeout(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)",173,177,"/**
* Configures FTP client timeout based on configuration.
* @param client FTP client to configure.
* @param conf Configuration object containing timeout setting.
*/
","* Set the FTPClient's timeout based on configuration.
   * FS_FTP_TIMEOUT is set as timeout (defaults to DEFAULT_TIMEOUT).",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FSBuilderSupport.java,getLong,"org.apache.hadoop.fs.impl.FSBuilderSupport:getLong(java.lang.String,long)",77,93,"/**
 * Retrieves a long value for the given key, using defVal if parsing fails.
 */","* Get a long value with resilience to unparseable values.
   * @param key key to log
   * @param defVal default value
   * @return long value",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,canBeSafelyDeleted,org.apache.hadoop.fs.shell.Delete$Rm:canBeSafelyDeleted(org.apache.hadoop.fs.shell.PathData),129,149,"/**
 * Determines if an item should be deleted based on safe delete settings.
 * @param item PathData object representing the item to be deleted.
 * @return True if the item should be deleted, false otherwise.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FileSystem:getDefaultBlockSize(),2753,2757,"/**
 * Calls m1() with a key and default value, returning a long.
 */","* Return the number of bytes that large input files should be optimally
   * be split into to minimize I/O time.
   * @deprecated use {@link #getDefaultBlockSize(Path)} instead
   * @return default block size.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,<init>,"org.apache.hadoop.fs.DF:<init>(java.io.File,org.apache.hadoop.conf.Configuration)",49,52,"/**
 * Constructs a DiskFailure with a given path and interval.
 * @param path File path to monitor.
 * @param interval Interval for disk failure checks.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getInterval,org.apache.hadoop.fs.GetSpaceUsed$Builder:getInterval(),58,67,"/**
* Retrieves the FS DU interval, using default if needed.
* Returns the interval as a long.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getJitter,org.apache.hadoop.fs.GetSpaceUsed$Builder:getJitter(),118,129,"/**
 * Returns the jitter value, from config if null.
 * @return long jitter value.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,ensureInitialized,org.apache.hadoop.io.nativeio.NativeIO:ensureInitialized(),1039,1048,"/**
 * Initializes the UID to User mapping cache with a timeout.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,"org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration,boolean)",106,128,"/**
 * Initializes the ShellBasedIdMapping with config and build flag.
 * @param conf Configuration object.
 * @param constructFullMapAtInit Flag to build full map at init.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",71,78,"/**
 * Constructs a DelegationTokenSecretManager with configuration and token kind.
 * @param conf Configuration object.
 * @param tokenKind Token identifier.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseDecayPeriodMillis,"org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayPeriodMillis(java.lang.String,org.apache.hadoop.conf.Configuration)",359,377,"/**
 * Gets the decay scheduler period in milliseconds from configuration.
 * @param ns Namespace key.
 * @param conf Configuration object.
 * @return Decay scheduler period.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceInit,org.apache.hadoop.util.JvmPauseMonitor:serviceInit(org.apache.hadoop.conf.Configuration),74,79,"/**
* Initializes thresholds from configuration.
* @param conf Configuration object providing threshold values.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getLong,"org.apache.hadoop.conf.ConfigurationWithLogging:getLong(java.lang.String,long)",97,102,"/**
 * Retrieves a value by name, logs it, and returns the value.
 * @param name The name of the value to retrieve.
 * @param defaultValue Default value if not found.
 */
","* See {@link Configuration#getLong(String, long)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,<init>,"org.apache.hadoop.ha.HealthMonitor:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)",115,135,"/**
 * Constructs a HealthMonitor with configuration and target.
 * @param conf Configuration object.
 * @param target Target to monitor.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,initBloomFilter,org.apache.hadoop.io.BloomMapFile$Writer:initBloomFilter(org.apache.hadoop.conf.Configuration),166,180,"/**
 * Initializes the bloom filter using configuration parameters.
 * @param conf Configuration object containing bloom filter settings.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getFloat,"org.apache.hadoop.conf.ConfigurationWithLogging:getFloat(java.lang.String,float)",77,82,"/**
 * Retrieves a value by name, logs it, and returns the value.
 * @param name The name of the value to retrieve.
 * @param defaultValue Default value if not found.
 */
","* See {@link Configuration#getFloat(String, float)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseDecayFactor,"org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayFactor(java.lang.String,org.apache.hadoop.conf.Configuration)",339,357,"/**
 * Retrieves decay factor from configuration, handling defaults and validation.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,initialize,"org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",87,99,"/**
* Initializes trash-related configurations from given configuration.
* @param conf Configuration object.
* @param fs FileSystem instance.
* @param home Home directory Path.
*/","* @deprecated Use {@link #initialize(Configuration, FileSystem)} instead.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,initialize,"org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",101,118,"/**
* Initializes trash operation parameters from configuration.
* @param conf Hadoop configuration object.
* @param fs FileSystem object.
*/
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,processRawArguments,org.apache.hadoop.fs.shell.FsCommand:processRawArguments(java.util.LinkedList),102,122,"/**
 * Processes command arguments, checks for missing fs.defaultFS, and executes.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,closeChildFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:closeChildFileSystems(org.apache.hadoop.fs.FileSystem),1966,1984,"/**
 * Closes child file systems if cache disabling is configured.
 * @param fs The parent FileSystem to process.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,isNestedMountPointSupported,org.apache.hadoop.fs.viewfs.ConfigUtil:isNestedMountPointSupported(org.apache.hadoop.conf.Configuration),270,272,"/**
 * Checks if nested mount point support is enabled in config.
 * @param conf Configuration object to check.
 * @return True if supported, false otherwise.
 */","* Check the bool config whether nested mount point is supported. Default: true
   * @param conf - from this conf
   * @return whether nested mount point is supported",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.viewfs.InodeTree)",1410,1426,"/**
 * Constructs an InternalDirOfViewFs with provided parameters.
 * @param dir Directory inode, cTime, ugi, URI, config, fsState
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createCompressor,org.apache.hadoop.io.compress.Lz4Codec:createCompressor(),111,120,"/**
 * Creates and returns an Lz4Compressor instance with config.
 * @return Lz4Compressor object with buffer size and LZ4HC flag.
 */","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,handleChecksumException,org.apache.hadoop.io.SequenceFile$Reader:handleChecksumException(org.apache.hadoop.fs.ChecksumException),2792,2801,"/**
 * Handles checksum errors: skips or throws exception based on config.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,getMultipleLinearRandomRetry,"org.apache.hadoop.io.retry.RetryUtils:getMultipleLinearRandomRetry(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String)",179,201,"/**
 * Creates a RetryPolicy based on configuration.
 * @param conf Configuration object.
 * @return RetryPolicy or null if disabled.
 */","* Return the MultipleLinearRandomRetry policy specified in the conf,
   * or null if the feature is disabled.
   * If the policy is specified in the conf but the policy cannot be parsed,
   * the default policy is returned.
   * 
   * Retry policy spec:
   *   N pairs of sleep-time and number-of-retries ""s1,n1,s2,n2,...""
   * 
   * @param conf configuration.
   * @param retryPolicyEnabledKey     conf property key for enabling retry
   * @param defaultRetryPolicyEnabled default retryPolicyEnabledKey conf value 
   * @param retryPolicySpecKey        conf property key for retry policy spec
   * @param defaultRetryPolicySpec    default retryPolicySpecKey conf value
   * @return the MultipleLinearRandomRetry policy specified in the conf,
   *         or null if the feature is disabled.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,isSupported,org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:isSupported(),283,283,"/**
* Abstract method; returns a boolean mask value.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addPrometheusServlet,org.apache.hadoop.http.HttpServer2:addPrometheusServlet(org.apache.hadoop.conf.Configuration),818,828,"/**
 * Initializes Prometheus support using the provided configuration.
 * @param conf Configuration object containing Prometheus settings.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addDefaultApps,"org.apache.hadoop.http.HttpServer2:addDefaultApps(org.eclipse.jetty.server.handler.ContextHandlerCollection,java.lang.String,org.apache.hadoop.conf.Configuration)",926,973,"/**
* Configures HTTP logs and static resources for Jetty server.
* @param parent Parent ContextHandlerCollection
* @param appDir Application directory
* @param conf Configuration object
*/","* Add default apps.
   *
   * @param parent contexthandlercollection.
   * @param appDir The application directory
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addDefaultServlets,org.apache.hadoop.http.HttpServer2:addDefaultServlets(org.apache.hadoop.conf.Configuration),985,995,"/**
 * Registers servlets for various configuration endpoints.
 * @param configuration The application configuration object.
 */","* Add default servlets.
   * @param configuration the hadoop configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,create,"org.apache.hadoop.metrics2.source.JvmMetrics:create(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSystem)",112,123,"/**
 * Creates JvmMetrics instance with given process/session name.
 * @param processName Process name.
 * @param sessionId Session ID.
 * @param ms MetricsSystem instance.
 * @return JvmMetrics object.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,initFilter,"org.apache.hadoop.security.HttpCrossOriginFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",39,52,"/**
 * Adds CORS filter if enabled in configuration.
 * @param container Filter container to add filter to.
 * @param conf Configuration object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$ListCommand:validate(),242,250,"/**
 * Initializes provider and fetches metadata.
 * Returns true if successful, false otherwise.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,getServerFailOverEnable,"org.apache.hadoop.ipc.CallQueueManager:getServerFailOverEnable(java.lang.String,org.apache.hadoop.conf.Configuration)",119,141,"/**
* Checks if IPC callqueue server failover is enabled.
* @param namespace Namespace to check.
* @param conf Configuration object.
* @return True if enabled, default otherwise.
*/","* Return boolean value configured by property 'ipc.<port>.callqueue.overflow.trigger.failover'
   * if it is present. If the config is not present, default config
   * (without port) is used to derive class i.e 'ipc.callqueue.overflow.trigger.failover',
   * and derived value is returned if configured. Otherwise, default value
   * {@link CommonConfigurationKeys#IPC_CALLQUEUE_SERVER_FAILOVER_ENABLE_DEFAULT} is returned.
   *
   * @param namespace Namespace ""ipc"" + ""."" + Server's listener port.
   * @param conf Configuration properties.
   * @return Value returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseBackOffByResponseTimeEnabled,"org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffByResponseTimeEnabled(java.lang.String,org.apache.hadoop.conf.Configuration)",467,472,"/**
 * Checks if response time enable key is enabled in configuration.
 * @param ns Namespace.
 * @param conf Configuration object.
 * @return Boolean indicating enablement status.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",1325,1342,"/**
 * Constructs a new Client with given Writable class, config, and socket factory.
 */","* Construct an IPC client whose values are of the given {@link Writable}
   * class.
   *
   * @param valueClass input valueClass.
   * @param conf input configuration.
   * @param factory input factory.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getClientBackoffEnable,"org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,org.apache.hadoop.conf.Configuration)",920,927,"/**
 * Checks if IPC backoff is enabled in the configuration.
 * @param prefix Configuration prefix.
 * @param conf Configuration object.
 * @return True if enabled, false otherwise.
 */
",* Get from config if client backoff is enabled on that port.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getClientBackoffEnable,"org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,int,org.apache.hadoop.conf.Configuration)",941,953,"/**
* Checks IPC backoff enable status from config, using namespace/port.
* @param namespace IPC namespace
* @param port Port number
* @param conf Configuration object
* @return True if enabled, false otherwise.
*/","* Return boolean value configured by property 'ipc.<port>.backoff.enable'
   * if it is present. If the config is not present, default config
   * (without port) is used to derive class i.e 'ipc.backoff.enable',
   * and derived value is returned if configured. Otherwise, default value
   * {@link CommonConfigurationKeys#IPC_BACKOFF_ENABLE_DEFAULT} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Value returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPasswordFromConfig,org.apache.hadoop.conf.Configuration:getPasswordFromConfig(java.lang.String),2513,2524,"/**
 * Extracts password characters from name, if fallback is enabled.
 * @param name The name to extract the password from.
 * @return Character array of the password, or null.
 */","* Fallback to clear text passwords in configuration.
   * @param name the property name.
   * @return clear text password or null",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getBoolean,"org.apache.hadoop.conf.ConfigurationWithLogging:getBoolean(java.lang.String,boolean)",67,72,"/**
 * Calls super.m1, logs the value, and returns the result.
 * @param name Parameter name.
 * @param defaultValue Default value.
 * @return Boolean value returned by super.m1.
 */","* See {@link Configuration#getBoolean(String, boolean)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileSystemClass,"org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration)",3559,3594,"/**
 * Finds a FileSystem implementation for the given scheme, using config or service files.
 * @param scheme Filesystem scheme (e.g., ""hdfs"")
 * @param conf Hadoop configuration object
 * @return FileSystem class or throws UnsupportedFileSystemException
 */","* Get the FileSystem implementation class of a filesystem.
   * This triggers a scan and load of all FileSystem implementations listed as
   * services and discovered via the {@link ServiceLoader}
   * @param scheme URL scheme of FS
   * @param conf configuration: can be null, in which case the check for
   * a filesystem binding declaration in the configuration is skipped.
   * @return the filesystem
   * @throws UnsupportedFileSystemException if there was no known implementation
   *         for the scheme.
   * @throws IOException if the filesystem could not be loaded",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,createFileSystem,"org.apache.hadoop.fs.AbstractFileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",169,181,"/**
 * Creates an AbstractFileSystem instance for the given URI.
 * @param uri The URI for the filesystem.
 * @param conf Configuration object.
 * @return AbstractFileSystem instance.
 */","* Create a file system instance for the specified uri using the conf. The
   * conf is used to find the class name that implements the file system. The
   * conf is also passed to the file system for its configuration.
   *
   * @param uri URI of the file system
   * @param conf Configuration for the file system
   * 
   * @return Returns the file system for the given URI
   *
   * @throws UnsupportedFileSystemException file system for <code>uri</code> is
   *           not found",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,loadMappingProviders,org.apache.hadoop.security.CompositeGroupsMapping:loadMappingProviders(),147,160,"/**
 * Iterates through mapping providers, configures them, logs errors.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolEngine,"org.apache.hadoop.ipc.RPC:getProtocolEngine(java.lang.Class,org.apache.hadoop.conf.Configuration)",220,230,"/**
 * Retrieves an RpcEngine for the given protocol, creating it if needed.
 * @param protocol The protocol class.
 * @param conf Configuration object.
 * @return The RpcEngine instance.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getQueueClass,"org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,org.apache.hadoop.conf.Configuration)",796,802,"/**
 * Retrieves a CallQueue implementation class based on configuration.
 * @param prefix Prefix for the queue key.
 * @param conf Configuration object.
 * @return CallQueue class.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getQueueClass,"org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)",816,827,"/**
 * Retrieves a BlockingQueue class for IPC call queue.
 * @param namespace IPC namespace.
 * @param port Port number.
 * @param conf Configuration object.
 * @return Class extending BlockingQueue.
 */","* Return class configured by property 'ipc.<port>.callqueue.impl' if it is
   * present. If the config is not present, default config (without port) is
   * used to derive class i.e 'ipc.callqueue.impl', and derived class is
   * returned if class value is present and valid. If default config is also
   * not present, default class {@link LinkedBlockingQueue} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Class returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getSchedulerClass,"org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,org.apache.hadoop.conf.Configuration)",829,853,"/**
* Resolves and configures the RpcScheduler implementation.
* @param prefix Prefix for configuration keys.
* @param conf Configuration object.
* @return RpcScheduler instance.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getSchedulerClass,"org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)",869,898,"/**
 * Retrieves the RpcScheduler class based on configuration.
 * @param namespace IPC namespace. @param port Port number. @param conf Configuration object.
 */","* Return class configured by property 'ipc.<port>.scheduler.impl' if it is
   * present. If the config is not present, and if property
   * 'ipc.<port>.callqueue.impl' represents FairCallQueue class,
   * return DecayRpcScheduler. If config 'ipc.<port>.callqueue.impl'
   * does not have value FairCallQueue, default config (without port) is used
   * to derive class i.e 'ipc.scheduler.impl'. If default config is also not
   * present, default class {@link DefaultRpcScheduler} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Class returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClass,"org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class,java.lang.Class)",2758,2772,"/**
 * Finds a class by name, validates it against an interface.
 * @param name Class name to find.
 * @param defaultValue Default class if not found.
 * @param xface Interface to validate against.
 * @return Class extending U or null if not found/invalid.
 */
","* Get the value of the <code>name</code> property as a <code>Class</code>
   * implementing the interface specified by <code>xface</code>.
   *   
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * An exception is thrown if the returned class does not implement the named
   * interface. 
   * 
   * @param name the conf key name.
   * @param defaultValue default value.
   * @param xface the interface implemented by the named class.
   * @param <U> Interface class type.
   * @return property value as a <code>Class</code>, 
   *         or <code>defaultValue</code>.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getInternal,"org.apache.hadoop.fs.FileSystem$Cache:getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key)",3689,3765,"/**
 * Retrieves a FileSystem, creating it if necessary.
 * @param uri URI of the filesystem
 * @param conf Configuration object
 * @param key Key associated with the filesystem
 * @return FileSystem object
 */","* Get the FS instance if the key maps to an instance, creating and
     * initializing the FS if it is not found.
     * If this is the first entry in the map and the JVM is not shutting down,
     * this registers a shutdown hook to close filesystems, and adds this
     * FS to the {@code toAutoClose} set if {@code ""fs.automatic.close""}
     * is set in the configuration (default: true).
     * @param uri filesystem URI
     * @param conf configuration
     * @param key key to store/retrieve this FileSystem in the cache
     * @return a cached or newly instantiated FileSystem.
     * @throws IOException If an I/O error occurred.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration),81,102,"/**
 * Constructs a SQLDelegationTokenSecretManager with configuration.
 * @param conf Hadoop configuration object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,setConf,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),61,72,"/**
 * Calls super.m1 and sets timeout from configuration.
 * @param conf Hadoop configuration object.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,getShutdownTimeout,org.apache.hadoop.util.ShutdownHookManager:getShutdownTimeout(org.apache.hadoop.conf.Configuration),180,191,"/**
 * Calculates service shutdown duration from configuration.
 * @param conf Configuration object; returns duration in milliseconds.
 */","* Get the shutdown timeout in seconds, from the supplied
   * configuration.
   * @param conf configuration to use.
   * @return a timeout, always greater than or equal to {@link #TIMEOUT_MINIMUM}",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,getCredentialProvider,org.apache.hadoop.security.alias.CredentialShell$Command:getCredentialProvider(),144,166,"/**
* Retrieves a CredentialProvider, prioritizing user-supplied ones.
* Returns the provider or null if none are valid.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPasswordFromCredentialProviders,org.apache.hadoop.conf.Configuration:getPasswordFromCredentialProviders(java.lang.String),2478,2506,"/**
* Retrieves password for a given name from credential providers.
* @param name Name of the credential to retrieve.
* @return Password as a char array, or null if not found.
*/","* Try and resolve the provided element name as a credential provider
   * alias.
   * @param name alias of the provisioned credential
   * @return password or null if not found
   * @throws IOException when error in fetching password",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,getKeyProvider,org.apache.hadoop.crypto.key.KeyShell$Command:getKeyProvider(),191,213,"/**
* Retrieves a KeyProvider, prioritizing user-supplied if available.
* Returns the provider or null if none are valid.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java,accept,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:accept(java.lang.Class),55,63,"/**
* Checks if a class is Avro serializable.
* @param c Class to check. Returns true if serializable.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawEncoderWithFallback,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",176,202,"/**
 * Creates a RawErasureEncoder using the provided configuration.
 * @param conf Configuration object.
 * @param codecName Codec name.
 * @param coderOptions ErasureCoderOptions object.
 * @return RawErasureEncoder instance.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawDecoderWithFallback,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",204,230,"/**
 * Creates a RawErasureDecoder using the provided codec and options.
 * @param conf Configuration object.
 * @param codecName Codec name.
 * @param coderOptions ErasureCoderOptions object.
 * @return RawErasureDecoder instance.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,createSession,"org.apache.hadoop.ha.SshFenceByTcpPort:createSession(java.lang.String,org.apache.hadoop.ha.SshFenceByTcpPort$Args)",118,128,"/**
 * Establishes an SSH session with the given host and args.
 * @param host The SSH host to connect to.
 * @param args SSH connection arguments.
 * @return An SSH session object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,refresh,org.apache.hadoop.security.authorize.ProxyServers:refresh(),31,33,"/**
 * Calls m1 with a default Configuration object.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,parseCapacityWeights,"org.apache.hadoop.ipc.CallQueueManager:parseCapacityWeights(int,java.lang.String,org.apache.hadoop.conf.Configuration)",417,440,"/**
 * Retrieves capacity weights for IPC call queue, using config or defaults.
 * @param priorityLevels Number of priority levels.
 * @param ns Namespace.
 * @param conf Configuration object.
 * @return Array of capacity weights.
 */","* Read the weights of capacity in callqueue and pass the value to
   * callqueue constructions.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,<init>,"org.apache.hadoop.ipc.metrics.RpcMetrics:<init>(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)",58,111,"/**
 * Initializes RPC metrics registry with server and configuration.
 * @param server RPC server instance.
 * @param conf Configuration object for metrics setup.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseThresholds,"org.apache.hadoop.ipc.DecayRpcScheduler:parseThresholds(java.lang.String,org.apache.hadoop.conf.Configuration,int)",379,407,"/**
 * Calculates decay thresholds as decimals from configuration.
 * @param ns namespace, conf configuration, numLevels levels
 * @return Array of decay threshold decimals.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,<init>,"org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)",56,79,"/**
 * Creates a WeightedRoundRobinMultiplexer with specified queues and weights.
 * @param aNumQueues Number of queues.
 * @param ns Namespace for configuration.
 * @param conf Hadoop configuration.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseBackOffResponseTimeThreshold,"org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffResponseTimeThreshold(java.lang.String,org.apache.hadoop.conf.Configuration,int)",433,456,"/**
 * Retrieves response time thresholds from config, or generates defaults.
 * @param ns namespace, conf configuration, numLevels priority levels
 * @return Array of response time thresholds.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getFilterInitializers,org.apache.hadoop.http.HttpServer2:getFilterInitializers(org.apache.hadoop.conf.Configuration),894,916,"/**
* Creates FilterInitializer array from config, ensuring specific initializers are present.
*/",Get an array of FilterConfiguration specified in the conf,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInstances,"org.apache.hadoop.conf.Configuration:getInstances(java.lang.String,java.lang.Class)",2787,2798,"/**
 * Retrieves a list of objects implementing the given interface.
 * @param name Name to lookup.
 * @param xface Interface to check.
 * @return List of objects implementing the interface.
 */
","* Get the value of the <code>name</code> property as a <code>List</code>
   * of objects implementing the interface specified by <code>xface</code>.
   * 
   * An exception is thrown if any of the classes does not exist, or if it does
   * not implement the named interface.
   * 
   * @param name the property name.
   * @param xface the interface implemented by the classes named by
   *        <code>name</code>.
   * @param <U> Interface class type.
   * @return a <code>List</code> of objects implementing <code>xface</code>.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,<init>,"org.apache.hadoop.io.DefaultStringifier:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)",60,73,"/**
 * Initializes a Stringifier with a configuration and class.
 * @param conf Configuration object.
 * @param c Class to serialize/deserialize.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,init,"org.apache.hadoop.io.SequenceFile$Writer:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,boolean,java.lang.Class,java.lang.Class,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,int)",1292,1354,"/**
* Initializes data writing context with provided configurations.
* Sets up serializers, compression, and prepares for data writing.
*/",Initialize.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,getFactory,org.apache.hadoop.util.ReflectionUtils:getFactory(org.apache.hadoop.conf.Configuration),330,335,"/**
 * Returns the SerializationFactory, creating it if it's null.
 * @param conf Configuration object for factory initialization.
 * @return SerializationFactory instance.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/IngressPortBasedResolver.java,setConf,org.apache.hadoop.security.IngressPortBasedResolver:setConf(org.apache.hadoop.conf.Configuration),65,79,"/**
* Configures port-to-QOP mapping based on INGRESS_PORT_SASL_CONFIGURED_PORTS.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,setConf,org.apache.hadoop.security.WhitelistBasedResolver:setConf(org.apache.hadoop.conf.Configuration),91,109,"/**
 * Initializes the IP white list and SASL properties from configuration.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,propagateOptions,"org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",159,166,"/**
 * Delegates to FutureIO.m1, passing all arguments unchanged.
 */","* Propagate options to any builder.
   * {@link FutureIO#propagateOptions(FSBuilder, Configuration, String, boolean)}
   * @param builder builder to modify
   * @param conf configuration to read
   * @param prefix prefix to scan/strip
   * @param mandatory are the options to be mandatory or optional?",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,propagateOptions,"org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",330,340,"/**
 * Configures a builder with optional and mandatory prefixes.
 * @param builder The FSBuilder to configure.
 * @param conf Configuration object.
 */","* Propagate options to any builder, converting everything with the
   * prefix to an option where, if there were 2+ dot-separated elements,
   * it is converted to a schema.
   * See {@link #propagateOptions(FSBuilder, Configuration, String, boolean)}.
   * @param builder builder to modify
   * @param conf configuration to read
   * @param optionalPrefix prefix for optional settings
   * @param mandatoryPrefix prefix for mandatory settings
   * @param <T> type of result
   * @param <U> type of builder
   * @return the builder passed in.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,getChangedProperties,"org.apache.hadoop.conf.ReconfigurableBase:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",99,103,"/**
* Delegates reconfiguration to the reconfigurationUtil.
* @param newConf The new configuration.
* @param oldConf The old configuration.
*/
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,doGet,"org.apache.hadoop.conf.ReconfigurationServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",199,212,"/**
 * Handles GET requests, processes data, and writes output.
 * @param req HTTP request object
 * @param resp HTTP response object
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/NativeLibraryChecker.java,main,org.apache.hadoop.util.NativeLibraryChecker:main(java.lang.String[]),45,156,"/**
 * Checks for native libraries; exits with code 1 if any fail.
 * Command-line args control what is checked.
 */","* A tool to test native library availability.
   * @param args args.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,getCompressorType,org.apache.hadoop.io.compress.BZip2Codec:getCompressorType(),137,140,"/**
* Returns the Bzip2 compressor class based on the configuration.
*/","* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of compressor needed by this codec.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,getDecompressorType,org.apache.hadoop.io.compress.BZip2Codec:getDecompressorType(),218,221,"/**
 * Returns the Bzip2Decompressor class using the provided configuration.
 */","* Get the type of {@link Decompressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of decompressor needed by this codec.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createDecompressor,org.apache.hadoop.io.compress.BZip2Codec:createDecompressor(),228,231,"/**
* Creates and returns a Bzip2Decompressor using the configuration.
*/","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping:reloadCachedMappings(),82,86,"/**
* Calls super.m1() and then calls m1() on m2().
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings(java.util.List),162,167,"/**
 * Calls m1() without arguments.
 * @param names Unused list of strings.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,<init>,org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String),510,512,"/**
 * Constructs a HadoopZookeeperFactory with a Zookeeper principal.
 * @param zkPrincipal Zookeeper principal string.
 */
","* Constructor for the helper class to configure the ZooKeeper client connection.
     * @param zkPrincipal Optional.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,lookupGroup,"org.apache.hadoop.security.LdapGroupsMapping:lookupGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext,int)",438,475,"/**
 * Retrieves group names for a search result, handling different scenarios.
 * @param result SearchResult object.
 * @param c DirContext object.
 * @param goUpHierarchy Hierarchy level to traverse.
 * @return Set of group names.
 */","* Perform the second query to get the groups of the user.
   *
   * If posixGroups is enabled, use use posix gid/uid to find.
   * Otherwise, use the general group member attribute to find it.
   *
   * @param result the result object returned from the prior user lookup.
   * @param c the context object of the LDAP connection.
   * @return a list of strings representing group names of the user.
   * @throws NamingException if unable to find group names",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HadoopKerberosName.java,main,org.apache.hadoop.security.HadoopKerberosName:main(java.lang.String[]),87,93,"/**
 * Processes arguments, converts to HadoopKerberosName, and prints.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getInstance,org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration),99,103,"/**
 * Creates a CryptoCodec using the configured cipher suite name.
 * @param conf Configuration object; retrieves cipher suite name.
 * @return CryptoCodec instance.
 */
","* Get crypto codec for algorithm/mode/padding in config value
   * hadoop.security.crypto.cipher.suite
   * 
   * @param conf
   *          the configuration
   * @return CryptoCodec the codec object Null value will be returned if no
   *         crypto codec classes with cipher suite configured.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,bind,"org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int)",685,688,"/**
 * Overloads m1 with null values for the timeout and selector.
 */","* A convenience method to bind to a given address and report 
   * better exceptions if the address is not a valid host.
   * @param socket the socket to bind
   * @param address the address to bind to
   * @param backlog the number of connections allowed in the queue
   * @throws BindException if the address can't be bound
   * @throws UnknownHostException if the address isn't a valid host name
   * @throws IOException other random errors from bind",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,"org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer)",3642,3645,"/**
* Calls m1 with a null value for the third parameter.
* @param propertyName Property name (can be null).
* @param out Writer to write to.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpConfiguration,"org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String,java.io.Writer)",3787,3804,"/**
* Dumps a configuration property to a writer, or throws an exception.
* @param config Configuration object
* @param propertyName Property name to dump
* @param out Writer to write to
*/","*  Writes properties and their attributes (final and resource)
   *  to the given {@link Writer}.
   *  <ul>
   *  <li>
   *  When propertyName is not empty, and the property exists
   *  in the configuration, the format of the output would be,
   *  <pre>
   *  {
   *    ""property"": {
   *      ""key"" : ""key1"",
   *      ""value"" : ""value1"",
   *      ""isFinal"" : ""key1.isFinal"",
   *      ""resource"" : ""key1.resource""
   *    }
   *  }
   *  </pre>
   *  </li>
   *
   *  <li>
   *  When propertyName is null or empty, it behaves same as
   *  {@link #dumpConfiguration(Configuration, Writer)}, the
   *  output would be,
   *  <pre>
   *  { ""properties"" :
   *      [ { key : ""key1"",
   *          value : ""value1"",
   *          isFinal : ""key1.isFinal"",
   *          resource : ""key1.resource"" },
   *        { key : ""key2"",
   *          value : ""value2"",
   *          isFinal : ""ke2.isFinal"",
   *          resource : ""key2.resource"" }
   *       ]
   *   }
   *  </pre>
   *  </li>
   *
   *  <li>
   *  When propertyName is not empty, and the property is not
   *  found in the configuration, this method will throw an
   *  {@link IllegalArgumentException}.
   *  </li>
   *  </ul>
   *  <p>
   * @param config the configuration
   * @param propertyName property name
   * @param out the Writer to write to
   * @throws IOException raised on errors performing I/O.
   * @throws IllegalArgumentException when property name is not
   *   empty and the property is not found in configuration
   *",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,formatZK,"org.apache.hadoop.ha.ZKFailoverController:formatZK(boolean,boolean)",282,299,"/**
 * Executes elector functionality, returns 0 on success, error code otherwise.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSystemSource,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSystemSource(),561,567,"/**
* Initializes and starts the metrics source adapter.
* Uses sourceConfigs or fallback config for configuration.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,java.lang.Object)",221,243,"/**
* Processes metrics source, sets name/desc, registers source.
* @param name Metric name, uses source description if null.
* @param desc Metric description, used if name is null.
* @param source The source object.
* @return The original source object.
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirs,org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path),808,811,"/**
* Delegates file processing to m1.
* @param f The Path object to process.
* @return True if processing succeeds, false otherwise.
*/","* Creates the specified directory hierarchy. Does not
   * treat existence as an error.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirs,"org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",813,816,"/**
* Sets the specified permission on the given file path.
* @param f the file path
* @param permission the permission to set
* @return true if successful, false otherwise
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,build,org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:build(),728,748,"/**
* Creates a data output stream with specified flags and options.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/BouncyCastleFipsKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",41,44,"/**
 * Initializes the FIPS key store provider with URI and configuration.
 * @param uri URI of the keystore.
 * @param conf Configuration object.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/JavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",40,43,"/**
 * Initializes a JavaKeyStoreProvider with a URI and configuration.
 * @param uri The URI of the keystore.
 * @param conf The configuration object.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalJavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",40,43,"/**
 * Initializes a LocalJavaKeyStoreProvider with URI and configuration.
 * @param uri The URI of the keystore.
 * @param conf Configuration object.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalBouncyCastleFipsKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",41,44,"/**
 * Initializes the FIPS keystore provider with URI and config.
 * @param uri keystore URI
 * @param conf configuration object
 * @throws IOException if an I/O error occurs
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,initSpnego,"org.apache.hadoop.http.HttpServer2:initSpnego(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Properties,java.lang.String,java.lang.String)",1356,1375,"/**
 * Registers a Kerberos authentication filter with the web app context.
 * @param conf Configuration object, auth prefixes, username/keytab keys.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java,getFilterConfigMap,"org.apache.hadoop.security.AuthenticationFilterInitializer:getFilterConfigMap(org.apache.hadoop.conf.Configuration,java.lang.String)",66,91,"/**
 * Configures filter properties based on configuration and prefix.
 * @param conf Configuration object.
 * @param prefix Prefix for configuration properties.
 * @return Map of filter configuration properties.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,createCuratorClient,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:createCuratorClient(org.apache.hadoop.conf.Configuration,java.lang.String)",185,241,"/**
 * Creates a CuratorFramework instance with configurations from the given config.
 * @param conf Configuration object.
 * @param namespace ZK namespace.
 * @return CuratorFramework instance.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,setJaasConfiguration,org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:setJaasConfiguration(org.apache.zookeeper.client.ZKClientConfig),585,597,"/**
* Configures JAAS for ZKClient. Requires kerberos principal & keytab.
* @param zkClientConfig ZKClient configuration object.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getServerPrincipal,org.apache.hadoop.security.SaslRpcClient:getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),303,356,"/**
 * Retrieves the server principal, validating against configuration.
 * @param authType Authentication type.
 * @return Server principal string.
 */","* Get the remote server's principal.  The value will be obtained from
   * the config and cross-checked against the server's advertised principal.
   * 
   * @param authType of the SASL client
   * @return String of the server's principal
   * @throws IOException - error determining configured principal",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,initProtocolMetaInfo,org.apache.hadoop.ipc.RPC$Server:initProtocolMetaInfo(org.apache.hadoop.conf.Configuration),1200,1209,"/**
 * Registers the Protocol Info service with the RPC framework.
 * @param conf Hadoop configuration object.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKeyProvider,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:createKeyProvider(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",231,242,"/**
* Creates a KeyProvider based on token service or KMSUtil config.
* @param token Token object containing service info.
* @param conf Configuration object.
* @return KeyProvider or null if URI is null.
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)",100,140,"/**
 * Initializes the load balancing KMS client provider.
 * @param uri Token service URI, providers, seed, config.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,decodeHarURI,"org.apache.hadoop.fs.HarFileSystem:decodeHarURI(java.net.URI,org.apache.hadoop.conf.Configuration)",218,254,"/**
 * Constructs a Har URI from a raw URI and configuration.
 * @param rawURI The raw URI to process.
 * @param conf Configuration object.
 * @return A constructed URI or default URI if invalid.
 */
","* decode the raw URI to get the underlying URI
   * @param rawURI raw Har URI
   * @return filtered URI of the underlying fileSystem",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration),288,290,"/**
* Creates a FileSystem instance.
* @param conf Hadoop configuration object
* @return FileSystem instance
*/
","* Returns the configured FileSystem implementation.
   * @param conf the configuration to use
   * @return FileSystem.
   * @throws IOException If an I/O error occurred.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,initialize,"org.apache.hadoop.fs.FileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",339,350,"/**
 * Initializes file system access based on URI and configuration.
 * @param name URI representing the file system.
 * @param conf Configuration object for file system access.
 */","* Initialize a FileSystem.
   *
   * Called after the new FileSystem instance is constructed, and before it
   * is ready for use.
   *
   * FileSystem implementations overriding this method MUST forward it to
   * their superclass, though the order in which it is done, and whether
   * to alter the configuration before the invocation are options of the
   * subclass.
   * @param name a URI whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration
   * @throws IOException on any failure to initialize this instance.
   * @throws IllegalArgumentException if the URI is considered invalid.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,org.apache.hadoop.fs.FileSystem:newInstance(org.apache.hadoop.conf.Configuration),621,623,"/**
* Creates a FileSystem instance.
* @param conf Hadoop configuration object
* @return FileSystem object
*/
","* Returns a unique configured FileSystem implementation for the default
   * filesystem of the supplied configuration.
   * This always returns a new FileSystem object.
   * @param conf the configuration to use
   * @return the new FS instance
   * @throws IOException FS creation or initialization failure.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,checkPath,org.apache.hadoop.fs.FileSystem:checkPath(org.apache.hadoop.fs.Path),792,825,"/**
 * Validates a path's filesystem against the expected one.
 * @param path The path to validate. Throws exception if invalid.
 */","* Check that a Path belongs to this FileSystem.
   *
   * The base implementation performs case insensitive equality checks
   * of the URIs' schemes and authorities. Subclasses may implement slightly
   * different checks.
   * @param path to check
   * @throws IllegalArgumentException if the path is not considered to be
   * part of this FileSystem.
   *",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getSocketAddr,"org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)",2539,2556,"/**
 * Resolves host and address to an InetSocketAddress.
 * @param hostProperty Host property key.
 * @param addressProperty Address property key.
 * @param defaultAddressValue Default address value.
 * @param defaultPort Default port.
 * @return InetSocketAddress object.
 */
","* Get the socket address for <code>hostProperty</code> as a
   * <code>InetSocketAddress</code>. If <code>hostProperty</code> is
   * <code>null</code>, <code>addressProperty</code> will be used. This
   * is useful for cases where we want to differentiate between host
   * bind address and address clients should use to establish connection.
   *
   * @param hostProperty bind host property name.
   * @param addressProperty address property name.
   * @param defaultAddressValue the default value
   * @param defaultPort the default port
   * @return InetSocketAddress",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FutureDataInputStreamBuilder.java,build,org.apache.hadoop.fs.FutureDataInputStreamBuilder:build(),47,50,"/**
* Returns a CompletableFuture for a masked FSDataInputStream.
* @return CompletableFuture<FSDataInputStream>
* @throws Exceptions on failure
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,open,"org.apache.hadoop.fs.sftp.SFTPFileSystem:open(org.apache.hadoop.fs.Path,int)",507,539,"/**
 * Opens an SFTP data input stream for a given path.
 * @param f the path to open
 * @param bufferSize the buffer size
 * @return FSDataInputStream object
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,create,"org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",545,589,"/**
 * Creates a data output stream on a file, handling permissions/existence.
 * @param f Path of the file
 * @return FSDataOutputStream object
 */","* A stream obtained via this call must be closed before using other APIs of
   * this class or else the invocation will block.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,rename,"org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",603,612,"/**
* Copies a file from src to dst using SFTP.
* @param src Source file path.
* @param dst Destination file path.
* @return True on success, false otherwise.
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,delete,"org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",614,623,"/**
 * Transfers a file or directory using SFTP.
 * @param f Path to the file/directory.
 * @param recursive Recursive flag for directories.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,listStatus,org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(org.apache.hadoop.fs.Path),625,634,"/**
 * Retrieves file statuses for a given path using an SFTP client.
 * @param f The Path object representing the file or directory.
 * @return An array of FileStatus objects.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory(),657,673,"/**
 * Retrieves the home directory path via SFTP channel.
 * @return Path object representing the home directory, null on error.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",688,697,"/**
 * Sets file permissions using an SFTP client.
 * @param f Path to the file. @param permission File permissions.
 * @return True if successful, false otherwise.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path),699,708,"/**
 * Retrieves FileStatus for a given Path using an SFTP channel.
 * @param f Path to retrieve status for.
 * @return FileStatus object or null if not found.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,read,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:read(long,byte[],int,int)",230,246,"/**
 * Reads data from a file with checksum verification.
 * @param position file position, b buffer, off offset, len length
 * @return Number of bytes read.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",91,96,"/**
 * Constructs a FutureDataInputStreamBuilderImpl.
 * @param fileSystem The file system to use.
 * @param path The path to the data stream.
 */
","* Constructor.
   * @param fileSystem owner FS.
   * @param path path",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",103,108,"/**
 * Constructs a FutureDataInputStreamBuilderImpl.
 * @param fileSystem The file system.
 * @param pathHandle The path handle.
 */
","* Constructor with PathHandle.
   * @param fileSystem owner FS.
   * @param pathHandle path handle",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,openFileOnInstance,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFileOnInstance(org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)",472,498,"/**
 * Opens an FSDataInputStream based on DynamicWrappedIO's state.
 * @param instance DynamicWrappedIO instance, determines open method.
 * @param fs FileSystem to use.
 * @param status FileStatus to open.
 * @param readPolicies Read policies to apply.
 * @return FSDataInputStream opened.
 */","* Open a file.
   * <p>
   * If the WrappedIO class is found, uses
   * {@link #fileSystem_openFile(FileSystem, Path, String, FileStatus, Long, Map)} with
   * {@link #PARQUET_READ_POLICIES} as the list of read policies and passing down
   * the file status.
   * <p>
   * If not, falls back to the classic {@code fs.open(Path)} call.
   * @param instance dynamic wrapped IO instance.
   * @param fs filesystem
   * @param status file status
   * @param readPolicies read policy to use
   * @return the input stream
   * @throws IOException any IO failure.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,getInputStreamForFile,org.apache.hadoop.security.alias.KeyStoreProvider:getInputStreamForFile(),63,66,"/**
* Returns an input stream based on internal method calls.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,loadFromPath,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadFromPath(org.apache.hadoop.fs.Path,char[])",291,298,"/**
 * Retrieves FsPermission from a Path, using provided password.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkAppend,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkAppend(org.apache.hadoop.fs.FileSystem),483,495,"/**
 * Checks if the FileSystem supports appending.
 * @param fs The FileSystem to check.
 * @return True if appending is supported, false otherwise.
 */
","* Test whether the file system supports append and return the answer.
   *
   * @param fs the target file system",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)",2934,2937,"/**
 * Constructs a Sorter with a Metadata object.
 * @param fs FileSystem, comparator, key/val classes, and config.
 */
","* Sort and merge using an arbitrary {@link RawComparator}.
     * @param fs input FileSystem.
     * @param comparator input RawComparator.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(org.apache.hadoop.conf.Configuration),72,76,"/**
 * Constructs a Bzip2Compressor with default direct buffer size.
 * @param conf Hadoop configuration object.
 */
","* Creates a new compressor, taking settings from the configuration.
   * @param conf configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,reinit,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reinit(org.apache.hadoop.conf.Configuration),108,122,"/**
 * Reinitializes the compressor with configuration, or defaults.
 * @param conf Configuration object, or null for defaults.
 */","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's block size and
   * and work factor.
   * 
   * @param conf Configuration storing new settings",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createCompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",279,281,"/**
* Wraps an OutputStream with compression.
* @param downStream Wrapped OutputStream
* @param compressor Compressor to use
* @param downStreamBufferSize Buffer size
* @throws IOException if an I/O error occurs
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,init,org.apache.hadoop.io.SequenceFile$Reader:init(boolean),2022,2161,"/**
 * Reads SequenceFile header and initializes readers.
 * @param tempReader indicates if this is a temp reader
 * @throws IOException if an I/O error occurs
 */","* Initialize the {@link Reader}
     * @param tmpReader <code>true</code> if we are constructing a temporary
     *                  reader {@link SequenceFile.Sorter.cloneFileAttributes}, 
     *                  and hence do not initialize every component; 
     *                  <code>false</code> otherwise.
     * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createDecompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",275,277,"/**
* Wraps an InputStream with decompression.
* @param downStream Input stream to decompress.
* @param decompressor Decompression algorithm.
* @param downStreamBufferSize Buffer size for downstream.
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,reinit,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reinit(org.apache.hadoop.conf.Configuration),112,120,"/**
 * Initializes the compressor with a new configuration from the given config.
 * @param conf Configuration object containing compression settings.
 */","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's compression level
   * and compression strategy.
   *
   * @param conf Configuration storing new settings",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressionBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getCompressionBufferSize(org.apache.hadoop.conf.Configuration),94,99,"/**
* Determines buffer size. Returns default if buffer size is 0.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getDecompressionBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getDecompressionBufferSize(org.apache.hadoop.conf.Configuration),101,106,"/**
* Returns buffer size, using ZStandardDecompressor.m2() if 0.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,writeStreamToFile,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:writeStreamToFile(java.io.InputStream,org.apache.hadoop.fs.shell.PathData,boolean,boolean)",499,512,"/**
 * Copies data from an InputStream to a PathData, optionally lazy.
 * @param in Input stream to copy from.
 * @param target Destination PathData.
 * @param lazyPersist Whether to lazy persist.
 * @param direct Whether to directly write.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,printToStdout,org.apache.hadoop.fs.shell.Display$Cat:printToStdout(java.io.InputStream),98,104,"/**
* Copies input stream to output stream using a buffer.
* @param in Input stream to copy from.
* @throws IOException if an I/O error occurs.
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareAppendValue,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendValue(int),554,575,"/**
 * Creates a DataOutputStream for writing a value.
 * @param length Value length or -1 to use buffer.
 * @return DataOutputStream for writing the value.
 */","* Obtain an output stream for writing a value into TFile. This may only be
     * called right after a key appending operation (the key append stream must
     * be closed).
     * 
     * @param length
     *          The expected length of the value. If length of the value is not
     *          known, set length = -1. Otherwise, the application must write
     *          exactly as many bytes as specified here before calling close on
     *          the returned output stream. Advertising the value size up-front
     *          guarantees that the value is encoded in one chunk, and avoids
     *          intermediate chunk buffering.
     * @throws IOException raised on errors performing I/O.
     * @return DataOutputStream.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion,org.apache.hadoop.conf.Configuration)",496,513,"/**
 * Constructs a RBlockState with compression, region, and input stream.
 * @param compressionAlgo Compression algorithm.
 * @param fsin Input stream for the block.
 * @param region Block region info.
 * @param conf Hadoop configuration.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.conf.Configuration)",119,139,"/**
 * Initializes a WBlockState with compression settings and streams.
 * @param compressionAlgo Compression algorithm.
 * @param fsOut FSDataOutputStream for writing.
 */","* @param compressionAlgo
       *          The compression algorithm to be used to for compression.
       * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,setConf,org.apache.hadoop.net.ScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration),135,139,"/**
 * Calls super.m1 and then calls m1 on the result of m2().
 */","* {@inheritDoc}.
   * <p>
   * This will get called in the superclass constructor, so a check is needed
   * to ensure that the raw mapping is defined before trying to relaying a null
   * configuration.
   * </p>
   * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,setConf,org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration),130,138,"/**
 * Calls super.m1 and sets dependencyScriptName from config.
 * @param conf Configuration object; null if not provided.
 */","* Set the configuration and extract the configuration parameters of interest
     * @param conf the new configuration",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createHttpsChannelConnector,"org.apache.hadoop.http.HttpServer2$Builder:createHttpsChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)",583,632,"/**
 * Creates a ServerConnector with SSL configuration.
 * @param server The server to connect to.
 * @param httpConfig HTTP configuration.
 * @return ServerConnector object.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getDelegationToken(java.lang.String),251,264,"/**
 * Retrieves a token using a provider, updates it, and logs it.
 * @param renewer Token renewer string.
 * @return Token object.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,renewDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token),266,274,"/**
 * Calls m3 to execute a Long-returning function with a token.
 * @param token The token to pass to the provider.
 * @return Long value returned by the provider.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,cancelDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token),276,285,"/**
 * Executes a provider function with a token.
 * @param token The token to pass to the provider.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,generateEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:generateEncryptedKey(java.lang.String),326,344,"/**
 * Retrieves an EncryptedKeyVersion by name.
 * @param encryptionKeyName Key name to retrieve.
 * @throws IOException, GeneralSecurityException on failure.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),346,364,"/**
 * Decrypts an encrypted key version using a provider.
 * @param encryptedKeyVersion Encrypted key version to decrypt
 * @return Decrypted KeyVersion object
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),366,384,"/**
 * Retrieves an EncryptedKeyVersion using a provider.
 * @param ekv The EncryptedKeyVersion to retrieve.
 * @return The retrieved EncryptedKeyVersion.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKeys(java.util.List),386,404,"/**
 * Processes a list of encrypted key versions using a KMS provider.
 * @param ekvs List of encrypted key version objects to process.
 * @throws IOException, GeneralSecurityException if processing fails.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersion(java.lang.String),406,414,"/**
 * Retrieves a KeyVersion for the given version name.
 * @param versionName The version name to retrieve.
 * @return KeyVersion object.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeys(),416,424,"/**
 * Calls m3 with a ProviderCallable to fetch a list of strings.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeysMetadata,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeysMetadata(java.lang.String[]),426,434,"/**
 * Retrieves Metadata array based on provided names.
 * @param names Names to fetch Metadata for.
 * @return Metadata array.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersions(java.lang.String),436,445,"/**
 * Retrieves KeyVersions by name using a provider.
 * @param name The name to search for KeyVersions.
 * @return List of KeyVersion objects.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCurrentKey(java.lang.String),447,455,"/**
 * Retrieves a KeyVersion by name using a provider.
 * @param name KeyVersion name to retrieve.
 * @return KeyVersion object.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getMetadata,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getMetadata(java.lang.String),457,465,"/**
 * Retrieves metadata by name using a provider.
 * @param name Metadata name to retrieve.
 * @return Metadata object or null if not found.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",467,476,"/**
 * Creates a KeyVersion using a provider.
 * @param name Key name, material data, options.
 * @return KeyVersion object.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",478,495,"/**
 * Retrieves a KeyVersion by name and options.
 * @param name key version name
 * @param options key version options
 * @return KeyVersion object
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,options,org.apache.hadoop.crypto.key.KeyProvider:options(org.apache.hadoop.conf.Configuration),433,435,"/**
 * Creates an Options object using the provided configuration.
 * @param conf Configuration object used to initialize Options.
 * @return New Options object.
 */
","* A helper function to create an options object.
   * @param conf the configuration to use
   * @return a new options object",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",142,145,"/**
 * Constructs a CryptoInputStream with default buffer size.
 * @param in Input stream.
 * @param codec CryptoCodec for encryption/decryption.
 * @param key Encryption key.
 * @param iv Initialization vector.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long,boolean)",130,135,"/**
 * Constructs a CryptoOutputStream with a default buffer size.
 * @param out Output stream.
 * @param codec CryptoCodec.
 * @param key Encryption key.
 * @param iv Initialization vector.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$ConnectionId:<init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)",1679,1709,"/**
 * Constructs a ConnectionId with address, protocol, ticket, and configs.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getTimeout,org.apache.hadoop.ipc.Client:getTimeout(org.apache.hadoop.conf.Configuration),202,213,"/**
 * Returns the IPC client timeout from config, or defaults to -1.
 */","* The time after which a RPC will timeout.
   * If ping is not enabled (via ipc.client.ping), then the timeout value is the 
   * same as the pingInterval.
   * If ping is enabled, then there is no timeout value.
   * 
   * @param conf Configuration
   * @return the timeout period in milliseconds. -1 if no timeout value is set
   * @deprecated use {@link #getRpcTimeout(Configuration)} instead",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,parseMetaData,org.apache.hadoop.fs.HarFileSystem$HarMetaData:parseMetaData(),1166,1236,"/**
* Reads master and archive index files to populate stores and archive.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,<init>,"org.apache.hadoop.ha.FailoverController:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceProtocol$RequestSource)",61,80,"/**
 * Constructs a FailoverController with configuration and request source.
 * @param conf Configuration object.
 * @param source Request source.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,connect,org.apache.hadoop.fs.ftp.FTPFileSystem:connect(),141,167,"/**
 * Creates and configures an FTPClient with settings from config.
 * @return Configured FTPClient object.
 * @throws IOException if connection or login fails.
 */","* Connect to the FTP server using configuration parameters *
   * 
   * @return An FTPClient instance
   * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FSBuilderSupport.java,getPositiveLong,"org.apache.hadoop.fs.impl.FSBuilderSupport:getPositiveLong(java.lang.String,long)",61,69,"/**
 * Returns value for key, using defVal if negative.
 * @param key option key
 * @param defVal default value if key's value is negative
 * @return Value associated with the key.
 */
","* Get a long value with resilience to unparseable values.
   * Negative values are replaced with the default.
   * @param key key to log
   * @param defVal default value
   * @return long value",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(),1282,1286,"/**
* Delegates m1() call to the fs object.
* @return long value returned by fs.m1()
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getServerDefaults,org.apache.hadoop.fs.FileSystem:getServerDefaults(),939,954,"/**
 * Creates FsServerDefaults object using configuration.
 * @return FsServerDefaults instance with default values.
 */","* Return a set of server default configuration values.
   * @return server default configuration values
   * @throws IOException IO failure
   * @deprecated use {@link #getServerDefaults(Path)} instead",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),2766,2768,"/**
 * Delegates to a default implementation of m1().
 * @param f Unused Path object.
 * @return Long value returned by the default m1().
 */","* Return the number of bytes that large input files should be optimally
   * be split into to minimize I/O time.  The given path will be used to
   * locate the actual filesystem.  The full path does not have to exist.
   * @param f path of file
   * @return the default block size for the path's filesystem",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize(),426,429,"/**
* Delegates m1() call to the fs object.
* @return long value returned by fs.m1()
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,reportChecksumFailure,"org.apache.hadoop.fs.LocalFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)",99,149,"/**
* Moves a bad file to a ""bad_files"" directory for later inspection.
*/","* Moves files to a bad file directory on the same device, so that their
   * storage will not be reused.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,<init>,org.apache.hadoop.fs.DU:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),43,48,"/**
 * Constructs a DU object using provided builder parameters.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,<init>,org.apache.hadoop.fs.CachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),60,66,"/**
 * Constructs CachingGetSpaceUsed with values from the builder.
 * @param builder Builder object containing configuration values.
 */
","* This is the constructor used by the builder.
   * All overriding classes should implement this.
   *
   * @param builder builder.
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/WindowsGetSpaceUsed.java,<init>,org.apache.hadoop.fs.WindowsGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),34,40,"/**
 * Constructs a WindowsGetSpaceUsed object.
 * @param builder Builder object containing configuration.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getOwner,org.apache.hadoop.io.nativeio.NativeIO:getOwner(java.io.FileDescriptor),934,954,"/**
* Gets the owner of a file descriptor. Uses Windows or POSIX methods.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration),135,137,"/**
 * Constructs a ShellBasedIdMapping with default shell execution mode.
 * @param conf Configuration object for mapping parameters.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initHM,org.apache.hadoop.ha.ZKFailoverController:initHM(),323,328,"/**
 * Initializes and configures the health monitor with callbacks.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,<init>,"org.apache.hadoop.fs.TrashPolicyDefault:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)",79,82,"/**
 * Constructs a TrashPolicyDefault with a FileSystem and Configuration.
 * @param fs The FileSystem to operate on.
 * @param conf The configuration to use.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,close,org.apache.hadoop.fs.viewfs.ViewFileSystem:close(),1986,2007,"/**
 * Executes method m1, potentially invoking cache operations or iterating mounts.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.DataOutputBuffer),2565,2584,"/**
 * Reads a record from the input stream.
 * @param buffer Output buffer to write the record to.
 * @return Key length or -1 if end of file.
 */","@deprecated Call {@link #nextRaw(DataOutputBuffer,SequenceFile.ValueBytes)}.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,getDefaultRetryPolicy,"org.apache.hadoop.io.retry.RetryUtils:getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String,java.lang.String)",59,85,"/**
* Creates a RetryPolicy based on config, or defaults to TRY_ONCE_THEN_FAIL.
* @param remoteExceptionToRetry Exception to retry on.
*/","* Return the default retry policy set in conf.
   * 
   * If the value retryPolicyEnabledKey is set to false in conf,
   * use TRY_ONCE_THEN_FAIL.
   * 
   * Otherwise, get the MultipleLinearRandomRetry policy specified in the conf
   * and then
   * (1) use multipleLinearRandomRetry for
   *     - remoteExceptionToRetry, or
   *     - IOException other than RemoteException, or
   *     - ServiceException; and
   * (2) use TRY_ONCE_THEN_FAIL for
   *     - non-remoteExceptionToRetry RemoteException, or
   *     - non-IOException.
   *     
   *
   * @param conf configuration.
   * @param retryPolicyEnabledKey     conf property key for enabling retry
   * @param defaultRetryPolicyEnabled default retryPolicyEnabledKey conf value 
   * @param retryPolicySpecKey        conf property key for retry policy spec
   * @param defaultRetryPolicySpec    default retryPolicySpecKey conf value
   * @param remoteExceptionToRetry    The particular RemoteException to retry
   * @return the default retry policy.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCodec,org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:getCodec(),273,273,"/**
 * Gets the compression codec.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createDecompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",275,277,"/**
* Delegates input stream processing to a Decompressor.
* @param downStream Input stream to be decompressed
* @param decompressor Decompression logic
* @param downStreamBufferSize Buffer size for downstream
* @throws IOException If an I/O error occurs
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createCompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",279,281,"/**
* Wraps an OutputStream with compression.
* @param downStream Output stream to wrap.
* @param compressor Compressor to use.
* @param downStreamBufferSize Buffer size for downstream.
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,init,"org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:init(java.lang.String,java.lang.String)",59,64,"/**
 * Returns the JvmMetrics instance, initializing if necessary.
 * @param processName Process name.
 * @param sessionId Session identifier.
 * @return JvmMetrics object.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)",1349,1351,"/**
 * Constructs a Client with a Writable class and Configuration.
 * @param valueClass Class of Writable object to be handled.
 * @param conf Hadoop configuration object.
 */
","* Construct an IPC client with the default SocketFactory.
   * @param valueClass input valueClass.
   * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,"org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,java.lang.Class)",50,68,"/**
 * Retrieves a Client from the cache or creates a new one.
 * @param conf Configuration object
 * @param factory SocketFactory for client connections
 * @param valueClass Writable class for data transfer
 * @return Client object
 */
","* Construct &amp; cache an IPC client with the user-provided SocketFactory
   * if no cached client exists.
   * 
   * @param conf Configuration
   * @param factory SocketFactory for client socket
   * @param valueClass Class of the expected response
   * @return an IPC client",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>(org.apache.hadoop.conf.Configuration),73,85,"/**
 * Constructs a FsUrlStreamHandlerFactory with the given configuration.
 * Initializes FileSystem and sets up protocol handlers.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,createURLStreamHandler,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler(java.lang.String),87,111,"/**
* Creates or retrieves a URLStreamHandler for the given protocol.
* Returns handler or null if unknown.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,excludeIncompatibleCredentialProviders,"org.apache.hadoop.security.ProviderUtils:excludeIncompatibleCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.Class)",141,199,"/**
 * Updates credential provider path, filtering based on file system.
 * @param config Configuration object to modify.
 * @param fileSystemClass Class of the filesystem being used.
 */","* There are certain integrations of the credential provider API in
   * which a recursive dependency between the provider and the hadoop
   * filesystem abstraction causes a problem. These integration points
   * need to leverage this utility method to remove problematic provider
   * types from the existing provider path within the configuration.
   *
   * @param config the existing configuration with provider path
   * @param fileSystemClass the class which providers must be compatible
   * @return Configuration clone with new provider path
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,get,"org.apache.hadoop.fs.AbstractFileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)",263,266,"/**
* Creates a FileSystem instance for the given URI and configuration.
* @param uri URI of the filesystem.
* @param conf Configuration object.
* @return AbstractFileSystem instance.
*/
","* The main factory method for creating a file system. Get a file system for
   * the URI's scheme and authority. The scheme of the <code>uri</code>
   * determines a configuration property name,
   * <tt>fs.AbstractFileSystem.<i>scheme</i>.impl</tt> whose value names the
   * AbstractFileSystem class.
   * 
   * The entire URI and conf is passed to the AbstractFileSystem factory method.
   * 
   * @param uri for the file system to be created.
   * @param conf which is passed to the file system impl.
   * 
   * @return file system for the given URI.
   * 
   * @throws UnsupportedFileSystemException if the file system for
   *           <code>uri</code> is not supported.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,setConf,org.apache.hadoop.security.CompositeGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),138,145,"/**
* Initializes the component using the provided configuration.
* @param conf Configuration object to initialize with.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.RpcClientUtil:getProtocolMetaInfoProxy(java.lang.Object,org.apache.hadoop.conf.Configuration)",179,187,"/**
 * Retrieves ProtocolMetaInfoPB using RPC, using provided config.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,build,org.apache.hadoop.ipc.RPC$Builder:build(),975,991,"/**
 * Creates and configures a Server instance using provided configurations.
 * @return Configured Server object.
 */","* @return Build the RPC Server.
     * @throws IOException on error
     * @throws HadoopIllegalArgumentException when mandatory fields are not set",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicy.java,getInstance,"org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",139,146,"/**
 * Creates and initializes a TrashPolicy based on configuration.
 * @param conf Configuration object.
 * @param fs FileSystem object.
 * @param home Home directory Path.
 * @return TrashPolicy instance.
 */
","* Get an instance of the configured TrashPolicy based on the value
   * of the configuration parameter fs.trash.classname.
   *
   * @param conf the configuration to be used
   * @param fs the file system to be used
   * @param home the home directory
   * @return an instance of TrashPolicy
   * @deprecated Use {@link #getInstance(Configuration, FileSystem)} instead.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicy.java,getInstance,"org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",156,162,"/**
 * Creates and initializes a TrashPolicy instance.
 * @param conf Configuration object.
 * @param fs FileSystem object.
 * @return TrashPolicy object.
 */
","* Get an instance of the configured TrashPolicy based on the value
   * of the configuration parameter fs.trash.classname.
   *
   * @param conf the configuration to be used
   * @param fs the file system to be used
   * @return an instance of TrashPolicy",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getMountTableConfigLoader,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountTableConfigLoader(org.apache.hadoop.conf.Configuration),181,203,"/**
 * Loads a MountTableConfigLoader implementation from configuration.
 * @param conf Configuration object to retrieve loader class from.
 * @return MountTableConfigLoader instance.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getKlass,org.apache.hadoop.fs.GetSpaceUsed$Builder:getKlass(),74,89,"/**
 * Returns the GetSpaceUsed class, using conf if available.
 * @return Class extending GetSpaceUsed, or configured class.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getInstance,"org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.net.InnerNode$Factory)",77,83,"/**
 * Creates a NetworkTopology instance using the provided factory.
 * @param conf Configuration object.
 * @param factory InnerNode factory.
 * @return NetworkTopology object.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String)",68,75,"/**
 * Creates a DomainNameResolver using the provided configuration.
 * @param conf Configuration object.
 * @param configKey Key to retrieve resolver class.
 */","* This function gets the instance based on the config.
   *
   * @param conf Configuration
   * @param configKey config key name.
   * @return Domain name resolver.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getInstance,org.apache.hadoop.security.SaslPropertiesResolver:getInstance(org.apache.hadoop.conf.Configuration),52,58,"/**
 * Resolves and instantiates a SaslPropertiesResolver class.
 * @param conf Configuration object used to find the class.
 * @return An instance of the resolved SaslPropertiesResolver.
 */
","* Returns an instance of SaslPropertiesResolver.
   * Looks up the configuration to see if there is custom class specified.
   * Constructs the instance by passing the configuration directly to the
   * constructor to achieve thread safety using final fields.
   * @param conf configuration.
   * @return SaslPropertiesResolver",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,<init>,"org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)",104,153,"/**
 * Constructs a Groups object, initializing various configuration settings.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateSasl,org.apache.hadoop.security.KDiag:validateSasl(java.lang.String),733,748,"/**
* Resolves and loads a SaslPropertiesResolver class.
* @param saslPropsResolverKey Key for resolving the resolver.
*/","* Try to load the SASL resolver.
   * @param saslPropsResolverKey key for the SASL resolver",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getInstance,org.apache.hadoop.security.authorize.ProxyUsers:getInstance(org.apache.hadoop.conf.Configuration),47,53,"/**
 * Gets an impersonation provider class and instantiates it.
 * @param conf Configuration object; defines provider class.
 * @return ImpersonationProvider instance.
 */
","* Returns an instance of ImpersonationProvider.
   * Looks up the configuration to see if there is custom class specified.
   * @param conf
   * @return ImpersonationProvider",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),84,100,"/**
* Initializes the random number generator using configuration.
* @param conf Hadoop configuration object.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,<init>,"org.apache.hadoop.util.ShutdownHookManager$HookEntry:<init>(java.lang.Runnable,int)",205,209,"/**
 * Creates a HookEntry with default shutdown timeout.
 * @param hook The runnable to execute.
 * @param priority Hook priority.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,shutdownExecutor,org.apache.hadoop.util.ShutdownHookManager:shutdownExecutor(org.apache.hadoop.conf.Configuration),142,162,"/**
 * Shuts down the ShutdownHookManager, handling timeout and interruption.
 * @param conf Configuration object for shutdown parameters.
 */","* Shutdown the executor thread itself.
   * @param conf the configuration containing the shutdown timeout setting.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPasswordFromCredentialProviders,"org.apache.hadoop.security.LdapGroupsMapping:getPasswordFromCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",893,906,"/**
 * Retrieves password for given alias from config, or uses default.
 * @param config Configuration object.
 * @param alias Password alias.
 * @param defaultPass Default password to use if not found.
 * @return Password string.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPassword,org.apache.hadoop.conf.Configuration:getPassword(java.lang.String),2418,2428,"/**
 * Retrieves password characters from m1 or m2 based on name.
 * @param name The name to use for password retrieval.
 * @return char array containing password or null if both fail.
 */
","* Get the value for a known password configuration element.
   * In order to enable the elimination of clear text passwords in config,
   * this method attempts to resolve the property name as an alias through
   * the CredentialProvider API and conditionally fallsback to config.
   * @param name property name
   * @return password
   * @throws IOException when error in fetching password",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawEncoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",131,137,"/**
 * Creates a RawErasureEncoder with given config, codec, and options.
 */","* Create RS raw encoder according to configuration.
   * @param conf configuration
   * @param coderOptions coder options that's used to create the coder
   * @param codec the codec to use. If null, will use the default codec
   * @return raw encoder",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawDecoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",146,152,"/**
 * Creates a RawErasureDecoder with given config, codec, and options.
 */","* Create RS raw decoder according to configuration.
   * @param conf configuration
   * @param coderOptions coder options that's used to create the coder
   * @param codec the codec to use. If null, will use the default codec
   * @return raw decoder",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,tryFence,"org.apache.hadoop.ha.SshFenceByTcpPort:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",80,115,"/**
 * Attempts to fence a target host using SSH.
 * @param target Service target with address info.
 * @param argsStr String containing SSH connection arguments.
 * @return True if fencing successful, false otherwise.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,isProxyServer,org.apache.hadoop.security.authorize.ProxyServers:isProxyServer(java.lang.String),47,52,"/**
 * Checks if a remote address is proxied.
 * @param remoteAddr The remote address to check.
 * @return True if the address is proxied, false otherwise.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,<init>,"org.apache.hadoop.ipc.CallQueueManager:<init>(java.lang.Class,java.lang.Class,boolean,int,java.lang.String,org.apache.hadoop.conf.Configuration)",78,96,"/**
 * Creates a CallQueueManager with specified configurations.
 * @param backingClass Queue implementation class.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,swapQueue,"org.apache.hadoop.ipc.CallQueueManager:swapQueue(java.lang.Class,java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration)",464,495,"/**
 * Initializes a new RpcScheduler and BlockingQueue.
 * @param schedulerClass Scheduler class, queue class, size, ns, config.
 */","* Replaces active queue with the newly requested one and transfers
   * all calls to the newQ before returning.
   *
   * @param schedulerClass input schedulerClass.
   * @param queueClassToUse input queueClassToUse.
   * @param maxSize input maxSize.
   * @param ns input ns.
   * @param conf input configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,create,"org.apache.hadoop.ipc.metrics.RpcMetrics:create(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)",115,118,"/**
 * Creates and registers RpcMetrics for the server.
 * @param server The server instance.
 * @param conf The configuration object.
 * @return Registered RpcMetrics object.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,int[],boolean,org.apache.hadoop.conf.Configuration)",119,154,"/**
 * Creates a FairCallQueue with specified priority levels, capacity,
 * and configuration.
 */","* Create a FairCallQueue.
   * @param priorityLevels the total size of all multi-level queue
   *                       priority policies
   * @param capacity the total size of all sub-queues
   * @param ns the prefix to use for configuration
   * @param capacityWeights the weights array for capacity allocation
   *                        among subqueues
   * @param serverFailOverEnabled whether or not to enable callqueue overflow trigger failover
   *                              for stateless servers when RPC call queue is filled
   * @param conf the configuration to read from
   * Notes: Each sub-queue has a capacity of `capacity / numSubqueues`.
   * The first or the highest priority sub-queue has an excess capacity
   * of `capacity % numSubqueues`",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,initializeWebServer,"org.apache.hadoop.http.HttpServer2:initializeWebServer(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String[])",726,795,"/**
 * Configures the web server with specified name, host, and paths.
 * @param name Server name, host, config, and path specifications.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseCostProvider,"org.apache.hadoop.ipc.DecayRpcScheduler:parseCostProvider(java.lang.String,org.apache.hadoop.conf.Configuration)",281,309,"/**
 * Retrieves a CostProvider based on namespace and configuration.
 * Returns DefaultCostProvider if none are found.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseIdentityProvider,"org.apache.hadoop.ipc.DecayRpcScheduler:parseIdentityProvider(java.lang.String,org.apache.hadoop.conf.Configuration)",312,337,"/**
* Retrieves an IdentityProvider based on configuration, or defaults.
* @param ns namespace
* @param conf configuration
* @return IdentityProvider instance
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,store,"org.apache.hadoop.io.DefaultStringifier:store(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.String)",110,117,"/**
* Writes item to configuration using a stringifier.
* @param conf Configuration object.
* @param item Item to be written.
* @param keyName Key name in configuration.
*/
","* Stores the item in the configuration with the given keyName.
   * 
   * @param <K>  the class of the item
   * @param conf the configuration to store
   * @param item the object to be stored
   * @param keyName the name of the key to use
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,load,"org.apache.hadoop.io.DefaultStringifier:load(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)",130,140,"/**
 * Retrieves a value from configuration, stringifies, and returns.
 * @param conf Configuration object.
 * @param keyName Key name in configuration.
 * @param itemClass Class of the value to retrieve.
 */","* Restores the object from the configuration.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use
   * @param keyName the name of the key to use
   * @param itemClass the class of the item
   * @return restored object
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,storeArray,"org.apache.hadoop.io.DefaultStringifier:storeArray(org.apache.hadoop.conf.Configuration,java.lang.Object[],java.lang.String)",153,171,"/**
 * Masks items using a stringifier and stores the result in config.
 * @param conf Configuration object.
 * @param items Items to mask.
 * @param keyName Key name for storing the masked string.
 */","* Stores the array of items in the configuration with the given keyName.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use 
   * @param items the objects to be stored
   * @param keyName the name of the key to use
   * @throws IndexOutOfBoundsException if the items array is empty
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,loadArray,"org.apache.hadoop.io.DefaultStringifier:loadArray(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)",184,203,"/**
 * Retrieves an array of items from configuration.
 * @param conf Configuration object.
 * @param keyName Key name in configuration.
 * @param itemClass Class of the items in the array.
 * @return Array of items or null if not found.
 */","* Restores the array of objects from the configuration.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use
   * @param keyName the name of the key to use
   * @param itemClass the class of the item
   * @return restored object
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",1257,1266,"/**
 * Creates a Writer with specified configuration and parameters.
 * @param fs FileSystem, Configuration, Path, classes, sizes, etc.
 */
","* Create the named file with write-progress reporter.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param bufferSize input bufferSize.
     * @param replication input replication.
     * @param blockSize input blockSize.
     * @param progress input progress.
     * @param metadata input metadata.
     * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,copy,"org.apache.hadoop.util.ReflectionUtils:copy(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.Object)",346,361,"/**
* Serializes src to buffer, deserializes to dst.
* @param conf Configuration object.
* @param src Source object to serialize.
* @param dst Destination object to deserialize to.
* @return Deserialized destination object.
*/
","* Make a copy of the writable object using serialization to a buffer.
   * @param src the object to copy from
   * @param dst the object to copy into, which is destroyed
   * @param <T> Generics Type.
   * @param conf configuration.
   * @return dst param (the copy)
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,propagateOptions,"org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",140,149,"/**
 * Delegates FSBuilder creation to FutureIO.
 * @param builder The FSBuilder instance.
 * @param conf Configuration object.
 * @param optionalPrefix Optional prefix string.
 * @param mandatoryPrefix Mandatory prefix string.
 */","* Propagate options to any builder.
   * {@link FutureIO#propagateOptions(FSBuilder, Configuration, String, String)}
   * @param builder builder to modify
   * @param conf configuration to read
   * @param optionalPrefix prefix for optional settings
   * @param mandatoryPrefix prefix for mandatory settings
   * @param <T> type of result
   * @param <U> type of builder
   * @return the builder passed in.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,run,org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:run(),116,162,"/**
* Applies configuration changes, logs results, and updates status.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,doGetGroups,"org.apache.hadoop.security.LdapGroupsMapping:doGetGroups(java.lang.String,int)",509,557,"/**
 * Retrieves a set of group names for a given user.
 * @param user User's DN.
 * @param goUpHierarchy Flag to search parent groups.
 * @return Set of group names.
 */","* Perform LDAP queries to get group names of a user.
   *
   * Perform the first LDAP query to get the user object using the user's name.
   * If one-query is enabled, retrieve the group names from the user object.
   * If one-query is disabled, or if it failed, perform the second query to
   * get the groups.
   *
   * @param user user name
   * @return a list of group names for the user. If the user can not be found,
   * return an empty string array.
   * @throws NamingException if unable to get group names",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,generateEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(java.lang.String),285,305,"/**
 * Retrieves an EncryptedKeyVersion for a given key name.
 * @param encryptionKeyName Name of the encryption key.
 * @return EncryptedKeyVersion object.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKeys(java.util.List),356,408,"/**
 * Processes a list of encrypted key versions, updating them.
 * @param ekvs List of encrypted key versions to process.
 * @throws IOException, GeneralSecurityException if errors occur.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),433,457,"/**
* Decrypts an encrypted key version and returns the KeyVersion.
* @param encryptedKeyVersion Encrypted key version to decrypt
* @return Decrypted KeyVersion object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,org.apache.hadoop.conf.Configuration:writeXml(java.io.Writer),3593,3595,"/**
 * Calls the overloaded method with null as the first argument.
 * @param out Writer object for output.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,writeResponse,"org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String,java.lang.String)",95,105,"/**
 * Writes property to Writer in specified format.
 * @param conf Configuration object.
 * @param out Writer to write to.
 * @param format Output format (JSON or XML).
 * @param propertyName Property to write.
 */",* Guts of the servlet - extracted for easy testing.,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSources,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSources(),539,543,"/**
* Initializes source filter and configs using provided configuration.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)",555,568,"/**
* Creates a data output stream for a file.
* @param f Path of the file
* @param overwrite Whether to overwrite existing files
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createSymlink,"org.apache.hadoop.fs.RawLocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1179,1202,"/**
* Creates a symbolic link.
* @param target Target path for the link.
* @param link Link path.
* @param createParent Whether to create parent directories.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getFilterProperties,"org.apache.hadoop.http.HttpServer2:getFilterProperties(org.apache.hadoop.conf.Configuration,java.util.List)",872,886,"/**
 * Merges filter configurations from prefixes into a Properties object.
 * @param conf Configuration object.
 * @param prefixes List of prefixes to merge.
 * @return Properties object containing merged configurations.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java,createFilterConfig,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:createFilterConfig(org.apache.hadoop.conf.Configuration),42,51,"/**
 * Builds a filter configuration map from the given configuration.
 * @param conf Configuration object containing proxy user settings
 * @return Map containing the filter configuration
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java,initFilter,"org.apache.hadoop.security.AuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",57,64,"/**
 * Adds an authentication filter to the container with given config.
 */","* Initializes hadoop-auth AuthenticationFilter.
   * <p>
   * Propagates to hadoop-auth AuthenticationFilter configuration all Hadoop
   * configuration properties prefixed with ""hadoop.http.authentication.""
   *
   * @param container The filter container
   * @param conf Configuration for run-time parameters",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration),159,183,"/**
 * Constructs a ZKDelegationTokenSecretManager with configuration.
 * @param conf Hadoop configuration object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,newZooKeeper,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean,org.apache.zookeeper.client.ZKClientConfig)",555,576,"/**
* Creates a ZooKeeper instance with specified configuration.
* @param connectString ZooKeeper connection string
* @return ZooKeeper instance
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,createSaslClient,org.apache.hadoop.security.SaslRpcClient:createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),211,270,"/**
 * Creates a SaslClient based on the provided authentication type.
 * @param authType Authentication type containing protocol details.
 * @return SaslClient object or null if authentication fails.
 */","* Try to create a SaslClient for an authentication type.  May return
   * null if the type isn't supported or the client lacks the required
   * credentials.
   * 
   * @param authType - the requested authentication method
   * @return SaslClient for the authType or null
   * @throws SaslException - error instantiating client
   * @throws IOException - misc errors",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,renew,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",189,208,"/**
* Renews a delegation token using the provided key provider.
* @param token The delegation token to renew.
* @param conf Configuration object.
* @return Renewed token duration.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,cancel,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",210,229,"/**
* Cancels a delegation token using a KeyProvider.
* @param token The token to cancel.
* @param conf Configuration object.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],org.apache.hadoop.conf.Configuration)",89,92,"/**
 * Constructs a LoadBalancingKMSClientProvider with a monotonic clock.
 * @param providerUri URI of the provider.
 * @param providers KMSClientProvider array.
 * @param conf Configuration object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)",94,98,"/**
 * Constructor for testing, uses a testing URI.
 * @param providers KMSClientProvider array
 * @param seed Random seed
 * @param conf Configuration object
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getFS,org.apache.hadoop.fs.FsShell:getFS(),79,84,"/**
* Returns the FileSystem instance, initializing it if null.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,initialize,"org.apache.hadoop.fs.sftp.SFTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",493,500,"/**
 * Calls super.m1, m2, m3, and sets the uriInfo.
 * @param uriInfo The URI information.
 * @param conf The configuration object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,<init>,"org.apache.hadoop.fs.DelegateToFileSystem:<init>(java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",49,57,"/**
 * Constructs a DelegateToFileSystem with URI, FileSystem impl, config.
 * @param theUri URI to delegate to.
 * @param theFsImpl FileSystem implementation.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
* Sets the URI for this object.
* @param name The URI to set.
* @param conf Configuration object.
*/
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,initialize,"org.apache.hadoop.fs.ftp.FTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",102,133,"/**
 * Configures FTP settings from URI and Configuration.
 * @param uri URI containing FTP details.
 * @param conf Configuration object to store settings.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,initialize,"org.apache.hadoop.fs.RawLocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",130,135,"/**
* Calls super.m1, then m2, and sets defaultBlockSize based on URI.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createFileSystem,"org.apache.hadoop.fs.FileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",3604,3629,"/**
 * Creates a FileSystem instance for the given URI and configuration.
 * @param uri The URI of the filesystem.
 * @param conf Hadoop configuration.
 * @return FileSystem instance.
 */","* Create and initialize a new instance of a FileSystem.
   * @param uri URI containing the FS schema and FS details
   * @param conf configuration to use to look for the FS instance declaration
   * and to pass to the {@link FileSystem#initialize(URI, Configuration)}.
   * @return the initialized filesystem.
   * @throws IOException problems loading or initializing the FileSystem",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,initialize,"org.apache.hadoop.fs.viewfs.ViewFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",310,384,"/**
 * Initializes the ViewFS state, including cache and inode tree.
 * @param theUri The URI for the ViewFS mount point.
 * @param conf Hadoop configuration.
 */","* Called after a new FileSystem instance is constructed.
   * @param theUri a uri whose authority section names the host, port, etc. for
   *        this FileSystem
   * @param conf the configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,createFileSystem,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",277,291,"/**
 * Creates a FileSystem instance based on URI and configuration.
 * @param uri The URI for the filesystem.
 * @param conf Configuration object.
 * @return FileSystem instance.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,initialize,"org.apache.hadoop.fs.FilterFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",92,104,"/**
 * Calls super.m1, then conditionally calls fs.m1 and updates swapScheme.
 */","Called after a new FileSystem instance is constructed.
   * @param name a uri whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,initialize,"org.apache.hadoop.fs.LocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",44,53,"/**
 * Processes a URI, potentially swapping the scheme if needed.
 * @param name The URI to process.
 * @param conf Configuration object.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,checkPath,org.apache.hadoop.fs.HarFileSystem:checkPath(org.apache.hadoop.fs.Path),338,341,"/**
* Delegates method call to the underlying filesystem object.
* @param path The path to operate on.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,makeQualified,org.apache.hadoop.fs.FileSystem:makeQualified(org.apache.hadoop.fs.Path),682,685,"/**
* Delegates path modification to m4 on the provided path.
* @param path The path to be modified.
* @return The modified path.
*/
","* Qualify a path to one which uses this FileSystem and, if relative,
   * made absolute.
   * @param path to qualify.
   * @return this path if it contains a scheme and authority and is absolute, or
   * a new path that includes a path and authority and is fully qualified
   * @see Path#makeQualified(URI, Path)
   * @throws IllegalArgumentException if the path has a schema/URI different
   * from this FileSystem.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,resolvePath,org.apache.hadoop.fs.FileSystem:resolvePath(org.apache.hadoop.fs.Path),975,978,"/**
* Applies masking operations to a Path.
* @param p the Path to mask
* @return the masked Path
*/
","* Return the fully-qualified path of path, resolving the path
   * through any symlinks or mount point.
   * @param p path to be resolved
   * @return fully qualified path
   * @throws FileNotFoundException if the path is not present
   * @throws IOException for any other error",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,checkPath,org.apache.hadoop.fs.FilterFileSystem:checkPath(org.apache.hadoop.fs.Path),146,149,"/**
 * Delegates method execution to the underlying file system.
 * @param path The path to operate on.
 */
",Check that a Path belongs to this FileSystem.,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,<init>,"org.apache.hadoop.fs.AvroFSInput:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",55,63,"/**
 * Initializes an AvroFSInput with a FileContext and Path.
 * @param fc FileContext for file operations.
 * @param p Path to the Avro file.
 */
","Construct given a {@link FileContext} and a {@link Path}.
   * @param fc filecontext.
   * @param p the path.
   * @throws IOException If an I/O error occurred.
   *",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,copy,"org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)",2208,2248,"/**
* Copies a file or directory from src to dst, optionally deleting src.
* @param src Source path
* @param dst Destination path
* @param deleteSource Whether to delete source after copy
* @param overwrite Whether to overwrite existing destination
* @return True if copy was successful
*/","* Copy from src to dst, optionally deleting src and overwriting dst.
     * @param src src.
     * @param dst dst.
     * @param deleteSource - delete src if true
     * @param overwrite  overwrite dst if true; throw IOException if dst exists
     *         and overwrite is false.
     *
     * @return true if copy is successful
     *
     * @throws AccessControlException If access is denied
     * @throws FileAlreadyExistsException If <code>dst</code> already exists
     * @throws FileNotFoundException If <code>src</code> does not exist
     * @throws ParentNotDirectoryException If parent of <code>dst</code> is not
     *           a directory
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>src</code> or <code>dst</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server
     * 
     * RuntimeExceptions:
     * @throws InvalidPathException If path <code>dst</code> is invalid",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory(),641,645,"/**
* Returns the result of calling the m1() method.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4913,4917,"/**
 * Constructs an FSDataInputStreamBuilder.
 * @param fileSystem The file system.
 * @param path The path to the file.
 */
","* Path Constructor.
     * @param fileSystem owner
     * @param path path to open.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",4924,4928,"/**
 * Constructs an FSDataInputStreamBuilder.
 * @param fileSystem The FileSystem object.
 * @param pathHandle The PathHandle object.
 */
","* Construct from a path handle.
     * @param fileSystem owner
     * @param pathHandle path handle of file to open.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,openFile,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)",449,454,"/**
 * Opens an FSDataInputStream with specified read policies.
 * @param fs FileSystem object
 * @param status FileStatus object
 * @param readPolicies Read policies string
 * @return FSDataInputStream object
 */
","* Open a file.
   * <p>
   * If the WrappedIO class is found, use it.
   * <p>
   * If not, falls back to the classic {@code fs.open(Path)} call.
   * @param fs filesystem
   * @param status file status
   * @param readPolicies read policy to use
   * @return the input stream
   * @throws IOException any IO failure.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,tryLoadFromPath,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadFromPath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",188,216,"/**
 * Retrieves FsPermission, handling corruption and backups.
 * @param path Path to check, @param backupPath backup path
 * @return FsPermission object
 */","* Try loading from the user specified path, else load from the backup
   * path in case Exception is not due to bad/wrong password.
   * @param path Actual path to load from
   * @param backupPath Backup path (_OLD)
   * @return The permissions of the loaded file
   * @throws NoSuchAlgorithmException
   * @throws CertificateException
   * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,loadAndReturnPerm,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadAndReturnPerm(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",252,272,"/**
 * Loads FsPermission from a keystore, deletes a path, returns permission.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,resetKeyStoreState,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:resetKeyStoreState(org.apache.hadoop.fs.Path),586,598,"/**
 * Resets the Keystore to a previous state using the given path.
 * @param path Path to the Keystore file.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)",2921,2924,"/**
 * Constructs a Sorter with a default WritableComparator.
 * @param fs FileSystem object
 * @param keyClass Class of the key to compare
 * @param valClass Class of the value
 * @param conf Configuration object
 */
","* Sort and merge files containing the named classes.
     * @param fs input FileSystem.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2Compressor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Compressor(org.apache.hadoop.conf.Configuration),98,101,"/**
 * Creates a compressor based on configuration.
 * @param conf Configuration object; determines compressor type.
 * @return A Bzip2Compressor or BZip2DummyCompressor.
 */
","* Return the appropriate implementation of the bzip2 compressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the bzip2 compressor.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,initialize,"org.apache.hadoop.io.SequenceFile$Reader:initialize(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,long,org.apache.hadoop.conf.Configuration,boolean)",1965,1989,"/**
* Initializes reader with input stream, start position, and length.
* @param filename file name, can be null
* @param in input stream
*/",Common work of the constructors.,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createOutputStream,"org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",137,144,"/**
 * Creates a CompressionOutputStream using provided OutputStream & compressor.
 */","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have compressed
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createCompressor,org.apache.hadoop.io.compress.ZStandardCodec:createCompressor(),162,167,"/**
* Creates and returns a ZStandardCompressor instance.
*/","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createInputStream,"org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",194,201,"/**
 * Creates and returns a compression input stream.
 * @param in Input stream to decompress.
 * @param decompressor Decompressor to use.
 * @return DecompressorStream object.
 */
","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createDecompressor,org.apache.hadoop.io.compress.ZStandardCodec:createDecompressor(),220,224,"/**
* Creates and returns a ZStandardDecompressor with configuration.
* Uses m1() and m2(conf) for initialization.
*/","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.ZStandardCodec:createDirectDecompressor(),236,241,"/**
 * Creates and returns a DirectDecompressor instance.
 * Uses the provided configuration to initialize it.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,createReader,"org.apache.hadoop.io.file.tfile.BCFile$Reader:createReader(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)",730,734,"/**
 * Creates a BlockReader using the provided algorithm and region.
 * @param compressAlgo Compression algorithm.
 * @param region Block region to read.
 * @return BlockReader instance.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,org.apache.hadoop.io.file.tfile.Compression$Algorithm)",345,363,"/**
* Creates a BlockAppender for a meta block with given name & algo.
* @param name Block name
* @param compressAlgo Compression algorithm
* @return BlockAppender instance
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareDataBlock,org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareDataBlock(),417,436,"/**
 * Creates a BlockAppender. Throws IllegalStateException if preconditions are not met.
 * @return BlockAppender instance
 */","* Create a Data Block and obtain an output stream for adding data into the
     * block. There can only be one BlockAppender stream active at any time.
     * Data Blocks may not be created after the first Meta Blocks. The caller
     * must call BlockAppender.close() to conclude the block creation.
     * 
     * @return The BlockAppender stream
     * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.conf.Configuration),103,106,"/**
 * Constructs a ScriptBasedMapping with the given configuration.
 * @param conf The configuration object for the mapping.
 */
","* Create an instance from the given configuration
   * @param conf configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,setConf,org.apache.hadoop.net.ScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration),85,89,"/**
* Calls super.m1 and then m2().m1 with the given configuration.
*/","* {@inheritDoc}.
   * <p>
   * This will get called in the superclass constructor, so a check is needed
   * to ensure that the raw mapping is defined before trying to relaying a null
   * configuration.
   * </p>
   * @param conf input Configuration.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,init,org.apache.hadoop.crypto.key.KeyShell:init(java.lang.String[]),80,168,"/**
* Processes command-line arguments and executes corresponding actions.
* @param args Command-line arguments to process.
* @return 0 on success, 1 on failure.
*/","* Parse the command line arguments and initialize the data.
   * <pre>
   * % hadoop key create keyName [-size size] [-cipher algorithm]
   *    [-provider providerPath]
   * % hadoop key roll keyName [-provider providerPath]
   * % hadoop key list [-provider providerPath]
   * % hadoop key delete keyName [-provider providerPath] [-i]
   * % hadoop key invalidateCache keyName [-provider providerPath]
   * </pre>
   * @param args Command line arguments.
   * @return 0 on success, 1 on failure.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataInputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",33,36,"/**
 * Creates a CryptoFSDataInputStream wrapping an FSDataInputStream.
 * @param in Input stream, codec, key, and IV for encryption.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long)",125,128,"/**
 * Creates a CryptoOutputStream with default authentication.
 * @param out Output stream.
 * @param codec CryptoCodec.
 * @param key Encryption key.
 * @param iv Initialization vector.
 * @param streamOffset Stream offset.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getConnectionId,"org.apache.hadoop.ipc.Client$ConnectionId:getConnectionId(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)",1798,1817,"/**
 * Creates a ConnectionId with provided address, protocol, ticket, timeout, and retry policy.
 */","* Returns a ConnectionId object. 
     * @param addr Remote address for the connection.
     * @param protocol Protocol for RPC.
     * @param ticket UGI
     * @param rpcTimeout timeout
     * @param conf Configuration object
     * @return A ConnectionId instance
     * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,open,"org.apache.hadoop.fs.ftp.FTPFileSystem:open(org.apache.hadoop.fs.Path,int)",276,306,"/**
 * Opens an input stream for a file using FTP, handling errors.
 * @param file Path to the file.
 * @param bufferSize Buffer size for the stream.
 * @return FSDataInputStream for reading the file.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,create,"org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",312,375,"/**
 * Creates a data output stream for writing to a file.
 * @param file Path to the file
 * @return FSDataOutputStream for writing
 */","* A stream obtained via this call must be closed before using other APIs of
   * this class or else the invocation will block.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,delete,"org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",400,409,"/**
 * Downloads a file or directory using FTP.
 * @param file Path to the file/directory.
 * @param recursive Recursive download for directories.
 * @return True on success, false otherwise.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,listStatus,org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.hadoop.fs.Path),468,477,"/**
 * Retrieves file statuses for a given path using an FTP client.
 * @param file the path to retrieve statuses for
 * @return An array of FileStatus objects.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path),500,509,"/**
 * Retrieves FileStatus for a given Path using an FTP client.
 * @param file the Path to get the FileStatus for
 * @return FileStatus object or null if not found
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",574,583,"/**
 * Sets file permissions using an FTP client.
 * @param file Path to the file. @param permission Permissions to set.
 * @return True if successful, false otherwise.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,rename,"org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",631,640,"/**
 * Copies a file from src to dst using FTP.
 * @param src Source file path
 * @param dst Destination file path
 * @return True if copy successful, false otherwise.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.ftp.FTPFileSystem:getHomeDirectory(),713,729,"/**
 * Retrieves the FTP home directory path.
 * @return Path object representing the home directory.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getServerDefaults,org.apache.hadoop.fs.HarFileSystem:getServerDefaults(),1260,1264,"/**
* Returns the FsServerDefaults object from the underlying fs object.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getServerDefaults,org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults(),158,162,"/**
* Delegates to the underlying fsImpl's m1() method.
* @return FsServerDefaults object from fsImpl
* @throws IOException if an I/O error occurs
*/
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getServerDefaults,org.apache.hadoop.fs.FileSystem:getServerDefaults(org.apache.hadoop.fs.Path),963,965,"/**
 * Returns the default FsServerDefaults.
 * @return FsServerDefaults object.
 */
","* Return a set of server default configuration values.
   * @param p path is used to identify an FS since an FS could have
   *          another FS that it could be delegating the call to
   * @return server default configuration values
   * @throws IOException IO failure",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getServerDefaults,org.apache.hadoop.fs.FilterFileSystem:getServerDefaults(),436,439,"/**
* Delegates to the underlying fs.m1() method.
* @return FsServerDefaults object from the underlying fs.
*/
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),1288,1292,"/**
* Delegates file size retrieval to the file system.
* @param f Path to the file
* @return File size as a long
*/
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean)",1089,1096,"/**
* Opens a data output stream to write to a file.
* @param f path to the file
* @param overwrite if true, overwrites existing files
* @return FSDataOutputStream object
*/
","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file to create
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an exception will be thrown.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)",1107,1114,"/**
 * Creates a data output stream at the specified path.
 * @param f path to create stream at
 * @param progress progressable object
 * @return FSDataOutputStream object
 */","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * Files are overwritten by default.
   * @param f the file to create
   * @param progress to report progress
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short)",1124,1131,"/**
 * Opens a data output stream for writing to a file.
 * @param f Path to the file.
 * @param replication Replication factor.
 * @return FSDataOutputStream object.
 */
","* Create an FSDataOutputStream at the indicated Path.
   * Files are overwritten by default.
   * @param f the file to create
   * @param replication the replication factor
   * @throws IOException IO failure
   * @return output stream1",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)",1143,1149,"/**
 * Opens a data output stream for writing to a file.
 * @param f Path to the file, replication factor, progressable.
 * @return FSDataOutputStream object.
 */","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * Files are overwritten by default.
   * @param f the file to create
   * @param replication the replication factor
   * @param progress to report progress
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int)",1161,1168,"/**
* Opens a data output stream at the given path.
* @param f Path to open. Overwrite & buffer size are params.
* @return FSDataOutputStream object.
*/","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file to create
   * @param overwrite if a path with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)",1183,1191,"/**
* Opens a data output stream for writing to a file.
* @param f Path to the file, overwrite, buffer size, progress.
* @return FSDataOutputStream object.
*/","* Create an {@link FSDataOutputStream} at the indicated Path
   * with write-progress reporting.
   *
   * The frequency of callbacks is implementation-specific; it may be ""none"".
   * @param f the path of the file to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param progress to report progress.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),976,988,"/**
 * Delegates m3 operation to the target file system.
 * @param f Path to resolve and delegate the operation on.
 * @return Long value returned by the target file system.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),442,445,"/**
 * Delegates file size retrieval to the file system.
 * @param f Path to the file
 * @return File size as a long
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,<init>,"org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",130,139,"/**
 * Creates a new FSDataOutputStreamBuilder.
 * @param fileSystem The FileSystem to use.
 * @param p The path for the output stream.
 */
","* Constructor.
   *
   * @param fileSystem file system.
   * @param p the path.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DFCachingGetSpaceUsed.java,<init>,org.apache.hadoop.fs.DFCachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),39,42,"/**
 * Constructs a DFCachingGetSpaceUsed with a Builder.
 * @param builder Builder object containing path and interval.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable),2462,2506,"/**
* Reads data into a Writable key, handling compressed/buffered modes.
* @param key The Writable key to populate.
* @return True if successful, false otherwise.
*/","* @return Read the next key in the file into <code>key</code>, skipping its
     * value.True if another entry exists, and false at end of file.
     *
     * @param key key.
     * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(java.lang.Object),2707,2752,"/**
 * Reads a key-value pair, validating the key class.
 * @param key The key object.
 * @return The read key object.
 */","* Read the next key in the file, skipping its
     * value.
     *
     * @param key input Object key.
     * @throws IOException raised on errors performing I/O.
     * @return Return null at end of file.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,initSingleton,"org.apache.hadoop.metrics2.source.JvmMetrics:initSingleton(java.lang.String,java.lang.String)",129,131,"/**
* Retrieves JvmMetrics for a process and session.
* @param processName Process identifier.
* @param sessionId Session identifier.
* @return JvmMetrics object.
*/
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",162,170,"/**
 * Constructs an Invoker with provided protocol, connection, config, etc.
 */","* This constructor takes a connectionId, instead of creating a new one.
     * @param protocol input protocol.
     * @param connId input connId.
     * @param conf input Configuration.
     * @param factory input factory.
     * @param alignmentContext Alignment context",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration),77,79,"/**
* Creates a Client object using the provided configuration.
* @param conf Configuration object for client setup.
* @return A Client object.
*/
","* Construct &amp; cache an IPC client with the default SocketFactory
   * and default valueClass if no cached client exists. 
   * 
   * @param conf Configuration
   * @return an IPC client",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,"org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",89,91,"/**
* Overloads m1 with default Writable class.
* @param conf Configuration object
* @param factory SocketFactory instance
* @return Client object
*/","* Construct &amp; cache an IPC client with the user-provided SocketFactory
   * if no cached client exists. Default response type is ObjectWritable.
   * 
   * @param conf Configuration
   * @param factory SocketFactory for client socket
   * @return an IPC client",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getClient,org.apache.hadoop.ipc.ProtobufRpcEngine2:getClient(org.apache.hadoop.conf.Configuration),370,376,"/**
 * Creates a client using the given configuration.
 * @param conf Configuration object for client creation.
 * @return A Client instance.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",170,178,"/**
 * Constructs an Invoker with provided protocol, connection, config, etc.
 */","* This constructor takes a connectionId, instead of creating a new one.
     *
     * @param protocol input protocol.
     * @param connId input connId.
     * @param conf input Configuration.
     * @param factory input factory.
     * @param alignmentContext Alignment context",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getClient,org.apache.hadoop.ipc.ProtobufRpcEngine:getClient(org.apache.hadoop.conf.Configuration),360,366,"/**
 * Creates a client using the provided configuration.
 * @param conf Configuration object for client creation.
 * @return A Client instance.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>(),69,71,"/**
 * Constructs a FsUrlStreamHandlerFactory with a default Configuration.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,isMethodSupported,"org.apache.hadoop.ipc.RpcClientUtil:isMethodSupported(java.lang.Object,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcKind,long,java.lang.String)",108,147,"/**
 * Checks if a method is supported by the RPC protocol version.
 * @param version RPC version to check
 * @return True if the method is supported, false otherwise.
 */","* Returns whether the given method is supported or not.
   * The protocol signatures are fetched and cached. The connection id for the
   * proxy provided is re-used.
   * @param rpcProxy Proxy which provides an existing connection id.
   * @param protocol Protocol for which the method check is required.
   * @param rpcKind The RpcKind for which the method check is required.
   * @param version The version at the client.
   * @param methodName Name of the method.
   * @return true if the method is supported, false otherwise.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,<init>,"org.apache.hadoop.ha.ZKFCRpcServer:<init>(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,org.apache.hadoop.ha.ZKFailoverController,org.apache.hadoop.security.authorize.PolicyProvider)",47,76,"/**
 * Constructs a ZKFCRpcServer with given configuration and components.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,<init>,"org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)",60,63,"/**
 * Constructs a Trash object with a FileSystem and Configuration.
 * @param fs The FileSystem to operate on.
 * @param conf The configuration to use.
 */
","* Construct a trash can accessor for the FileSystem provided.
   * @param fs the FileSystem
   * @param conf a Configuration
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,build,org.apache.hadoop.fs.GetSpaceUsed$Builder:build(),144,176,"/**
 * Gets the space used object, falling back to default implementation.
 * @return GetSpaceUsed object
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getInstance,org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration),73,75,"/**
 * Retrieves the network topology using the default factory.
 * @param conf Configuration object for topology retrieval.
 * @return NetworkTopology object representing the topology.
 */
","* Get an instance of NetworkTopology based on the value of the configuration
   * parameter net.topology.impl.
   * 
   * @param conf the configuration to be used
   * @return an instance of NetworkTopology",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",55,59,"/**
 * Resolves a domain name using a configuration and host.
 * @param conf Configuration object.
 * @param host Hostname to resolve.
 * @param configKey Key for configuration lookup.
 * @return DomainNameResolver object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setConfigurationInternal,org.apache.hadoop.security.SecurityUtil:setConfigurationInternal(org.apache.hadoop.conf.Configuration),105,125,"/**
 * Configures security settings based on the provided Hadoop configuration.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,<init>,"org.apache.hadoop.security.SaslRpcClient:<init>(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",118,125,"/**
 * Constructs a SaslRpcClient with user info, protocol, address, and config.
 */
","* Create a SaslRpcClient that can be used by a RPC client to negotiate
   * SASL authentication with a RPC server
   * @param ugi - connecting user
   * @param protocol - RPC protocol
   * @param serverAddr - InetSocketAddress of remote server
   * @param conf - Configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,<init>,org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration),100,102,"/**
 * Constructs a Groups object with a default Timer.
 * @param conf Configuration object for initialization.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,"org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String)",70,80,"/**
 * Configures impersonation provider with a proxy user prefix.
 * @param conf Hadoop configuration object
 * @param proxyUserPrefix Prefix for proxy user
 */","* Refreshes configuration using the specified Proxy user prefix for
   * properties.
   *
   * @param conf configuration
   * @param proxyUserPrefix proxy user configuration prefix",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),55,59,"/**
 * Calls super.m1 and then m3 with a value from the configuration.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,addShutdownHook,"org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int)",294,305,"/**
 * Adds a shutdown hook with a given priority.
 * @param shutdownHook Runnable to execute during shutdown.
 * @param priority Hook priority.
 */","* Adds a shutdownHook with a priority, the higher the priority
   * the earlier will run. ShutdownHooks with same priority run
   * in a non-deterministic order.
   *
   * @param shutdownHook shutdownHook <code>Runnable</code>
   * @param priority priority of the shutdownHook.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getPasswordString,"org.apache.hadoop.http.HttpServer2$Builder:getPasswordString(org.apache.hadoop.conf.Configuration,java.lang.String)",443,450,"/**
 * Retrieves a character array from configuration for a given name.
 * @param conf Configuration object. @param name Name to retrieve.
 * @return Character array or null if not found.
 */
","* A wrapper of {@link Configuration#getPassword(String)}. It returns
     * <code>String</code> instead of <code>char[]</code>.
     *
     * @param conf the configuration
     * @param name the property name
     * @return the password string or null",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPassword,"org.apache.hadoop.security.LdapGroupsMapping:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",914,927,"/**
 * Retrieves password for alias, falling back to default if unavailable.
 * @param conf Configuration object, alias password alias, defaultPass default password
 * @return Password string.
 */
","* Passwords should not be stored in configuration. Use
   * {@link #getPasswordFromCredentialProviders(
   *            Configuration, String, String)}
   * to avoid reading passwords from a configuration file.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,getPassword,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",302,315,"/**
 * Retrieves password for an alias from configuration, uses default if unavailable.
 * @param conf Configuration object. @param alias Alias to retrieve.
 * @param defaultPass Default password to use if alias not found.
 * @return Password string.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getZKAuthInfos,"org.apache.hadoop.security.SecurityUtil:getZKAuthInfos(org.apache.hadoop.conf.Configuration,java.lang.String)",775,791,"/**
 * Retrieves ZK authentication information from configuration.
 * @param conf Configuration object.
 * @param configKey Key for authentication configuration.
 * @return List of ZKAuthInfo objects or empty list on failure.
 */","* Utility method to fetch ZK auth info from the configuration.
   *
   * @param conf configuration.
   * @param configKey config key.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @throws ZKUtil.BadAuthFormatException if the auth format is invalid
   * @return ZKAuthInfo List.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,checkCreateRSRawEncoder,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:checkCreateRSRawEncoder(),52,59,"/**
 * Returns the RawErasureEncoder instance, creating it if null.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,checkCreateRSRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateRSRawEncoder(),61,67,"/**
 * Returns the RawErasureEncoder instance, creating it if null.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,checkCreateXorRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateXorRawEncoder(),69,76,"/**
 * Returns the XOR RawErasureEncoder, creating it if null.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,checkCreateXorRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateXorRawEncoder(),75,81,"/**
 * Returns the XOR RawErasureEncoder, creating it if null.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),40,50,"/**
 * Creates an ErasureCodingStep using the provided block group.
 * @param blockGroup ECBlockGroup to be encoded.
 * @return An ErasureCodingStep object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),40,51,"/**
 * Creates an ErasureDecodingStep with the given block group.
 * @param blockGroup ECBlockGroup to be used for erasure decoding.
 * @return An ErasureDecodingStep object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,checkCreateRSRawDecoder,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:checkCreateRSRawDecoder(),52,58,"/**
* Returns a RawErasureDecoder instance, creating it if null.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,checkCreateRSRawDecoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateRSRawDecoder(),67,73,"/**
 * Returns the RawErasureDecoder instance, creating it if null.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshCallQueue,org.apache.hadoop.ipc.Server:refreshCallQueue(org.apache.hadoop.conf.Configuration),903,915,"/**
 * Initializes the call queue with configuration and namespace details.
 * @param conf Configuration object for queue parameters.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,org.apache.hadoop.conf.Configuration)",88,94,"/**
 * Constructs a FairCallQueue with default capacity weights.
 * @param priorityLevels Number of priority levels.
 * @param capacity Queue capacity.
 * @param ns Namespace.
 * @param conf Configuration object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,boolean,org.apache.hadoop.conf.Configuration)",96,102,"/**
 * Constructs a FairCallQueue with default queue weights.
 * @param priorityLevels Number of priority levels.
 * @param capacity Queue capacity.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)",236,279,"/**
 * Constructs a DecayRpcScheduler with given configuration.
 * @param numLevels Priority levels.
 * @param ns Namespace.
 * @param conf Configuration object.
 */
","* Create a decay scheduler.
   * @param numLevels number of priority levels
   * @param ns config prefix, so that we can configure multiple schedulers
   *           in a single instance.
   * @param conf configuration to use.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,clone,"org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)",218,227,"/**
 * Creates a clone of an object using the provided configuration.
 * @param orig The object to clone.
 * @param conf Configuration object for cloning.
 * @return A clone of the original object.
 */
","* Make a copy of a writable object using serialization to a buffer.
   *
   * @param <T> Generics Type T.
   * @param orig The object to copy
   * @param conf input Configuration.
   * @return The copied object",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.LdapGroupsMapping:getGroupsSet(java.lang.String),726,760,"/**
* Retrieves user groups, retrying on authentication/naming errors.
* @param user User identifier. Returns empty set on failure.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),327,354,"/**
 * Encrypts a key version.
 * @param ekv The encrypted key version to encrypt.
 * @return EncryptedKeyVersion object.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,org.apache.hadoop.conf.Configuration:writeXml(java.io.OutputStream),3589,3591,"/**
* Delegates to m1 with a UTF-8 wrapped OutputStreamWriter.
* @param out The output stream to wrap.
*/","* Write out the non-default properties in this configuration to the given
   * {@link OutputStream} using UTF-8 encoding.
   *
   * @param out the output stream to write to.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,writeResponse,"org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String)",107,110,"/**
* Calls m1 with a null value for the fourth parameter.
* @param conf Configuration object
* @param out Writer to write output to
* @param format Format string
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configure,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configure(java.lang.String),481,486,"/**
 * Initializes metrics configuration with a given prefix and performs related actions.
 * @param prefix Prefix for the metrics configuration.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",547,553,"/**
 * Creates a data output stream. Overwrites if true.
 * @param f Path to the file.
 * @param overwrite Overwrite existing file.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",592,600,"/**
 * Calls m1 with permission set, returns FSDataOutputStream.
 * @param f Path
 * @param permission FsPermission
 * @param overwrite boolean
 * @param bufferSize int
 * @param replication short
 * @param blockSize long
 * @param progress Progressable
 * @return FSDataOutputStream
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",602,610,"/**
 * Opens a data output stream for writing to a file.
 * @param f path to file, permission, overwrite, etc.
 * @return FSDataOutputStream object
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,constructSecretProvider,"org.apache.hadoop.http.HttpServer2:constructSecretProvider(org.apache.hadoop.http.HttpServer2$Builder,javax.servlet.ServletContext)",862,870,"/**
 * Creates a SignerSecretProvider using config and ServletContext.
 * @param b Builder object containing configuration
 * @param ctx ServletContext for authentication
 * @return SignerSecretProvider instance
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java,initFilter,"org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",53,58,"/**
 * Adds a ProxyUserAuthenticationFilter to the container with config.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",98,101,"/**
 * Constructs a ZKSecretManager with a configuration and token kind.
 * @param conf Configuration object.
 * @param tokenKind Token type identifier.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,newZooKeeper,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean)",547,553,"/**
 * Creates a ZooKeeper instance with default configuration.
 * @param connectString ZooKeeper connection string
 * @param sessionTimeout Session timeout in milliseconds
 * @param watcher Watcher object for events
 * @param canBeReadOnly Whether the connection can be read-only
 * @return ZooKeeper instance
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,selectSaslClient,org.apache.hadoop.security.SaslRpcClient:selectSaslClient(java.util.List),154,187,"/**
 * Selects a SaslAuth type based on client capabilities.
 * @param authTypes list of supported authentication types
 * @return Selected SaslAuth type, or null if none suitable.
 */","* Instantiate a sasl client for the first supported auth type in the
   * given list.  The auth type must be defined, enabled, and the user
   * must possess the required credentials, else the next auth is tried.
   * 
   * @param authTypes to attempt in the given order
   * @return SaslAuth of instantiated client
   * @throws AccessControlException - client doesn't support any of the auths
   * @throws IOException - misc errors",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,<init>,"org.apache.hadoop.fs.ftp.FtpFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",50,53,"/**
 * Constructs an FTPFileSystem for interacting with an FTP server.
 * @param theUri The URI representing the FTP server location.
 * @param conf The configuration object.
 */
","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFs.java,<init>,"org.apache.hadoop.fs.HarFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",28,31,"/**
 * Constructs a HarFileSystem for reading from the given URI.
 * @param theUri The URI to read from.
 * @param conf Hadoop configuration.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,<init>,"org.apache.hadoop.fs.local.RawLocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",55,59,"/**
 * Constructs a RawLocalFs instance for accessing local file system.
 * @param theUri The URI of the file system.
 * @param conf Hadoop configuration.
 */
","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.HttpsFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
 * Initializes the URI for this object.
 * @param name The URI to set.
 * @param conf Configuration object.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.HttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
* Sets the URI for this object.
* @param name The URI to set.
* @param conf Configuration object (unused).
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,initialize,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",150,179,"/**
 * Initializes ViewFileSystemOverloadScheme with URI and configuration.
 * Sets config flags and loads mount table configuration if available.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,initialize,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",135,140,"/**
 * Calls super.m1 and then calls m2 with the provided configuration.
 */","* Called after a new FileSystem instance is constructed.
   * @param name a uri whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkDependencies,"org.apache.hadoop.fs.FileUtil:checkDependencies(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",336,353,"/**
* Prevents copying a file to itself or a subdirectory.
* @param srcFS Source file system.
* @param src Source path.
* @param dstFS Destination file system.
* @param dst Destination path.
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",95,104,"/**
 * Constructs a MultipartUploaderBuilderImpl.
 * @param fileSystem The file system to use.
 * @param p The path to upload.
 */
","* Constructor.
   *
   * @param fileSystem fileSystem.
   * @param p path.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.FileStatus)",156,166,"/**
 * Initializes PathData with file system, path string, and status.
 * @param fs FileSystem object
 * @param pathString Path as string
 * @param stat FileStatus object
 * @throws IOException if an I/O error occurs
 */
","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it.
   * @param fs the FileSystem
   * @param pathString a String of the path
   * @param stat the FileStatus (may be null if the path doesn't exist)",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hasPathCapability,"org.apache.hadoop.fs.FileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",3487,3501,"/**
 * Checks if a path has the specified capability.
 * @param path Path to check. @param capability Capability string.
 */","* The base FileSystem implementation generally has no knowledge
   * of the capabilities of actual implementations.
   * Unless it has a way to explicitly determine the capabilities,
   * this method returns false.
   * {@inheritDoc}",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.FileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),4973,4978,"/**
 * Processes a path and returns a new Path object.
 * @param path The input path to process.
 * @return A new Path object.
 */
","* Return path of the enclosing root for a given path.
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   *
   * Call makeQualified on the param path to ensure its part of the correct filesystem.
   *
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,makeQualified,org.apache.hadoop.fs.FilterFileSystem:makeQualified(org.apache.hadoop.fs.Path),124,139,"/**
 * Resolves a path, potentially swapping the scheme.
 * @param path Input path to resolve.
 * @return Resolved path object.
 */
",Make sure that a path specifies a FileSystem.,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,resolvePath,org.apache.hadoop.fs.HarFileSystem:resolvePath(org.apache.hadoop.fs.Path),343,346,"/**
* Delegates Path m1 operation to the underlying FileSystem.
* @param p The Path to operate on.
* @return The result of the Path m1 operation.
*/
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,getFileStatus,"org.apache.hadoop.fs.shell.find.BaseExpression:getFileStatus(org.apache.hadoop.fs.shell.PathData,int)",279,290,"/**
 * Retrieves FileStatus, potentially following linked files.
 * @param item PathData containing file status and filesystem.
 * @param depth Recursion depth for linked files.
 * @return FileStatus object.
 */","* Returns the {@link FileStatus} from the {@link PathData} item. If the
   * current options require links to be followed then the returned file status
   * is that of the linked file.
   *
   * @param item
   *          PathData
   * @param depth
   *          current depth in the process directories
   * @return FileStatus
   * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,resolvePath,org.apache.hadoop.fs.viewfs.ViewFileSystem:resolvePath(org.apache.hadoop.fs.Path),412,420,"/**
 * Resolves a Path by delegating to the file system.
 * @param f The Path to resolve.
 * @return The resolved Path.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,resolvePath,org.apache.hadoop.fs.FilterFileSystem:resolvePath(org.apache.hadoop.fs.Path),157,160,"/**
* Delegates Path m1 operation to the underlying filesystem.
* @param p The Path to operate on.
* @return The result of the Path m1 operation.
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,fullPath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:fullPath(org.apache.hadoop.fs.Path),91,97,"/**
* Constructs a Path object based on path and workingDir.
* Returns a new Path or modifies the existing one.
*/","* @param path
   * @return  full path including the chroot",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,copy,"org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2173,2178,"/**
* Moves a file or directory from src to dst.
* @param src Source path.
* @param dst Destination path.
*/
","* Copy file from src to dest. See
     * {@link #copy(Path, Path, boolean, boolean)}
     *
     * @param src src.
     * @param dst dst.
     * @throws AccessControlException If access is denied.
     * @throws FileAlreadyExistsException If file <code>src</code> already exists.
     * @throws FileNotFoundException if next file does not exist any more.
     * @throws ParentNotDirectoryException If parent of <code>src</code> is not a
     * directory.
     * @throws UnsupportedFileSystemException If file system for
     * <code>src/dst</code> is not supported.
     * @throws IOException If an I/O error occurred.
     * @return if success copy true, not false.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createDataInputStreamBuilder,"org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4877,4883,"/**
 * Creates an FSDataInputStreamBuilder for a given path.
 * @param fileSystem The FileSystem object.
 * @param path The path to create the stream for.
 */","* Create instance of the standard {@link FSDataInputStreamBuilder} for the
   * given filesystem and path.
   * @param fileSystem owner
   * @param path path to read
   * @return a builder.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createDataInputStreamBuilder,"org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",4892,4898,"/**
 * Creates an FSDataInputStreamBuilder for a given filesystem and path.
 * @param fileSystem The filesystem.
 * @param pathHandle The path handle.
 * @return An FSDataInputStreamBuilder object.
 */
","* Create instance of the standard {@link FSDataInputStreamBuilder} for the
   * given filesystem and path handle.
   * @param fileSystem owner
   * @param pathHandle path handle of file to open.
   * @return a builder.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,tryLoadIncompleteFlush,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadIncompleteFlush(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",229,250,"/**
 * Determines FsPermission based on paths; initializes if needed.
 * @param oldPath The old path.
 * @param newPath The new path.
 * @return FsPermission object.
 */","* The KeyStore might have gone down during a flush, In which case either the
   * _NEW or _OLD files might exists. This method tries to load the KeyStore
   * from one of these intermediate files.
   * @param oldPath the _OLD file created during flush
   * @param newPath the _NEW file created during flush
   * @return The permissions of the loaded file
   * @throws IOException
   * @throws NoSuchAlgorithmException
   * @throws CertificateException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createCompressor,org.apache.hadoop.io.compress.BZip2Codec:createCompressor(),147,150,"/**
 * Creates and returns a Bzip2 compressor using the provided configuration.
 */","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getMetaBlock,org.apache.hadoop.io.file.tfile.BCFile$Reader:getMetaBlock(java.lang.String),701,710,"/**
 * Creates a BlockReader for the given name.
 * @param name Block name; throws MetaBlockDoesNotExist if not found.
 */","* Stream access to a Meta Block.
     * 
     * @param name
     *          meta block name
     * @return BlockReader input stream for reading the meta block.
     * @throws IOException
     * @throws MetaBlockDoesNotExist
     *           The Meta Block with the given name does not exist.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDataBlock,org.apache.hadoop.io.file.tfile.BCFile$Reader:getDataBlock(int),720,728,"/**
 * Retrieves a BlockReader for the specified block index.
 * @param blockIndex Index of the block to retrieve.
 * @throws IndexOutOfBoundsException if blockIndex is out of bounds.
 */","* Stream access to a Data Block.
     * 
     * @param blockIndex
     *          0-based data block index.
     * @return BlockReader input stream for reading the data block.
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Writer:close(),303,339,"/**
* Writes a new data block to the data index.
* Handles errors and ensures block appender is not active.
*/","* Close the BCFile Writer. Attempting to use the Writer after calling
     * <code>close</code> is not allowed and may lead to undetermined results.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)",381,385,"/**
 * Creates a BlockAppender with the given name and compression.
 * @param name Block name.
 * @param compressionName Compression name.
 * @return BlockAppender object.
 */
","* Create a Meta Block and obtain an output stream for adding data into the
     * block. There can only be one BlockAppender stream active at any time.
     * Regular Blocks may not be created after the first Meta Blocks. The caller
     * must call BlockAppender.close() to conclude the block creation.
     * 
     * @param name
     *          The name of the Meta Block. The name must not conflict with
     *          existing Meta Blocks.
     * @param compressionName
     *          The name of the compression algorithm to be used.
     * @return The BlockAppender stream
     * @throws IOException
     * @throws MetaBlockAlreadyExists
     *           If the meta block with the name already exists.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String),403,406,"/**
 * Creates a BlockAppender with the given name, using default meta.
 * @param name Appender name.
 * @return BlockAppender instance.
 */
","* Create a Meta Block and obtain an output stream for adding data into the
     * block. The Meta Block will be compressed with the same compression
     * algorithm as data blocks. There can only be one BlockAppender stream
     * active at any time. Regular Blocks may not be created after the first
     * Meta Blocks. The caller must call BlockAppender.close() to conclude the
     * block creation.
     * 
     * @param name
     *          The name of the Meta Block. The name must not conflict with
     *          existing Meta Blocks.
     * @return The BlockAppender stream
     * @throws MetaBlockAlreadyExists
     *           If the meta block with the name already exists.
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,initDataBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:initDataBlock(),639,644,"/**
 * Initializes the block appender if it's not already initialized.
 */","* Check if we need to start a new data block.
     * 
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",42,47,"/**
 * Constructs a CryptoFSDataOutputStream.
 * @param out FSDataOutputStream, codec, key, and iv for encryption.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",120,123,"/**
 * Creates a CryptoOutputStream with default update mode.
 * @param out Output stream.
 * @param codec CryptoCodec.
 * @param key Encryption key.
 * @param iv Initialization vector.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.ftp.FTPFileSystem:getWorkingDirectory(),707,711,"/**
* Returns the result of calling the m1() method.
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getServerDefaults,org.apache.hadoop.fs.HarFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),1266,1269,"/**
* Delegates to the underlying file system's m1 method.
* @param f The path to use for the operation.
* @return The FsServerDefaults object.
*/
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getServerDefaults,org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),164,167,"/**
* Delegates to the underlying file system implementation.
* @param f the path
* @return FsServerDefaults object
*/
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),1004,1013,"/**
 * Retrieves server defaults for a given path.
 * @param f Path to resolve; throws NotInMountpointException.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getServerDefaults,org.apache.hadoop.fs.FilterFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),452,455,"/**
* Delegates to the underlying FileSystem's m1 method.
* @param f Path to be passed to the underlying m1 method.
* @return FsServerDefaults object returned by fs.m1(f)
*/
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processArguments(java.util.LinkedList),392,424,"/**
 * Copies data from input paths to the destination path.
 * @param args List of PathData objects representing input paths
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path),1077,1079,"/**
 * Opens a file for writing.
 * @param f Path to the file
 * @return FSDataOutputStream for writing
 */
","* Create an FSDataOutputStream at the indicated Path.
   * Files are overwritten by default.
   * @param f the file to create
   * @throws IOException IO failure
   * @return output stream.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,close,org.apache.hadoop.io.BloomMapFile$Writer:close(),192,204,"/**
 * Writes bloom filter to a file.
 * Writes the bloom filter to a file specified by BLOOM_FILE_NAME.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,createLogFile,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createLogFile(org.apache.hadoop.fs.Path),688,716,"/**
* Finds a valid output path by incrementing an ID.
* @param initial Initial path to attempt writing to.
* @throws IOException if a valid path cannot be found.
*/","* Create a new log file and return the {@link FSDataOutputStream}. If a
   * file with the specified path already exists, add a suffix, starting with 1
   * and try again. Keep incrementing the suffix until a nonexistent target
   * path is found.
   *
   * Once the file is open, update {@link #currentFSOutStream},
   * {@link #currentOutStream}, and {@#link #currentFilePath} are set
   * appropriately.
   *
   * @param initial the target path
   * @throws IOException thrown if the call to see if the exists fails",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,createOrAppendLogFile,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createOrAppendLogFile(org.apache.hadoop.fs.Path),789,820,"/**
* Initializes output stream for a file. Tries two methods to create.
* @param targetFile Path to the file to be written to.
* @throws IOException if file creation fails.
*/","* Create a new log file and return the {@link FSDataOutputStream}. If a
   * file with the specified path already exists, open the file for append
   * instead.
   *
   * Once the file is open, update {@link #currentFSOutStream},
   * {@link #currentOutStream}, and {@#link #currentFilePath}.
   *
   * @param initial the target path
   * @throws IOException thrown if the call to see the append operation fails.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,save,"org.apache.hadoop.util.JsonSerialization:save(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Object,boolean)",293,297,"/**
* Calls m2 with a Path obtained from fs.m1, overwriting if needed.
*/","* Save to a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten
   * @param instance instance
   * @throws IOException IO exception.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",1231,1238,"/**
 * Creates a Writer with specified FileSystem, Configuration, and metadata.
 * @param fs FileSystem, conf Configuration, name Path, keyClass, valClass
 */
","* Create the named file with write-progress reporter.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param progress input progress.
     * @param metadata input metadata.
     * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNewFile,org.apache.hadoop.fs.FileSystem:createNewFile(org.apache.hadoop.fs.Path),1495,1503,"/**
* Processes a file path; returns true on success, false otherwise.
*/","* Creates the given Path as a brand-new zero-length file.  If
   * create fails, or if it already existed, return false.
   * <i>Important: the default implementation is not atomic</i>
   * @param f path to use for create
   * @throws IOException IO failure
   * @return if create new file success true,not false.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4690,4692,"/**
 * Constructs a FileSystemDataOutputStreamBuilder.
 * @param fileSystem The file system.
 * @param p The path.
 */
","* Constructor.
     * @param fileSystem owner
     * @param p path to create",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,"org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",2518,2530,"/**
* Writes a value associated with a key, throws IOException if mismatch.
* @param key The key to write the value for.
* @param val The value to write.
* @return True if more data is available.
*/
","* Read the next key/value pair in the file into <code>key</code> and
     * <code>val</code>.
     * @return Returns true if such a pair exists and false when at
     * end of file.
     *
     * @param key input key.
     * @param val input val.
     * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,read,org.apache.hadoop.fs.shell.Display$TextRecordInputStream:read(),236,257,"/**
 * Processes input, updates key/val, writes to output, and returns.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",143,152,"/**
 * Constructs an Invoker with a connection ID and fallback auth.
 * @param protocol RPC protocol class
 * @param addr Socket address
 * @param ticket UserGroupInformation ticket
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",80,88,"/**
 * Creates a ProtocolProxy for the given protocol.
 * @param protocol Protocol class.
 * @return ProtocolProxy instance.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",121,130,"/**
 * Creates a ProtocolProxy for a specific protocol.
 * @param connId Connection ID.
 * @param conf Configuration.
 * @param factory Socket factory.
 * @return ProtocolProxy object.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getClient,org.apache.hadoop.ipc.WritableRpcEngine:getClient(org.apache.hadoop.conf.Configuration),279,283,"/**
 * Delegates client creation to a static inner class.
 * @param conf Configuration object used for client setup.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",219,230,"/**
 * Constructs an Invoker with provided configuration and parameters.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",103,111,"/**
 * Creates a ProtocolProxy for the given protocol.
 * @param protocol Protocol class.
 * @return ProtocolProxy instance.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",128,137,"/**
 * Creates a ProtocolProxy for a specific protocol.
 * @param connId Connection ID. @param conf Configuration.
 * @param factory Socket factory. Returns a ProtocolProxy.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",150,159,"/**
 * Constructs an Invoker with additional parameters.
 * @param fallbackToSimpleAuth Whether to fallback to simple auth
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),60,67,"/**
* Calls a method on a remote service using RPC.
* @param methodName Name of the method to call remotely.
* @return True if the call was successful, false otherwise.
*/
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),71,78,"/**
 * Calls RPC method.
 * @param methodName Name of the RPC method to call.
 * @return True if successful, false otherwise.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),106,113,"/**
 * Calls a method on the RPC service.
 * @param methodName Name of the method to call.
 * @return True if the call was successful, false otherwise.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),60,67,"/**
 * Calls a remote method via RPC.
 * @param methodName Name of the remote method to call.
 * @return True if call successful, false otherwise.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),61,66,"/**
 * Calls RPC method.
 * @param methodName Name of the RPC method to call.
 * @return True if call successful, false otherwise.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initRPC,org.apache.hadoop.ha.ZKFailoverController:initRPC(),330,334,"/**
 * Binds and starts the ZKFC RpcServer using configuration and address.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,<init>,org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.conf.Configuration),50,52,"/**
 * Constructs a Trash object using the provided Configuration.
 * @param conf Hadoop configuration object.
 * @throws IOException if an I/O error occurs.
 */
","* Construct a trash can accessor.
   * @param conf a Configuration
   * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,main,org.apache.hadoop.fs.DU:main(java.lang.String[]),91,102,"/**
 * Calculates and prints disk space used in a directory.
 * @param args Command-line arguments; first is directory path.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.String)",50,53,"/**
* Delegates to m2 with URI's host extracted via m1.
* @param conf Configuration object.
* @param uri URI object.
* @param configKey Key for configuration.
* @return DomainNameResolver object.
*/
","* Create a domain name resolver to convert the domain name in the config to
   * the actual IP addresses of the Namenode/Router/RM.
   *
   * @param conf Configuration to get the resolver from.
   * @param uri the url that the resolver will be used against
   * @param configKey The config key name suffixed with
   *                  the nameservice/yarnservice.
   * @return Domain name resolver.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setConfiguration,org.apache.hadoop.security.SecurityUtil:setConfiguration(org.apache.hadoop.conf.Configuration),98,103,"/**
 * Updates the configuration object.
 * @param conf The configuration object to update.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,<init>,org.apache.hadoop.security.UserGroupInformation$TestingGroups:<init>(org.apache.hadoop.security.Groups),1575,1578,"/**
 * Constructs a TestingGroups with a given underlying implementation.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingService,org.apache.hadoop.security.Groups:getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration),471,481,"/**
 * Returns the Groups instance, creating it if it doesn't exist.
 * @param conf Configuration object for initializing Groups.
 */
","* Get the groups being used to map user-to-groups.
   * @param conf configuration.
   * @return the groups being used to map user-to-groups.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingServiceWithLoadedConfiguration,org.apache.hadoop.security.Groups:getUserToGroupsMappingServiceWithLoadedConfiguration(org.apache.hadoop.conf.Configuration),488,495,"/**
 * Initializes and returns the Groups instance using the given config.
 */","* Create new groups used to map user-to-groups with loaded configuration.
   * @param conf configuration.
   * @return the groups being used to map user-to-groups.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,init,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:init(javax.servlet.FilterConfig),53,58,"/**
 * Initializes the filter configuration.
 * @param filterConfig Filter configuration object.
 * @throws ServletException if initialization fails.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:init(javax.servlet.FilterConfig),177,202,"/**
 * Initializes the filter, configuring authentication handlers and proxies.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration),86,88,"/**
* Calls m1 with the CONF_HADOOP_PROXYUSER property.
* @param conf Hadoop configuration object
*/
","* Refreshes configuration using the default Proxy user prefix for properties.
   * @param conf configuration",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,deleteOnExit,org.apache.hadoop.fs.FileContext:deleteOnExit(org.apache.hadoop.fs.Path),1706,1724,"/**
* Adds a file to the set of files to be deleted on exit.
* @param f The Path object representing the file to add.
* @return True if successful, false otherwise.
*/","* Mark a path to be deleted on JVM shutdown.
   * 
   * @param f the existing path to delete.
   *
   * @return  true if deleteOnExit is successful, otherwise false.
   *
   * @throws AccessControlException If access is denied
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,register,org.apache.hadoop.service.launcher.ServiceShutdownHook:register(int),61,64,"/**
 * Executes m1 and registers this object with the shutdown hook.
 * @param priority Priority level for shutdown hook registration.
 */","* Register the service for shutdown with Hadoop's
   * {@link ShutdownHookManager}.
   * @param priority shutdown hook priority",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,startupShutdownMessage,"org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.slf4j.Logger)",805,828,"/**
* Logs startup info, registers signal loggers (Unix), and sets shutdown hook.
*/","* Print a log message for starting up and shutting down
   * @param clazz the class of the server
   * @param args arguments
   * @param log the target log object",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,loadSSLConfiguration,org.apache.hadoop.http.HttpServer2$Builder:loadSSLConfiguration(),455,483,"/**
 * Initializes SSL configuration properties from the SSLFactory.
 * Handles potential IOExceptions if properties are missing.
 */",* Load SSL properties from the SSL configuration.,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,loadSslConf,org.apache.hadoop.security.LdapGroupsMapping:loadSslConf(org.apache.hadoop.conf.Configuration),874,891,"/**
* Initializes keystore and truststore using configuration.
* @param sslConf Configuration object containing SSL settings.
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPasswordForBindUser,org.apache.hadoop.security.LdapGroupsMapping:getPasswordForBindUser(java.lang.String),977,991,"/**
* Retrieves a password based on a key prefix, using fallback strategies.
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,createTrustManagersFromConfiguration,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createTrustManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,java.lang.String,long)",105,150,"/**
 * Initializes the trust manager with specified SSL configuration.
 * @param mode SSL mode, truststore details, and reload interval.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,createKeyManagersFromConfiguration,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createKeyManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,long)",160,205,"/**
 * Initializes the KeyManagers using keystore details from config.
 * @param mode SSLFactory mode
 * @param keystoreType Keystore type
 * @param storesReloadInterval Reload interval in milliseconds
 */","* Implements logic of initializing the KeyManagers with the options
   * to reload keystores.
   * @param mode client or server
   * @param keystoreType The keystore type.
   * @param storesReloadInterval The interval to check if the keystore certificates
   *                             file has changed.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getZKAuths,org.apache.hadoop.util.curator.ZKCuratorManager:getZKAuths(org.apache.hadoop.conf.Configuration),120,123,"/**
 * Retrieves ZK authentication information from the configuration.
 * @param conf Configuration object containing ZK authentication details.
 * @return List of ZKAuthInfo objects.
 */
","* Utility method to fetch ZK auth info from the configuration.
   *
   * @param conf configuration.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @throws ZKUtil.BadAuthFormatException if the auth format is invalid
   * @return ZKAuthInfo List.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initZK,org.apache.hadoop.ha.ZKFailoverController:initZK(),341,382,"/**
 * Initializes the ActiveStandbyElector with configurations from the provided context.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),41,50,"/**
 * Creates an ErasureEncodingStep using the provided block group.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),48,59,"/**
 * Creates an HHXORErasureEncodingStep with encoded blocks.
 * @param blockGroup ECBlockGroup to encode.
 * @return HHXORErasureEncodingStep object.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),41,50,"/**
 * Creates an ErasureDecodingStep with input/output blocks & decoder.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),49,65,"/**
 * Creates an HHXORErasureDecodingStep with provided block groups.
 * @param blockGroup ECBlockGroup used for erasure decoding.
 * @return HHXORErasureDecodingStep instance.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroupsSet(java.lang.String),92,105,"/**
 * Returns a set of groups for a user, applying case conversion.
 * @param user User identifier.
 * @return Set of groups, converted to uppercase/lowercase.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getGroups,org.apache.hadoop.security.LdapGroupsMapping:getGroups(java.lang.String),357,360,"/**
 * Returns a list of strings derived from user input.
 * @param user The input string.
 * @return A new list containing strings derived from the input.
 */
","* Returns list of groups for a user.
   * 
   * The LdapCtx which underlies the DirContext object is not thread-safe, so
   * we need to block around this whole method. The caching infrastructure will
   * ensure that performance stays in an acceptable range.
   *
   * @param user get groups for this user
   * @return list of groups for a given user",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,main,org.apache.hadoop.conf.Configuration:main(java.lang.String[]),3945,3947,"/**
* Initializes the configuration and redirects output stream.
*/","For debugging.  List non-default properties to the terminal and exit.
   * @param args the argument to be parsed.
   * @throws Exception exception.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,start,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:start(),178,194,"/**
 * Starts the metrics system, notifies callbacks, and logs start.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,<init>,org.apache.hadoop.http.HttpServer2:<init>(org.apache.hadoop.http.HttpServer2$Builder),699,724,"/**
 * Constructs a new HttpServer2 instance using the provided Builder.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",118,125,"/**
 * Creates a DelegationTokenManager using ZK or file-based secret management.
 * @param conf Hadoop configuration.
 * @param tokenKind Token kind.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,saslConnect,org.apache.hadoop.security.SaslRpcClient:saslConnect(org.apache.hadoop.ipc.Client$IpcStreams),365,455,"/**
* Negotiates authentication method with remote peer via SASL.
* @param ipcStreams streams for communication
* @return Authentication method used after negotiation
*/","* Do client side SASL authentication with server via the given IpcStreams.
   *
   * @param ipcStreams ipcStreams.
   * @return AuthMethod used to negotiate the connection
   * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,<init>,org.apache.hadoop.fs.local.RawLocalFs:<init>(org.apache.hadoop.conf.Configuration),42,44,"/**
 * Constructs a RawLocalFs instance using the default local FS URI.
 * @param conf Hadoop configuration object.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,<init>,"org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",37,41,"/**
 * Creates a FileSystemMultipartUploaderBuilder.
 * @param fileSystem The file system to use.
 * @param path The path to upload to.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String)",111,113,"/**
 * Constructs a PathData with a file system and path string.
 * @param fs The file system.
 * @param pathString The path string.
 * @throws IOException If an I/O error occurs.
 */
","* Looks up the file status for a path.  If the path
   * doesn't exist, then the status will be null
   * @param fs the FileSystem for the path
   * @param pathString a string for a path 
   * @throws IOException if anything goes wrong",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getDirectoryContents,org.apache.hadoop.fs.shell.PathData:getDirectoryContents(),274,285,"/**
 * Lists directory contents as PathData objects.
 * Returns an array of PathData, sorted alphabetically.
 */","* Returns a list of PathData objects of the items contained in the given
   * directory.
   * @return list of PathData objects for its children
   * @throws IOException if anything else goes wrong...",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,maybeIgnoreMissingDirectory,"org.apache.hadoop.fs.FileUtil:maybeIgnoreMissingDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.FileNotFoundException)",2094,2110,"/**
 * Throws FileNotFoundException if directory listing is inconsistent.
 * Logs if directory is missing and throws the exception.
 */","* Method to call after a FNFE has been raised on a treewalk, so as to
   * decide whether to throw the exception (default), or, if the FS
   * supports inconsistent directory listings, to log and ignore it.
   * If this returns then the caller should ignore the failure and continue.
   * @param fs filesystem
   * @param path path
   * @param e exception caught
   * @throws FileNotFoundException the exception passed in, if rethrown.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.DelegateToFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",287,292,"/**
* Delegates to the underlying FileSystem implementation.
* @param path Path to operate on.
* @param capability Capability string.
* @return Result of the delegated operation.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
 * Checks path capability; returns true for read-only connector.
 * @param path Path to check. @param capability Capability string.
 */","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.RawLocalFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1315,1332,"/**
* Checks if path has capability, returns true for supported cases.
* @param path Path to check.
* @param capability Capability to check for.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.viewfs.ViewFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1350,1371,"/**
 * Checks if a path has the specified capability.
 * @param path The path to check.
 * @param capability The capability to check for.
 */","* Reject the concat operation; forward the rest to the viewed FS.
   * @param path path to query the capability of.
   * @param capability string to query the stream support for.
   * @return the capability
   * @throws IOException if there is no resolved FS, or it raises an IOE.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),1373,1389,"/**
* Resolves a path, handling file not found and mount point issues.
* @param path The path to resolve.
* @return Resolved Path object.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path),1941,1958,"/**
 * Resolves path, handling FileNotFountException and returning a Path.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.FilterFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),735,738,"/**
* Delegates Path m1 operation to the underlying file system.
* @param path The Path object to operate on.
* @return The result of the m1 operation.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,fileSystem_getEnclosingRoot,"org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_getEnclosingRoot(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",202,204,"/**
* Delegates path processing to FileSystem's m1 method.
* @param fs The FileSystem to use.
* @param path The path to process.
* @return The processed Path.
*/
","* Return path of the enclosing root for a given path.
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   * @param fs filesystem
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.FilterFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",740,753,"/**
* Delegates capability check based on path capabilities.
* @param path Path to check. @param capability Capability string.
* @throws IOException if an I/O error occurs.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ViewFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",694,760,"/**
* Renames a file or directory from src to dst.
* Resolves paths and handles file system renaming.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",193,199,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to the file; permission, overwrite, bufferSize, etc.
 * @return FSDataOutputStream
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",201,208,"/**
 * Creates a data output stream for the given file path.
 * @param f path to the file
 * @return FSDataOutputStream
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",210,214,"/**
* Calls super.m2 with a modified Path and recursive flag.
* @param f the Path object
* @param recursive recursive flag
* @throws IOException if an I/O error occurs
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",223,228,"/**
* Calls super.m3 with a ViewFsFileStatus, passing start and len.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),230,234,"/**
* Calculates checksum for a file path, using modified input.
* @param f Path to the file
* @return FileChecksum object
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",236,240,"/**
* Calculates checksum of a file.
* @param f Path to the file. @param length File length.
* @return FileChecksum object.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileStatus(org.apache.hadoop.fs.Path),242,246,"/**
* Retrieves FileStatus, applying m1(f) before passing to super.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),248,251,"/**
* Delegates m2 processing after applying m1 to the Path.
* @param f The Path to process.
* @return The processed Path.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStatus(org.apache.hadoop.fs.Path),259,262,"/**
* Calls super.m2 with the result of m1(p).
* @param p The path to process.
* @return FsStatus returned by super.m2.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatus(org.apache.hadoop.fs.Path),264,268,"/**
* Gets file statuses for a path, using a modified path.
* @param f The path to get file statuses for.
* @return An array of FileStatus objects.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),270,274,"/**
 * Returns a remote iterator for the given path.
 * Delegates to super, passing a modified path.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",276,280,"/**
* Sets file permissions using parent's permission setting logic.
* @param f Path to the file. @param permission Desired permissions.
* @return True if successful, false otherwise.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,mkdirs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path),282,285,"/**
* Delegates m2 processing after applying m1 to the Path.
* @param f The Path to process.
* @return Result of the delegated m2 call.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:open(org.apache.hadoop.fs.Path,int)",287,291,"/**
 * Opens an input stream on a file. Delegates to superclass.
 * @param f Path to the file.
 * @param bufferSize Buffer size for the stream.
 * @return FSDataInputStream object.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",293,297,"/**
* Calls super.m2 with a modified path.
* @param f Path object
* @param bufferSize buffer size
* @param progress Progressable object
* @return FSDataOutputStream
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",299,304,"/**
* Copies a file from src to dst using m1 to transform paths.
* @param src source path
* @param dst destination path
* @return True if copy successful, false otherwise.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",306,311,"/**
 * Calls super.m2 with a normalized path, username, and groupname.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",313,317,"/**
* Calls super.m2 with a normalized path and given permission.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",319,323,"/**
* Delegates m2 call to superclass, using result of m1(f).
* @param f Path object
* @param replication replication factor
* @throws IOException if an I/O error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",325,329,"/**
 * Calls super.m2 with modified Path, mtime, and atime.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",331,335,"/**
* Calls super.m2 with a modified path, applying ACL specifications.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",337,341,"/**
* Calls super.m2 with a modified path, applying ACL specifications.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),343,346,"/**
* Calls super.m2 with the result of m1(path).
* @param path The path to process.
* @throws IOException if an I/O error occurs.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAcl(org.apache.hadoop.fs.Path),348,351,"/**
* Calls super.m2 with the result of m1(path).
* @param path The path to process.
* @throws IOException if an I/O error occurs.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",353,356,"/**
* Calls super.m2 with the result of m1(path) and aclSpec.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getAclStatus(org.apache.hadoop.fs.Path),358,361,"/**
* Delegates AclStatus retrieval, applying m1(Path) first.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",363,367,"/**
* Calls super.m2 with a modified path.
* @param path Path to operate on.
* @param name Attribute name.
* @param value Attribute value.
* @param flag Attribute set flags.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",369,372,"/**
* Calls super.m2 with a modified path and name.
* @param path The path to use.
* @param name The name to use.
* @return Byte array returned by super.m2.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path),374,377,"/**
* Calls super.m2 with the result of m1(path).
* @param path Path to process; returns a Map<String, byte[]>
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",379,383,"/**
* Delegates method execution to the superclass.
* @param path Input path. @param names List of names.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:truncate(org.apache.hadoop.fs.Path,long)",385,388,"/**
* Calls super.m2 with a modified path.
* @param path Path to modify.
* @param newLength New length for the file.
* @return Result of super.m2 call.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listXAttrs(org.apache.hadoop.fs.Path),390,393,"/**
* Delegates processing to the parent class after initial path handling.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",395,398,"/**
* Calls super.m2 with a modified path.
* @param path The path to modify.
* @param name The name associated with the path.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",400,403,"/**
* Calls super.m2 with a modified path.
* @param path The input path.
* @param name The name to use in the call.
* @return The result of the super call.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",405,409,"/**
* Calls super.m2 with a modified path.
* @param path The path to modify.
* @param snapshotOldName Old snapshot name.
* @param snapshotNewName New snapshot name.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",411,415,"/**
* Calls super.m2 with a modified snapshot directory.
* @param snapshotDir The snapshot directory path.
* @param snapshotName The name of the snapshot.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,resolvePath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:resolvePath(org.apache.hadoop.fs.Path),417,420,"/**
* Delegates processing to superclass after applying m1.
* @param p The input Path object.
* @return The processed Path object.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getContentSummary(org.apache.hadoop.fs.Path),422,425,"/**
* Delegates content summary retrieval to fs.m2 after processing Path.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),427,430,"/**
* Calculates quota usage for a file.
* @param f Path to the file.
* @return QuotaUsage object.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),439,442,"/**
* Calls super.m2() with the result of m1(f).
* @param f the Path object
* @return long value returned by super.m2()
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),449,452,"/**
* Calls super.m2 with the result of m1(f).
* @param f The input Path object.
* @return A short value returned by super.m2.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),464,467,"/**
* Creates a BlockStoragePolicySpi using a modified source path.
* @param src The source path to use.
* @return A BlockStoragePolicySpi object.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),469,472,"/**
* Calls super.m2 with the result of m1(src).
* @param src Path object; source path.
* @throws IOException if an I/O error occurs.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",474,477,"/**
* Calls super.m2 with a modified Path from src, using policyName.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),479,482,"/**
* Calls super.m2 with a modified Path obtained from m1(src).
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createFile,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createFile(org.apache.hadoop.fs.Path),484,487,"/**
* Calls super.m2() with the result of m1(path).
* @param path Path to be processed.
* @return FSDataOutputStreamBuilder object.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFile,org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.Path),4761,4765,"/**
* Creates a FutureDataInputStreamBuilder for the given path.
* @param path Path to the data stream; returns a builder.
*/","* Open a file for reading through a builder API.
   * Ultimately calls {@link #open(Path, int)} unless a subclass
   * executes the open command differently.
   *
   * The semantics of this call are therefore the same as that of
   * {@link #open(Path, int)} with one special point: it is in
   * {@code FSDataInputStreamBuilder.build()} in which the open operation
   * takes place -it is there where all preconditions to the operation
   * are checked.
   * @param path file path
   * @return a FSDataInputStreamBuilder object to build the input stream
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFile,org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.PathHandle),4780,4785,"/**
* Creates a FutureDataInputStreamBuilder for the given PathHandle.
* @param pathHandle Path to create the stream builder for.
* @return A FutureDataInputStreamBuilder instance.
*/","* Open a file for reading through a builder API.
   * Ultimately calls {@link #open(PathHandle, int)} unless a subclass
   * executes the open command differently.
   *
   * If PathHandles are unsupported, this may fail in the
   * {@code FSDataInputStreamBuilder.build()}  command,
   * rather than in this {@code openFile()} operation.
   * @param pathHandle path handle.
   * @return a FSDataInputStreamBuilder object to build the input stream
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,locateKeystore,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:locateKeystore(),145,176,"/**
 * Loads keystore password and sets file permissions.
 * Uses env var, file, or default password. Throws IOException.
 */","* Open up and initialize the keyStore.
   * @throws IOException If there is a problem reading the password file
   * or a problem reading the keystore.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,checkTFileDataIndex,org.apache.hadoop.io.file.tfile.TFile$Reader:checkTFileDataIndex(),882,893,"/**
 * Initializes tfileIndex if not already initialized.
 * Reads and creates TFileIndex object.
 */","* Lazily loading the TFile index.
     * 
     * @throws IOException",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getMetaBlock,org.apache.hadoop.io.file.tfile.TFile$Reader:getMetaBlock(java.lang.String),967,970,"/**
* Delegates BCF reader method.
* @param name The name to pass to the reader.
* @return DataInputStream returned by reader.
*/
","* Stream access to a meta block.``
     * 
     * @param name
     *          The name of the meta block.
     * @return The input stream.
     * @throws IOException
     *           on I/O error.
     * @throws MetaBlockDoesNotExist
     *           If the meta block with the name does not exist.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)",617,645,"/**
 * Initializes a Reader with input stream, file length, and configuration.
 */","* Constructor
     * 
     * @param fin
     *          FS input stream.
     * @param fileLength
     *          Length of the corresponding file
     * @throws IOException",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockReader,org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockReader(int),2035,2037,"/**
* Returns a BlockReader for the specified block index.
* @param blockIndex Index of the block to read.
* @return BlockReader object.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)",595,606,"/**
 * Creates a DataOutputStream for a Meta Block.
 * @param name Block name, compressName compression name.
 * @return DataOutputStream for writing the Meta Block.
 */","* Obtain an output stream for creating a meta block. This function may not
     * be called when there is a key append stream or value append stream
     * active. No more key-value insertion is allowed after a meta data block
     * has been added to TFile.
     * 
     * @param name
     *          Name of the meta block.
     * @param compressName
     *          Name of the compression algorithm to be used. Must be one of the
     *          strings returned by
     *          {@link TFile#getSupportedCompressionAlgorithms()}.
     * @return A DataOutputStream that can be used to write Meta Block data.
     *         Closing the stream would signal the ending of the block.
     * @throws IOException raised on errors performing I/O.
     * @throws MetaBlockAlreadyExists
     *           the Meta Block with the same name already exists.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer:close(),300,343,"/**
 * Closes the TFile, writing metadata and index to the block appender.
 */","* Close the Writer. Resources will be released regardless of the exceptions
     * being thrown. Future close calls will have no effect.
     * 
     * The underlying FSDataOutputStream is not closed.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareMetaBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String),623,632,"/**
 * Creates a Meta Block with the given name.
 * @param name Name of the Meta Block.
 * @return DataOutputStream for writing to the Meta Block.
 */","* Obtain an output stream for creating a meta block. This function may not
     * be called when there is a key append stream or value append stream
     * active. No more key-value insertion is allowed after a meta data block
     * has been added to TFile. Data will be compressed using the default
     * compressor as defined in Writer's constructor.
     * 
     * @param name
     *          Name of the meta block.
     * @return A DataOutputStream that can be used to write Meta Block data.
     *         Closing the stream would signal the ending of the block.
     * @throws IOException raised on errors performing I/O.
     * @throws MetaBlockAlreadyExists
     *           the Meta Block with the same name already exists.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareAppendKey,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendKey(int),527,537,"/**
 * Creates a new KeyRegister DataOutputStream of specified length.
 * @param length Length of the key to be registered.
 * @return A new KeyRegister DataOutputStream.
 */
","* Obtain an output stream for writing a key into TFile. This may only be
     * called when there is no active Key appending stream or value appending
     * stream.
     * 
     * @param length
     *          The expected length of the key. If length of the key is not
     *          known, set length = -1. Otherwise, the application must write
     *          exactly as many bytes as specified here before calling close on
     *          the returned output stream.
     * @return The key appending output stream.
     * @throws IOException raised on errors performing I/O.
     *",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),459,462,"/**
 * Retrieves FsServerDefaults using a modified Path.
 * @param f The Path to use, modified by m1().
 * @return FsServerDefaults object.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)",517,557,"/**
* Copies a file or directory to a destination file system.
* @param src Source file/directory.
* @param dstFS Destination file system.
* @param dst Destination path.
* @param deleteSource Delete source after copy?
*/","* Copy local files to a FileSystem.
   *
   * @param src src.
   * @param dstFS dstFs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,innerComplete,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerComplete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)",195,243,"/**
 * Creates a PathHandle for a multipart upload, handling file operations.
 * @param multipartUploadId Upload handle for multipart upload.
 * @param filePath Path to the file.
 * @param handleMap Map of part handles.
 * @return PathHandle for the created file.
 */","* The upload complete operation.
   * @param multipartUploadId the ID of the upload
   * @param filePath path
   * @param handleMap map of handles
   * @return the path handle
   * @throws IOException failure",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,touch,org.apache.hadoop.fs.shell.TouchCommands$Touch:touch(org.apache.hadoop.fs.shell.PathData),162,175,"/**
 * Processes a PathData item, creating it if it doesn't exist.
 * @param item PathData object containing file system and path.
 * @throws IOException if an I/O error occurs.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,touchz,org.apache.hadoop.fs.shell.TouchCommands$Touchz:touchz(org.apache.hadoop.fs.shell.PathData),88,90,"/**
* Executes file system operations on the given path data.
* @param item PathData object containing file system and path.
* @throws IOException if an I/O error occurs.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",742,749,"/**
 * Creates a data output stream for a file.
 * @param fs FileSystem object
 * @param file Path of the file
 * @param permission FsPermission for the file
 * @return FSDataOutputStream object
 */
","* Create a file with the provided permission.
   *
   * The permission of the file is set to be the provided permission as in
   * setPermission, not permission{@literal &~}umask
   *
   * The HDFS implementation is implemented using two RPCs.
   * It is understood that it is inefficient,
   * but the implementation is thread-safe. The other option is to change the
   * value of umask in configuration to be 0, but it is not thread-safe.
   *
   * @param fs FileSystem
   * @param file the name of the file to be created
   * @param permission the permission of the file
   * @return an output stream
   * @throws IOException IO failure",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)",1209,1215,"/**
 * Creates a Writer with specified filesystem, config, path, and classes.
 * @param fs Filesystem. @param conf Configuration. @param name Path.
 */
","* Create the named file.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,rollLogDir,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDir(),661,673,"/**
 * Creates a log file, appending or overwriting based on allowAppend.
 */","* Create a new directory based on the current interval and a new log file in
   * that directory.
   *
   * @throws IOException thrown if an error occurs while creating the
   * new directory or new log file",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,readIndex,org.apache.hadoop.io.MapFile$Reader:readIndex(),577,631,"/**
 * Reads keys and positions from an index, building a key list.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,next,"org.apache.hadoop.io.MapFile$Reader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",811,814,"/**
* Delegates to the underlying data structure's m1 method.
* @param key The key to use.
* @param val The value to associate with the key.
* @return Result of the underlying m1 method.
*/
","* Read the next key/value pair in the map into <code>key</code> and
     * <code>val</code>.  Returns true if such a pair exists and false when at
     * the end of the map.
     *
     * @param key WritableComparable.
     * @param val Writable.
     * @return if such a pair exists true,not false.
     * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",106,119,"/**
 * Creates a ProtocolProxy for a given protocol class.
 * @param protocol Protocol class to proxy.
 * @return ProtocolProxy instance.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",348,367,"/**
 * Creates a ProtocolProxy for the given protocol.
 * @param protocol Protocol interface.
 * @return ProtocolProxy object.
 * @throws IOException if an error occurs.
 */","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input ticket.
   * @param conf input configuration.
   * @param factory input factory.
   * @param rpcTimeout input rpcTimeout.
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @param fallbackToSimpleAuth input fallbackToSimpleAuth.
   * @param alignmentContext input alignmentContext.
   * @return ProtocolProxy.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",113,126,"/**
 * Creates a ProtocolProxy for a given protocol.
 * @param protocol Protocol class.
 * @return ProtocolProxy instance.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processArguments,org.apache.hadoop.fs.shell.Delete$Expunge:processArguments(java.util.LinkedList),244,273,"/**
* Processes file system trash operations based on configuration.
* @param args PathData objects (unused in this implementation)
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getTrash,org.apache.hadoop.fs.FsShell:getTrash(),86,91,"/**
 * Returns the Trash object, creating it if it's null.
 * Uses m1() to initialize the Trash object.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingService,org.apache.hadoop.security.Groups:getUserToGroupsMappingService(),462,464,"/**
 * Returns a Groups object, using the default Configuration.
 */","* Get the groups being used to map user-to-groups.
   * @return the groups being used to map user-to-groups.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,initialize,"org.apache.hadoop.security.UserGroupInformation:initialize(org.apache.hadoop.conf.Configuration,boolean)",309,354,"/**
 * Initializes Kerberos authentication settings based on configuration.
 * @param conf Hadoop configuration object
 * @param overrideNameRules Whether to override name rules
 */","* Initialize UGI and related classes.
   * @param conf the configuration to use",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,org.apache.hadoop.security.authorize.AccessControlList:<init>(),73,74,"/**
 * Constructs a new, empty AccessControlList.
 */",* This constructor exists primarily for AccessControlList to be Writable.,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String),85,87,"/**
 * Constructs an ACL from a string representation.
 * @param aclString String containing ACL data.
 */
","* Construct a new ACL from a String representation of the same.
   * 
   * The String is a a comma separated list of users and groups.
   * The user list comes first and is separated by a space followed 
   * by the group list. For e.g. ""user1,user2 group1,group2""
   * 
   * @param aclString String representation of the ACL",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,"org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String,java.lang.String)",97,99,"/**
 * Constructs an ACL with users and groups.
 * @param users Comma-separated list of users.
 * @param groups Comma-separated list of groups.
 */
","* Construct a new ACL from String representation of users and groups
   * 
   * The arguments are comma separated lists
   * 
   * @param users comma separated list of users
   * @param groups comma separated list of groups",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(),58,61,"/**
 * Calls m1 with a default Configuration object.
 */",* refresh Impersonation rules,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,coreServiceLaunch,"org.apache.hadoop.service.launcher.ServiceLauncher:coreServiceLaunch(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)",571,647,"/**
 * Launches and executes a service, handling configuration and shutdown.
 * @param conf Service configuration
 * @param instance Service instance
 * @return Exit code of the service execution
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,build,org.apache.hadoop.http.HttpServer2$Builder:build(),485,564,"/**
 * Creates and configures an HttpServer2 instance with specified endpoints.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,initializeBindUsers,org.apache.hadoop.security.LdapGroupsMapping:initializeBindUsers(),950,975,"/**
 * Configures bind users from configuration, or uses defaults.
 * Populates `bindUsers` and sets `currentBindUser`.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,init,org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:init(org.apache.hadoop.security.ssl.SSLFactory$Mode),252,300,"/**
 * Initializes SSL components based on the provided mode.
 * Configures keystore, truststore, and key/trust managers.
 */","* Initializes the keystores of the factory.
   *
   * @param mode if the keystores are to be used in client or server mode.
   * @throws IOException thrown if the keystores could not be initialized due
   * to an IO error.
   * @throws GeneralSecurityException thrown if the keystores could not be
   * initialized due to a security error.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,"org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List,boolean)",149,193,"/**
 * Initializes a CuratorFramework client with ZooKeeper configuration.
 * @param authInfos List of AuthInfo objects; may be modified.
 * @param sslEnabled Enables SSL connection to ZooKeeper.
 */","* Start the connection to the ZooKeeper ensemble.
   *
   * @param authInfos  List of authentication keys.
   * @param sslEnabled If the connection should be SSL/TLS encrypted.
   * @throws IOException            If the connection cannot be started.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doRun,org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[]),202,270,"/**
* Starts the failover controller, handling ZK connection and args.
* Returns error code or 0 on success.
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,getGroups,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroups(java.lang.String),76,90,"/**
 * Transforms user groups based on the 'rule' (upper/lower/none).
 * @param user User identifier. Returns list of transformed group names.
 */","* Returns list of groups for a user.
     * This calls {@link LdapGroupsMapping}'s getGroups and applies the
     * configured rules on group names before returning.
     *
     * @param user get groups for this user
     * @return list of groups for a given user",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,init,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:init(java.lang.String),148,176,"/**
 * Initializes the metrics system with a given prefix.
 * @param prefix Prefix for metrics names.
 * @return MetricsSystem instance.
 */","* Initialized the metrics system with a prefix.
   * @param prefix  the system will look for configs with the prefix
   * @return the metrics system object itself",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,initTokenManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:initTokenManager(java.util.Properties),148,163,"/**
* Initializes the DelegationTokenManager with properties.
* @param config Properties object for configuring the manager.
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupSaslConnection,org.apache.hadoop.ipc.Client$Connection:setupSaslConnection(org.apache.hadoop.ipc.Client$IpcStreams),571,579,"/**
 * Initializes and returns a SaslRpcClient, using remote ID details.
 * @param streams IpcStreams object for RPC communication.
 * @return SaslRpcClient instance.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalFs.java,<init>,org.apache.hadoop.fs.local.LocalFs:<init>(org.apache.hadoop.conf.Configuration),36,38,"/**
 * Constructs a LocalFs instance using the provided configuration.
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,suffix,org.apache.hadoop.fs.shell.PathData:suffix(java.lang.String),241,243,"/**
 * Creates a PathData object with the given extension.
 * @param extension file extension to append to the base path
 * @return PathData object representing the combined path
 */
","* Returns a new PathData with the given extension.
   * @param extension for the suffix
   * @return PathData
   * @throws IOException shouldn't happen",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getPathDataForChild,org.apache.hadoop.fs.shell.PathData:getPathDataForChild(org.apache.hadoop.fs.shell.PathData),307,310,"/**
 * Creates a PathData object for a child path, ensuring parent is a directory.
 * @param child The child path to create data for.
 * @return A PathData object representing the child path.
 */
","* Creates a new object for a child entry in this directory
   * @param child the basename will be appended to this object's path
   * @return PathData for the child
   * @throws IOException if this object does not exist or is not a directory",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,recursePath,org.apache.hadoop.fs.shell.Command:recursePath(org.apache.hadoop.fs.shell.PathData),449,466,"/**
 * Processes a PathData item, calling m4 with either m3 or m5.
 * @param item The PathData object to process.
 * @throws IOException if an I/O error occurs.
 */","*  Gets the directory listing for a path and invokes
   *  {@link #processPaths(PathData, PathData...)}
   *  @param item {@link PathData} for directory to recurse into
   *  @throws IOException if anything goes wrong...",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.HttpsFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
 * Checks path capabilities, returns true if read-only.
 * Otherwise, delegates to superclass.
 */","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.HttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
 * Checks path capabilities, returning true for FS_READ_ONLY.
 * @param path Path to check. @param capability Capability string.
 */","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",495,499,"/**
* Calls super.m2 with a modified path.
* @param path The path to modify.
* @param capability Capability string.
* @throws IOException if an I/O error occurs
*/
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.ChecksumFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1128,1140,"/**
 * Checks path capability, returns false for append/concat, else calls super.
 */","* Disable those operations which the checksummed FS blocks.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path),217,221,"/**
* Calls m1 with the provided Path and default 'true' value.
* @param f The Path object to process.
* @return True if successful, false otherwise.
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,updateFileStatus,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:updateFileStatus(org.apache.hadoop.fs.Path),131,136,"/**
* Retrieves file status. Uses m2 if possible, otherwise m3.
* @param f The Path of the file to get status for.
* @throws IOException if an I/O error occurs.
*/
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,listStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:listStatus(org.apache.hadoop.fs.Path),804,845,"/**
 * Retrieves FileStatus for a given path, handling exceptions.
 * @param f Path to retrieve FileStatus for.
 * @return FileStatus array or throws exception if errors.
 */","* Returns the closest non-failing destination's result.
   *
   * @param f given path
   * @return array of file statuses according to nfly modes
   * @throws FileNotFoundException
   * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.NflyFSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",866,873,"/**
 * Checks file access permissions for all nodes.
 * @param f Path to check. @param permission Permission to check against.
 * @return True if all nodes pass the permission check.
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,rename,"org.apache.hadoop.fs.viewfs.NflyFSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",739,765,"/**
* Renames files from src to dst, handling exceptions and logging.
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize(),434,437,"/**
* Calls m2 with the result of m1(rootPath).
* Returns a long value.
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication(),444,447,"/**
* Returns a short value calculated from rootPath via m1.
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",462,504,"/**
* Copies a file or directory from srcFS to dstFS.
* @param deleteSource Whether to delete source after copy.
*/","* Copy a file/directory tree within/between filesystems.
   * <p>
   * returns true if the operation succeeded. When deleteSource is true,
   * this means ""after the copy, delete(source) returned true""
   * If the destination is a directory, and mkdirs (dest) fails,
   * the operation will return false rather than raise any exception.
   * </p>
   * The overwrite flag is about overwriting files; it has no effect about
   * handing an attempt to copy a file atop a directory (expect an IOException),
   * or a directory over a path which contains a file (mkdir will fail, so
   * ""false"").
   * <p>
   * The operation is recursive, and the deleteSource operation takes place
   * as each subdirectory is copied. Therefore, if an operation fails partway
   * through, the source tree may be partially deleted.
   * </p>
   * @param srcFS source filesystem
   * @param srcStatus status of source
   * @param dstFS destination filesystem
   * @param dst path of source
   * @param deleteSource delete the source?
   * @param overwrite overwrite files at destination?
   * @param conf configuration to use when opening files
   * @return true if the operation succeeded.
   * @throws IOException failure",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.io.File,boolean,org.apache.hadoop.conf.Configuration)",578,605,"/**
 * Copies a file or directory. Deletes source if specified.
 * @param srcFS FileSystem of source.
 * @param dst Destination file.
 * @return True if copy successful.
 */
",Copy FileSystem files to local files.,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,openFile,org.apache.hadoop.fs.shell.PathData:openFile(java.lang.String),632,639,"/**
 * Opens a data input stream with specified policy.
 * @param policy Policy string for opening the file.
 * @return FSDataInputStream object.
 */","* Open a file.
   * @param policy fadvise policy.
   * @return an input stream
   * @throws IOException failure",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFile,org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.Path),709,713,"/**
* Delegates to the FileSystem's m1 method for FutureDataInputStreamBuilder.
* @param path Path to the file.
* @return FutureDataInputStreamBuilder object.
*/
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,openFile,"org.apache.hadoop.io.SequenceFile$Reader:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int,long)",2002,2012,"/**
 * Opens a data input stream for a file.
 * @param fs Filesystem, file path, buffer size, length.
 * @return FSDataInputStream
 */","* Override this method to specialize the type of
     * {@link FSDataInputStream} returned.
     * @param fs The file system used to open the file.
     * @param file The file being read.
     * @param bufferSize The buffer size used to read the file.
     * @param length The length being read if it is {@literal >=} 0.
     *               Otherwise, the length is not available.
     * @return The opened stream.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,load,"org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus)",264,283,"/**
 * Reads data from a file system path, handling potential errors.
 * @param fs Filesystem, path to read, and optional status.
 * @return Data read from the file or throws an exception.
 */","* Load from a Hadoop filesystem.
   * If a file status is supplied, it's passed in to the openFile()
   * call so that FS implementations can optimize their opening.
   * @param fs filesystem
   * @param path path
   * @param status status of the file to open.
   * @return a loaded object
   * @throws PathIOException JSON parse problem
   * @throws EOFException file status references an empty file
   * @throws IOException IO problems",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFile,org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.PathHandle),715,719,"/**
* Delegates to the FileSystem's m1 method.
* @param pathHandle Path handle to process.
* @return FutureDataInputStreamBuilder object.
*/
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFirstKey,org.apache.hadoop.io.file.tfile.TFile$Reader:getFirstKey(),901,904,"/**
* Delegates to m1() and returns tfileIndex.m2().
* Throws IOException if tfileIndex.m2() does.
*/","* Get the first key in the TFile.
     * 
     * @return The first key in the TFile.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLastKey,org.apache.hadoop.io.file.tfile.TFile$Reader:getLastKey(),912,915,"/**
* Delegates to m1() and returns tfileIndex.m2().
*/","* Get the last key in the TFile.
     * 
     * @return The last key in the TFile.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockContainsKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockContainsKey(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",985,995,"/**
 * Finds the block index for a key, considering greater flag.
 * @param key The key to search for.
 * @param greater True if seeking greater than key.
 * @return Location object or end if not found.
 */","* if greater is true then returns the beginning location of the block
     * containing the key strictly greater than input key. if greater is false
     * then returns the beginning location of the block greater than equal to
     * the input key
     * 
     * @param key
     *          the input key
     * @param greater
     *          boolean flag
     * @return
     * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationByRecordNum,org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationByRecordNum(long),997,1000,"/**
* Retrieves a location record from the transaction file index.
* @param recNum Record number to retrieve.
* @return Location object or null if not found.
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumByLocation,org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),1002,1005,"/**
* Delegates location-based retrieval to tfileIndex.
* @param location Location object for retrieval.
* @return Long value returned by tfileIndex.m2()
*/
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKeyNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getKeyNear(long),1063,1068,"/**
 * Reads a key from a block at the given offset.
 * @param offset Offset to the block; returns null if not found.
 */","* Get a sample key that is within a block whose starting offset is greater
     * than or equal to the specified offset.
     * 
     * @param offset
     *          The file offset.
     * @return the key that fits the requirement; or null if no such key exists
     *         (which could happen if the offset is close to the end of the
     *         TFile).
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)",802,818,"/**
 * Initializes a Reader with an FSDataInputStream, file length, and config.
 * @param fsdis Input stream for file data.
 * @param fileLength Length of the file.
 * @param conf Hadoop configuration.
 * @throws IOException If an I/O error occurs.
 */","* Constructor
     * 
     * @param fsdis
     *          FS input stream of the TFile.
     * @param fileLength
     *          The length of TFile. This is required because we have no easy
     *          way of knowing the actual size of the input file through the
     *          File input stream.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,initBlock,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:initBlock(int),1548,1559,"/**
 * Initializes block reader for a specific block index.
 * @param blockIndex Index of the block to read.
 */","* Load a compressed block for reading. Expecting blockIndex is valid.
       * 
       * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,append,"org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],int,int,byte[],int,int)",380,413,"/**
* Writes key/value data to streams, validating buffer offsets/lengths.
*/","* Adding a new key-value pair to TFile.
     * 
     * @param key
     *          buffer for key.
     * @param koff
     *          offset in key buffer.
     * @param klen
     *          length of key.
     * @param value
     *          buffer for value.
     * @param voff
     *          offset in value buffer.
     * @param vlen
     *          length of value.
     * @throws IOException
     *           Upon IO errors.
     *           <p>
     *           If an exception is thrown, the TFile will be in an inconsistent
     *           state. The only legitimate call after that would be close",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults(),454,457,"/**
* Returns FsServerDefaults using a derived root path.
* @return FsServerDefaults object
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processPath,org.apache.hadoop.fs.shell.TouchCommands$Touch:processPath(org.apache.hadoop.fs.shell.PathData),148,151,"/**
* Processes a PathData item by calling m1.
* @param item The PathData object to process.
* @throws IOException if an I/O error occurs.
*/
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processNonexistentPath,org.apache.hadoop.fs.shell.TouchCommands$Touch:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),153,160,"/**
* Processes a PathData item. Throws PathNotFoundException if invalid.
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processPath,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processPath(org.apache.hadoop.fs.shell.PathData),67,77,"/**
 * Validates PathData: checks if it's a directory or non-zero length.
 * @param item PathData object to validate.
 * @throws IOException if validation fails.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processNonexistentPath,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),79,86,"/**
 * Processes a PathData item. Throws PathNotFoundException if invalid.
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,getOutputStreamForKeystore,org.apache.hadoop.security.alias.KeyStoreProvider:getOutputStreamForKeystore(),52,56,"/**
 * Creates and returns a FileSystem data output stream.
 * @return FSDataOutputStream - stream for writing data.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,writeToNew,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:writeToNew(org.apache.hadoop.fs.Path),607,620,"/**
 * Stores the keystore to the specified path.
 * @param newPath Path to store the keystore.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,midKey,org.apache.hadoop.io.MapFile$Reader:midKey(),649,657,"/**
 * Returns the median key. Returns null if no keys are present.
 */","* Get the key at approximately the middle of the file. Or null if the
     *  file is empty.
     *
     * @throws IOException raised on errors performing I/O.
     * @return WritableComparable.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,finalKey,org.apache.hadoop.io.MapFile$Reader:finalKey(org.apache.hadoop.io.WritableComparable),665,681,"/**
 * Processes data based on the provided key.
 * @param key The key used for data processing.
 * @throws IOException if an I/O error occurs.
 */
","* Reads the final key from the file.
     *
     * @param key key to read into
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seekInternal,"org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable,boolean)",721,780,"/**
* Finds the position for a key, adjusting seek index as needed.
* @param key The key to search for.
* @param before True to find the position before the key.
* @return Comparison result: <0 if key is before, >0 otherwise.
*/
","* Positions the reader at the named key, or if none such exists, at the
     * key that falls just before or just after dependent on how the
     * <code>before</code> parameter is set.
     * 
     * @param before - IF true, and <code>key</code> does not exist, position
     * file at entry that falls just before <code>key</code>.  Otherwise,
     * position file at record that sorts just after.
     * @return  0   - exact match found
     *          < 0 - positioned at next record
     *          1   - no more records in file",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,mergePass,org.apache.hadoop.io.MapFile$Merger:mergePass(),1101,1146,"/**
 * Reads and writes key-value pairs from multiple readers.
 * Reads until all readers are exhausted or an error occurs.
 */","* Merge all input files to output map file.<br>
     * 1. Read first key/value from all input files to keys/values array. <br>
     * 2. Select the least key and corresponding value. <br>
     * 3. Write the selected key and value to output file. <br>
     * 4. Replace the already written key/value in keys/values arrays with the
     * next key/value from the selected input <br>
     * 5. Repeat step 2-4 till all keys are read. <br>",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",97,104,"/**
 * Creates a ProtocolProxy. Overloads m1 with null arguments.
 * @param protocol Protocol class.
 * @return ProtocolProxy object.
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",299,307,"/**
 * Creates a ProtocolProxy. Overloads m1 with null arguments.
 * @param protocol Protocol class.
 * @return ProtocolProxy object.
 */","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type T
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input ticket.
   * @param conf input configuration.
   * @param factory input factory.
   * @param rpcTimeout input rpcTimeout.
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",322,330,"/**
 * Creates a ProtocolProxy with extracted connection details.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @return ProtocolProxy object.
 */","* Construct a client-side proxy object with a ConnectionId.
   *
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param connId input ConnectionId.
   * @param conf input Configuration.
   * @param factory input factory.
   * @param alignmentContext Alignment context
   * @throws IOException raised on errors performing I/O.
   * @return ProtocolProxy.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",93,101,"/**
 * Creates a ProtocolProxy. Overloads m1 with null arguments.
 * @param protocol Protocol class.
 * @return ProtocolProxy object.
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getCurrentTrashDir,org.apache.hadoop.fs.FsShell:getCurrentTrashDir(),125,127,"/**
* Delegates m2() call to the result of m1().
* @return Path object returned by m1().m2()
*/
","* Returns the Trash object associated with this shell.
   * @return Path to the trash
   * @throws IOException upon error",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getCurrentTrashDir,org.apache.hadoop.fs.FsShell:getCurrentTrashDir(org.apache.hadoop.fs.Path),135,137,"/**
* Delegates Path processing to m1().m2(path).
* @param path The Path to process.
* @return Processed Path.
*/
","* Returns the current trash location for the path specified
   * @param path to be deleted
   * @return path to the trash
   * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,ensureInitialized,org.apache.hadoop.security.UserGroupInformation:ensureInitialized(),295,303,"/**
 * Executes m2 if m1 fails, synchronizing access to prevent conflicts.
 */","* A method to initialize the fields that depend on a configuration.
   * Must be called before useKerberos or groups is used.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setConfiguration,org.apache.hadoop.security.UserGroupInformation:setConfiguration(org.apache.hadoop.conf.Configuration),362,366,"/**
 * Calls m1 with the provided configuration and true flag.
 * @param conf The Hadoop configuration object.
 */","* Set the static configuration for UGI.
   * In particular, set the security authentication mechanism and the
   * group look up service.
   * @param conf the configuration to use",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,refreshWithLoadedConfiguration,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refreshWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",152,200,"/**
 * Initializes ACLs and machine lists based on configuration and provider.
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,init,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:init(java.lang.String),68,101,"/**
 * Processes configuration entries for users, groups, and hosts.
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getSip,org.apache.hadoop.security.authorize.ProxyUsers:getSip(),116,124,"/**
 * Lazily initializes and returns the ImpersonationProvider instance.
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchService,"org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)",485,539,"/**
 * Executes service, handles exceptions, and creates an ExitException.
 * @param conf Configuration object
 * @param instance Service instance
 * @return ExitUtil.ExitException representing service exit status
 */","* Launch a service catching all exceptions and downgrading them to exit codes
   * after logging.
   *
   * Sets {@link #serviceException} to this value.
   * @param conf configuration to use
   * @param instance optional instance of the service.
   * @param processedArgs command line after the launcher-specific arguments
   * have been stripped out.
   * @param addShutdownHook should a shutdown hook be added to terminate
   * this service on shutdown. Tests should set this to false.
   * @param execute execute/wait for the service to stop.
   * @return an exit exception, which will have a status code of 0 if it worked",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,setConf,org.apache.hadoop.security.LdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),767,864,"/**
 * Initializes LDAP configuration from provided Configuration object.
 * @param conf Configuration object containing LDAP settings.
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List),138,140,"/**
* Calls m1 with authInfos and false as the second argument.
* @param authInfos List of authentication information.
* @throws IOException if an I/O error occurs.
*/
","* Start the connection to the ZooKeeper ensemble.
   * @param authInfos List of authentication keys.
   * @throws IOException If the connection cannot be started.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:init(java.util.Properties),127,132,"/**
 * Initializes resources using the provided configuration.
 * @param config Properties object containing configuration details.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalFs.java,<init>,"org.apache.hadoop.fs.local.LocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",49,52,"/**
 * Constructs a LocalFs object using the provided configuration.
 * @param theUri The URI of the local file system.
 * @param conf The configuration object.
 */
","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,copyStreamToTarget,"org.apache.hadoop.fs.shell.CommandWithDestination:copyStreamToTarget(java.io.InputStream,org.apache.hadoop.fs.shell.PathData)",418,434,"/**
* Copies data from an InputStream to a PathData target.
* @param in Input stream containing data to copy.
* @param target PathData to copy data to.
*/","* If direct write is disabled ,copies the stream contents to a temporary
   * file ""target._COPYING_"". If the copy is successful, the temporary file
   * will be renamed to the real path, else the temporary file will be deleted.
   * if direct write is enabled , then creation temporary file is skipped.
   *
   * @param in     the input stream for the copy
   * @param target where to store the contents of the stream
   * @throws IOException if copy fails",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getTargetPath,org.apache.hadoop.fs.shell.CommandWithDestination:getTargetPath(org.apache.hadoop.fs.shell.PathData),330,342,"/**
 * Processes PathData based on conditions, returning a PathData object.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,cleanupAllTmpFiles,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:cleanupAllTmpFiles(),399,407,"/**
 * Deletes temporary files for each node, handling potential errors.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,commit,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:commit(),409,444,"/**
* Commits changes to NflyNodes, handling exceptions and setting timestamps.
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,delete,"org.apache.hadoop.fs.viewfs.NflyFSystem:delete(org.apache.hadoop.fs.Path,boolean)",768,793,"/**
* Attempts to process files in nodes, recursively if specified.
* @param f Path to file. @param recursive Whether to recurse.
* @return True if successful, false otherwise.
*/
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",426,433,"/**
 * Copies a file or directory.
 * @param srcFS, dstFS FileSystems. @param src, dst Paths.
 * @param deleteSource, overwrite Flags. @param conf Configuration.
 */
","* Copy files between FileSystems.
   *
   * @param srcFS srcFs.
   * @param src src.
   * @param dstFS dstFs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param overwrite overwrite.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,repairAndOpen,"org.apache.hadoop.fs.viewfs.NflyFSystem:repairAndOpen(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode[],org.apache.hadoop.fs.Path,int)",636,713,"/**
* Repairs and opens a data stream based on node statuses.
* @param mrNodes Array of MRNflyNode objects.
* @param f Path to the file.
* @param bufferSize Size of the buffer.
* @return FSDataInputStream or null if failed.
*/","* Iterate all available nodes in the proximity order to attempt repair of all
   * FileNotFound nodes.
   *
   * @param mrNodes work set copy of nodes
   * @param f path to repair and open
   * @param bufferSize buffer size for read RPC
   * @return the closest/most recent replica stream AFTER repair",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.File,boolean,org.apache.hadoop.conf.Configuration)",570,575,"/**
 * Copies a file from a FileSystem to a local file.
 * @param srcFS FileSystem object
 * @param src Path to source file
 * @param dst Destination file
 * @param deleteSource Delete source after copy?
 * @param conf Configuration object
 * @return True if copy succeeds, false otherwise.
 */","* Copy FileSystem files to local files.
   *
   * @param srcFS srcFs.
   * @param src src.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,openForSequentialIO,org.apache.hadoop.fs.shell.PathData:openForSequentialIO(),621,624,"/**
 * Opens a sequential file input stream with default policy.
 * @return FSDataInputStream for sequential file access.
 */","* Open a file for sequential IO.
   * <p>
   * This uses FileSystem.openFile() to request sequential IO;
   * the file status is also passed in.
   * Filesystems may use to optimize their IO.
   * </p>
   * @return an input stream
   * @throws IOException failure",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,dumpToOffset,org.apache.hadoop.fs.shell.Head:dumpToOffset(org.apache.hadoop.fs.shell.PathData),72,77,"/**
 * Copies data from a PathData input stream to System.out.
 * @param item PathData object to read from
 * @throws IOException if an I/O error occurs
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,dumpFromOffset,"org.apache.hadoop.fs.shell.Tail:dumpFromOffset(org.apache.hadoop.fs.shell.PathData,long)",105,121,"/**
* Reads data from a file and updates offset.
* @param item PathData object, offset starting position.
* @return Updated offset after reading.
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,openFile,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:openFile(org.apache.hadoop.fs.Path),489,493,"/**
* Delegates to super.m2 with a modified Path.
* @param path The Path to be processed.
* @return FutureDataInputStreamBuilder object.
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,load,"org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",248,250,"/**
* Calls m1 with a null resolver.
* @param fs FileSystem to use.
* @param path Path to process.
* @return Result of processing the path.
*/","* Load from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return a loaded object
   * @throws PathIOException JSON parse problem
   * @throws IOException IO problems",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumNear(long),1048,1050,"/**
* Applies m1 and m2 to the offset.
* @param offset The input offset value.
* @return The processed offset value.
*/
","* Get the RecordNum for the first key-value pair in a compressed block
     * whose byte offset in the TFile is greater than or equal to the specified
     * offset.
     * 
     * @param offset
     *          the user supplied offset.
     * @return the RecordNum to the corresponding entry. If no such entry
     *         exists, it returns the total entry count.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNum,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:getRecordNum(),1629,1631,"/**
* Reads data using reader.m1 with currentLocation.
* @return Long value returned by reader.m1.
*/
","* Get the RecordNum corresponding to the entry pointed by the cursor.
       * @return The RecordNum corresponding to the entry pointed by the cursor.
       * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.TFile$Reader$Location,org.apache.hadoop.io.file.tfile.TFile$Reader$Location)",1281,1303,"/**
 * Initializes a Scanner with a Reader and location boundaries.
 * @param reader Reader to read from; @param begin, @param end Location bounds.
 * @throws IOException if an I/O error occurs.
 */
","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param begin
       *          Begin location of the scan.
       * @param end
       *          End location of the scan.
       * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),1396,1429,"/**
* Validates location and adjusts state based on its properties.
* @param l The Location object to validate.
*/","* Move the cursor to the new location. The entry returned by the previous
       * entry() call will be invalid.
       * 
       * @param l
       *          new cursor location. It must fall between the begin and end
       *          location of the scanner.
       * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,advance,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:advance(),1521,1541,"/**
* Processes location data, potentially advancing the reader.
*/","* Move the cursor to the next key-value pair. The entry returned by the
       * previous entry() call will be invalid.
       * 
       * @return true if the cursor successfully moves. False when cursor is
       *         already at the end location and cannot be advanced.
       * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,append,"org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],byte[])",355,357,"/**
* Overloaded method to write a byte array to a destination.
* @param key Key for encryption/decryption.
* @param value Data to be written.
*/","* Adding a new key-value pair to the TFile. This is synonymous to
     * append(key, 0, key.length, value, 0, value.length)
     * 
     * @param key
     *          Buffer for key.
     * @param value
     *          Buffer for value.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,flush,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:flush(),529,584,"/**
 * Renames files, updates metadata, and handles potential IO errors.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seekInternal,org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable),704,707,"/**
* Calls m1 with the given key and default flag (false).
* @param key The key to pass to the overloaded method.
* @throws IOException if an I/O error occurs.
*/
","* Positions the reader at the named key, or if none such exists, at the
     * first entry after the named key.
     *
     * @return  0   - exact match found
     *          < 0 - positioned at next record
     *          1   - no more records in file",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getClosest,"org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,boolean)",859,875,"/**
 * Filters and processes key-value pairs based on a comparison.
 * @param key The key to compare.
 * @param val The value to process.
 * @param before Flag indicating comparison direction.
 * @return Next key or null if filtered.
 */","* Finds the record that is the closest match to the specified key.
     * 
     * @param key       - key that we're trying to find
     * @param val       - data value if key is found
     * @param before    - IF true, and <code>key</code> does not exist, return
     * the first entry that falls just before the <code>key</code>.  Otherwise,
     * return the record that sorts just after.
     * @return          - the key that was the closest match or null if eof.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",90,95,"/**
 * Creates a ProtocolProxy. Overloads m1 with null as the async handler.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @param addr Remote address.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",86,91,"/**
 * Creates a ProtocolProxy. Overloads m1 with null as the async handler.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @param addr Remote address.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isAuthenticationMethodEnabled,org.apache.hadoop.security.UserGroupInformation:isAuthenticationMethodEnabled(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod),391,396,"/**
 * Checks if the given method matches the authenticationMethod.
 * @param method Authentication method to compare.
 * @return True if methods match, false otherwise.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isKerberosKeyTabLoginRenewalEnabled,org.apache.hadoop.security.UserGroupInformation:isKerberosKeyTabLoginRenewalEnabled(),398,404,"/**
 * Checks if kerberos keytab login renewal is enabled.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKerberosLoginRenewalExecutor,org.apache.hadoop.security.UserGroupInformation:getKerberosLoginRenewalExecutor(),406,412,"/**
 * Returns the kerberos login renewal executor.
 * Calls m1() before returning the executor.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createUserForTesting,"org.apache.hadoop.security.UserGroupInformation:createUserForTesting(java.lang.String,java.lang.String[])",1607,1620,"/**
 * Creates a UserGroupInformation object with provided user and groups.
 * @param user User name.
 * @param userGroups Array of user group names.
 * @return UserGroupInformation object.
 */
","* Create a UGI for testing HDFS and MapReduce
   * @param user the full user principal name
   * @param userGroups the names of the groups that the user belongs to
   * @return a fake user for running unit tests",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createProxyUserForTesting,"org.apache.hadoop.security.UserGroupInformation:createProxyUserForTesting(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.String[])",1634,1645,"/**
 * Creates a UserGroupInformation object, populating group memberships.
 * @param user User identifier.
 * @param realUser Existing UserGroupInformation.
 * @param userGroups Array of user group names.
 * @return UserGroupInformation object.
 */
","* Create a proxy user UGI for testing HDFS and MapReduce
   * 
   * @param user
   *          the full user principal name for effective user
   * @param realUser
   *          UGI of the real user
   * @param userGroups
   *          the names of the groups that the user belongs to
   * @return a fake user for running unit tests",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroups,org.apache.hadoop.security.UserGroupInformation:getGroups(),1790,1799,"/**
 * Retrieves a list of groups for the current user.
 * Returns an empty list if an IO error occurs.
 */","* Get the group names for this user. {@link #getGroupsSet()} is less
   * expensive alternative when checking for a contained element.
   * @return the list of users with the primary group first. If the command
   *    fails, it returns an empty list.
   * @deprecated Use {@link #getGroupsSet()} instead.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupsSet,org.apache.hadoop.security.UserGroupInformation:getGroupsSet(),1806,1814,"/**
 * Retrieves groups for the user. Returns empty set on failure.
 */","* Get the groups names for the user as a Set.
   * @return the set of users with the primary group first. If the command
   *     fails, it returns an empty set.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,doSubjectLogin,"org.apache.hadoop.security.UserGroupInformation:doSubjectLogin(javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$LoginParams)",2042,2073,"/**
* Obtains UserGroupInformation based on subject and login params.
* @param subject Subject for authentication, may be null.
* @param params Login parameters, may be null.
* @return UserGroupInformation object.
* @throws IOException if login fails.
*/","* Login a subject with the given parameters.  If the subject is null,
   * the login context used to create the subject will be attached.
   * @param subject to login, null for new subject.
   * @param params for login, null for externally managed ugi.
   * @return UserGroupInformation for subject
   * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,init,org.apache.hadoop.fs.FsShell:init(),100,109,"/**
 * Initializes and configures the command factory.
 * Sets up default commands and registers the factory.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshServiceAclWithLoadedConfiguration,"org.apache.hadoop.ipc.Server:refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",778,782,"/**
* Delegates authorization management to serviceAuthorizationManager.
* @param conf Configuration object. @param provider Policy provider.
*/","* Refresh the service authorization ACL for the service handled by this server
   * using the specified Configuration.
   *
   * @param conf input Configuration.
   * @param provider input provider.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getTestProvider,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getTestProvider(),52,59,"/**
 * Returns the DefaultImpersonationProvider instance, initializing it if needed.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",99,102,"/**
 * Delegates method m2 to the singleton instance of m1.
 * @param user UserGroupInformation object
 * @param remoteAddress Remote address string
 */","* Authorize the superuser which is doing doAs.
   * {@link #authorize(UserGroupInformation, InetAddress)} should be preferred
   * to avoid possibly re-resolving the ip address.
   *
   * @param user ugi of the effective or proxy user which contains a real user
   * @param remoteAddress the ip address of client
   * @throws AuthorizationException Authorization Exception.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)",111,114,"/**
 * Delegates method m2 to the singleton instance of m1.
 * @param user UserGroupInformation object
 * @param remoteAddress Remote address object
 */","* Authorize the superuser which is doing doAs.
   *
   * @param user ugi of the effective or proxy user which contains a real user
   * @param remoteAddress the inet address of client
   * @throws AuthorizationException Authorization Exception.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getDefaultImpersonationProvider,org.apache.hadoop.security.authorize.ProxyUsers:getDefaultImpersonationProvider(),140,143,"/**
* Returns the DefaultImpersonationProvider instance.
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchService,"org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)",464,469,"/**
* Delegates to overloaded method with null argument.
* @param conf Configuration object.
* @param processedArgs List of processed arguments.
*/","* Launch a service catching all exceptions and downgrading them to exit codes
   * after logging.
   *
   * Sets {@link #serviceException} to this value.
   * @param conf configuration to use
   * @param processedArgs command line after the launcher-specific arguments
   * have been stripped out.
   * @param addShutdownHook should a shutdown hook be added to terminate
   * this service on shutdown. Tests should set this to false.
   * @param execute execute/wait for the service to stop.
   * @return an exit exception, which will have a status code of 0 if it worked",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,setConf,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),56,66,"/**
 * Initializes a rule based on the configuration.
 * @param conf Configuration object; retrieves conversion rule.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,org.apache.hadoop.util.curator.ZKCuratorManager:start(),129,131,"/**
* Calls m1 with an empty list as an argument.
*/","* Start the connection to the ZooKeeper ensemble.
   * @throws IOException If the connection cannot be started.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,init,org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:init(java.util.Properties),99,126,"/**
* Initializes authentication schemes from configuration.
* @param config Servlet configuration properties.
* @throws ServletException if schemes are misconfigured.
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,copyFileToTarget,"org.apache.hadoop.fs.shell.CommandWithDestination:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",350,367,"/**
* Copies data from src PathData to target, preserving xattrs.
* @param src Source PathData
* @param target Target PathData
*/","* Copies the source file to the target.
   * @param src item to copy
   * @param target where to copy the item
   * @throws IOException if copy fails",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPathArgument,org.apache.hadoop.fs.shell.CommandWithDestination:processPathArgument(org.apache.hadoop.fs.shell.PathData),247,274,"/**
 * Compares source and destination paths, throws exception if identical.
 * @param src PathData object representing the source path.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,recursePath,org.apache.hadoop.fs.shell.CommandWithDestination:recursePath(org.apache.hadoop.fs.shell.PathData),299,328,"/**
* Processes PathData, updating 'dst' and handling directory creation.
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,close,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:close(),378,397,"/**
* Closes output streams and ensures sufficient replication.
* Throws IOException if replication is insufficient.
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)",366,371,"/**
* Moves a file/directory from src to dst.
* @param srcFS, src, dstFS, dst, deleteSource, conf move details
* @throws IOException if an I/O error occurs
*/
","* Copy files between FileSystems.
   * @param srcFS src fs.
   * @param src src.
   * @param dstFS dst fs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @return if copy success true, not false.
   * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",373,411,"/**
* Copies multiple files from srcFS to dstFS.
* @param srcs source paths, @param dst destination path
* @return true if all copies succeed, false otherwise.
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,open,"org.apache.hadoop.fs.viewfs.NflyFSystem:open(org.apache.hadoop.fs.Path,int)",579,621,"/**
 * Opens a data input stream for a path, handling exceptions and repair.
 * @param f the path to open
 * @param bufferSize the buffer size
 * @return FSDataInputStream or throws exception
*/","* Category: READ.
   *
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * @return input stream according to nfly flags (closest, most recent)
   * @throws IOException
   * @throws FileNotFoundException iff all destinations generate this exception",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$Merge:processArguments(java.util.LinkedList),91,114,"/**
* Copies PathData items, skipping files based on exit code/flags.
* @param items LinkedList of PathData objects to copy.
* @throws IOException if an I/O error occurs during the process.
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,getInputStream,org.apache.hadoop.fs.shell.Display$Cat:getInputStream(org.apache.hadoop.fs.shell.PathData),106,109,"/**
 * Delegates InputStream retrieval to item's m1() method.
 * @param item PathData object; provides InputStream.
 * @return InputStream obtained from item.
 */
",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,processPath,org.apache.hadoop.fs.shell.Head:processPath(org.apache.hadoop.fs.shell.PathData),63,70,"/**
 * Processes a PathData item, throwing exception if it's a directory.
 * @param item The PathData item to process.
 * @throws IOException If an I/O error occurs or item is a directory.
 */
",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,processPath,org.apache.hadoop.fs.shell.Tail:processPath(org.apache.hadoop.fs.shell.PathData),88,103,"/**
* Follows a path, throwing exception if directory.
* Uses m3 to update offset, delays with Thread.sleep.
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(),1077,1079,"/**
 * Creates a new Scanner for parsing input from this stream.
 * @return A Scanner instance.
 */
","* Get a scanner than can scan the whole TFile.
     * 
     * @return The scanner object. A valid Scanner is always returned even if
     *         the TFile is empty.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByRecordNum,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByRecordNum(long,long)",1194,1202,"/**
 * Creates a Scanner for records between beginRecNum and endRecNum.
 * @param beginRecNum Start record number (>= 0).
 * @param endRecNum End record number (<= total records).
 */","* Create a scanner that covers a range of records.
     * 
     * @param beginRecNum
     *          The RecordNum for the first record (inclusive).
     * @param endRecNum
     *          The RecordNum for the last record (exclusive). To scan the whole
     *          file, either specify endRecNum==-1 or endRecNum==getEntryCount().
     * @return The TFile scanner that covers the specified range of records.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,long,long)",1264,1268,"/**
 * Creates a Scanner with specified character offsets.
 * @param reader Reader object to read from.
 * @param offBegin Start offset (inclusive).
 * @param offEnd End offset (exclusive).
 */
","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param offBegin
       *          Begin byte-offset of the scan.
       * @param offEnd
       *          End byte-offset of the scan.
       * @throws IOException
       * 
       *           The offsets will be rounded to the beginning of a compressed
       *           block whose offset is greater than or equal to the specified
       *           offset.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",1366,1385,"/**
 * Recursively searches for a location based on a key.
 * @param key The key to search for.
 * @param beyond Flag to indicate searching beyond the key.
 * @return True if found, false otherwise.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,rewind,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:rewind(),1437,1439,"/**
* Calls method m1 with the beginLocation value.
*/","* Rewind to the first entry in the scanner. The entry returned by the
       * previous entry() call will be invalid.
       * 
       * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seek,org.apache.hadoop.io.MapFile$Reader:seek(org.apache.hadoop.io.WritableComparable),692,694,"/**
* Checks if the value for the given key is zero.
* @param key The key to check.
* @return True if the value is zero, false otherwise.
*/
","* Positions the reader at the named key, or if none such exists, at the
     * first entry after the named key.  Returns true iff the named key exists
     * in this map.
     *
     * @param key key.
     * @throws IOException raised on errors performing I/O.
     * @return if the named key exists in this map true, not false.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getClosest,"org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",842,846,"/**
* Calls m1 with the key, val, and false for the 'isBinary' flag.
*/","* Finds the record that is the closest match to the specified key.
     * Returns <code>key</code> or if it does not exist, at the first entry
     * after the named key.
     * 
     * @param key key that we're trying to find.
     * @param val data value if key is found.
     * @return the key that was the closest match or null if eof.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isSecurityEnabled,org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled(),387,389,"/**
* Returns true if authentication fails using SIMPLE method.
*/","* Determine if UserGroupInformation is using Kerberos to determine
   * user identities or is relying on simple authentication
   * 
   * @return true if UGI is working in a secure environment",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logoutUserFromKeytab,org.apache.hadoop.security.UserGroupInformation:logoutUserFromKeytab(),1158,1189,"/**
* Logs out the user, initiating Kerberos logout sequence.
* Throws KerberosAuthException if logout fails.
*/","* Log the current user out who previously logged in using keytab.
   * This method assumes that the user logged in by calling
   * {@link #loginUserFromKeytab(String, String)}.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if a failure occurred in logout,
   * or if the user did not log in by invoking loginUserFromKeyTab() before.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,checkStat,"org.apache.hadoop.io.SecureIOUtils:checkStat(java.io.File,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",282,303,"/**
* Checks file owner against expected owner, throws IOException if mismatch.
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getPrimaryGroupName,org.apache.hadoop.security.UserGroupInformation:getPrimaryGroupName(),1655,1661,"/**
 * Retrieves a group name. Throws IOException if no primary group exists.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupNames,org.apache.hadoop.security.UserGroupInformation:getGroupNames(),1778,1781,"/**
* Converts a collection of strings to a string array.
*/","* Get the group names for this user. {@link #getGroupsSet()} is less
   * expensive alternative when checking for a contained element.
   * @return the list of users with the primary group first. If the command
   *    fails, it returns an empty list.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,isUserInList,org.apache.hadoop.security.authorize.AccessControlList:isUserInList(org.apache.hadoop.security.UserGroupInformation),235,249,"/**
* Checks if user has access based on ACLs, groups, or real UGI.
* @param ugi UserGroupInformation to check.
* @return True if access is allowed, false otherwise.
*/","* Checks if a user represented by the provided {@link UserGroupInformation}
   * is a member of the Access Control List. If user was proxied and
   * USE_REAL_ACLS + the real user name is in the control list, then treat this
   * case as if user were in the ACL list.
   * @param ugi UserGroupInformation to check if contained in the ACL
   * @return true if ugi is member of the list or if USE_REAL_ACLS + real user
   * is in the list",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUGIFromSubject,org.apache.hadoop.security.UserGroupInformation:getUGIFromSubject(javax.security.auth.Subject),651,664,"/**
 * Obtains UserGroupInformation from a Subject.
 * @param subject The Subject to extract information from.
 * @throws IOException if an error occurs during the process.
 */
","* Create a UserGroupInformation from a Subject with Kerberos principal.
   *
   * @param subject             The KerberosPrincipal to use in UGI.
   *                            The creator of subject is responsible for
   *                            renewing credentials.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if the kerberos login fails
   * @return UserGroupInformation",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createLoginUser,org.apache.hadoop.security.UserGroupInformation:createLoginUser(javax.security.auth.Subject),731,803,"/**
 * Obtains UserGroupInformation, potentially using a proxy user.
 * @param subject Subject to authenticate.
 * @return UserGroupInformation object.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,doFilter,"org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",60,105,"/**
 * Processes a request, potentially proxying user authentication.
 * Uses m1, m2, m3, m4, m5, m7, m8, m9, m12, m13, m14, m10, m15.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,doFilter,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",243,308,"/**
 * Processes request with authentication, sets UGI, and delegates.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,managementOperation,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",220,355,"/**
 * Processes delegation token operations based on request parameters.
 * @param token Authentication token, may be null.
 * @param request HTTP request.
 * @param response HTTP response.
 * @return True if request should continue, false otherwise.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.conf.Configuration)",134,138,"/**
 * Calls m1 with default configuration.
 * @param user UserGroupInformation object
 * @param remoteAddress Remote address string
 * @throws AuthorizationException if authorization fails
 */","* This function is kept to provide backward compatibility.
   * @param user user.
   * @param remoteAddress remote address.
   * @param conf configuration.
   * @throws AuthorizationException Authorization Exception.
   * @deprecated use {@link #authorize(UserGroupInformation, String)} instead.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,authorizeConnection,org.apache.hadoop.ipc.Server$Connection:authorizeConnection(),3018,3039,"/**
 * Authorizes a connection, proxies user if applicable, and records metrics.
 */","* Authorize proxy users to access this server
     * @throws RpcServerException - user is not allowed to proxy",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,copyFileToTarget,"org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",140,154,"/**
 * Calls super.m2, potentially executing within a thread pool.
 * Handles IOExceptions during the call to super.m2.
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPath,"org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",287,297,"/**
* Copies or skips a PathData based on its status.
* @param src Source PathData.
* @param dst Destination PathData.
*/","* Called with a source and target destination pair
   * @param src for the operation
   * @param dst for the operation
   * @throws IOException if anything goes wrong",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.HarFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",845,849,"/**
* Copies a file or directory.
* @param delSrc Delete source after copy.
* @param src Source path.
* @param dst Destination path.
*/",* copies the file in the har filesystem to a local file.,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,rename,"org.apache.hadoop.fs.RawLocalFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",624,644,"/**
* Moves a file from src to dst, handling Windows and fallback copy.
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",991,996,"/**
* Copies a file or directory from src to dst, deleting src if delSrc is true.
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1002,1007,"/**
* Copies a file or directory.
* @param delSrc Whether to delete src after copy.
* @param src Source path.
* @param dst Destination path.
*/
","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.LocalFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,87,"/**
* Copies a file or directory.
* @param delSrc Delete source after copy.
* @param src Source path.
* @param dst Destination path.
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.LocalFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",89,93,"/**
* Copies a source Path to a destination Path, deleting source if specified.
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processPath,org.apache.hadoop.fs.shell.Display$Cat:processPath(org.apache.hadoop.fs.shell.PathData),88,96,"/**
 * Checks if path is a directory and verifies checksum.
 * @param item PathData object containing file information.
 * @throws IOException if an I/O error occurs.
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByByteRange,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByByteRange(long,long)",1094,1096,"/**
 * Creates a Scanner for a portion of the input stream.
 * @param offset Start offset.
 * @param length Length of the stream to scan.
 * @return A Scanner object.
 */
","* Get a scanner that covers a portion of TFile based on byte offsets.
     * 
     * @param offset
     *          The beginning byte offset in the TFile.
     * @param length
     *          The length of the region.
     * @return The actual coverage of the returned scanner tries to match the
     *         specified byte-region but always round up to the compression
     *         block boundaries. It is possible that the returned scanner
     *         contains zero key-value pairs even if length is positive.
     * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1318,1331,"/**
 * Constructs a Scanner with custom block keys.
 * @param reader Reader to read from.
 * @param beginKey Block begin key.
 * @param endKey Block end key.
 */","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param beginKey
       *          Begin key of the scan. If null, scan from the first
       *          &lt;K, V&gt; entry of the TFile.
       * @param endKey
       *          End key of the scan. If null, scan up to the last &lt;K, V&gt;
       *          entry of the TFile.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[],int,int)",1361,1364,"/**
* Calls m1 with a ByteArray initialized from the byte array.
* @param key byte array key
* @param keyOffset offset in the byte array
* @param keyLen length of the key
* @throws IOException if an I/O error occurs
*/
","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. The entry returned by the previous entry() call will
       * be invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @return true if we find an equal key; false otherwise.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[],int,int)",1477,1480,"/**
* Calls m1 with a ByteArray created from the provided key data.
*/","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. The entry returned by the previous entry() call will
       * be invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[],int,int)",1508,1511,"/**
* Calls m1 with a ByteArray derived from the provided key data.
*/","* Move the cursor to the first entry whose key is strictly greater than
       * the input key. The entry returned by the previous entry() call will be
       * invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,seek,org.apache.hadoop.io.SetFile$Reader:seek(org.apache.hadoop.io.WritableComparable),130,134,"/**
* Delegates the call to the parent class's m1 method.
* @param key The key to pass to the superclass method.
* @return The result of the superclass's m1 method.
*/
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,get,"org.apache.hadoop.io.MapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",823,830,"/**
* Processes a key-value pair. Returns val if key matches, else null.
*/","* Return the value for the named key, or null if none exists.
     * @param key key.
     * @param val val.
     * @return Writable if such a pair exists true,not false.
     * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,commit,org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit(),189,235,"/**
* Attempts to authenticate a user, utilizing various methods.
* Returns true if successful, otherwise throws LoginException.
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",579,587,"/**
 * Creates a ProtocolProxy. Uses SaslRpcServer if UserGroupInformation.m1() is true.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T
   * @param protocol protocol class
   * @param clientVersion client's version
   * @param connId client connection identifier
   * @param conf configuration
   * @param factory socket factory
   * @param alignmentContext StateID alignment context
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean)",661,677,"/**
 * Creates a ProtocolProxy with configured parameters.
 * @param protocol Protocol class.
 * @return ProtocolProxy instance.
 */","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @param fallbackToSimpleAuth set to true or false during calls to indicate if
   *   a secure client falls back to simple auth
   * @return the proxy
   * @throws IOException if any error occurs",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",698,715,"/**
 * Creates a ProtocolProxy with configured parameters.
 * @param protocol Protocol class.
 * @return ProtocolProxy instance.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @param fallbackToSimpleAuth set to true or false during calls to indicate
   *   if a secure client falls back to simple auth
   * @param alignmentContext state alignment context
   * @param <T> Generics Type T.
   * @return the proxy
   * @throws IOException if any error occurs",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setFallBackToSimpleAuth,org.apache.hadoop.ipc.Client$Connection:setFallBackToSimpleAuth(java.util.concurrent.atomic.AtomicBoolean),858,890,"/**
 * Configures fallback authentication based on auth method & settings.
 * @param fallbackToSimpleAuth AtomicBoolean to control fallback.
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenForRandomRead,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)",126,143,"/**
 * Opens a RandomAccessFile and verifies file ownership.
 * @param f File to open, mode access mode, owner/group expected values.
 * @throws IOException if an I/O error occurs.
 */","* @return Same as openForRandomRead except that it will run even if security is off.
   * This is used by unit tests.
   *
   * @param f input f.
   * @param mode input mode.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenFSDataInputStream,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenFSDataInputStream(java.io.File,java.lang.String,java.lang.String)",174,192,"/**
 * Opens a FSDataInputStream, checks owner/group permissions.
 * @param file The file to open.
 * @param expectedOwner Expected owner string.
 * @param expectedGroup Expected group string.
 * @return FSDataInputStream or null if permissions fail.
 */","* Same as openFSDataInputStream except that it will run even if security is
   * off. This is used by unit tests.
   *
   * @param file input file.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.
   * @return FSDataInputStream.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenForRead,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRead(java.io.File,java.lang.String,java.lang.String)",224,241,"/**
 * Opens a FileInputStream and verifies file ownership.
 * @param f file to open
 * @param expectedOwner expected owner
 * @param expectedGroup expected group
 * @return FileInputStream
 */
","* @return Same as openForRead() except that it will run even if security is off.
   * This is used by unit tests.
   * @param f input f.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path),1080,1087,"/**
 * Creates a FileStatus object for a given path.
 * @param f the path for which to create the FileStatus
 * @return A FileStatus object representing the path.
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path),1089,1128,"/**
 * Retrieves a FileStatus for a given path, handling symbolic links.
 * @param f The Path to get the FileStatus for.
 * @return A FileStatus object.
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path),1406,1413,"/**
* Sets ACL status for a path.
* Sets ugi, permissions, and sets recursion flag.
* @param path The path to set ACL status on.
* @return AclStatus object.
*/
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path),1544,1551,"/**
* Creates a FileStatus object for the given path.
* @param f Path to create status for.
* @return FileStatus object.
*/
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path),1554,1620,"/**
 * Retrieves FileStatus for paths, handling links and fallbacks.
 * @param f path to check
 * @return Array of FileStatus objects
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path),1832,1839,"/**
 * Sets ACL status for a path.
 * Sets permissions, user/group IDs, and flags.
 * @param path the path to set ACL status on
 * @throws IOException if an I/O error occurs
 */
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpUGI,"org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",666,690,"/**
* Logs user group information, including credentials and authentication details.
*/","* Dump a UGI.
   *
   * @param title title of this section
   * @param ugi UGI to dump
   * @throws IOException",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,print,org.apache.hadoop.security.UserGroupInformation:print(),2022,2032,"/**
 * Prints user and group information to the console.
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,isUserAllowed,org.apache.hadoop.security.authorize.AccessControlList:isUserAllowed(org.apache.hadoop.security.UserGroupInformation),251,253,"/**
 * Delegates to m1 with the provided UserGroupInformation.
 * @param ugi UserGroupInformation object to pass to m1
 * @return Result of m1's execution.
 */
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getLoginUser,org.apache.hadoop.security.UserGroupInformation:getLoginUser(),673,698,"/**
 * Retrieves or creates a UserGroupInformation object.
 * Returns the UserGroupInformation or creates a new one.
 */","* Get the currently logged in user.  If no explicit login has occurred,
   * the user will automatically be logged in with either kerberos credentials
   * if available, or as the local OS user, based on security settings.
   * @return the logged in user
   * @throws IOException if login fails",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromSubject,org.apache.hadoop.security.UserGroupInformation:loginUserFromSubject(javax.security.auth.Subject),725,729,"/**
* Processes a Subject, potentially throwing an IOException.
* @param subject The Subject to process.
*/","* Log in a user using the given subject
   * @param subject the subject to use when logging in a user, or null to
   * create a new subject.
   *
   * If subject is not null, the creator of subject is responsible for renewing
   * credentials.
   *
   * @throws IOException if login fails",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processConnectionContext,org.apache.hadoop.ipc.Server$Connection:processConnectionContext(org.apache.hadoop.ipc.RpcWritable$Buffer),2678,2724,"/**
 * Processes RPC buffer to extract connection context and user.
 * @param buffer RPC buffer containing connection details.
 * @throws RpcServerException if connection context is already read.
 */","Reads the connection context following the connection header
     * @throws RpcServerException - if the header cannot be
     *         deserialized, or the user is not authorized",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPath,org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData),276,279,"/**
* Calls m2 with src and the result of m1(src).
* @param src PathData object to process.
* @throws IOException if an I/O error occurs.
*/
",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.RawLocalFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",877,880,"/**
* Copies a file from the source path to the destination path.
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1174,1181,"/**
 * Creates a Scanner with specified begin and end keys.
 * @param beginKey Start key for the Scanner.
 * @param endKey End key for the Scanner.
 */
","* Get a scanner that covers a specific key range.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[]),1343,1345,"/**
* Calls overloaded method with key and full length.
* @param key byte array key
* @throws IOException if I/O error occurs
*/
","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. Synonymous to seekTo(key, 0, key.length). The entry
       * returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @return true if we find an equal key.
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[]),1460,1462,"/**
* Calls overloaded method with key and full length.
* @param key The byte array key.
* @throws IOException if an I/O error occurs.
*/
","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. Synonymous to lowerBound(key, 0, key.length). The
       * entry returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[]),1491,1493,"/**
 * Calls m1 with the entire key array.
 * @param key The byte array key to use.
 * @throws IOException if an I/O error occurs.
 */
","* Move the cursor to the first entry whose key is strictly greater than
       * the input key. Synonymous to upperBound(key, 0, key.length). The entry
       * returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,get,org.apache.hadoop.io.SetFile$Reader:get(org.apache.hadoop.io.WritableComparable),156,163,"/**
 * Processes a key; returns it or null based on m1's result.
 */","* Read the matching key from a set into <code>key</code>.
     *
     * @param key input key.
     * @return Returns <code>key</code>, or null if no match exists.
     * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,get,"org.apache.hadoop.io.BloomMapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",281,288,"/**
* Calls super.m2 if m1(key) returns true; otherwise, returns null.
*/","* Fast version of the
     * {@link MapFile.Reader#get(WritableComparable, Writable)} method. First
     * it checks the Bloom filter for the existence of the key, and only if
     * present it performs the real get operation. This yields significant
     * performance improvements for get operations on sparsely populated files.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",558,563,"/**
 * Creates a ProtocolProxy with specified parameters.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @return ProtocolProxy object.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T
   * @param protocol protocol class
   * @param clientVersion client's version
   * @param connId client connection identifier
   * @param conf configuration
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",631,641,"/**
 * Creates a ProtocolProxy with default proxy settings.
 * @param protocol Protocol class.
 * @return ProtocolProxy object.
 */","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @return the proxy
   * @throws IOException if any error occurs",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupIOstreams,org.apache.hadoop.ipc.Client$Connection:setupIOstreams(java.util.concurrent.atomic.AtomicBoolean),764,856,"/**
 * Establishes an IPC connection, handling authentication and retries.
 * @param fallbackToSimpleAuth Whether to fallback to simple auth.
 */","Connect to the server and set up the I/O streams. It then sends
     * a header to the server and starts
     * the connection thread that waits for responses.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openForRandomRead,"org.apache.hadoop.io.SecureIOUtils:openForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)",107,114,"/**
 * Opens a RandomAccessFile, using m2 if UserGroupInformation.m1() is true.
 * @param f file to open
 * @param mode access mode (e.g., ""r"", ""rw"")
 * @param expectedOwner owner string
 * @param expectedGroup group string
 * @return RandomAccessFile object
 */
","* @return Open the given File for random read access, verifying the expected user/
   * group constraints if security is enabled.
   * 
   * Note that this function provides no additional security checks if hadoop
   * security is disabled, since doing the checks would be too expensive when
   * native libraries are not available.
   * 
   * @param f file that we are trying to open
   * @param mode mode in which we want to open the random access file
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO error occurred or if the user/group does
   * not match when security is enabled.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openFSDataInputStream,"org.apache.hadoop.io.SecureIOUtils:openFSDataInputStream(java.io.File,java.lang.String,java.lang.String)",156,162,"/**
 * Opens a file input stream, checking permissions if needed.
 * @param file The file to open.
 * @param expectedOwner Expected owner of the file.
 * @param expectedGroup Expected group of the file.
 */
","* Opens the {@link FSDataInputStream} on the requested file on local file
   * system, verifying the expected user/group constraints if security is
   * enabled.
   * @param file absolute path of the file
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO Error occurred or the user/group does not
   * match if security is enabled
   * @return FSDataInputStream.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openForRead,"org.apache.hadoop.io.SecureIOUtils:openForRead(java.io.File,java.lang.String,java.lang.String)",208,214,"/**
 * Opens a FileInputStream, checking user/group permissions if needed.
 * @param f file to open, expectedOwner, expectedGroup for checks
 * @throws IOException if an I/O error occurs
 */
","* Open the given File for read access, verifying the expected user/group
   * constraints if security is enabled.
   *
   * @return Note that this function provides no additional checks if Hadoop
   * security is disabled, since doing the checks would be too expensive
   * when native libraries are not available.
   *
   * @param f the file that we are trying to open
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO Error occurred, or security is enabled and
   * the user/group does not match",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getLinkTarget(org.apache.hadoop.fs.Path),1334,1338,"/**
* Applies m2 after m1 to the input Path.
* @param f The Path to be processed.
* @return The processed Path.
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getContentSummary(org.apache.hadoop.fs.Path),1659,1678,"/**
* Calculates content summary recursively from a Path.
* @param f Path to summarize; returns ContentSummary object.
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStatus(org.apache.hadoop.fs.Path),1680,1694,"/**
* Calculates FsStatus summary by recursively traversing Path.
* @param p The path to traverse.
* @return FsStatus object containing the summary.
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,userHasAdministratorAccess,"org.apache.hadoop.http.HttpServer2:userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)",1727,1734,"/**
 * Checks if the remote user is an admin based on ACL.
 * @param servletContext Servlet context holding admin ACL.
 * @param remoteUser Remote user's name.
 * @return True if the user is an admin, false otherwise.
 */
","* Get the admin ACLs from the given ServletContext and check if the given
   * user is in the ACL.
   *
   * @param servletContext the context containing the admin ACL.
   * @param remoteUser the remote user to check for.
   * @return true if the user is present in the ACL, false if no ACL is set or
   *         the user is not present",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,authorize,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,org.apache.hadoop.conf.Configuration,java.net.InetAddress)",88,138,"/**
 * Authorizes a user for a protocol, checking ACLs and host access.
 * @param user UserGroupInformation object
 * @param protocol Protocol to authorize
 */","* Authorize the user to access the protocol being used.
   * 
   * @param user user accessing the service 
   * @param protocol service being accessed
   * @param conf configuration to use
   * @param addr InetAddress of the client
   * @throws AuthorizationException on authorization failure",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,authorize,"org.apache.hadoop.security.authorize.DefaultImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)",108,135,"/**
 * Checks user authorization based on ACL and proxy host.
 * @param user UserGroupInformation object
 * @param remoteAddress Remote address of the connection
 */",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCurrentUser,org.apache.hadoop.security.UserGroupInformation:getCurrentUser(),583,594,"/**
 * Gets UserGroupInformation; returns default if subject is null/unverified.
 */","* Return the current user, including any doAs in the current stack.
   * @return the current user
   * @throws IOException if login fails",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginKeytabBased,org.apache.hadoop.security.UserGroupInformation:isLoginKeytabBased(),1417,1421,"/**
* Delegates to m1().m2().
*/","* Did the login happen via keytab.
   * @return true or false
   * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginTicketBased,org.apache.hadoop.security.UserGroupInformation:isLoginTicketBased(),1428,1430,"/**
* Delegates to m1().m2(). Returns the result of that call.
*/","* Did the login happen via ticket cache.
   * @return true or false
   * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsLoginUserOrFatal,org.apache.hadoop.security.SecurityUtil:doAsLoginUserOrFatal(java.security.PrivilegedAction),508,522,"/**
 * Executes action with elevated privileges if conditions are met.
 * @param action The PrivilegedAction to execute.
 * @return The result of the action.
 */
","* Perform the given action as the daemon's login user. If the login
   * user cannot be determined, this will log a FATAL error and exit
   * the whole JVM.
   *
   * @param action action.
   * @param <T> generic type T.
   * @return generic type T.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsLoginUser,org.apache.hadoop.security.SecurityUtil:doAsLoginUser(java.security.PrivilegedExceptionAction),533,536,"/**
* Executes a privileged action and returns the result.
* @param action The privileged action to execute.
* @return The result of the action.
*/
","* Perform the given action as the daemon's login user. If an
   * InterruptedException is thrown, it is converted to an IOException.
   *
   * @param action the action to perform
   * @param <T> Generics Type T.
   * @return the result of the action
   * @throws IOException in the event of error",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,cedeActive,org.apache.hadoop.ha.ZKFailoverController:cedeActive(int),575,588,"/**
 * Executes a task under user's privilege, handling exceptions.
 * @param millisToCede Timeout for the privileged action.
 */","* Request from graceful failover to cede active role. Causes
   * this ZKFC to transition its local node to standby, then quit
   * the election for the specified period of time, after which it
   * will rejoin iff it is healthy.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,gracefulFailoverToYou,org.apache.hadoop.ha.ZKFailoverController:gracefulFailoverToYou(),630,643,"/**
 * Executes m1 within a privileged action using UserGroupInformation.
 */","* Coordinate a graceful failover to this node.
   * @throws ServiceFailedException if the node fails to become active
   * @throws IOException some other error occurs",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(byte[],byte[])",1132,1137,"/**
 * Delegates to another m1 method with ByteArray objects.
 * @param beginKey Byte array for the beginning key.
 * @param endKey Byte array for the ending key.
 * @return Scanner object.
 */
","* Get a scanner that covers a portion of TFile based on keys.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1155,1159,"/**
* Delegates to m1, returns a Scanner.
* @param beginKey Start key for scanning.
* @param endKey End key for scanning.
*/","* Get a scanner that covers a specific key range.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.
     * 
     * @deprecated Use {@link #createScannerByKey(RawComparable, RawComparable)}
     *             instead.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",535,543,"/**
* Creates a ProtocolProxy, delegates to m2 with a default timeout.
* @param protocol Protocol class
* @param clientVersion Client version
* @param addr Remote address
* @param ticket UserGroupInformation
* @param conf Configuration
* @param factory SocketFactory
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param ticket user group information
   * @param conf configuration to use
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",604,613,"/**
 * Delegates to m1 and calls its m2() method.
 * @param protocol Protocol class.
 * @return Result of m1().m2()
 */
","* Construct a client-side proxy that implements the named protocol,
   * talking to a server at the named address.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @return the proxy
   * @throws IOException if any error occurs",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,hasAdministratorAccess,"org.apache.hadoop.http.HttpServer2:hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1686,1716,"/**
 * Checks if the user is authorized to access a page.
 * @param servletContext Servlet context.
 * @param request HTTP request.
 * @param response HTTP response.
 * @return True if authorized, false otherwise.
 */","* Does the user sending the HttpServletRequest has the administrator ACLs? If
   * it isn't the case, response will be modified to send an error to the user.
   *
   * @param servletContext servletContext.
   * @param request request.
   * @param response used to send the error response if user does not have admin access.
   * @return true if admin-authorized, false otherwise
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,authorize,"org.apache.hadoop.ipc.Server:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.net.InetAddress)",3806,3821,"/**
 * Authorizes a user for a protocol, using the service manager.
 * @param user UserGroupInformation object
 * @param protocolName Protocol name to authorize
 * @param addr Address of the remote host
 */","* Authorize the incoming client connection.
   * 
   * @param user client user
   * @param protocolName - the protocol
   * @param addr InetAddress of incoming connection
   * @throws AuthorizationException when the client isn't authorized to talk the protocol",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getHomeDirectory,org.apache.hadoop.fs.FileSystem:getHomeDirectory(),2445,2456,"/**
 * Gets the user's home directory path.
 * Uses UGI username or system property if UGI fails.
 */","Return the current user's home directory in this FileSystem.
   * The default implementation returns {@code ""/user/$USER/""}.
   * @return the path.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,checkAccessPermissions,"org.apache.hadoop.fs.FileSystem:checkAccessPermissions(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction)",2855,2877,"/**
 * Checks file access permissions based on user and action.
 * @param stat FileStatus object.
 * @param mode FsAction to be performed.
 */","* This method provides the default implementation of
   * {@link #access(Path, FsAction)}.
   *
   * @param stat FileStatus to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws IOException for any error",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(),280,283,"/**
 * Constructs a ViewFileSystem with current user and timestamp.
 */","* This is the  constructor with the signature needed by
   * {@link FileSystem#createFileSystem(URI, Configuration)}
   *
   * After this constructor is called initialize() is called.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",227,290,"/**
 * Initializes a ViewFs object with the given URI and configuration.
 * @param theUri The URI for the ViewFs.
 * @param conf Hadoop configuration object.
 */
","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   *
   * @param theUri which must be that of ViewFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree:<init>(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI,boolean)",617,770,"/**
 * Initializes the InodeTree with configuration and view name.
 * @param config Hadoop configuration object
 * @param viewName View name for the view filesystem
 * @param theUri URI representing the view filesystem
 */
","* Create Inode Tree from the specified mount-table specified in Config.
   *
   * @param config the mount table keys are prefixed with
   *               FsConstants.CONFIG_VIEWFS_PREFIX.
   * @param viewName the name of the mount table
   *                 if null use defaultMT name.
   * @param theUri heUri.
   * @param initingUriAsFallbackOnNoMounts initingUriAsFallbackOnNoMounts.
   * @throws UnsupportedFileSystemException file system for <code>uri</code> is
   *                                        not found.
   * @throws URISyntaxException if the URI does not have an authority
   *                            it is badly formed.
   * @throws FileAlreadyExistsException there is a file at the path specified
   *                                    or is discovered on one of its ancestors.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration,long)",3881,3889,"/**
 * Initializes a Key object with URI, configuration, and unique ID.
 * @param uri The URI to parse.
 * @param conf Configuration object.
 * @param unique Unique identifier.
 */
",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.AbstractFileSystem:getHomeDirectory(),461,472,"/**
 * Creates a Path object representing the user's home directory.
 * Uses username from UGI or system property if UGI fails.
 */","* Return the current user's home directory in this file system.
   * The default implementation returns ""/user/$USER/"".
   * 
   * @return current user's home directory.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",283,335,"/**
 * Connects to a URL with a token, potentially using a delegation token.
 * @param url URL to connect to
 * @param token Authentication token
 * @param doAs User to execute as
 * @return HttpURLConnection object
 */","* Returns an authenticated {@link HttpURLConnection}. If the Delegation
   * Token is present, it will be used taking precedence over the configured
   * <code>Authenticator</code>. If the <code>doAs</code> parameter is not NULL,
   * the request will be done on behalf of the specified <code>doAs</code> user.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @param doAs user to do the the request on behalf of, if NULL the request is
   * as self.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,authenticate,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",134,153,"/**
 * Authenticates with a URL, using delegation token if available.
 * @param url URL to authenticate.
 * @param token Authentication token.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getBestUGI,"org.apache.hadoop.security.UserGroupInformation:getBestUGI(java.lang.String,java.lang.String)",606,615,"/**
 * Gets UserGroupInformation based on ticket cache path/user.
 * @param ticketCachePath Cache path, or null.
 * @param user User name, or null.
 * @return UserGroupInformation object.
 */
","* Find the most appropriate UserGroupInformation to use
   *
   * @param ticketCachePath    The Kerberos ticket cache path, or NULL
   *                           if none is specfied
   * @param user               The user name, or NULL if none is specified.
   *
   * @return                   The most appropriate UserGroupInformation
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromKeytabAndReturnUGI,"org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytabAndReturnUGI(java.lang.String,java.lang.String)",1388,1399,"/**
 * Authenticates user using provided principal and keytab path.
 * @param user User principal.
 * @param path Keytab file path.
 * @return UserGroupInformation object.
 */
","* Log a user in from a keytab file. Loads a user identity from a keytab
   * file and login them in. This new user does not affect the currently
   * logged-in user.
   * @param user the principal name to load from the keytab
   * @param path the path to the keytab file
   * @throws IOException if the keytab file can't be read
   * @return UserGroupInformation.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logAllUserInfo,"org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.slf4j.Logger,org.apache.hadoop.security.UserGroupInformation)",1999,2010,"/**
 * Logs user group information details.
 * @param log Logger instance for logging.
 * @param ugi UserGroupInformation object.
 */","* Log all (current, real, login) UGI and token info into specified log.
   * @param ugi - UGI
   * @param log - log.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,<init>,org.apache.hadoop.security.alias.UserProvider:<init>(),44,47,"/**
 * Initializes the UserProvider with current user and credentials.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsCurrentUser,org.apache.hadoop.security.SecurityUtil:doAsCurrentUser(java.security.PrivilegedExceptionAction),547,550,"/**
 * Executes a privileged action and returns the result.
 * @param action The action to execute.
 * @return The result of the action.
 */
","* Perform the given action as the daemon's current user. If an
   * InterruptedException is thrown, it is converted to an IOException.
   *
   * @param action the action to perform
   * @param <T> generic type T.
   * @return the result of the action
   * @throws IOException in the event of error",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,<init>,org.apache.hadoop.security.SaslRpcServer:<init>(org.apache.hadoop.security.SaslRpcServer$AuthMethod),89,120,"/**
 * Constructs a SaslRpcServer with the given authentication method.
 * @param authMethod The authentication method to use.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,create,"org.apache.hadoop.security.SaslRpcServer:create(org.apache.hadoop.ipc.Server$Connection,java.util.Map,org.apache.hadoop.security.token.SecretManager)",122,173,"/**
 * Creates a SaslServer based on the authentication method.
 * @param connection The connection object.
 * @param saslProperties SASL properties.
 * @return SaslServer instance.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,<init>,org.apache.hadoop.crypto.key.UserProvider:<init>(org.apache.hadoop.conf.Configuration),47,51,"/**
 * Initializes a UserProvider with a Configuration and current user.
 * @param conf Configuration object for Hadoop setup.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDoAsUser,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDoAsUser(),1129,1134,"/**
 * Returns the proxy token if UGI auth is PROXY, otherwise null.
 */","* Get the doAs user name.
   *
   * 'actualUGI' is the UGI of the user creating the client
   * It is possible that the creator of the KMSClientProvier
   * calls this method on behalf of a proxyUser (the doAsUser).
   * In which case this call has to be made as the proxy user.
   *
   * @return the doAs user name.
   * @throws IOException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.io.retry.RetryPolicy,long)",411,453,"/**
 * Creates a ProtocolProxy, retrying on connection errors.
 * @param protocol Protocol class.
 * @return ProtocolProxy object.
 * @throws IOException if connection fails after timeout.
 */","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param rpcTimeout timeout for each RPC
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @param timeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,shouldAuthenticateOverKrb,org.apache.hadoop.ipc.Client$Connection:shouldAuthenticateOverKrb(),556,569,"/**
 * Checks Kerberos authentication based on user information.
 * Returns true if authentication is valid, false otherwise.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRestrictParserDefault,org.apache.hadoop.conf.Configuration$Resource:getRestrictParserDefault(java.lang.Object),288,299,"/**
 * Checks if resource access is permitted based on user group.
 * Returns true if permitted, false otherwise.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection$1:run(),1082,1109,"/**
* Processes RPC requests, handling connections and errors.
* Logs start/stop events and any unexpected exceptions.
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doKerberosRelogin,org.apache.hadoop.ipc.Server:doKerberosRelogin(),3407,3425,"/**
* Initiates a re-login process based on IPC server status.
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,run,org.apache.hadoop.ha.ZKFailoverController:run(java.lang.String[]),173,199,"/**
 * Executes a privileged action and handles exceptions.
 * @param args Arguments passed to the privileged action.
 * @return Integer result of the privileged action.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,cedeActive,org.apache.hadoop.ha.ZKFCRpcServer:cedeActive(int),91,96,"/**
 * Delegates to Zookeeper functionality, passing the provided time.
 * @param millisToCede time in milliseconds to cede to Zookeeper
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,gracefulFailover,org.apache.hadoop.ha.ZKFCRpcServer:gracefulFailover(),98,102,"/**
* Executes m1 and m2 on the Zookeeper framework client.
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(byte[],byte[])",1113,1117,"/**
 * Delegates scanner creation to m1.
 * @param beginKey Start key for scanning.
 * @param endKey End key for scanning.
 * @return Scanner object.
 */","* Get a scanner that covers a portion of TFile based on keys.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.
     * 
     * @deprecated Use {@link #createScannerByKey(byte[], byte[])} instead.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",488,494,"/**
 * Creates a ProtocolProxy with default UserGroupInformation.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @param addr Socket address.
 * @param conf Configuration object.
 * @param factory Socket factory.
 * @return ProtocolProxy object.
 */","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",511,519,"/**
 * Calls the m2 method of the result of m1.
 * @param protocol Protocol class.
 * @return Result of the chained method call.
 */","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   *
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input tocket.
   * @param conf input conf.
   * @param factory input factory.
   * @return the protocol proxy.
   * @throws IOException raised on errors performing I/O.
   *",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",46,54,"/**
 * Creates a ZKFCProtocolClientSideTranslatorPB using provided config.
 * @param addr Socket address, conf Configuration, timeout Timeout in ms
 * @throws IOException if an I/O error occurs
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",74,82,"/**
 * Creates a HAServiceProtocolClientSideTranslatorPB proxy.
 * @param addr Socket address.
 * @param conf Hadoop configuration.
 * @param socketFactory Socket factory.
 * @param timeout Socket timeout.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doGet,"org.apache.hadoop.log.LogLevel$Servlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",325,360,"/**
* Processes log level submission request.
* Retrieves log name and level from request, displays results.
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/AdminAuthorizedServlet.java,doGet,"org.apache.hadoop.http.AdminAuthorizedServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",36,45,"/**
 * Calls super.m3 if HttpServer2.m2 returns true.
 * @param request HTTP request object.
 * @param response HTTP response object.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,isInstrumentationAccessAllowed,"org.apache.hadoop.http.HttpServer2:isInstrumentationAccessAllowed(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1660,1674,"/**
 * Checks access based on admin configuration.
 * @param servletContext Servlet context.
 * @param request HTTP request.
 * @param response HTTP response.
 * @return True if access is allowed.
 */
","* Checks the user has privileges to access to instrumentation servlets.
   * <p>
   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to FALSE
   * (default value) it always returns TRUE.
   * <p>
   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to TRUE
   * it will check that if the current user is in the admin ACLS. If the user is
   * in the admin ACLs it returns TRUE, otherwise it returns FALSE.
   *
   * @param servletContext the servlet context.
   * @param request the servlet request.
   * @param response the servlet response.
   * @return TRUE/FALSE based on the logic decribed above.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.DelegateToFileSystem:getHomeDirectory(),169,172,"/**
 * Delegates the call to the underlying file system implementation.
 * @return Path object returned by the underlying implementation.
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getTrashRoot,org.apache.hadoop.fs.FileSystem:getTrashRoot(org.apache.hadoop.fs.Path),3439,3442,"/**
 * Creates a masked path using a prefix and internal methods.
 * @param path The input path (unused in this method).
 * @return A new Path object with a masked path.
 */
","* Get the root directory of Trash for current user when the path specified
   * is deleted.
   *
   * @param path the trash root of the path to be determined.
   * @return the default implementation returns {@code /user/$USER/.Trash}",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getTrashRoots,org.apache.hadoop.fs.FileSystem:getTrashRoots(boolean),3452,3478,"/**
 * Retrieves trash roots, either for current user or all users.
 * @param allUsers true to get all user trash roots, false for current.
 * @return Collection of FileStatus objects representing trash roots.
 */","* Get all the trash roots for current user or all users.
   *
   * @param allUsers return trash roots for all users if true.
   * @return all the trash root directories.
   *         Default FileSystem returns .Trash under users' home directories if
   *         {@code /user/$USER/.Trash} exists.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.FilterFileSystem:getHomeDirectory(),297,300,"/**
* Delegates m1() call to the underlying file system.
* @return Path object representing the result.
*/
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,access,"org.apache.hadoop.fs.FileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",2840,2844,"/**
 * Executes a file system action on a given path.
 * @param path The path to operate on.
 * @param mode The file system action to perform.
 */","* Checks if the user can access a path.  The mode specifies which access
   * checks to perform.  If the requested permissions are granted, then the
   * method returns normally.  If access is denied, then the method throws an
   * {@link AccessControlException}.
   * <p>
   * The default implementation calls {@link #getFileStatus(Path)}
   * and checks the returned permissions against the requested permissions.
   *
   * Note that the {@link #getFileStatus(Path)} call will be subject to
   * authorization checks.
   * Typically, this requires search (execute) permissions on each directory in
   * the path's prefix, but this is implementation-defined.  Any file system
   * that provides a richer authorization model (such as ACLs) may override the
   * default implementation so that it checks against that model instead.
   * <p>
   * In general, applications should avoid using this method, due to the risk of
   * time-of-check/time-of-use race conditions.  The permissions on a file may
   * change immediately after the access call returns.  Most applications should
   * prefer running specific file system actions as the desired user represented
   * by a {@link UserGroupInformation}.
   *
   * @param path Path to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws FileNotFoundException if the path does not exist
   * @throws IOException see specific implementation",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,access,"org.apache.hadoop.fs.AbstractFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",1046,1050,"/**
 * Sets file access permissions.
 * @param path Path to the file.
 * @param mode File access mode.
 */","* The specification of this method matches that of
   * {@link FileContext#access(Path, FsAction)}
   * except that an UnresolvedLinkException may be thrown if a symlink is
   * encountered in the path.
   *
   * @param path the path.
   * @param mode fsaction mode.
   * @throws AccessControlException access control exception.
   * @throws FileNotFoundException file not found exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",392,396,"/**
 * Initializes the ViewFileSystem with a URI and configuration.
 * @param theUri The URI for the file system.
 * @param conf Configuration object.
 * @throws IOException if an I/O error occurs.
 */
","* Convenience Constructor for apps to call directly.
   * @param theUri which must be that of ViewFileSystem
   * @param conf conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:<init>(),123,125,"/**
 * Constructs a ViewFileSystemOverloadScheme.
 * Initializes the scheme, potentially throwing IOException.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,org.apache.hadoop.fs.viewfs.ViewFs:<init>(org.apache.hadoop.conf.Configuration),213,216,"/**
 * Constructs a ViewFs object using a default URI and configuration.
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",3877,3879,"/**
 * Constructs a Key with a URI and Configuration.
 * @param uri The URI for the key.
 * @param conf The configuration for the key.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUnique,"org.apache.hadoop.fs.FileSystem$Cache:getUnique(java.net.URI,org.apache.hadoop.conf.Configuration)",3671,3674,"/**
 * Creates and returns a FileSystem object for the given URI.
 * @param uri The URI of the file system.
 * @param conf Hadoop configuration.
 * @return FileSystem object.
 */
",The objects inserted into the cache using this method are all unique.,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ChRootedFs:getHomeDirectory(),151,154,"/**
 * Delegates to the underlying FileSystem's m1() method.
 * @return Path object returned by the FileSystem's m1()
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,<init>,"org.apache.hadoop.fs.FileContext:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)",243,271,"/**
 * Initializes a FileContext with a default filesystem and configuration.
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getHomeDirectory,org.apache.hadoop.fs.FileContext:getHomeDirectory(),577,579,"/**
* Delegates m1() call to the default file system.
* @return Path object returned by defaultFS.m1()
*/
","* Return the current user's home directory in this file system.
   * The default implementation returns ""/user/$USER/"".
   * @return the home directory",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getHomeDirectory,org.apache.hadoop.fs.FilterFs:getHomeDirectory(),83,86,"/**
* Delegates m1() call to the underlying file system.
* @return Path object returned by the file system's m1()
*/
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",230,235,"/**
* Delegates to self or super based on token type.
* @param url URL to connect to.
* @param token Authentication token.
* @return HttpURLConnection object.
*/
","* Returns an authenticated {@link HttpURLConnection}, it uses a Delegation
   * Token only if the given auth token is an instance of {@link Token} and
   * it contains a Delegation Token, otherwise use the configured
   * {@link DelegationTokenAuthenticator} to authenticate the connection.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)",268,280,"/**
 * Gets a FileSystem object for the given URI, using provided config and user.
 * @param uri The URI of the file system.
 * @param conf Hadoop configuration.
 * @param user User name for authentication.
 * @return FileSystem object.
 */","* Get a FileSystem instance based on the uri, the passed in
   * configuration and the user.
   * @param uri of the filesystem
   * @param conf the configuration to use
   * @param user to perform the get as
   * @return the filesystem instance
   * @throws IOException failure to load
   * @throws InterruptedException If the {@code UGI.doAs()} call was
   * somehow interrupted.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,"org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)",571,583,"/**
 * Gets a FileSystem object for the given URI, using provided config and user.
 * @param uri The URI of the file system.
 * @param conf Hadoop configuration.
 * @param user User name for authentication.
 * @return FileSystem object.
 */","* Returns the FileSystem for this URI's scheme and authority and the
   * given user. Internally invokes {@link #newInstance(URI, Configuration)}
   * @param uri uri of the filesystem.
   * @param conf the configuration to use
   * @param user to perform the get as
   * @return filesystem instance
   * @throws IOException if the FileSystem cannot be instantiated.
   * @throws InterruptedException If the {@code UGI.doAs()} call was
   *         somehow interrupted.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUGIFromTicketCache,"org.apache.hadoop.security.UserGroupInformation:getUGIFromTicketCache(java.lang.String,java.lang.String)",627,638,"/**
 * Authenticates a user, returning a UserGroupInformation object.
 * @param ticketCache Kerberos ticket cache path.
 * @param user User principal name.
 * @return UserGroupInformation object.
 */
","* Create a UserGroupInformation from a Kerberos ticket cache.
   * 
   * @param user                The principal name to load from the ticket
   *                            cache
   * @param ticketCache     the path to the ticket cache file
   *
   * @throws IOException        if the kerberos login fails
   * @return UserGroupInformation.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,loginFromKeytab,org.apache.hadoop.security.KDiag:loginFromKeytab(),628,657,"/**
 * Authenticates using a keytab or as the current user.
 * Relogins UGI if a keytab is provided.
 */","* Log in from a keytab, dump the UGI, validate it, then try and log in again.
   *
   * That second-time login catches JVM/Hadoop compatibility problems.
   * @throws IOException Keytab loading problems",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromKeytab,"org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytab(java.lang.String,java.lang.String)",1127,1147,"/**
 * Authenticates a user using a keytab file.
 * @param user User's username.
 * @param path Keytab file path.
 */","* Log a user in from a keytab file. Loads a user identity from a keytab
   * file and logs them in. They become the currently logged-in user.
   * @param user the principal name to load from the keytab
   * @param path the path to the keytab file
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if it's a kerberos login exception.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logAllUserInfo,org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.apache.hadoop.security.UserGroupInformation),2017,2020,"/**
* Calls m1 with the provided UserGroupInformation and a default logger.
* @param ugi UserGroupInformation object
* @throws IOException if an I/O error occurs
*/
","* Log all (current, real, login) UGI and token info into UGI debug log.
   * @param ugi - UGI
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getActualUgi,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getActualUgi(),1167,1190,"/**
* Returns the UserGroupInformation, potentially using a fallback UGI.
* Handles Kerberos delegation token and credential checks.
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,buildNegotiateResponse,org.apache.hadoop.ipc.Server:buildNegotiateResponse(java.util.List),3445,3467,"/**
 * Creates a RpcSaslProto based on the provided authentication methods.
 * @param authMethods List of authentication methods to consider.
 * @return RpcSaslProto object representing the negotiation result.
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,createSaslServer,org.apache.hadoop.ipc.Server$Connection:createSaslServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod),2621,2626,"/**
 * Creates a SaslRpcServer with provided authMethod and properties.
 * @param authMethod Sasl authentication method to use
 * @return SaslRpcServer instance
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)",366,372,"/**
 * Creates a ProtocolProxy. Delegates to internal method m2.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @param addr Socket address.
 * @param conf Configuration.
 * @param connTimeout Connection timeout.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param connTimeout time in milliseconds before giving up
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,long)",387,394,"/**
 * Calls m2() on the result of m1(), passing protocol, version, addr, etc.
 */","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param rpcTimeout timeout for each RPC
   * @param timeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,java.lang.String)",261,263,"/**
 * Constructs a Resource with a default parser.
 * @param resource The resource object.
 * @param name Resource name.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",467,473,"/**
 * Delegates to m1 and returns the result of m1's m2() call.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @param addr Socket address.
 * @param conf Configuration object.
 * @param factory Socket factory.
 * @return Result of m1().m2()
 */","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param conf input Configuration.
   * @param factory input factory.
   * @throws IOException raised on errors performing I/O.
   * @return proxy.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",773,780,"/**
 * Creates a ProtocolProxy with provided protocol, version, address, config.
 * @param protocol Protocol class
 * @param clientVersion Client version
 * @param addr Socket address
 * @param conf Configuration object
 * @return ProtocolProxy instance
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   * 
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param conf input configuration.
   * @param <T> Generics Type T.
   * @return a protocol proxy
   * @throws IOException if the thread is interrupted.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,getUgmProtocol,org.apache.hadoop.tools.GetGroupsBase:getUgmProtocol(),97,105,"/**
 * Retrieves user mappings protocol using RPC.
 * @return GetUserMappingsProtocol object.
 */","* Get a client of the {@link GetUserMappingsProtocol}.
   * @return A {@link GetUserMappingsProtocol} client proxy.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getZKFCProxy,"org.apache.hadoop.ha.HAServiceTarget:getZKFCProxy(org.apache.hadoop.conf.Configuration,int)",164,173,"/**
 * Creates a ZKFCProtocol with specified configuration and timeout.
 * @param conf Configuration object.
 * @param timeoutMs Timeout in milliseconds.
 * @return ZKFCProtocol object.
 */","* @return a proxy to the ZKFC which is associated with this HA service.
   * @param conf configuration.
   * @param timeoutMs timeout in milliseconds.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxyForAddress,"org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,int,java.net.InetSocketAddress)",146,156,"/**
 * Creates a HAServiceProtocol client with given config, timeout, retries, and address.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,isInstrumentationAccessAllowed,"org.apache.hadoop.jmx.JMXJsonServlet:isInstrumentationAccessAllowed(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",152,156,"/**
* Delegates request handling to HttpServer2.
* @param request HTTP request
* @param response HTTP response
* @return Result of the delegated handling
*/
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,doGet,"org.apache.hadoop.http.HttpServer2$StackServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1745,1758,"/**
 * Handles a JSP request, writing output and logging activity.
 * @param request HTTP request object
 * @param response HTTP response object
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileOutputServlet.java,doGet,"org.apache.hadoop.http.ProfileOutputServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",48,77,"/**
 * Handles request, authorizes access, and serves file or triggers refresh.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,doGet,"org.apache.hadoop.http.ProfileServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",187,327,"/**
 * Starts an async profiler session based on request parameters.
 * @param req HttpServletRequest containing profiling options.
 * @param resp HttpServletResponse to send the result.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,doGet,"org.apache.hadoop.conf.ConfServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",57,83,"/**
 * Processes a request, sets content type, and writes output.
 * @param request HTTP request object
 * @param response HTTP response object
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,moveToTrash,org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash(org.apache.hadoop.fs.Path),129,204,"/**
* Moves a file or directory to the trash.
* @param path Path to the file/directory to move.
* @throws IOException if an I/O error occurs during the move.
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,getCurrentTrashDir,org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(),241,244,"/**
 * Creates a Path object using fs.m1(null) and CURRENT.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,getCurrentTrashDir,org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(org.apache.hadoop.fs.Path),246,249,"/**
 * Creates a masked path using the filesystem's m1 function.
 * @param path The input path to mask.
 * @return A new Path object with the masked path.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getTrashRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoot(org.apache.hadoop.fs.Path),1180,1220,"/**
 * Determines the trash root path for a given path.
 * @param path The path to find the trash root for.
 * @return The trash root path.
 */","* Get the trash root directory for current user when the path
   * specified is deleted.
   *
   * If FORCE_INSIDE_MOUNT_POINT flag is not set, return the default trash root
   * from targetFS.
   *
   * When FORCE_INSIDE_MOUNT_POINT is set to true,
   * <ol>
   *   <li>
   *     If the trash root for path p is in the same mount point as path p,
   *       and one of:
   *       <ol>
   *         <li>The mount point isn't at the top of the target fs.</li>
   *         <li>The resolved path of path is root (in fallback FS).</li>
   *         <li>The trash isn't in user's target fs home directory
   *            get the corresponding viewFS path for the trash root and return
   *            it.
   *         </li>
   *       </ol>
   *   </li>
   *   <li>
   *     else, return the trash root under the root of the mount point
   *     (/{mntpoint}/.Trash/{user}).
   *   </li>
   * </ol>
   *
   * These conditions handle several different important cases:
   * <ul>
   *   <li>File systems may need to have more local trash roots, such as
   *         encryption zones or snapshot roots.</li>
   *   <li>The fallback mount should use the user's home directory.</li>
   *   <li>Cloud storage systems should not use trash in an implicity defined
   *        home directory, per a container, unless it is the fallback fs.</li>
   * </ul>
   *
   * @param path the trash root of the path to be determined.
   * @return the trash root path.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getTrashRoot,org.apache.hadoop.fs.FilterFileSystem:getTrashRoot(org.apache.hadoop.fs.Path),689,692,"/**
 * Delegates Path m1 operation to the underlying FileSystem.
 * @param path The Path to operate on.
 * @return The result of the operation.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,run,org.apache.hadoop.fs.TrashPolicyDefault$Emptier:run(),278,320,"/**
 * Empties the trash, periodically deleting files based on interval.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(java.util.Date),212,220,"/**
 * Creates checkpoints for trash roots, iterating through them.
 * @param date The date for the checkpoint.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(boolean),232,239,"/**
 * Deletes checkpoints in trash roots, optionally immediately.
 * @param deleteImmediately Whether to delete immediately.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getTrashRoots,org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoots(boolean),1231,1297,"/**
* Retrieves trash roots, considering all users or not.
* @param allUsers true to include all users' trash, false otherwise.
* @return Collection of FileStatus objects representing trash roots.
*/","* Get all the trash roots for current user or all users.
   *
   * When FORCE_INSIDE_MOUNT_POINT is set to true, we also return trash roots
   * under the root of each mount point, with their viewFS paths.
   *
   * @param allUsers return trash roots for all users if true.
   * @return all Trash root directories.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getTrashRoots,org.apache.hadoop.fs.FilterFileSystem:getTrashRoots(boolean),694,697,"/**
 * Delegates to FileSystem's m1 method.
 * @param allUsers if true, includes all user files; otherwise, not.
 * @return Collection of FileStatus objects.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,testAccess,"org.apache.hadoop.fs.shell.Test:testAccess(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.permission.FsAction)",110,118,"/**
 * Executes a file system action on a PathData item.
 * @param item PathData object. @param action FsAction to perform.
 * @return True on success, false if access fails.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,access,"org.apache.hadoop.fs.viewfs.ViewFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",576,582,"/**
 * Executes an action on a file system path.
 * @param path Path to the file.
 * @param mode Action to perform.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,access,"org.apache.hadoop.fs.FilterFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",470,474,"/**
 * Delegates the file system operation to the underlying file system.
 * @param path The file system path.
 * @param mode The file system action to perform.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,access,"org.apache.hadoop.fs.viewfs.ChRootedFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",208,211,"/**
* Calls myFs.m2 with a modified path and given action.
* @param path The path to operate on.
* @param mode The FsAction to apply.
*/
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,access,"org.apache.hadoop.fs.viewfs.ViewFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",428,434,"/**
 * Executes an action on a file system path.
 * @param path Path to the file.
 * @param mode Action to perform.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,access,"org.apache.hadoop.fs.FilterFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",132,137,"/**
 * Executes an action on a path using the file system.
 * @param path The path to operate on.
 * @param mode The file system action to perform.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(org.apache.hadoop.conf.Configuration),403,405,"/**
 * Constructs a ViewFileSystem with a default URI, using the given config.
 * @param conf Configuration object for file system parameters.
 */
","* Convenience Constructor for apps to call directly.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,addFileSystemForTesting,"org.apache.hadoop.fs.FileSystem:addFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",240,244,"/**
 * Adds a URI to the cache using the provided configuration and filesystem.
 */","* This method adds a FileSystem instance to the cache so that it can
   * be retrieved later. It is only for testing.
   * @param uri the uri to store it under
   * @param conf the configuration to store it under
   * @param fs the FileSystem to store
   * @throws IOException if the current user cannot be determined.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,removeFileSystemForTesting,"org.apache.hadoop.fs.FileSystem:removeFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",246,250,"/**
 * Maps a URI to a FileSystem entry, updating the cache.
 * @param uri URI to map. @param conf Configuration. @param fs FileSystem.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem$Cache:get(java.net.URI,org.apache.hadoop.conf.Configuration)",3665,3668,"/**
 * Gets a FileSystem object for the given URI and configuration.
 * @param uri The URI of the file system.
 * @param conf Hadoop configuration.
 * @return FileSystem object.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,"org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",594,611,"/**
 * Retrieves a FileSystem based on the provided URI and config.
 * @param uri The URI for the filesystem.
 * @param config Configuration object.
 * @return A FileSystem object.
 */
","* Returns the FileSystem for this URI's scheme and authority.
   * The entire URI is passed to the FileSystem instance's initialize method.
   * This always returns a new FileSystem object.
   * @param uri FS URI
   * @param config configuration to use
   * @return the new FS instance
   * @throws IOException FS creation or initialization failure.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,"org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)",373,376,"/**
 * Creates a new FileContext using the provided file system and config.
 */","* Create a FileContext with specified FS as default using the specified
   * config.
   * 
   * @param defFS default fs.
   * @param aConf configutration.
   * @return new FileContext with specified FS as default.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",230,235,"/**
 * Delegates to either self or super based on token type.
 * @param url URL to connect to.
 * @param token Authentication token.
 * @return HttpURLConnection object.
 */","* Returns an authenticated {@link HttpURLConnection}, it uses a Delegation
   * Token only if the given auth token is an instance of {@link Token} and
   * it contains a Delegation Token, otherwise use the configured
   * {@link DelegationTokenAuthenticator} to authenticate the connection.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,execute,org.apache.hadoop.security.KDiag:execute(),282,420,"/**
* Performs security diagnostics and login checks.
* Logs system properties, environment variables, and config.
*/","* Execute diagnostics.
   * <p>
   * Things it would be nice if UGI made accessible
   * <ol>
   *   <li>A way to enable JAAS debug programatically</li>
   *   <li>Access to the TGT</li>
   * </ol>
   * @return true if security was enabled and all probes were successful
   * @throws KerberosDiagsFailure explicitly raised failure
   * @throws Exception other security problems",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,maybeDoLoginFromKeytabAndPrincipal,org.apache.hadoop.security.token.DtUtilShell:maybeDoLoginFromKeytabAndPrincipal(java.lang.String[]),82,106,"/**
 * Parses arguments, extracts principal/keytab, and returns modified args.
 */","* Parse arguments looking for Kerberos keytab/principal.
   * If both are found: remove both from the argument list and attempt login.
   * If only one of the two is found: remove it from argument list, log warning
   * and do not attempt login.
   * If neither is found: return original args array, doing nothing.
   * Return the pruned args array if either flag is present.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,main,org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[]),2300,2318,"/**
 * Retrieves and displays UserGroupInformation, optionally from keytab.
 * @param args Command-line arguments; keytab principal and file.
 */","* A test method to print out the current user's UGI.
   * @param args if there are two arguments, read the user from the keytab
   * and print it out.
   * @throws Exception Exception.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,login,"org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)",309,329,"/**
 * Logs in the user using the provided keytab and principal.
 * @param conf Hadoop configuration.
 * @param keytabFileKey Keytab file key.
 * @param userNameKey User name key.
 * @param hostname Hostname to use for principal.
 */","* Login as a principal specified in config. Substitute $host in user's Kerberos principal 
   * name with hostname. If non-secure mode - return. If no keytab available -
   * bail out with an exception
   * 
   * @param conf
   *          conf to use
   * @param keytabFileKey
   *          the key to look for keytab file in conf
   * @param userNameKey
   *          the key to look for user's Kerberos principal name in conf
   * @param hostname
   *          hostname to use for substitution
   * @throws IOException if the config doesn't specify a keytab",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createConnection,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createConnection(java.net.URL,java.lang.String)",502,537,"/**
 * Creates and configures an HttpURLConnection with delegation token auth.
 * @param url URL to connect to
 * @param method HTTP method (e.g., GET, POST)
 * @return HttpURLConnection object
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDelegationToken(java.lang.String),1025,1059,"/**
 * Obtains a delegation token from the URL, using renewer.
 * @param renewer renewer string for token acquisition
 * @return Token object or null on failure
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,renewDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token),1061,1087,"/**
 * Renews a delegation token.
 * @param dToken The delegation token to renew.
 * @return The new expiration time in milliseconds.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,cancelDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token),1089,1116,"/**
 * Cancels a delegation token using a privileged action.
 * @param dToken The delegation token to cancel.
 * @throws IOException if an I/O error occurs.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)",3307,3405,"/**
 * Constructs a Server instance with specified configurations.
 * @param bindAddress Server bind address.
 * @param port Server port.
 */
","* Constructs a server listening on the named port and address.  Parameters passed must
   * be of the named class.  The <code>handlerCount</code> determines
   * the number of handler threads that will be used to process calls.
   * If queueSizePerHandler or numReaders are not -1 they will be used instead of parameters
   * from configuration. Otherwise the configuration will be picked up.
   * 
   * If rpcRequestClass is null then the rpcRequestClass must have been 
   * registered via {@link #registerProtocolEngine(RPC.RpcKind,
   *  Class, RPC.RpcInvoker)}
   * This parameter has been retained for compatibility with existing tests
   * and usage.
   *
   * @param bindAddress input bindAddress.
   * @param port input port.
   * @param rpcRequestClass input rpcRequestClass.
   * @param handlerCount input handlerCount.
   * @param numReaders input numReaders.
   * @param queueSizePerHandler input queueSizePerHandler.
   * @param conf input Configuration.
   * @param serverName input serverName.
   * @param secretManager input secretManager.
   * @param portRangeConfig input portRangeConfig.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,buildSaslNegotiateResponse,org.apache.hadoop.ipc.Server$Connection:buildSaslNegotiateResponse(),2603,2619,"/**
* Constructs the negotiate message for token authentication.
* Returns the negotiate message or negotiates token auth.
*/","* Process the Sasl's Negotiate request, including the optimization of 
     * accelerating token negotiation.
     * @return the response to Negotiate request - the list of enabled 
     *         authMethods and challenge if the TOKENS are supported. 
     * @throws SaslException - if attempt to generate challenge fails.
     * @throws IOException - if it fails to create the SASL server for Tokens",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",326,332,"/**
 * Creates a ProtocolProxy with a default timeout.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @param addr Socket address.
 * @param conf Configuration object.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)",346,351,"/**
 * Executes a protocol function and returns the result.
 * @param protocol Protocol class to execute.
 * @return Result of the protocol function.
 */
","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param connTimeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String)",998,1000,"/**
 * Processes a resource by creating a Resource object and passing it to m1.
 * @param in Input stream for the resource.
 * @param name Resource name.
 */
","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param in InputStream to deserialize the object from.
   * @param name the name of the resource because InputStream.toString is not
   * very descriptive some times.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object),253,255,"/**
 * Constructs a Resource with the given object and its string representation.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",728,734,"/**
 * Delegates to m1 and calls its m2 method.
 * @param protocol Class to instantiate.
 * @param clientVersion Client version.
 * @param addr Socket address.
 * @param conf Configuration object.
 * @return Result of m1.m2()
 */
","* Construct a client-side proxy object with the default SocketFactory.
    *
    * @param <T> Generics Type T.
    * @param protocol input protocol.
    * @param clientVersion input clientVersion.
    * @param addr input addr.
    * @param conf input Configuration.
    * @return a proxy instance
    * @throws IOException  if the thread is interrupted.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,run,org.apache.hadoop.tools.GetGroupsBase:run(java.lang.String[]),62,79,"/**
 * Prints group memberships for given usernames, or default user.
 * @param args Array of usernames to check; uses default if empty.
 * @return 0
 */
","* Get the groups for the users given and print formatted output to the
   * {@link PrintStream} configured earlier.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,gracefulFailoverThroughZKFCs,org.apache.hadoop.ha.HAAdmin:gracefulFailoverThroughZKFCs(org.apache.hadoop.ha.HAServiceTarget),276,290,"/**
 * Attempts failover to a node.
 * @param toNode The target HAServiceTarget node.
 * @return 0 on success, -1 on failure.
 */
","* Initiate a graceful failover by talking to the target node's ZKFC.
   * This sends an RPC to the ZKFC, which coordinates the failover.
   *
   * @param toNode the node to fail to
   * @return status code (0 for success)
   * @throws IOException if failover does not succeed",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,cedeRemoteActive,"org.apache.hadoop.ha.ZKFailoverController:cedeRemoteActive(org.apache.hadoop.ha.HAServiceTarget,int)",749,756,"/**
 * Retrieves and initializes a ZKFCProtocol from a remote HAServiceTarget.
 * @param remote Remote HAServiceTarget.
 * @param timeout Timeout in milliseconds.
 * @return ZKFCProtocol object.
 */","* Ask the remote zkfc to cede its active status and wait for the specified
   * timeout before attempting to claim leader status.
   * @param remote node to ask
   * @param timeout amount of time to cede
   * @return the {@link ZKFCProtocol} used to talk to the ndoe
   * @throws IOException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getHealthMonitorProxy,"org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int,int)",131,138,"/**
 * Connects to a service using configured parameters and retries.
 * @param conf Configuration object. @param timeoutMs Timeout in ms.
 * @param retries Number of connection retries.
 * @return HAServiceProtocol object.
 */
",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxyForAddress,"org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,java.net.InetSocketAddress)",140,144,"/**
 * Calls m1 with a default retry count.
 * @param conf Configuration object.
 * @param timeoutMs Timeout in milliseconds.
 * @param addr Remote address.
 * @return HAServiceProtocol object.
 */
",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,doGet,"org.apache.hadoop.jmx.JMXJsonServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",175,232,"/**
 * Processes JMX requests, serializes results to JSON, and returns.
 */","* Process a GET request for the specified resource.
   * 
   * @param request
   *          The servlet request we are processing
   * @param response
   *          The servlet response we are creating",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(),206,210,"/**
 * Calls m1 with a Date object as the argument.
 * Throws IOException if an I/O error occurs.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(),222,225,"/**
 * Calls m1 with the 'flag' parameter set to false.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpointsImmediately,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpointsImmediately(),227,230,"/**
 * Calls m1 with true as an argument.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,processPath,org.apache.hadoop.fs.shell.Test:processPath(org.apache.hadoop.fs.shell.PathData),77,108,"/**
 * Evaluates a condition based on flag and PathData.
 * Sets exitCode to 1 if condition fails.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,access,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",253,257,"/**
* Calls super.m2 with a modified path, potentially changing access.
* @param path The path to operate on.
* @param mode The access control mode.
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)",536,558,"/**
 * Retrieves a FileSystem instance from cache or creates a new one.
 * @param uri The URI of the filesystem.
 * @param conf Hadoop configuration.
 * @return A FileSystem object.
 */","* Get a FileSystem for this URI's scheme and authority.
   * <ol>
   * <li>
   *   If the configuration has the property
   *   {@code ""fs.$SCHEME.impl.disable.cache""} set to true,
   *   a new instance will be created, initialized with the supplied URI and
   *   configuration, then returned without being cached.
   * </li>
   * <li>
   *   If the there is a cached FS instance matching the same URI, it will
   *   be returned.
   * </li>
   * <li>
   *   Otherwise: a new FS instance will be created, initialized with the
   *   configuration and URI, cached and returned to the caller.
   * </li>
   * </ol>
   * @param uri uri of the filesystem.
   * @param conf configrution.
   * @return filesystem instance.
   * @throws IOException if the FileSystem cannot be instantiated.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstanceLocal,org.apache.hadoop.fs.FileSystem:newInstanceLocal(org.apache.hadoop.conf.Configuration),631,634,"/**
 * Retrieves the LocalFileSystem instance for the configuration.
 * @param conf Hadoop configuration object
 * @return LocalFileSystem instance
 */","* Get a unique local FileSystem object.
   * @param conf the configuration to configure the FileSystem with
   * @return a new LocalFileSystem object.
   * @throws IOException FS creation or initialization failure.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/FsGetter.java,getNewInstance,"org.apache.hadoop.fs.viewfs.FsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",42,45,"/**
 * Gets a FileSystem for the given URI and configuration.
 * @param uri URI of the file system
 * @param conf Hadoop configuration
 * @return FileSystem instance
 */
","* Gets new file system instance of given uri.
   * @param uri uri.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return file system.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getNewInstance,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",234,250,"/**
 * Returns a FileSystem for the given URI, or defaults if mismatch.
 * @param uri The URI for the filesystem.
 * @param conf The configuration.
 * @return A FileSystem object.
 */
",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatusForFallbackLink,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatusForFallbackLink(),1244,1265,"/**
* Retrieves FileStatus array, handling fallback filesystem.
* Returns empty array if no status found.
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem),385,388,"/**
* Creates a FileContext using the provided file system.
* @param defaultFS The default file system to use.
* @return A FileContext object.
*/
","* Create a FileContext for specified file system using the default config.
   * 
   * @param defaultFS default fs.
   * @return a FileContext with the specified AbstractFileSystem
   *                 as the default FS.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,"org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI,org.apache.hadoop.conf.Configuration)",456,473,"/**
 * Creates a FileContext using provided URI and config.
 * @param defaultFsUri URI of the default file system.
 * @param aConf Hadoop configuration.
 * @return FileContext object.
 */","* Create a FileContext for specified default URI using the specified config.
   * 
   * @param defaultFsUri defaultFsUri.
   * @param aConf configrution.
   * @return new FileContext for specified uri
   * @throws UnsupportedFileSystemException If the file system with specified is
   *           not supported
   * @throws RuntimeException If the file system specified is supported but
   *         could not be instantiated, or if login fails.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,run,org.apache.hadoop.security.KDiag:run(java.lang.String[]),197,244,"/**
 * Parses arguments, loads resources, and configures the system.
 * Returns 0 on success, -1 on failure.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,init,org.apache.hadoop.security.token.DtUtilShell:init(java.lang.String[]),116,177,"/**
 * Processes command-line arguments and sets up operations.
 * @param args Command-line arguments passed to the function.
 * @return 0 on success, 1 on failure.
 */","* Parse the command line arguments and initialize subcommand.
   * Also will attempt to perform Kerberos login if both -principal and -keytab
   * flags are passed in args array.
   * @param args args.
   * @return 0 if the argument(s) were recognized, 1 otherwise
   * @throws Exception Exception.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,login,"org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",287,292,"/**
 * Calls m2 with a generated principal name based on the configuration.
 * @param conf Configuration object.
 * @param keytabFileKey Keytab file key.
 * @param userNameKey User name key.
 */
","* Login as a principal specified in config. Substitute $host in
   * user's Kerberos principal name with a dynamically looked-up fully-qualified
   * domain name of the current host.
   * 
   * @param conf
   *          conf to use
   * @param keytabFileKey
   *          the key to look for keytab file in conf
   * @param userNameKey
   *          the key to look for user's Kerberos principal name in conf
   * @throws IOException if login fails",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,call,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class,int)",544,608,"/**
 * Executes a request, handles retries, and parses JSON response.
 * @param conn HttpURLConnection, expectedResponse, klass, authRetryCount
 * @return Parsed object of type T or null if not found
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,<init>,"org.apache.hadoop.ipc.RPC$Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)",1189,1198,"/**
 * Constructs a Server instance with the given configuration parameters.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,org.apache.hadoop.conf.Configuration)",3264,3271,"/**
 * Constructs a Server with specified address, port, class, and handlers.
 * @param bindAddress Server bind address.
 * @param port Server port.
 * @param paramClass Writable class.
 * @param handlerCount Handler count.
 * @param conf Configuration object.
 */
",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager)",3273,3280,"/**
 * Constructs a Server object with the given configuration parameters.
 * @param bindAddress Bind address, port, RPC request class, etc.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processSaslMessage,org.apache.hadoop.ipc.Server$Connection:processSaslMessage(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2327,2385,"/**
 * Processes a Sasl message and returns a Sasl response.
 * @param saslMessage Incoming Sasl message to process.
 * @return Sasl response or null based on the state.
 * @throws SaslException, IOException, AccessControlException, InterruptedException
 */","* Process a saslMessge.
     * @param saslMessage received SASL message
     * @return the sasl response to send back to client
     * @throws SaslException if authentication or generating response fails, 
     *                       or SASL protocol mixup
     * @throws IOException if a SaslServer cannot be created
     * @throws AccessControlException if the requested authentication type 
     *         is not supported or trying to re-attempt negotiation.
     * @throws InterruptedException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",305,312,"/**
 * Calls m1 and then m2, returning the result of m2.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @param addr Socket address.
 * @param conf Configuration object.
 */
","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.lang.String),923,925,"/**
 * Creates a Resource with the given name and passes it to m1.
 * @param name The name of the resource to create.
 */
","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param name resource to be added, the classpath is examined for a file 
   *             with that name.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.net.URL),941,943,"/**
 * Processes a URL by creating a Resource and passing it to m1.
 */","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param url url of the resource to be added, the local filesystem is 
   *            examined directly to find the resource, without referring to 
   *            the classpath.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path),959,961,"/**
 * Processes a file using m1.
 * @param file The Path object representing the file to process.
 */","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param file file-path of resource to be added, the local filesystem is
   *             examined directly to find the resource, without referring to 
   *             the classpath.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream),980,982,"/**
 * Processes an input stream by wrapping it in a Resource and calling m1.
 */","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * WARNING: The contents of the InputStream will be cached, by this method. 
   * So use this sparingly because it does increase the memory consumption.
   * 
   * @param in InputStream to deserialize the object from. In will be read from
   * when a get or set is called next.  After it is read the stream will be
   * closed.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",66,72,"/**
 * Initializes the client with address and configuration.
 * @param addr Socket address for the connection.
 * @param conf Hadoop configuration object.
 */
",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doGracefulFailover,org.apache.hadoop.ha.ZKFailoverController:doGracefulFailover(),660,739,"/**
 * Attempts to become the active node, handling failover logic.
 * Throws ServiceFailedException if failover fails.
 */","* Coordinate a graceful failover. This proceeds in several phases:
   * 1) Pre-flight checks: ensure that the local node is healthy, and
   * thus a candidate for failover.
   * 2a) Determine the current active node. If it is the local node, no
   * need to failover - return success.
   * 2b) Get the other nodes
   * 3a) Ask the other nodes to yield from election for a number of seconds
   * 3b) Ask the active node to yield from the election for a number of seconds.
   * 4) Allow the normal election path to run in other threads. Wait until
   * we either become unhealthy or we see an election attempt recorded by
   * the normal code path.
   * 5) Allow the old active to rejoin the election, so a future
   * failback is possible.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,createProxy,org.apache.hadoop.ha.HealthMonitor:createProxy(),191,193,"/**
* Retrieves the HAServiceProtocol based on configuration.
* @return HAServiceProtocol object.
* @throws IOException if an I/O error occurs.
*/
","* Connect to the service to be monitored. Stubbed out for easier testing.
   *
   * @throws IOException raised on errors performing I/O.
   * @return HAServiceProtocol.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getHealthMonitorProxy,"org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int)",126,129,"/**
* Calls m1 with a default retry count.
* @param conf Configuration object.
* @param timeoutMs Timeout in milliseconds.
* @return HAServiceProtocol object.
*/
","* Returns a proxy to connect to the target HA service for health monitoring.
   * If {@link #getHealthMonitorAddress()} is implemented to return a non-null
   * address, then this proxy will connect to that address.  Otherwise, the
   * returned proxy defaults to using {@link #getAddress()}, which means this
   * method's behavior is identical to {@link #getProxy(Configuration, int)}.
   *
   * @param conf configuration.
   * @param timeoutMs timeout in milliseconds
   * @return a proxy to connect to the target HA service for health monitoring
   * @throws IOException if there is an error",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxy,"org.apache.hadoop.ha.HAServiceTarget:getProxy(org.apache.hadoop.conf.Configuration,int)",100,103,"/**
* Creates a HAServiceProtocol with given config and timeout.
* @param conf Configuration object
* @param timeoutMs Timeout in milliseconds
* @return HAServiceProtocol instance
*/
","* @return a proxy to connect to the target HA Service.
   * @param timeoutMs timeout in milliseconds.
   * @param conf Configuration.
   * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,initialize,"org.apache.hadoop.fs.HarFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",128,175,"/**
 * Initializes the HarFileSystem, fetches metadata, and validates paths.
 */","* Initialize a Har filesystem per har archive. The 
   * archive home directory is the top level directory
   * in the filesystem that contains the HAR archive.
   * Be careful with this method, you do not want to go 
   * on creating new Filesystem instances per call to 
   * path.getFileSystem().
   * the uri of Har is 
   * har://underlyingfsscheme-host:port/archivepath.
   * or 
   * har:///archivepath. This assumes the underlying filesystem
   * to be used in case not specified.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,moveToAppropriateTrash,"org.apache.hadoop.fs.Trash:moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",77,122,"/**
 * Checks if a path is trash-enabled, potentially updating trash interval.
 * @param fs Filesystem.
 * @param p Path to check.
 * @param conf Configuration.
 * @throws IOException if trash configuration retrieval fails.
 */","* In case of the symlinks or mount points, one has to move the appropriate
   * trashbin in the actual volume of the path p being deleted.
   *
   * Hence we get the file system of the fully-qualified resolved-path and
   * then move the path p to the trashbin in that volume,
   * @param fs - the filesystem of path p
   * @param p - the path being deleted - to be moved to trash
   * @param conf - configuration
   * @return false if the item is already in the trash or trash is disabled
   * @throws IOException on error",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,connect,org.apache.hadoop.fs.FsUrlConnection:connect(),55,75,"/**
 * Connects to a file system, establishing an input stream.
 * @throws IOException if connection fails or URI is invalid.
 */",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",88,90,"/**
 * Constructs a PathData object from a path string and configuration.
 * @param pathString Path string to create PathData from.
 * @param conf Hadoop configuration.
 * @throws IOException if an I/O error occurs.
 */
","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it
   * @param pathString a string for a path
   * @param conf the configuration file
   * @throws IOException if anything goes wrong...",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFSofPath,"org.apache.hadoop.fs.FileSystem:getFSofPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",427,435,"/**
 * Obtains a FileSystem for a given path and configuration.
 * @param absOrFqPath Path to the filesystem.
 * @param conf Hadoop configuration.
 * @return FileSystem object.
 */
",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getNamed,"org.apache.hadoop.fs.FileSystem:getNamed(java.lang.String,org.apache.hadoop.conf.Configuration)",477,481,"/**
 * Creates a FileSystem object for the given name and configuration.
 * @param name File system name.
 * @param conf Hadoop configuration.
 * @return FileSystem object.
 */","* @deprecated call {@link #get(URI, Configuration)} instead.
   *
   * @param name name.
   * @param conf configuration.
   * @return file system.
   * @throws IOException If an I/O error occurred.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLocal,org.apache.hadoop.fs.FileSystem:getLocal(org.apache.hadoop.conf.Configuration),508,511,"/**
 * Retrieves the LocalFileSystem instance for the given configuration.
 * @param conf Hadoop configuration object
 * @return LocalFileSystem instance
 */
","* Get the local FileSystem.
   * @param conf the configuration to configure the FileSystem with
   * if it is newly instantiated.
   * @return a LocalFileSystem
   * @throws IOException if somehow the local FS cannot be instantiated.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",124,127,"/**
 * Constructs a ChRootedFileSystem with a given URI and configuration.
 * @param uri The URI of the filesystem.
 * @param conf Hadoop configuration.
 * @throws IOException if an I/O error occurs.
 */
","* Constructor.
   * @param uri base file system
   * @param conf configuration
   * @throws IOException",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/FsGetter.java,get,"org.apache.hadoop.fs.viewfs.FsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)",55,57,"/**
 * Gets a FileSystem instance for the given URI and configuration.
 * @param uri The URI of the file system.
 * @param conf The configuration settings.
 * @return A FileSystem object.
 */
","* Gets file system instance of given uri.
   *
   * @param uri uri.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return FileSystem.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,get,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)",258,275,"/**
 * Retrieves a FileSystem based on URI scheme matching.
 * @param uri The URI to resolve.
 * @param conf Configuration object.
 * @return A FileSystem instance.
 */","* When ViewFileSystemOverloadScheme scheme and target uri scheme are
     * matching, it will not take advantage of FileSystem cache as it will
     * create instance directly. For caching needs please set
     * ""fs.viewfs.enable.inner.cache"" to true.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getFileSystem,org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration),365,367,"/**
* Gets a FileSystem instance using the provided Configuration.
* @param conf Hadoop configuration object
* @return A FileSystem instance
*/
","* Return the FileSystem that owns this Path.
   *
   * @param conf the configuration to use when resolving the FileSystem
   * @return the FileSystem that owns this Path
   * @throws java.io.IOException thrown if there's an issue resolving the
   * FileSystem",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getFileSystem,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getFileSystem(),458,476,"/**
 * Retrieves a FileSystem, using supplied or connecting to base path.
 * @return FileSystem object, or null if supplied filesystem is null.
 */","* Return the supplied file system for testing or otherwise get a new file
   * system.
   *
   * @return the file system to use
   * @throws MetricsException thrown if the file system could not be retrieved",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,get,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:get(java.net.URI,org.apache.hadoop.conf.Configuration)",129,153,"/**
 * Retrieves or creates a FileSystem for the given URI and config.
 * @param uri The URI of the filesystem.
 * @param config Configuration for filesystem creation.
 * @return The FileSystem object.
 */",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path),1159,1226,"/**
 * Retrieves FileStatus for paths within a directory.
 * @param f the path to the directory
 * @return An array of FileStatus objects.
 */","* {@inheritDoc}
     *
     * Note: listStatus on root(""/"") considers listing from fallbackLink if
     * available. If the same directory name is present in configured mount
     * path as well as in fallback link, then only the configured mount path
     * will be listed in the returned result.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI),440,443,"/**
 * Gets a FileContext for the default filesystem URI.
 * @param defaultFsUri URI of the default filesystem
 * @return FileContext object
 */","* Create a FileContext for specified URI using the default config.
   * 
   * @param defaultFsUri defaultFsUri.
   * @return a FileContext with the specified URI as the default FS.
   * 
   * @throws UnsupportedFileSystemException If the file system for
   *           <code>defaultFsUri</code> is not supported",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.conf.Configuration),485,496,"/**
 * Gets a FileContext for the default filesystem.
 * @param aConf Hadoop configuration object
 * @throws UnsupportedFileSystemException if scheme is missing
 */","* Create a FileContext using the passed config. Generally it is better to use
   * {@link #getFileContext(URI, Configuration)} instead of this one.
   * 
   * 
   * @param aConf configration.
   * @return new FileContext
   * @throws UnsupportedFileSystemException If file system in the config
   *           is not supported",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLocalFSFileContext,org.apache.hadoop.fs.FileContext:getLocalFSFileContext(org.apache.hadoop.conf.Configuration),506,509,"/**
 * Creates a FileContext for the local file system.
 * @param aConf Configuration object for file system operations.
 * @throws UnsupportedFileSystemException if local FS is unsupported.
 */","* @param aConf - from which the FileContext is configured
   * @return a FileContext for the local file system using the specified config.
   * 
   * @throws UnsupportedFileSystemException If default file system in the config
   *           is not supported
   *",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,init,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:init(org.apache.commons.configuration2.SubsetConfiguration),233,265,"/**
 * Configures the component using provided SubsetConfiguration.
 * @param metrics2Properties Configuration properties to apply.
 */",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,call,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class)",539,542,"/**
 * Delegates to overloaded method with default authRetry.
 * @param conn HttpURLConnection, jsonOutput, expectedResponse, klass
 * @return Instance of klass, or null if not found.
 */",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",479,493,"/**
 * Constructs a Server with specified configuration and protocol details.
 */","* Construct an RPC server.
     *
     * @param protocolClass the class of protocol
     * @param protocolImpl the protocolImpl whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param numReaders number of read threads
     * @param queueSizePerHandler the size of the queue contained
     *                            in each Handler
     * @param verbose whether each call should be logged
     * @param secretManager the server-side secret manager for each token type
     * @param portRangeConfig A config parameter that can be used to restrict
     * the range of ports used when port is 0 (an ephemeral port)
     * @param alignmentContext provides server state info on client responses
     * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",495,535,"/**
 * Constructs a Server with specified configuration and protocol.
 * @param protocolClass Protocol class, or null to derive from impl.
 */","* Construct an RPC server.
     * @param protocolClass - the protocol being registered
     *     can be null for compatibility with old usage (see below for details)
     * @param protocolImpl the protocol impl that will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param alignmentContext provides server state info on client responses
     * @param numReaders input numReaders.
     * @param portRangeConfig input portRangeConfig.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param secretManager input secretManager.
     * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,saslProcess,org.apache.hadoop.ipc.Server$Connection:saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2242,2314,"/**
 * Processes incoming SASL message, authenticates client, and sets context.
 * @param saslMessage Incoming SASL message to process.
 */","* Process saslMessage and send saslResponse back
     * @param saslMessage received SASL message
     * @throws RpcServerException setup failed due to SASL negotiation
     *         failure, premature or invalid connection context, or other state 
     *         errors. This exception needs to be sent to the client. This 
     *         exception will wrap {@link RetriableException}, 
     *         {@link InvalidToken}, {@link StandbyException} or 
     *         {@link SaslException}.
     * @throws IOException if sending reply fails
     * @throws InterruptedException",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,readSSLConfiguration,"org.apache.hadoop.security.ssl.SSLFactory:readSSLConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.ssl.SSLFactory$Mode)",162,184,"/**
 * Creates an SSL configuration based on the provided mode.
 * @param conf Base configuration.
 * @param mode Client or server mode.
 * @return SSL configuration.
 */",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,refresh,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refresh(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",140,150,"/**
 * Loads policy file, configures policy, and applies it.
 * @param conf Configuration object.
 * @param provider Policy provider.
 */
",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/HCFSMountTableConfigLoader.java,load,"org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:load(java.lang.String,org.apache.hadoop.conf.Configuration)",57,113,"/**
 * Loads the latest mount table configuration from the specified path.
 * @param mountTableConfigPath Path to the mount table configuration.
 * @param conf Hadoop configuration object.
 */","* Loads the mount-table configuration from hadoop compatible file system and
   * add the configuration items to given configuration. Mount-table
   * configuration format should be suffixed with version number.
   * Format: {@literal mount-table.<versionNumber>.xml}
   * Example: mount-table.1.xml
   * When user wants to update mount-table, the expectation is to upload new
   * mount-table configuration file with monotonically increasing integer as
   * version number. This API loads the highest version number file. We can
   * also configure single file path directly.
   *
   * @param mountTableConfigPath : A directory path where mount-table files
   *          stored or a mount-table file path. We recommend to configure
   *          directory with the mount-table version files.
   * @param conf : to add the mount table as resource.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,tryConnect,org.apache.hadoop.ha.HealthMonitor:tryConnect(),170,183,"/**
 * Attempts to establish a proxy connection.
 * Sets proxy to m5() result or null on failure, updates state.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,isOtherTargetNodeActive,"org.apache.hadoop.ha.HAAdmin:isOtherTargetNodeActive(java.lang.String,boolean)",187,215,"/**
 * Checks if nodes are active, returns true if any fail activation.
 * @param targetNodeToActivate Node to activate
 * @param forceActive Forces activation despite errors.
 */","* Checks whether other target node is active or not
   * @param targetNodeToActivate
   * @return true if other target node is active or some other exception 
   * occurred and forceActive was set otherwise false
   * @throws IOException",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,transitionToStandby,org.apache.hadoop.ha.HAAdmin:transitionToStandby(org.apache.commons.cli.CommandLine),217,234,"/**
 * Transitions a service to standby.
 * @param cmd Command line arguments; expects one argument.
 * @return 0 on success, -1 on failure.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkHealth,org.apache.hadoop.ha.HAAdmin:checkHealth(org.apache.commons.cli.CommandLine),292,309,"/**
 * Checks service health using provided command line arguments.
 * @param cmd Command line arguments; expects one argument.
 * @return 0 on success, -1 on failure.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,getServiceState,org.apache.hadoop.ha.HAAdmin:getServiceState(org.apache.commons.cli.CommandLine),311,324,"/**
* Retrieves and prints service state.
* @param cmd CommandLine object containing arguments.
* @return 0 on success, -1 on failure.
*/
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,getAllServiceState,org.apache.hadoop.ha.HAAdmin:getAllServiceState(),443,464,"/**
 * Iterates through targets, retrieves protocol info, and prints.
 * Returns 0 on success, -1 on failure to get service IDs.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,becomeActive,org.apache.hadoop.ha.ZKFailoverController:becomeActive(),408,443,"/**
 * Transitions a target to the active state, logging and recording attempts.
 * @throws ServiceFailedException if activation fails.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,becomeStandby,org.apache.hadoop.ha.ZKFailoverController:becomeStandby(),514,529,"/**
* Transitions a target to standby state, logging status.
* Sets service state to STANDBY after successful transition.
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doCedeActive,org.apache.hadoop.ha.ZKFailoverController:doCedeActive(int),590,623,"/**
 * Cedes the active role, transitioning the node to standby.
 * @param millisToCede Milliseconds to delay joining the election.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,preFailoverChecks,"org.apache.hadoop.ha.FailoverController:preFailoverChecks(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean)",109,156,"/**
 * Attempts failover to target service, throwing exception on failure.
 * @param from Source service, @param target Target service
 */","* Perform pre-failover checks on the given service we plan to
   * failover to, eg to prevent failing over to a service (eg due
   * to it being inaccessible, already active, not healthy, etc).
   *
   * An option to ignore toSvc if it claims it is not ready to
   * become active is provided in case performing a failover will
   * allow it to become active, eg because it triggers a log roll
   * so the standby can learn about new blocks and leave safemode.
   *
   * @param from currently active service
   * @param target service to make active
   * @param forceActive ignore toSvc if it reports that it is not ready
   * @throws FailoverFailedException if we should avoid failover",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,tryGracefulFence,org.apache.hadoop.ha.FailoverController:tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget),168,186,"/**
* Attempts to gracefully make a service standby.
* @param svc The HAServiceTarget to make standby.
* @return True if successful, false otherwise.
*/","* Try to get the HA state of the node at the given address. This
   * function is guaranteed to be ""quick"" -- ie it has a short timeout
   * and no retries. Its only purpose is to avoid fencing a node that
   * has already restarted.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,moveToTrash,org.apache.hadoop.fs.shell.Delete$Rm:moveToTrash(org.apache.hadoop.fs.shell.PathData),151,167,"/**
* Attempts to process a PathData item, handling IO exceptions.
* @param item The PathData object to process.
* @return True if processing was successful, false otherwise.
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,getInputStream,org.apache.hadoop.fs.FsUrlConnection:getInputStream(),77,83,"/**
* Returns an input stream. Initializes if null.
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,recursePath,org.apache.hadoop.fs.shell.find.Find:recursePath(org.apache.hadoop.fs.shell.PathData),345,371,"/**
 * Processes a PathData item, potentially linking and skipping.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isPathRecursable,org.apache.hadoop.fs.shell.find.Find:isPathRecursable(org.apache.hadoop.fs.shell.PathData),373,392,"/**
* Checks if a PathData item meets certain criteria.
* Returns true if conditions are met, false otherwise.
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,expandArgument,org.apache.hadoop.fs.shell.Head:expandArgument(java.lang.String),56,61,"/**
 * Creates a PathData list from the input string and m1().
 * @param arg Input string used to create PathData.
 * @return List containing a single PathData object.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,expandArgument,org.apache.hadoop.fs.shell.Tail:expandArgument(java.lang.String),81,86,"/**
 * Creates a PathData list from the input string argument.
 * @param arg String argument used to create PathData.
 * @return List of PathData objects.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemLinkResolver.java,resolve,"org.apache.hadoop.fs.FileSystemLinkResolver:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",71,111,"/**
 * Resolves a path, following symlinks until resolved or error.
 * @param filesys Filesystem object.
 * @param path Path to resolve.
 * @return Resolved object of type T.
 */","* Attempt calling overridden {@link #doCall(Path)} method with
   * specified {@link FileSystem} and {@link Path}. If the call fails with an
   * UnresolvedLinkException, it will try to resolve the path and retry the call
   * by calling {@link #next(FileSystem, Path)}.
   * @param filesys FileSystem with which to try call
   * @param path Path with which to try call
   * @return Generic type determined by implementation
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",100,102,"/**
 * Creates a PathData object using a local URI and configuration.
 * @param localPath The local URI path.
 * @param conf The Hadoop configuration.
 */
","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it
   * @param localPath a local URI
   * @param conf the configuration file
   * @throws IOException if anything goes wrong...",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",2571,2576,"/**
 * Copies files from source paths to the destination path.
 * @param delSrc Deletes source files after successful copy.
 * @param overwrite Overwrites existing files at destination.
 * @param srcs Source file paths.
 * @param dst Destination path.
 */
","* The src files are on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param overwrite whether to overwrite an existing file
   * @param srcs array of paths which are source
   * @param dst path
   * @throws IOException IO failure",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2588,2593,"/**
* Copies a file or directory.
* @param delSrc Delete source file/dir after copy.
* @param overwrite Overwrite existing files.
* @param src Source path.
* @param dst Destination path.
*/
","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param overwrite whether to overwrite an existing file
   * @param src path
   * @param dst path
   * @throws IOException IO failure",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",2648,2658,"/**
* Copies a file or directory.
* @param delSrc Delete source file after copy.
* @param src Source path.
* @param dst Destination path.
* @param useRawLocalFileSystem Use raw local filesystem.
*/","* The src file is under this filesystem, and the dst is on the local disk.
   * Copy it from the remote filesystem to the local dst name.
   * delSrc indicates if the src will be removed
   * or not. useRawLocalFileSystem indicates whether to use RawLocalFileSystem
   * as the local file system or not. RawLocalFileSystem is non checksumming,
   * So, It will not create any crc files at local.
   *
   * @param delSrc
   *          whether to delete the src
   * @param src
   *          path
   * @param dst
   *          path
   * @param useRawLocalFileSystem
   *          whether to use RawLocalFileSystem as local file system or not.
   *
   * @throws IOException for any IO error",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,confChanged,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:confChanged(org.apache.hadoop.conf.Configuration),309,360,"/**
 * Creates a Context object with local directories from configuration.
 * @param conf Hadoop configuration object
 * @return Context object with configured local directories
 */","This method gets called everytime before any read/write to make sure
     * that any change to localDirs is reflected immediately.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLocalPath,"org.apache.hadoop.conf.Configuration:getLocalPath(java.lang.String,java.lang.String)",2828,2848,"/**
 * Finds a valid local path by iterating through directories.
 * @param dirsProp Property containing directory paths.
 * @param path The path to create.
 * @return A Path object if found, otherwise throws IOException.
 */","* Get a local file under a directory named by <i>dirsProp</i> with
   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,
   * then one is chosen based on <i>path</i>'s hash code.  If the selected
   * directory does not exist, an attempt is made to create it.
   *
   * @param dirsProp directory in which to locate the file.
   * @param path file-path.
   * @return local file under the directory with the given path.
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,java.net.URI,org.apache.hadoop.conf.Configuration)",94,97,"/**
 * Constructs a NflyNode with a ChRootedFileSystem.
 * @param hostName Node hostname.
 * @param rackName Node rack name.
 * @param uri URI for the filesystem.
 * @param conf Configuration object.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getRawFileSystem,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getRawFileSystem(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",328,340,"/**
 * Resolves a FileSystem for the given path using the configuration.
 * @param path Path to resolve.
 * @param conf Configuration object.
 * @return Resolved FileSystem.
 */","* This is an admin only API to give access to its child raw file system, if
   * the path is link. If the given path is an internal directory(path is from
   * mount paths tree), it will initialize the file system of given path uri
   * directly. If path cannot be resolved to any internal directory or link, it
   * will throw NotInMountpointException. Please note, this API will not return
   * chrooted file system. Instead, this API will get actual raw file system
   * instances.
   *
   * @param path - fs uri path
   * @param conf - configuration
   * @throws IOException raised on errors performing I/O.
   * @return file system.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getMountPathInfo,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountPathInfo(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",351,372,"/**
 * Resolves mount path info for a given path and configuration.
 * @param path Path to resolve.
 * @param conf Configuration object.
 * @return MountPathInfo object.
 */","* Gets the mount path info, which contains the target file system and
   * remaining path to pass to the target file system.
   *
   * @param path the path.
   * @param conf configuration.
   * @return mount path info.
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,expandAsGlob,"org.apache.hadoop.fs.shell.PathData:expandAsGlob(java.lang.String,org.apache.hadoop.conf.Configuration)",344,396,"/**
 * Extracts path data based on a glob pattern.
 * @param pattern glob pattern to match
 * @param conf Hadoop configuration
 * @return Array of PathData objects or null if no match
 */","* Expand the given path as a glob pattern.  Non-existent paths do not
   * throw an exception because creation commands like touch and mkdir need
   * to create them.  The ""stat"" field will be null if the path does not
   * exist.
   * @param pattern the pattern to expand as a glob
   * @param conf the hadoop configuration
   * @return list of {@link PathData} objects.  if the pattern is not a glob,
   * and does not exist, the list will contain a single PathData with a null
   * stat 
   * @throws IOException anything else goes wrong...",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",1894,1932,"/**
 * Constructs a Reader with configuration and options.
 * @param conf Hadoop configuration
 * @param opts Reader options
 * @throws IOException if file or stream option is missing
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,initBloomFilter,"org.apache.hadoop.io.BloomMapFile$Reader:initBloomFilter(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",237,254,"/**
 * Loads a DynamicBloomFilter from a file.
 * @param dirName Directory containing the bloom filter.
 * @param conf Hadoop configuration.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFileDumper.java,dumpInfo,"org.apache.hadoop.io.file.tfile.TFileDumper:dumpInfo(java.lang.String,java.io.PrintStream,org.apache.hadoop.conf.Configuration)",96,295,"/**
* Prints file metadata to the output stream.
* Reads and displays properties of a file.
*/","* Dump information about TFile.
   * 
   * @param file
   *          Path string of the TFile
   * @param out
   *          PrintStream to output the information.
   * @param conf
   *          The configuration object.
   * @throws IOException",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageFile,"org.apache.hadoop.security.Credentials:readTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",225,241,"/**
 * Reads credentials from a token storage file.
 * @param filename Path to the credentials file.
 * @param conf Hadoop configuration.
 * @return Credentials object.
 */","* Convenience method for reading a token storage file and loading its Tokens.
   * @param filename filename.
   * @param conf configuration.
   * @throws IOException  raised on errors performing I/O.
   * @return Credentials.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageFile,"org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials$SerializedFormat)",341,347,"/**
 * Writes data to a file using the provided format and configuration.
 * @param filename Path to the output file.
 * @param conf Configuration settings.
 * @param format Serialized format to use.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.KeyStoreProvider:initFileSystem(java.net.URI),81,85,"/**
 * Calls super.m1, then initializes 'fs' using m2, m3, and m4.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,<init>,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",129,138,"/**
 * Initializes the JavaKeyStoreProvider with URI and configuration.
 * @param uri URI of the keystore.
 * @param conf Hadoop configuration.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,getLibJars,org.apache.hadoop.util.GenericOptionsParser:getLibJars(org.apache.hadoop.conf.Configuration),372,389,"/**
* Constructs a classpath URL array from libjars specified in config.
* @param conf Hadoop configuration object
* @return URL array of libjars or null if none are valid.
*/","* If libjars are set in the conf, parse the libjars.
   * @param conf input Configuration.
   * @return libjar urls
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,initFs,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:initFs(),271,304,"/**
* Attempts to create a file system and handles potential errors.
* Returns true on success, throws exception if ignoreError is false.
*/","* Initialize the connection to HDFS and create the base directory. Also
   * launch the flush thread.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLocalFSFileContext,org.apache.hadoop.fs.FileContext:getLocalFSFileContext(),426,429,"/**
 * Returns a FileContext for the local file system.
 * @throws UnsupportedFileSystemException if local FS is unsupported
 */","* @return a FileContext for the local file system using the default config.
   * @throws UnsupportedFileSystemException If the file system for
   *           {@link FsConstants#LOCAL_FS_URI} is not supported.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,<init>,org.apache.hadoop.fs.shell.Display$AvroFileInputStream:<init>(org.apache.hadoop.fs.FileStatus),278,289,"/**
 * Initializes AvroFileInputStream from a FileStatus, reading and encoding.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(),416,419,"/**
 * Creates a FileContext using the default configuration.
 * @return FileContext object.
 * @throws UnsupportedFileSystemException if FS is not supported.
 */","* Create a FileContext using the default config read from the
   * $HADOOP_CONFIG/core.xml, Unspecified key-values for config are defaulted
   * from core-defaults.xml in the release jar.
   * 
   * @throws UnsupportedFileSystemException If the file system from the default
   *           configuration is not supported
   * @return file context.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersion(java.lang.String),616,624,"/**
 * Retrieves a KeyVersion object by versionName.
 * @param versionName The version name to retrieve.
 * @return KeyVersion object.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCurrentKey(java.lang.String),626,634,"/**
 * Retrieves the latest version of a key by name.
 * @param name Key name; retrieves the corresponding KeyVersion.
 * @throws IOException if an I/O error occurs.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeys(),636,644,"/**
 * Retrieves a list of keys names from a REST resource.
 * @return List of keys names as Strings.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeysMetadata,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeysMetadata(java.lang.String[]),675,694,"/**
 * Retrieves metadata for specified key names.
 * @param keyNames Array of key names to fetch metadata for.
 * @return Array of Metadata objects.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKeyInternal,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKeyInternal(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",696,722,"/**
* Creates a key version with specified name, material, and options.
* @param name Key name
* @param material Key material
* @param options Key options
* @return KeyVersion object
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,invalidateCache,org.apache.hadoop.crypto.key.kms.KMSClientProvider:invalidateCache(java.lang.String),741,750,"/**
 * Invalidates cache for a resource by name.
 * @param name The name of the resource to invalidate.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),801,835,"/**
 * Decrypts an encrypted key version and returns a KeyVersion object.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),837,864,"/**
* Re-encrypts an encrypted key version using KMS.
* @param ekv The encrypted key version to re-encrypt.
* @return The new encrypted key version.
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKeys(java.util.List),866,906,"/**
 * Re-encrypts a list of encrypted key versions in batch.
 * @param ekvs List of encrypted key versions to re-encrypt.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersions(java.lang.String),908,923,"/**
* Retrieves key versions by name.
* @param name Key name to fetch versions for.
* @return List of KeyVersion objects or an empty list.
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getMetadata,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getMetadata(java.lang.String),925,933,"/**
 * Retrieves metadata for a given name.
 * @param name The name to retrieve metadata for.
 * @return Metadata object.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,deleteKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:deleteKey(java.lang.String),935,941,"/**
 * Deletes a resource by name using HTTP DELETE.
 * @param name The name of the resource to delete.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getServer,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",380,390,"/**
 * Creates and returns a new RPC server instance.
 * @param protocol RPC protocol class
 * @return RPC server object
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",446,455,"/**
 * Constructs a Server instance with specified configuration parameters.
 */","* Construct an RPC server.
     * 
     * @param protocolClass the class of protocol
     * @param protocolImpl the protocolImpl whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param portRangeConfig A config parameter that can be used to restrict
     * the range of ports used when port is 0 (an ephemeral port)
     * @param alignmentContext provides server state info on client responses
     * @param secretManager input secretManager.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param numReaders input numReaders.
     * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getServer,"org.apache.hadoop.ipc.WritableRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",371,382,"/**
 * Creates and returns a new RPC server instance.
 * @param protocolClass Protocol class for the server.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String)",466,476,"/**
 * Constructs a Server with specified parameters.
 * Delegates to another constructor.
 */","* Construct an RPC server.
     * @param protocolClass - the protocol being registered
     *     can be null for compatibility with old usage (see below for details)
     * @param protocolImpl the protocol impl that will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param secretManager input secretManager.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param portRangeConfig input portRangeConfig.
     * @param numReaders input numReaders.
     *
     * @deprecated use Server#Server(Class, Object,
     *      Configuration, String, int, int, int, int, boolean, SecretManager)
     * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,saslReadAndProcess,org.apache.hadoop.ipc.Server$Connection:saslReadAndProcess(org.apache.hadoop.ipc.RpcWritable$Buffer),2178,2196,"/**
* Processes a SASL message from the buffer.
* Wraps data if configured, otherwise processes normally.
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,<init>,"org.apache.hadoop.security.ssl.SSLFactory:<init>(org.apache.hadoop.security.ssl.SSLFactory$Mode,org.apache.hadoop.conf.Configuration)",136,160,"/**
 * Constructs an SSLFactory with the given mode and configuration.
 */","* Creates an SSLFactory.
   *
   * @param mode SSLFactory mode, client or server.
   * @param conf Hadoop configuration from where the SSLFactory configuration
   * will be read.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshServiceAcl,"org.apache.hadoop.ipc.Server:refreshServiceAcl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",767,769,"/**
 * Delegates service authorization management to the provider.
 * @param conf Configuration object.
 * @param provider PolicyProvider instance.
 */
","* Refresh the service authorization ACL for the service handled by this server.
   *
   * @param conf input Configuration.
   * @param provider input PolicyProvider.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,loopUntilConnected,org.apache.hadoop.ha.HealthMonitor:loopUntilConnected(),161,168,"/**
 * Retries connecting until a proxy is established.
 * Retries with connectRetryInterval, calls m1() on each attempt.
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,transitionToActive,org.apache.hadoop.ha.HAAdmin:transitionToActive(org.apache.commons.cli.CommandLine),155,178,"/**
 * Transitions a service to active state.
 * @param cmd Command line arguments; validates input.
 * @return 0 on success, -1 on failure.
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doFence,org.apache.hadoop.ha.ZKFailoverController:doFence(org.apache.hadoop.ha.HAServiceTarget),543,566,"/**
 * Attempts graceful failover; fences target if unsuccessful.
 * @param target The HAServiceTarget to transition/fence.
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,failover,"org.apache.hadoop.ha.FailoverController:failover(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean,boolean)",198,260,"/**
 * Performs failover from {@code fromSvc} to {@code toSvc}.
 * Fences {@code fromSvc} if possible, makes {@code toSvc} active.
 */","* Failover from service 1 to service 2. If the failover fails
   * then try to failback.
   *
   * @param fromSvc currently active service
   * @param toSvc service to make active
   * @param forceFence to fence fromSvc even if not strictly necessary
   * @param forceActive try to make toSvc active even if it is not ready
   * @throws FailoverFailedException if the failover fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processPath,org.apache.hadoop.fs.shell.Delete$Rm:processPath(org.apache.hadoop.fs.shell.PathData),110,127,"/**
 * Deletes a path if conditions are met, throws exceptions on failure.
 * @param item PathData object representing the path to delete.
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,expandArgument,org.apache.hadoop.fs.shell.CopyCommands$Put:expandArgument(java.lang.String),295,304,"/**
 * Creates PathData objects from a URI or string argument.
 * @param arg URI string or path argument
 * @return List of PathData objects
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Merge:processOptions(java.util.LinkedList),71,89,"/**
 * Processes command arguments, sets delimiters, and initializes paths.
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,expandArgument,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:expandArgument(java.lang.String),357,375,"/**
* Processes an argument, reads stdin or creates PathData.
* @param arg Argument string; may be a URI or file path.
* @return List of PathData objects.
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getLocalDestination,org.apache.hadoop.fs.shell.CommandWithDestination:getLocalDestination(java.util.LinkedList),182,195,"/**
 * Creates a PathData object, handling URI syntax exceptions.
 * @param args List of arguments; used for path determination.
 * @throws IOException if URI syntax is invalid and not on Windows.
 */","*  The last arg is expected to be a local path, if only one argument is
   *  given then the destination will be the current directory 
   *  @param args is the list of arguments
   * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",2530,2533,"/**
* Copies files from multiple source paths to a destination path.
* @param srcs array of source paths
* @param dst destination path
*/
","* The src files is on the local disk.  Add it to filesystem at
   * the given dst name, removing the source afterwards.
   * @param srcs source paths
   * @param dst path
   * @throws IOException IO failure",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",360,365,"/**
* Delegates file system operation to the underlying file system.
* @param delSrc Whether to delete source files.
* @param overwrite Whether to overwrite existing files.
* @param srcs Source file paths.
* @param dst Destination path.
*/","* The src files are on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2556,2559,"/**
* Calls m1 with default copy-attributes.
* @param delSrc Whether to delete the source after copy.
* @param src Source path.
* @param dst Destination path.
*/","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param src path
   * @param dst path
   * @throws IOException IO failure.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",372,377,"/**
 * Delegates file operation to the underlying filesystem.
 * @param delSrc Delete source file after copying.
 * @param overwrite Overwrite destination if it exists.
 * @param src Source path.
 * @param dst Destination path.
 */","* The src file is on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2624,2627,"/**
* Calls m1 with the additional parameter set to false.
* @param delSrc Whether to delete the source after copying.
* @param src Source path.
* @param dst Destination path.
*/","* Copy it a file from a remote filesystem to the local one.
   * delSrc indicates if the src will be removed or not.
   * @param delSrc whether to delete the src
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)",394,492,"/**
* Finds a local directory for a file, considering size and write check.
* @param pathStr file path, @param size file size, @param conf Configuration
* @param checkWrite whether to check write access
* @return Path object representing the chosen directory
*/","Get a path from the local FS. If size is known, we go
     *  round-robin over the set of disks (via the configured dirs) and return
     *  the first complete path which has enough space.
     *  
     *  If size is not known, use roulette selection -- pick directories
     *  with probability proportional to their available space.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathToRead,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",518,539,"/**
* Finds a file path within configured local directories.
* @param pathStr The file path to search for.
* @param conf Hadoop configuration.
* @return Path object if found, otherwise throws exception.
*/","Get a path from the local FS for reading. We search through all the
     *  configured dirs for the file's existence and return the complete
     *  path to the file when we find one",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAllLocalPathsToRead,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",603,610,"/**
 * Creates an iterable of paths from a string path and configuration.
 * @param pathStr Path string.
 * @param conf Hadoop configuration.
 * @return Iterable of Path objects.
 */
","* Get all of the paths that currently exist in the working directories.
     * @param pathStr the path underneath the roots
     * @param conf the configuration to look up the roots in
     * @return all of the paths that exist under any of the roots
     * @throws IOException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet,org.apache.hadoop.fs.viewfs.FsGetter)",228,275,"/**
 * Initializes the NflyFSystem with provided URIs, config, and replication factors.
 * @param uris URIs for the file system destinations
 * @param conf Hadoop configuration
 * @param minReplication Minimum replication factor
 * @param nflyFlags Nfly key flags
 * @param fsGetter FileSystem getter
 * @throws IOException if minimum replication is not met
 */
","* Creates a new Nfly instance.
   *
   * @param uris the list of uris in the mount point
   * @param conf configuration object
   * @param minReplication minimum copies to commit a write op
   * @param nflyFlags modes such readMostRecent
   * @param fsGetter to get the file system instance with the given uri
   * @throws IOException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,runAll,org.apache.hadoop.fs.shell.Command:runAll(),128,142,"/**
 * Processes input paths, returns -1 if an error occurs.
 * Iterates through 'args', processes each path using m3/m4.
 */","* For each source path, execute the command
   * 
   * @return 0 if it runs successfully; -1 if it fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,expandArgument,org.apache.hadoop.fs.shell.Command:expandArgument(java.lang.String),264,271,"/**
 * Retrieves a list of PathData objects for the given argument.
 * @param arg argument to retrieve PathData for
 * @return List of PathData objects
 */
","* Expand the given argument into a list of {@link PathData} objects.
   * The default behavior is to expand globs.  Commands may override to
   * perform other expansions on an argument.
   * @param arg string pattern to expand
   * @return list of {@link PathData} objects
   * @throws IOException if anything goes wrong...",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getRemoteDestination,org.apache.hadoop.fs.shell.CommandWithDestination:getRemoteDestination(java.util.LinkedList),203,221,"/**
 * Parses path arguments and sets the destination PathData.
 * @param args LinkedList of arguments; determines path resolution.
 * @throws IOException if path resolution fails.
 */
","*  The last arg is expected to be a remote path, if only one argument is
   *  given then the destination will be the remote user's directory 
   *  @param args is the list of arguments
   *  @throws PathIOException if path doesn't exist or matches too many times",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",1942,1946,"/**
 * Constructs a Reader with a qualified file path.
 * @param fs Filesystem.
 * @param file Path to the file.
 * @param conf Configuration object.
 * @throws IOException if an I/O error occurs.
 */
","* Construct a reader by opening a file from the given file system.
     * @param fs The file system used to open the file.
     * @param file The file being read.
     * @param conf Configuration
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Reader(Configuration, Option...) instead.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,int,long,long,org.apache.hadoop.conf.Configuration)",1958,1962,"/**
 * Constructs a Reader with configured stream, start, and length.
 */","* Construct a reader by the given input stream.
     * @param in An input stream.
     * @param buffersize unused
     * @param start The starting position.
     * @param length The length being read.
     * @param conf Configuration
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Reader(Configuration, Reader.Option...) instead.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,createDataFileReader,"org.apache.hadoop.io.MapFile$Reader:createDataFileReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",568,575,"/**
 * Creates a SequenceFile reader with specified options.
 * @param dataFile Path to the SequenceFile.
 * @param conf Hadoop configuration.
 * @param options Reader options.
 * @return SequenceFile.Reader instance.
 */
","* Override this method to specialize the type of
     * {@link SequenceFile.Reader} returned.
     *
     * @param dataFile data file.
     * @param conf configuration.
     * @param options options.
     * @throws IOException raised on errors performing I/O.
     * @return SequenceFile.Reader.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawKey,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawKey(),3832,3857,"/**
* Reads a key from the input stream.
* Returns true if successful, false otherwise.
*/","* Fills up the rawKey object with the key returned by the Reader.
       * @return true if there is a key returned; false, otherwise
       * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1071,1195,"/**
 * Creates a Writer with provided Configuration and options.
 * @param conf Hadoop configuration object.
 * @param opts Writer options.
 * @throws IOException if an I/O error occurs.
 */
","* Construct a uncompressed writer from a set of options.
     * @param conf the configuration to use
     * @param opts the options used when creating the writer
     * @throws IOException if it fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,main,org.apache.hadoop.io.file.tfile.TFile:main(java.lang.String[]),2348,2366,"/**
 * Dumps TFile(s) to the console, using specified configuration.
 * @param args Command-line arguments: TFile paths.
 */","* Dumping the TFile information.
   * 
   * @param args
   *          A list of TFile paths.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageFile,"org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",335,339,"/**
* Calls m1 with SerializedFormat.WRITABLE as the default format.
* @param filename Path to the file.
* @param conf Configuration object.
*/
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,doFormattedWrite,"org.apache.hadoop.security.token.DtFileOperations:doFormattedWrite(java.io.File,java.lang.String,org.apache.hadoop.security.Credentials,org.apache.hadoop.conf.Configuration)",104,114,"/**
 * Saves credentials to a file in specified format.
 * @param f file to save to, format is the serialization format
 */
","Write out a Credentials object as a local file.
   *  @param f a local File object.
   *  @param format a string equal to FORMAT_PB or FORMAT_JAVA.
   *  @param creds the Credentials object to be written out.
   *  @param conf a Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,rollLogDirIfNeeded,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDirIfNeeded(),504,542,"/**
 * Flushes metrics data, creates new log file if needed.
 * Updates current log file based on flush schedule.
 */","* Check the current directory against the time stamp.  If they're not
   * the same, create a new directory and a new log file in that directory.
   *
   * @throws MetricsException thrown if an error occurs while creating the
   * new directory or new log file",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,getJarsInDirectory,"org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String,boolean)",1777,1796,"/**
* Lists files matching a glob pattern, locally or remotely.
* @param path glob path to search.
* @param useLocal if true, searches locally.
* @return List of Path objects found.
*/","* Returns all jars that are in the directory. It is useful in expanding a
   * wildcard path to return all jars from the directory to use in a classpath.
   *
   * @param path the path to the directory. The path may include the wildcard.
   * @param useLocal use local.
   * @return the list of jars as URLs, or an empty list if there are no jars, or
   * the directory does not exist",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,getInputStream,org.apache.hadoop.fs.shell.Display$Text:getInputStream(org.apache.hadoop.fs.shell.PathData),123,173,"/**
 * Reads input stream, handles compression, and returns a stream.
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",724,728,"/**
* Retrieves a KeyVersion by name and options.
* @param name Key name. @param options Key options.
* @return KeyVersion object.
*/
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",730,739,"/**
 * Creates a KeyVersion.
 * @param name Key name.
 * @param material Material data.
 * @param options Options for KeyVersion creation.
 * @return KeyVersion object.
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersionInternal,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersionInternal(java.lang.String,byte[])",752,768,"/**
* Creates a KeyVersion resource.
* @param name Key name
* @param material Material data (base64 encoded)
* @return KeyVersion object
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,invalidateCache,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:invalidateCache(java.lang.String),319,324,"/**
* Calls m1 on each KMSClientProvider for the given key name.
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getServer,"org.apache.hadoop.ipc.ProtobufRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",368,378,"/**
 * Creates a new RPC server with the provided configuration.
 * @param protocol RPC protocol class
 * @return RPC server instance
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)",413,418,"/**
 * Constructs a Server with default settings.
 * @param protocolClass Protocol class.
 * @param protocolImpl Protocol implementation.
 */
","Construct an RPC server.
     * @param protocolClass class
     * @param protocolImpl the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager)",436,445,"/**
 * Constructs a Server with specified parameters.
 * @param protocolImpl Protocol implementation
 * @param conf Configuration object
 */
","* Construct an RPC server.
     * @param protocolImpl the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param numReaders input numberReaders.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param secretManager input secretManager.
     * 
     * @deprecated use Server#Server(Class, Object, 
     *      Configuration, String, int, int, int, int, boolean, SecretManager)
     * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processRpcOutOfBandRequest,"org.apache.hadoop.ipc.Server$Connection:processRpcOutOfBandRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)",2984,3012,"/**
 * Processes out-of-band RPC headers based on call ID.
 * @param header RPC request header
 * @param buffer Data buffer for processing
 */","* Establish RPC connection setup by negotiating SASL if required, then
     * reading and authorizing the connection header
     * @param header - RPC header
     * @param buffer - stream to request payload
     * @throws RpcServerException - setup failed due to SASL
     *         negotiation failure, premature or invalid connection context,
     *         or other state errors. This exception needs to be sent to the 
     *         client.
     * @throws IOException - failed to send a response back to the client
     * @throws InterruptedException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,connect,org.apache.hadoop.log.LogLevel$CLI:connect(java.net.URL),260,284,"/**
 * Opens a URL connection with authentication and SSL if HTTPS.
 * @param url The URL to open.
 * @return URLConnection object.
 */","* Connect to the URL. Supports HTTP/HTTPS and supports SPNEGO
     * authentication. It falls back to simple authentication if it fails to
     * initiate SPNEGO.
     *
     * @param url the URL address of the daemon servlet
     * @return a connected connection
     * @throws Exception if it can not establish a connection.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",378,428,"/**
 * Creates a KMS client provider with given URI and configuration.
 * @param uri KMS endpoint URI
 * @param conf Configuration object
 * @throws IOException if an I/O error occurs
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,runCmd,org.apache.hadoop.ha.HAAdmin:runCmd(java.lang.String[]),391,441,"/**
* Processes command-line arguments and dispatches to appropriate handlers.
* @param argv command-line arguments
* @return int result code or -1 on failure
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,fenceOldActive,org.apache.hadoop.ha.ZKFailoverController:fenceOldActive(byte[]),532,541,"/**
 * Fences an active target, handling exceptions and recording failures.
 * @param data Input data used to create the target.
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2518,2521,"/**
* Calls m1 with copy-only mode false, moving src to dst.
* @param src Source path
* @param dst Destination path
*/
","* The src file is on the local disk.  Add it to filesystem at
   * the given dst name and the source is kept intact afterwards
   * @param src path
   * @param dst path
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2542,2545,"/**
* Copies a file from src to dst using m1.
* @param src Source file path.
* @param dst Destination file path.
*/
","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name, removing the source afterwards.
   * @param src local path
   * @param dst path
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",349,353,"/**
* Delegates m1 operation to the underlying file system.
* @param delSrc If true, deletes the source path after copy.
* @param src Source path.
* @param dst Destination path.
*/","* The src file is on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2601,2603,"/**
* Calls m1 with copy-only set to false.
* @param src Source path.
* @param dst Destination path.
*/","* Copy it a file from the remote filesystem to the local one.
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveToLocalFile,"org.apache.hadoop.fs.FileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2612,2614,"/**
* Copies a file from src to dst.
* @param src Source file path.
* @param dst Destination file path.
*/
","* Copy a file to the local filesystem, then delete it from the
   * remote filesystem (if successfully copied).
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",384,388,"/**
* Delegates method execution to the underlying filesystem.
* @param delSrc Whether to delete the source after copying.
* @param src Source path.
* @param dst Destination path.
*/","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.
   * delSrc indicates if the src will be removed or not.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)",162,167,"/**
 * Delegates path creation to AllocatorPerContext.
 * @param pathStr Path string. @param size Size. @param conf Configuration.
 * @param checkWrite Whether to check write access.
 */","Get a path from the local FS. Pass size as 
   *  SIZE_UNKNOWN if not known apriori. We
   *  round-robin over the set of disks (via the configured dirs) and return
   *  the first complete path which has enough space 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @param checkWrite ensure that the path is writable
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createTmpFileForWrite,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",500,512,"/**
 * Creates a file with the given prefix in the specified directory.
 * @param pathStr File path string.
 * @param size File size.
 * @param conf Configuration object.
 * @return The created File object.
 */
","Creates a file on the local FS. Pass size as 
     * {@link LocalDirAllocator.SIZE_UNKNOWN} if not known apriori. We
     *  round-robin over the set of disks (via the configured dirs) and return
     *  a file on the first path which has enough space. The file is guaranteed
     *  to go away when the JVM exits.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathToRead,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",177,181,"/**
* Delegates path resolution to AllocatorPerContext.
* @param pathStr Path string to resolve.
* @param conf Configuration object.
* @return Resolved Path object.
*/","Get a path from the local FS for reading. We search through all the
   *  configured dirs for the file's existence and return the complete
   *  path to the file when we find one 
   *  @param pathStr the requested file (this will be searched)
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAllLocalPathsToRead,"org.apache.hadoop.fs.LocalDirAllocator:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",190,198,"/**
 * Retrieves a list of paths based on the provided path string and config.
 * @param pathStr Path string to search.
 * @param conf Configuration object.
 * @return Iterable of Path objects.
 */
","* Get all of the paths that currently exist in the working directories.
   * @param pathStr the path underneath the roots
   * @param conf the configuration to look up the roots in
   * @return all of the paths that exist under any of the roots
   * @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet)",213,216,"/**
 * Constructs NflyFSystem with URI, config, minReplication, and flags.
 * Delegates to the primary constructor.
 */
","* Creates a new Nfly instance.
   *
   * @param uris the list of uris in the mount point
   * @param conf configuration object
   * @param minReplication minimum copies to commit a write op
   * @param nflyFlags modes such readMostRecent
   * @throws IOException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,createFileSystem,"org.apache.hadoop.fs.viewfs.NflyFSystem:createFileSystem(java.net.URI[],org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.viewfs.FsGetter)",944,971,"/**
 * Creates an NflyFSystem with specified URIs, config, minRepl, and flags.
 * @param uris URIs for the filesystem
 * @param conf Hadoop configuration
 * @return NflyFSystem object
 */","* Initializes an nfly mountpoint in viewfs.
   *
   * @param uris destinations to replicate writes to
   * @param conf file system configuration
   * @param settings comma-separated list of k=v pairs.
   * @return an Nfly filesystem
   * @throws IOException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,expandArguments,org.apache.hadoop.fs.shell.Command:expandArguments(java.util.LinkedList),243,254,"/**
 * Expands string arguments into PathData objects.
 * @param args List of string arguments to expand.
 * @return LinkedList of PathData objects.
 */
","*  Expands a list of arguments into {@link PathData} objects.  The default
   *  behavior is to call {@link #expandArgument(String)} on each element
   *  which by default globs the argument.  The loop catches IOExceptions,
   *  increments the error count, and displays the exception.
   * @param args strings to expand into {@link PathData} objects
   * @return list of all {@link PathData} objects the arguments
   * @throws IOException if anything goes wrong...",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,open,"org.apache.hadoop.io.MapFile$Reader:open(org.apache.hadoop.fs.Path,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",532,556,"/**
* Initializes reader with data and index files.
* @param dir directory containing files; comparator, conf, options
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,adjustPriorityQueue,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:adjustPriorityQueue(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor),3598,3609,"/**
 * Processes a segment descriptor.
 * @param ms The segment descriptor to process.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1609,1620,"/**
 * Initializes a BlockCompressWriter with configuration and options.
 * @param conf Hadoop configuration object
 * @param options Compression options
 * @throws IOException if an I/O error occurs
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$RecordCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1539,1542,"/**
 * Initializes a RecordCompressWriter with a configuration and options.
 */",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,getTokenFile,"org.apache.hadoop.security.token.DtFileOperations:getTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)",175,220,"/**
 * Fetches a token, potentially aliased, using DtFetcher implementations.
 * @param tokenFile File containing credentials.
 */","Fetch a token from a service and save to file in the local filesystem.
   *  @param tokenFile a local File object to hold the output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias overwrite service field of fetched token with this text.
   *  @param service use a DtFetcher implementation matching this service text.
   *  @param url pass this URL to fetcher after stripping any http/s prefix.
   *  @param renewer pass this renewer to the fetcher.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,aliasTokenFile,"org.apache.hadoop.security.token.DtFileOperations:aliasTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",230,243,"/**
* Processes token file, aliases tokens for a service, and saves credentials.
*/","Alias a token from a file and save back to file in the local filesystem.
   *  @param tokenFile a local File object to hold the input and output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias overwrite service field of fetched token with this text.
   *  @param service only apply alias to tokens matching this service text.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,appendTokenFiles,"org.apache.hadoop.security.token.DtFileOperations:appendTokenFiles(java.util.ArrayList,java.lang.String,org.apache.hadoop.conf.Configuration)",251,264,"/**
 * Merges tokens from multiple files into a single Credentials object.
 * @param tokenFiles List of token files to merge.
 * @param fileFormat File format string.
 * @param conf Configuration object.
 */","Append tokens from list of files in local filesystem, saving to last file.
   *  @param tokenFiles list of local File objects.  Last file holds the output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,removeTokenFromFile,"org.apache.hadoop.security.token.DtFileOperations:removeTokenFromFile(boolean,java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",275,291,"/**
 * Processes tokens, cancels if requested, and saves new credentials.
 * @param cancel Cancel token processing.
 */","Remove a token from a file in the local filesystem, matching alias.
   *  @param cancel cancel token as well as remove from file.
   *  @param tokenFile a local File object.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias remove only tokens matching alias; null matches all.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.
   *  @throws InterruptedException if the thread is interrupted.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,renewTokenFile,"org.apache.hadoop.security.token.DtFileOperations:renewTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",301,313,"/**
 * Renews tokens from a file, checks aliases, and saves updated credentials.
 */","Renew a token from a file in the local filesystem, matching alias.
   *  @param tokenFile a local File object.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias renew only tokens matching alias; null matches all.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.
   *  @throws InterruptedException if the thread is interrupted.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,importTokenFile,"org.apache.hadoop.security.token.DtFileOperations:importTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,java.lang.String,org.apache.hadoop.conf.Configuration)",323,339,"/**
 * Adds a token to credentials and saves it to a file.
 * @param tokenFile File to store token.
 * @param fileFormat File format.
 * @param alias Token alias.
 * @param base64 Base64 encoded token.
 * @param conf Hadoop configuration.
 */","Import a token from a base64 encoding into the local filesystem.
   * @param tokenFile A local File object.
   * @param fileFormat A string equal to FORMAT_PB or FORMAT_JAVA, for output.
   * @param alias overwrite Service field of fetched token with this text.
   * @param base64 urlString Encoding of the token to import.
   * @param conf Configuration object passed along.
   * @throws IOException Error to import the token into the file.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,putMetrics,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),822,861,"/**
 * Writes metrics record to output stream, handling errors.
 * @param record The metrics record to write.
 */",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,getJarsInDirectory,org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String),1764,1766,"/**
 * Retrieves a list of paths from the given path string.
 * @param path The path string to process.
 * @return A list of Path objects.
 */
","* Returns all jars that are in the directory. It is useful in expanding a
   * wildcard path to return all jars from the directory to use in a classpath.
   * It operates only on local paths.
   *
   * @param path the path to the directory. The path may include the wildcard.
   * @return the list of jars as URLs, or an empty list if there are no jars, or
   * the directory does not exist locally",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,expandWildcard,"org.apache.hadoop.util.GenericOptionsParser:expandWildcard(java.util.List,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)",495,512,"/**
* Collects JAR paths from a directory.
* @param finalPaths List to store JAR paths.
* @param path Path of the directory to scan.
* @param fs FileSystem object.
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String),771,775,"/**
* Retrieves a KeyVersion by name.
* @param name The name of the KeyVersion to retrieve.
* @return A KeyVersion object.
*/
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String,byte[])",777,786,"/**
 * Creates a KeyVersion object.
 * @param name Key name.
 * @param material Key material.
 * @return KeyVersion object.
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,deleteKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:deleteKey(java.lang.String),497,507,"/**
 * Executes a provider function with the given name.
 * @param name The name to pass to the provider function.
 */",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String,byte[])",509,520,"/**
 * Creates a key version with the given name and material.
 * @param name Key version name.
 * @param material Key material.
 * @return The created KeyVersion object.
 */",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String),522,541,"/**
 * Retrieves a KeyVersion by name.
 * @param name Key version name.
 * @return KeyVersion object.
 * @throws NoSuchAlgorithmException, IOException
 */",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)",398,402,"/**
 * Constructor for Server (deprecated). Delegates to another constructor.
 */
","* Construct an RPC server.
     * @param instance the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * 
     * @deprecated Use #Server(Class, Object, Configuration, String, int)
     * @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processOneRpc,org.apache.hadoop.ipc.Server$Connection:processOneRpc(java.nio.ByteBuffer),2785,2823,"/**
 * Processes an RPC request from a ByteBuffer, handling errors and retries.
 */","* Process one RPC Request from buffer read from socket stream 
     *  - decode rpc in a rpc-Call
     *  - handle out-of-band RPC requests such as the initial connectionContext
     *  - A successfully decoded RpcCall will be deposited in RPC-Q and
     *    its response will be sent later when the request is processed.
     * 
     * Prior to this call the connectionHeader (""hrpc..."") has been handled and
     * if SASL then SASL has been established and the buf we are passed
     * has been unwrapped from SASL.
     * 
     * @param bb - contains the RPC request header and the rpc request
     * @throws IOException - internal error that should not be returned to
     *         client, typically failure to respond to client
     * @throws InterruptedException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,process,org.apache.hadoop.log.LogLevel$CLI:process(java.lang.String),292,311,"/**
 * Reads and processes data from a URL, extracting lines containing MARKER.
 * @param urlString The URL to connect to and read from.
 */","* Configures the client to send HTTP/HTTPS request to the URL.
     * Supports SPENGO for authentication.
     * @param urlString URL and query string to the daemon's web UI
     * @throws Exception if unable to connect",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createProviders,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProviders(org.apache.hadoop.conf.Configuration,java.net.URL,int,java.lang.String)",311,326,"/**
 * Creates KMSClientProvider array from hostnames.
 * @param conf Configuration object.
 * @param origUrl Original URL.
 * @return KMSClientProvider array.
 */",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,run,org.apache.hadoop.ha.HAAdmin:run(java.lang.String[]),347,361,"/**
 * Delegates to m5, handling IllegalArgumentException and IOException.
 * Returns -1 on error; otherwise returns the value of m5.
 */",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,completeLocalOutput,"org.apache.hadoop.fs.FileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2686,2689,"/**
* Copies a file from a temporary location to the output file system.
* @param fsOutputFile Output file path.
* @param tmpLocalFile Temporary file path.
*/
","* Called when we're all done writing to the target.
   * A local FS will do nothing, because we've written to exactly the
   * right place.
   * A remote FS will copy the contents of tmpLocalFile to the correct target at
   * fsOutputFile.
   * @param fsOutputFile path of output file
   * @param tmpLocalFile path to local tmp file
   * @throws IOException IO failure",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1019,1043,"/**
* Copies a Path to a destination, handling CRC copies as needed.
* @param src Source Path
* @param dst Destination Path
* @param copyCrc Whether to copy CRC files
*/
","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.
   * If src and dst are directories, the copyCrc parameter
   * determines whether to copy CRC files.
   * @param src src path.
   * @param dst dst path.
   * @param copyCrc copy csc flag.
   * @throws IOException if an I/O error occurs.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",145,148,"/**
 * Creates a Path object.
 * @param pathStr Path string.
 * @param size File size.
 * @param conf Hadoop configuration.
 * @return Path object.
 */
","Get a path from the local FS. Pass size as 
   *  SIZE_UNKNOWN if not known apriori. We
   *  round-robin over the set of disks (via the configured dirs) and return
   *  the first complete path which has enough space 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createTmpFileForWrite,"org.apache.hadoop.fs.LocalDirAllocator:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",211,215,"/**
 * Delegates file creation to AllocatorPerContext.
 * @param pathStr File path. @param size File size. @param conf Configuration.
 * @return File object.
 */
","Creates a temporary file in the local FS. Pass size as -1 if not known 
   *  apriori. We round-robin over the set of disks (via the configured dirs) 
   *  and select the first complete path which has enough space. A file is
   *  created on this directory. The file is guaranteed to go away when the
   *  JVM exits.
   *  @param pathStr prefix for the temporary file
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @return a unique temporary file
   *  @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processRawArguments,org.apache.hadoop.fs.shell.Command:processRawArguments(java.util.LinkedList),229,232,"/**
* Processes a list of strings, calls m1, then m2.
* @param args List of strings to be processed.
* @throws IOException if an I/O error occurs.
*/
","* Allows commands that don't use paths to handle the raw arguments.
   * Default behavior is to expand the arguments via
   * {@link #expandArguments(LinkedList)} and pass the resulting list to
   * {@link #processArguments(LinkedList)} 
   * @param args the list of argument strings
   * @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",490,499,"/**
 * Opens a Reader for a directory of SequenceFiles.
 * @param dir Directory containing SequenceFiles.
 * @param conf Hadoop Configuration object.
 * @param opts Reader options.
 */
",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:next(),3565,3591,"/**
* Processes a segment descriptor, updating internal state.
* Returns true on success, false otherwise.
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",274,294,"/**
* Creates a Writer instance with specified compression options.
* @param conf Configuration object.
* @param opts Writer options.
* @return Writer instance.
*/","* Create a new Writer with the given options.
   * @param conf the configuration to use
   * @param opts the options to create the file with
   * @return a new Writer
   * @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Get:execute(),241,245,"/**
* Calls DtFileOperations.m2 with provided parameters.
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Edit:execute(),271,277,"/**
 * Processes each token file using DtFileOperations.
 * Iterates through tokenFiles and calls m2 for each.
 */",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Append:execute(),289,292,"/**
* Executes file operations using provided data.
* @throws Exception if an error occurs during processing
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Remove:execute(),320,326,"/**
 * Processes each token file using DtFileOperations.
 * Iterates through tokenFiles and calls m2 for each.
 */",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Renew:execute(),350,355,"/**
 * Processes each token file using DtFileOperations.
 * Iterates through tokenFiles and applies formatting.
 */",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Import:execute(),381,385,"/**
* Calls DtFileOperations.m2 with provided parameters and m1() result.
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,createJarWithClassPath,"org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Map)",1668,1753,"/**
* Creates a classpath JAR with resolved entries, handling wildcards.
* @param inputClassPath classpath string to resolve
* @return Array of classpath JAR path and unexpanded classpath
*/","* Create a jar file at the given path, containing a manifest with a classpath
   * that references all specified entries.
   *
   * Some platforms may have an upper limit on command line length.  For example,
   * the maximum command line length on Windows is 8191 characters, but the
   * length of the classpath may exceed this.  To work around this limitation,
   * use this method to create a small intermediate jar with a manifest that
   * contains the full classpath.  It returns the absolute path to the new jar,
   * which the caller may set as the classpath for a new process.
   *
   * Environment variable evaluation is not supported within a jar manifest, so
   * this method expands environment variables before inserting classpath entries
   * to the manifest.  The method parses environment variables according to
   * platform-specific syntax (%VAR% on Windows, or $VAR otherwise).  On Windows,
   * environment variables are case-insensitive.  For example, %VAR% and %var%
   * evaluate to the same value.
   *
   * Specifying the classpath in a jar manifest does not support wildcards, so
   * this method expands wildcards internally.  Any classpath entry that ends
   * with * is translated to all files at that path with extension .jar or .JAR.
   *
   * @param inputClassPath String input classpath to bundle into the jar manifest
   * @param pwd Path to working directory to save jar
   * @param targetDir path to where the jar execution will have its working dir
   * @param callerEnv Map {@literal <}String, String{@literal >} caller's
   * environment variables to use for expansion
   * @return String[] with absolute path to new jar in position 0 and
   *   unexpanded wild card entry path in position 1
   * @throws IOException if there is an I/O error while writing the jar file",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,constructUrlsFromClasspath,org.apache.hadoop.util.ApplicationClassLoader:constructUrlsFromClasspath(java.lang.String),107,126,"/**
* Converts classpath string to an array of URLs.
* Parses classpath entries, adding file and jar URLs.
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,validateFiles,"org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String,boolean)",425,488,"/**
 * Masks file paths, expands wildcards if enabled, and returns a string.
 * @param files Comma-separated file paths.
 * @param expandWildcard Whether to expand wildcard characters.
 * @return Masked file paths string.
 */","* takes input as a comma separated list of files
   * and verifies if they exist. It defaults for file:///
   * if the files specified do not have a scheme.
   * it returns the paths uri converted defaulting to file:///.
   * So an input of  /home/user/file1,/home/user/file2 would return
   * file:///home/user/file1,file:///home/user/file2.
   *
   * @param files the input files argument
   * @param expandWildcard whether a wildcard entry is allowed and expanded. If
   * true, any directory followed by a wildcard is a valid entry and is replaced
   * with the list of jars in that directory. It is used to support the wildcard
   * notation in a classpath.
   * @return a comma-separated list of validated and qualified paths, or null
   * if the input files argument is null",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,readAndProcess,org.apache.hadoop.ipc.Server$Connection:readAndProcess(),2478,2565,"/**
* Reads and validates RPC headers, allocates data buffer.
* Returns count, or -1 on error.
*/","* This method reads in a non-blocking fashion from the channel: 
     * this method is called repeatedly when data is present in the channel; 
     * when it has enough data to process one rpc it processes that rpc.
     * 
     * On the first pass, it processes the connectionHeader, 
     * connectionContext (an outOfBand RPC) and at most one RPC request that 
     * follows that. On future passes it will process at most one RPC request.
     *  
     * Quirky things: dataLengthBuffer (4 bytes) is used to read ""hrpc"" OR 
     * rpc request length.
     *    
     * @return -1 in case of error, else num bytes read so far
     * @throws IOException - internal error that should not be returned to
     *         client, typically failure to respond to client
     * @throws InterruptedException - if the thread is interrupted.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,unwrapPacketAndProcessRpcs,org.apache.hadoop.ipc.Server$Connection:unwrapPacketAndProcessRpcs(byte[]),2733,2767,"/**
 * Processes input data using SASL unwrap and reads data.
 * @param inBuf byte array containing data to be processed
 */","* Process a wrapped RPC Request - unwrap the SASL packet and process
     * each embedded RPC request 
     * @param inBuf - SASL wrapped request of one or more RPCs
     * @throws IOException - SASL packet cannot be unwrapped
     * @throws InterruptedException",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doGetLevel,org.apache.hadoop.log.LogLevel$CLI:doGetLevel(),236,238,"/**
* Logs the class name via HTTP request to the specified URL.
*/","* Send HTTP/HTTPS request to get log level.
     *
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doSetLevel,org.apache.hadoop.log.LogLevel$CLI:doSetLevel(),246,249,"/**
 * Sends a log level request to the specified URL.
 */","* Send HTTP/HTTPS request to set log level.
     *
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createProvider,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)",279,309,"/**
 * Creates a KeyProvider for a given URI and configuration.
 * @param providerUri URI of the key provider.
 * @param conf Hadoop configuration.
 * @return KeyProvider or null if the scheme doesn't match.
 */","* This provider expects URIs in the following form :
     * {@literal kms://<PROTO>@<AUTHORITY>/<PATH>}
     *
     * where :
     * - PROTO = http or https
     * - AUTHORITY = {@literal <HOSTS>[:<PORT>]}
     * - HOSTS = {@literal <HOSTNAME>[;<HOSTS>]}
     * - HOSTNAME = string
     * - PORT = integer
     *
     * This will always create a {@link LoadBalancingKMSClientProvider}
     * if the uri is correct.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,completeLocalOutput,"org.apache.hadoop.fs.FilterFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",408,412,"/**
* Delegates the call to the underlying file system object.
* @param fsOutputFile Output file path.
* @param tmpLocalFile Temporary file path.
*/
","* Called when we're all done writing to the target.  A local FS will
   * do nothing, because we've written to exactly the right place.  A remote
   * FS will copy the contents of tmpLocalFile to the correct target at
   * fsOutputFile.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,createTmpFileForWrite,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",830,838,"/**
 * Creates a file with a given prefix in the allocated directory.
 * @param pathStr Path string, size, and configuration for file creation.
 * @return The created file object.
 */
","* Demand create the directory allocator, then create a temporary file.
     * This does not mark the file for deletion when a process exits.
     * {@link LocalDirAllocator#createTmpFileForWrite(String, long, Configuration)}.
     *
     * @param pathStr prefix for the temporary file.
     * @param size    the size of the file that is going to be written.
     * @param conf    the Configuration object.
     * @return a unique temporary file.
     * @throws IOException IO problems",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,org.apache.hadoop.conf.Configuration)",129,132,"/**
 * Delegates to overloaded method with default size.
 * @param pathStr Path string.
 * @param conf Configuration object.
 * @return Path object.
 */
","Get a path from the local FS. This method should be used if the size of 
   *  the file is not known apriori. We go round-robin over the set of disks
   *  (via the configured dirs) and return the first complete path where
   *  we could create the parent directory of the passed path. 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,run,org.apache.hadoop.fs.shell.Command:run(java.lang.String[]),184,201,"/**
 * Processes command-line arguments, handles errors, and returns exit code.
 */","* Invokes the command handler.  The default behavior is to process options,
   * expand arguments, and then process each argument.
   * <pre>
   * run
   * |{@literal ->} {@link #processOptions(LinkedList)}
   * \{@literal ->} {@link #processRawArguments(LinkedList)}
   *      |{@literal ->} {@link #expandArguments(LinkedList)}
   *      |   \{@literal ->} {@link #expandArgument(String)}*
   *      \{@literal ->} {@link #processArguments(LinkedList)}
   *          |{@literal ->} {@link #processArgument(PathData)}*
   *          |   |{@literal ->} {@link #processPathArgument(PathData)}
   *          |   \{@literal ->} {@link #processPaths(PathData, PathData...)}
   *          |        \{@literal ->} {@link #processPath(PathData)}*
   *          \{@literal ->} {@link #processNonexistentPath(PathData)}
   * </pre>
   * Most commands will chose to implement just
   * {@link #processOptions(LinkedList)} and {@link #processPath(PathData)}
   * 
   * @param argv the list of command line arguments
   * @return the exit code for the command
   * @throws IllegalArgumentException if called with invalid arguments",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",101,104,"/**
* Constructs a Reader with a FileSystem, file path, and Configuration.
*/
","* Construct an array reader for the named file.
     * @param fs FileSystem.
     * @param file file.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",124,127,"/**
 * Constructs a Reader with a FileSystem, directory name, and comparator.
 */","* Construct a set reader for the named set using the named comparator.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param comparator input comparator.
     * @param conf input Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",510,514,"/**
 * Constructs a Reader with a directory path.
 * @param fs FileSystem object
 * @param dirName Directory name
 * @param conf Configuration object
 * @throws IOException if an I/O error occurs
 */
","* Construct a map reader for the named map.
     * @deprecated
     *
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",526,530,"/**
 * Constructs a Reader with a directory path, config, and comparator.
 * (Deprecated: Use the other constructor instead.)
 */
","* Construct a map reader for the named map using the named comparator.
     * @deprecated
     *
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param comparator WritableComparator.
     * @param conf Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",213,217,"/**
 * Constructs a Reader from a directory, configuration, and options.
 */",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cloneFileAttributes,"org.apache.hadoop.io.SequenceFile$Sorter:cloneFileAttributes(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)",3406,3422,"/**
 * Creates a Writer from input and output paths, using Reader/Writer configs.
 * @param inputFile Input file path.
 * @param outputFile Output file path.
 * @param prog Progressable object for tracking progress.
 * @return Writer object.
 */
","* Clones the attributes (like compression of the input file and creates a 
     * corresponding Writer
     * @param inputFile the path of the input file whose attributes should be 
     * cloned
     * @param outputFile the path of the output file 
     * @param prog the Progressable to report status during the file write
     * @return Writer
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,fix,"org.apache.hadoop.io.MapFile:fix(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.conf.Configuration)",934,1014,"/**
 * Processes a SequenceFile, creating an index if not dryrun.
 * @param fs Filesystem to operate on.
 * @param dir Directory containing the SequenceFile.
 */","* This method attempts to fix a corrupt MapFile by re-creating its index.
   * @param fs filesystem
   * @param dir directory containing the MapFile data and index
   * @param keyClass key class (has to be a subclass of Writable)
   * @param valueClass value class (has to be a subclass of Writable)
   * @param dryrun do not perform any changes, just report what needs to be done
   * @param conf configuration.
   * @return number of valid entries in this MapFile, or -1 if no fixing was needed
   * @throws Exception Exception.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,flush,"org.apache.hadoop.io.SequenceFile$Sorter$SortPass:flush(int,int,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,boolean)",3217,3251,"/**
 * Writes data segments to output files, handling compression and indexing.
 */",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])",312,357,"/**
 * Creates a Writer instance for writing sorted data to a directory.
 * @param conf Hadoop configuration
 * @param dirName Directory to write the data
 * @param opts Writer options
 */",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)",308,315,"/**
 * Delegates to the primary m5 method with provided arguments.
 */","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",330,339,"/**
 * Delegates to the primary m6 method with provided arguments.
 * @param fs FileSystem, configuration, path, key/val classes, compression
 * @return Writer object
 */","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",355,366,"/**
 * Delegates to internal m7 method with provided parameters.
 * @param fs FileSystem, conf Configuration, name Path, keyClass, valClass, compressionType, progress
 * @return Writer object
 */","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param progress The Progressable object to track progress.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",382,392,"/**
 * Creates a Writer instance with specified parameters.
 * @param fs FileSystem, conf Configuration, name Path, etc.
 * @return Writer instance
 */","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",410,423,"/**
 * Delegates to internal m8 method with provided parameters.
 * @param fs FileSystem, conf Configuration, name Path, etc.
 */","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",444,461,"/**
 * Delegates to a more complete m11 method with pre-configured parameters.
 */","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param bufferSize buffer size for the underlaying outputstream.
   * @param replication replication factor for the file.
   * @param blockSize block size for the file.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",539,551,"/**
 * Creates a writer for the given file, using helper methods.
 * @param fs FileSystem, conf Configuration, name Path, classes, codec, progress
 * @return Writer object
 */","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)",567,577,"/**
 * Delegates to the primary m6 method with adjusted parameters.
 * @param conf Configuration object.
 */","* Construct the preferred type of 'raw' SequenceFile Writer.
   * @param conf The configuration.
   * @param out The stream on top which the writer is to be constructed.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",592,600,"/**
 * Delegates to the internal m5 method with prepared arguments.
 * @param conf Configuration object.
 * @return Writer object.
 */","* Construct the preferred type of 'raw' SequenceFile Writer.
   * @param conf The configuration.
   * @param out The stream on top which the writer is to be constructed.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,createJarWithClassPath,"org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,java.util.Map)",1632,1635,"/**
 * Delegates to overloaded method with pwd as the output directory.
 * @param inputClassPath classpath string
 * @param pwd Path object
 * @param callerEnv environment variables
 * @return String array
 */
",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,<init>,"org.apache.hadoop.util.ApplicationClassLoader:<init>(java.lang.String,java.lang.ClassLoader,java.util.List)",102,105,"/**
 * Constructs an ApplicationClassLoader with URLs from classpath.
 * @param classpath Comma-separated classpath string
 * @param parent Parent classloader
 * @param systemClasses System class list
 * @throws MalformedURLException if classpath is invalid
 */
",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,validateFiles,org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String),405,407,"/**
 * Processes files string. Overloads with default flag value.
 * @param files String of files to process.
 * @throws IOException if an I/O error occurs.
 */
","* Takes input as a comma separated list of files
   * and verifies if they exist. It defaults for file:///
   * if the files specified do not have a scheme.
   * it returns the paths uri converted defaulting to file:///.
   * So an input of  /home/user/file1,/home/user/file2 would return
   * file:///home/user/file1,file:///home/user/file2.
   *
   * This method does not recognize wildcards.
   *
   * @param files the input files argument
   * @return a comma-separated list of validated and qualified paths, or null
   * if the input files argument is null",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRead,org.apache.hadoop.ipc.Server$Listener:doRead(java.nio.channels.SelectionKey),1641,1671,"/**
* Processes a connection, handling reads and potential exceptions.
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,sendLogLevelRequest,org.apache.hadoop.log.LogLevel$CLI:sendLogLevelRequest(),126,139,"/**
 * Executes operation (GETLEVEL or SETLEVEL) via m1/m2.
 * Throws exception if operation is invalid.
 */","* Send HTTP/HTTPS request to the daemon.
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,create,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",807,817,"/**
 * Creates a DataBlock on disk.
 * @param index block index, limit block size, statistics upload stats.
 * @return DataBlock object.
 */
","* Create a temp file and a {@link DiskBlock} instance to manage it.
     *
     * @param index      block index.
     * @param limit      limit of the block.
     * @param statistics statistics to update.
     * @return the new block.
     * @throws IOException IO problems",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getTempFilePath,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getTempFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",649,658,"/**
 * Creates a temporary file path for caching.
 * @param conf Hadoop configuration
 * @param localDirAllocator Local directory allocator
 * @return Path object representing the temporary file
 */","* Create temporary file based on the file path retrieved from local dir allocator
   * instance. The file is created with .bin suffix. The created file has been granted
   * posix file permissions available in TEMP_FILE_ATTRS.
   *
   * @param conf the configuration.
   * @param localDirAllocator the local dir allocator instance.
   * @return path of the file created.
   * @throws IOException if IO error occurs while local dir allocator tries to retrieve path
   * from local FS or file creation fails or permission set fails.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,run,org.apache.hadoop.fs.FsShell:run(java.lang.String[]),300,351,"/**
 * Executes a command, traces its execution, and returns its exit code.
 */",* run,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",112,114,"/**
 * Constructs a Reader with a FileSystem, directory name, and configuration.
 */","* Construct a set reader for the named set.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param conf input Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",219,223,"/**
 * Constructs a Reader with a Path, using the provided Configuration.
 * @param fs FileSystem object
 * @param dirName Directory name
 * @param conf Configuration object
 * @throws IOException if an I/O error occurs
 */
",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,boolean)",225,229,"/**
 * Constructs a Reader with a given FileSystem and directory name.
 * @param fs FileSystem object
 * @param dirName Directory name
 */
",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",231,235,"/**
 * Constructs a Reader with a Path, configuration, and comparator.
 * @param fs Filesystem, dirName directory name, comparator comparator
 */
",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:merge(),3623,3723,"/**
 * Merges sorted segments into larger ones until a threshold is met.
 */","This is the single level merge that is called multiple times 
       * depending on the factor size and the number of segments
       * @return RawKeyValueIterator
       * @throws IOException",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,run,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:run(boolean),3100,3179,"/**
 * Processes input files, extracts data, and handles compression.
 * Returns the number of processed segments.
 */",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.io.SequenceFile$CompressionType)",82,89,"/**
 * Constructs a SequenceFile writer with given configuration.
 * @param conf Hadoop configuration, fs FileSystem, dirName directory
 */
","* Create a set naming the element comparator and compression type.
     *
     * @param conf input Configuration.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param comparator input comparator.
     * @param compress input compress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])",158,164,"/**
 * Constructs a Writer with given configuration, directory, and options.
 */
",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)",50,55,"/**
 * Constructs a Writer with given config, filesystem, file, and value class.
 */
","* Create the named file for values of the named class.
     *
     * @param conf configuration.
     * @param fs file system.
     * @param file file.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",68,77,"/**
 * Constructs a Writer with given configuration, file system, and classes.
 * @param file Output file path.
 * @param valClass Writable value class.
 */
","* Create the named file for values of the named class.
     *
     * @param conf configuration.
     * @param fs file system.
     * @param file file.
     * @param valClass valClass.
     * @param compress compress.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)",112,117,"/**
 * Constructs a Writer with a directory path.
 * @param conf Configuration object.
 * @param fs FileSystem object.
 * @param dirName Directory name.
 * @param keyClass Key class.
 * @param valClass Value class.
 */
","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",132,139,"/**
 * Constructs a Writer with specified configuration, FS, dir, classes,
 * compression, and progress. Delegates to another constructor.
 */
","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",155,162,"/**
 * Constructs a Writer with specified configuration and parameters.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Output directory name
 * @param keyClass Class of the key
 * @param valClass Class of the value
 * @param compress Compression type
 * @param codec Compression codec
 * @param progress Progressable object
 * @throws IOException if an I/O error occurs
 */
","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @param codec codec.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",175,181,"/**
 * Constructs a Writer with a directory path.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Directory name
 * @param keyClass Key class
 * @param valClass Value class
 * @param compress Compression type
 */
","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)",192,198,"/**
 * Constructs a Writer with a directory path.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Directory name
 * @param comparator Comparator for writable values
 * @param valClass Class of writable values
 */
","Create the named map using the named key comparator. 
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",210,216,"/**
 * Constructs a Writer with specified configuration and compression.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Output directory name
 * @param comparator Comparator for writable values
 * @param compress Compression type
 */","Create the named map using the named key comparator.
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress compress.
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",231,239,"/**
 * Constructs a Writer with specified parameters.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Output directory name
 * @param comparator Comparator for writable values
 * @param valClass Class of the writable values
 * @param compress Compression type
 * @param progress Progress reporter
 */
","* Create the named map using the named key comparator.
     * @deprecated Use Writer(Configuration, Path, Option...)} instead.
     *
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress CompressionType.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",255,263,"/**
 * Constructs a Writer with specified configuration and parameters.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Output directory name
 */
","* Create the named map using the named key comparator.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress CompressionType.
     * @param codec codec.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,open,"org.apache.hadoop.io.MapFile$Merger:open(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",1059,1090,"/**
 * Initializes readers and checks key/value class consistency.
 * @param inMapFiles Input map files.
 * @param outMapFile Output map file.
 */",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",513,522,"/**
 * Creates a Writer. Delegates to m2, passing CreateResult.
 * @param fc FileContext
 * @return Writer object
 */","* Construct the preferred type of SequenceFile Writer.
   * @param fc The context for the specified file.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @param createFlag gives the semantics of create: overwrite, append etc.
   * @param opts file creation options; see {@link CreateOpts}.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Classpath.java,main,org.apache.hadoop.util.Classpath:main(java.lang.String[]),64,113,"/**
 * Processes command-line arguments and performs actions based on flags.
 */","* Main entry point.
   *
   * @param args command-line arguments",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,createClassLoader,"org.apache.hadoop.util.RunJar:createClassLoader(java.io.File,java.io.File)",344,383,"/**
 * Creates a ClassLoader, either ApplicationClassLoader or URLClassLoader.
 * @param file The file to be loaded.
 * @param workDir The working directory.
 * @return A ClassLoader instance.
 */","* Creates a classloader based on the environment that was specified by the
   * user. If HADOOP_USE_CLIENT_CLASSLOADER is specified, it creates an
   * application classloader that provides the isolation of the user class space
   * from the hadoop classes and their dependencies. It forms a class space for
   * the user jar as well as the HADOOP_CLASSPATH. Otherwise, it creates a
   * classloader that simply adds the user jar to the classpath.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,processGeneralOptions,org.apache.hadoop.util.GenericOptionsParser:processGeneralOptions(org.apache.commons.cli.CommandLine),291,364,"/**
 * Processes command-line arguments and configures the Hadoop configuration.
 * @param line CommandLine object containing command-line options.
 */","* Modify configuration according user-specified generic options.
   *
   * @param line User-specified generic options",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRunLoop,org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop(),1486,1527,"/**
 * Processes pending connections and reads from selection keys.
 */",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,run,org.apache.hadoop.log.LogLevel$CLI:run(java.lang.String[]),109,119,"/**
 * Executes m2 and m3, handles exceptions, and returns 0 or -1.
 */",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getCacheFilePath,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getCacheFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",496,500,"/**
* Delegates path creation to m1.
* @param conf Hadoop configuration.
* @param localDirAllocator Local directory allocator.
* @return Path object.
*/
","* Return temporary file created based on the file path retrieved from local dir allocator.
   *
   * @param conf The configuration object.
   * @param localDirAllocator Local dir allocator instance.
   * @return Path of the temporary file created.
   * @throws IOException if IO error occurs while local dir allocator tries to retrieve path
   * from local FS or file creation fails or permission set fails.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,isCacheSpaceAvailable,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:isCacheSpaceAvailable(long,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",621,633,"/**
 * Checks if enough cache space is available for a file.
 * @param fileSize Size of the file to be cached.
 * @param conf Configuration object.
 * @param localDirAllocator Local directory allocator.
 * @return True if cache space is available, false otherwise.
 */
","* Determine if the cache space is available on the local FS.
   *
   * @param fileSize The size of the file.
   * @param conf The configuration.
   * @param localDirAllocator Local dir allocator instance.
   * @return True if the given file size is less than the available free space on local FS,
   * False otherwise.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(java.util.List,org.apache.hadoop.fs.Path)",3313,3319,"/**
 * Creates and returns a RawKeyValueIterator for merging segments.
 * @param segments List of segment descriptors to merge.
 * @param tmpDir Temporary directory for merging.
 * @return RawKeyValueIterator for the merged data.
 */
","* Merges the list of segments of type <code>SegmentDescriptor</code>
     * @param segments the list of SegmentDescriptors
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIterator
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,int,org.apache.hadoop.fs.Path)",3349,3364,"/**
 * Creates a MergeQueue with segment descriptors from input paths.
 * @param inNames Input paths to merge.
 * @param deleteInputs Whether to keep input paths.
 * @return MergeQueue object for merging.
 */","* Merges the contents of files passed in Path[]
     * @param inNames the array of path names
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @param factor the factor that will be used as the maximum merge fan-in
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3375,3394,"/**
 * Merges input paths into a single file.
 * @param inNames Input paths to merge.
 * @param tempDir Temporary directory for merging.
 * @param deleteInputs Whether to delete input paths after merge.
 * @return Iterator for the merged key-value pairs.
 */","* Merges the contents of files passed in Path[]
     * @param inNames the array of path names
     * @param tempDir the directory for creating temp files during merge
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",3479,3489,"/**
 * Creates and returns a RawKeyValueIterator for merging.
 * @param inName Input path
 * @param indexIn Index path
 * @param tmpDir Temporary directory
 * @return RawKeyValueIterator
 */
","Used by mergePass to merge the output of the sort
     * @param inName the name of the input file containing sorted segments
     * @param indexIn the offsets of the sorted segments
     * @param tmpDir the relative directory to store intermediate results in
     * @return RawKeyValueIterator
     * @throws IOException",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sortPass,org.apache.hadoop.io.SequenceFile$Sorter:sortPass(boolean),3064,3076,"/**
 * Performs a sort pass, returns status.
 * @param deleteInput flag to delete input files
 * @return Status code of the sort pass
 */",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",65,70,"/**
 * Creates a Writer with a key class.
 * @param conf Configuration object.
 * @param fs FileSystem object.
 * @param dirName Directory name.
 * @param keyClass Key class.
 * @param compress Compression type.
 */
","* Create a set naming the element class and compression type.
     *
     * @param conf input Configuration.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param keyClass input keyClass.
     * @param compress input compress.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",90,97,"/**
 * Constructs a Writer with specified configuration and parameters.
 * @param conf Hadoop configuration, FS, dir, key/val classes, codec, progress.
 */",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",99,106,"/**
 * Constructs a Writer with specified configuration, file system,
 * directory, key/value classes, compression, and progress.
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",108,115,"/**
 * Constructs a Writer with a directory path.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Directory name
 * @param keyClass Key class
 * @param valClass Value class
 * @param compress Compression type
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",117,125,"/**
 * Constructs a Writer with pre-configured parameters.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Output directory name
 */",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",127,134,"/**
 * Constructs a Writer with specified configuration and parameters.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Output directory name
 * @param comparator Comparator for writable values
 * @param valClass Class of writable values
 * @param compress Compression type
 * @param progress Progressable object
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",136,142,"/**
 * Constructs a Writer with specified configuration and parameters.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Directory name
 * @param comparator Comparator for writable values
 * @param compress Compression type
 * @throws IOException if an I/O error occurs
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)",144,149,"/**
 * Deprecated constructor. Creates a Writer with given config, FS, dir, comparator, and value class.
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)",151,156,"/**
 * Constructs a Writer with a directory path.
 * @param conf Configuration object.
 * @param fs FileSystem object.
 * @param dirName Directory name.
 * @param keyClass Key class.
 * @param valClass Value class.
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,main,org.apache.hadoop.io.MapFile:main(java.lang.String[]),1160,1191,"/**
 * Maps an input file to an output file using a MapFile reader.
 * @param args Command-line arguments: input file, output file.
 */",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)",50,53,"/**
 * Constructs a Writer with a FileSystem, directory name, and key class.
 */","* Create the named set for keys of the named class.
     * @deprecated pass a Configuration too
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param keyClass input keyClass.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,merge,"org.apache.hadoop.io.MapFile$Merger:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)",1039,1053,"/**
 * Processes map files, optionally deletes inputs, and writes output.
 */","* Merge multiple MapFiles to one Mapfile.
     *
     * @param inMapFiles input inMapFiles.
     * @param deleteInputs deleteInputs.
     * @param outMapFile input outMapFile.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,boolean,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)",480,496,"/**
 * Creates a Writer with specified parameters, delegating to m9.
 * @param fs FileSystem, conf Configuration, name Path, etc.
 * @return Writer object
 */","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param bufferSize buffer size for the underlaying outputstream.
   * @param replication replication factor for the file.
   * @param blockSize block size for the file.
   * @param createParent create parent directory if non-existent
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,run,org.apache.hadoop.util.RunJar:run(java.lang.String[]),248,334,"/**
 * Executes a JAR file, running its main class with provided arguments.
 */",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,parseGeneralOptions,"org.apache.hadoop.util.GenericOptionsParser:parseGeneralOptions(org.apache.commons.cli.Options,java.lang.String[])",572,588,"/**
 * Parses command-line options.
 * @param opts Options object to parse.
 * @param args Command-line arguments.
 * @return True if parsing was successful, false otherwise.
 */","* Parse the user-specified options, get the generic options, and modify
   * configuration accordingly.
   *
   * @param opts Options to use for parsing args.
   * @param args User-specified arguments
   * @return true if the parse was successful",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Listener$Reader:run(),1472,1484,"/**
 * Executes a task and ensures readSelector is closed afterward.
 */",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,put,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:put(int,java.nio.ByteBuffer,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",370,413,"/**
 * Writes data to a block, handling locking, validation, and prefetching.
 */","* Puts the given block in this cache.
   *
   * @param blockNumber the block number, used as a key for blocks map.
   * @param buffer buffer contents of the given block to be added to this cache.
   * @param conf the configuration.
   * @param localDirAllocator the local dir allocator instance.
   * @throws IOException if either local dir allocator fails to allocate file or if IO error
   * occurs while writing the buffer content to the file.
   * @throws IllegalArgumentException if buffer is null, or if buffer.limit() is zero or negative.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)",3331,3337,"/**
 * Creates a RawKeyValueIterator. Uses factor if inNames is small.
 * @param inNames Input paths.
 * @param deleteInputs Delete inputs after processing.
 * @param tmpDir Temporary directory.
 * @return RawKeyValueIterator object.
 */
","* Merges the contents of files passed in Path[] using a max factor value
     * that is already set
     * @param inNames the array of path names
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,mergePass,org.apache.hadoop.io.SequenceFile$Sorter:mergePass(org.apache.hadoop.fs.Path),3458,3470,"/**
 * Merges data from a RawKeyValueIterator to a Writer.
 * @param tmpDir Temporary directory for processing.
 * @throws IOException If an I/O error occurs.
 */",sort calls this to generate the final merged output,,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,main,org.apache.hadoop.util.RunJar:main(java.lang.String[]),244,246,"/**
* Executes a JAR file using the RunJar class.
* @param args Command-line arguments passed to RunJar.
*/
","Run a Hadoop job jar.  If the main class is not in the jar's manifest,
   * then it must be provided on the command line.
   *
   * @param args args.
   * @throws Throwable error.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])",178,182,"/**
 * Parses generic options using provided configuration, options, and args.
 */","* Create a <code>GenericOptionsParser</code> to parse given options as well 
   * as generic Hadoop options. 
   * 
   * The resulting <code>CommandLine</code> object can be obtained by 
   * {@link #getCommandLine()}.
   * 
   * @param conf the configuration to modify  
   * @param options options built by the caller 
   * @param args User-specified arguments
   * @throws IOException raised on errors performing I/O.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sortAndIterate,"org.apache.hadoop.io.SequenceFile$Sorter:sortAndIterate(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3032,3052,"/**
 * Creates a RawKeyValueIterator based on input files and segments.
 * @param inFiles Input file paths.
 * @param tempDir Temporary directory.
 * @param deleteInput Whether to delete input files.
 * @return RawKeyValueIterator or null if no segments.
 */","* Perform a file sort from a set of input files and return an iterator.
     * @param inFiles the files to be sorted
     * @param tempDir the directory where temp files are created during sort
     * @param deleteInput should the input files be deleted as they are read?
     * @return iterator the RawKeyValueIterator
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",3445,3455,"/**
 * Processes input files, writes results to output file.
 * @param inFiles Input file paths.
 * @param outFile Output file path.
 */","Merge the provided files.
     * @param inFiles the array of input path names
     * @param outFile the final output file
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,"org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3009,3022,"/**
 * Merges input files into a single output file.
 * @param inFiles array of input file paths
 * @param outFile path to output file
 * @param deleteInput whether to delete input files after merge
 */
","* Perform a file sort from a set of input files into an output file.
     * @param inFiles the files to be sorted
     * @param outFile the sorted output file
     * @param deleteInput should the input files be deleted as they are read?
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,<init>,"org.apache.hadoop.service.launcher.ServiceLauncher$MinimalGenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])",1081,1084,"/**
 * Constructs a MinimalGenericOptionsParser with config, options, and args.
 */",,,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.commons.cli.Options,java.lang.String[])",135,138,"/**
 * Constructs a GenericOptionsParser with a default Configuration.
 * @param opts Options object to parse.
 * @param args Command-line arguments.
 * @throws IOException If an I/O error occurs.
 */
","* Create an options parser with the given options to parse the args.
   * @param opts the options
   * @param args the command line arguments
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,org.apache.hadoop.util.GenericOptionsParser:<init>(java.lang.String[]),145,148,"/**
 * Constructs a GenericOptionsParser with default configuration and options.
 * @param args Command-line arguments to parse.
 * @throws IOException If an I/O error occurs.
 */
","* Create an options parser to parse the args.
   * @param args the command line arguments
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,java.lang.String[])",161,164,"/**
 * Constructs a GenericOptionsParser with a default Options object.
 * @param conf Configuration object.
 * @param args Command-line arguments.
 * @throws IOException If an I/O error occurs.
 */
","* Create a <code>GenericOptionsParser</code> to parse only the generic
   * Hadoop arguments.
   * 
   * The array of string arguments other than the generic arguments can be 
   * obtained by {@link #getRemainingArgs()}.
   * 
   * @param conf the <code>Configuration</code> to modify.
   * @param args command-line arguments.
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,"org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",3060,3062,"/**
* Calls m1 with a single input file and default options.
* @param inFile Input file path.
* @param outFile Output file path.
*/
","* The backwards compatible interface to sort.
     * @param inFile the input file to sort.
     * @param outFile the sorted output file.
     * @throws IOException raised on errors performing I/O.",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,createGenericOptionsParser,"org.apache.hadoop.service.launcher.ServiceLauncher:createGenericOptionsParser(org.apache.hadoop.conf.Configuration,java.lang.String[])",979,982,"/**
 * Creates a MinimalGenericOptionsParser with config and arguments.
 * @param conf Configuration object
 * @param argArray Command-line arguments
 * @return MinimalGenericOptionsParser instance
 */
","* Override point: create a generic options parser or subclass thereof.
   * @param conf Hadoop configuration
   * @param argArray array of arguments
   * @return a generic options parser to parse the arguments
   * @throws IOException on any failure",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,main,org.apache.hadoop.util.ConfTest:main(java.lang.String[]),227,300,"/**
* Parses command-line arguments and validates input files.
* Parses options, handles errors, and validates files.
*/",,,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,run,"org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])",62,83,"/**
 * Executes a tool with configuration and arguments.
 * @param conf Configuration object.
 * @param tool Tool to execute.
 * @param args Command-line arguments.
 * @return Tool's exit code.
 */","* Runs the given <code>Tool</code> by {@link Tool#run(String[])}, after 
   * parsing with the given generic arguments. Uses the given 
   * <code>Configuration</code>, or builds one if null.
   * 
   * Sets the <code>Tool</code>'s configuration with the possibly modified 
   * version of the <code>conf</code>.  
   * 
   * @param conf <code>Configuration</code> for the <code>Tool</code>.
   * @param tool <code>Tool</code> to run.
   * @param args command-line arguments to the tool.
   * @return exit code of the {@link Tool#run(String[])} method.
   * @throws Exception Exception.",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,parseCommandArgs,"org.apache.hadoop.service.launcher.ServiceLauncher:parseCommandArgs(org.apache.hadoop.conf.Configuration,java.util.List)",917,970,"/**
 * Parses command line arguments, extracts configuration files/classes.
 * @param conf Configuration object
 * @param args Command line arguments
 * @return Remaining arguments after parsing
 */","* Parse the command arguments, extracting the service class as the last
   * element of the list (after extracting all the rest).
   *
   * The field {@link #commandOptions} field must already have been set.
   * @param conf configuration to use
   * @param args command line argument list
   * @return the remaining arguments
   * @throws ServiceLaunchException if processing of arguments failed",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,exec,"org.apache.hadoop.security.KDiag:exec(org.apache.hadoop.conf.Configuration,java.lang.String[])",1052,1056,"/**
 * Executes a KDiag tool runner with the given configuration and arguments.
 * @param conf Hadoop configuration.
 * @param argv Command-line arguments.
 * @return Exit code from the tool runner.
 */
","* Inner entry point, with no logging or system exits.
   *
   * @param conf configuration
   * @param argv argument list
   * @return an exception
   * @throws Exception Exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,main,org.apache.hadoop.security.token.DtUtilShell:main(java.lang.String[]),393,395,"/**
 * Executes a DtUtilShell command with given arguments.
 * @param args Command-line arguments for the shell.
 * @throws Exception if an error occurs during execution.
 */",,,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,main,org.apache.hadoop.security.alias.CredentialShell:main(java.lang.String[]),534,537,"/**
 * Executes a Hadoop ToolRunner with provided configuration and args.
 * @param args Command-line arguments for the Hadoop tool.
 * @throws Exception if an error occurs during execution.
 */
","* Main program.
   *
   * @param args
   *          Command line arguments
   * @throws Exception exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,main,org.apache.hadoop.crypto.key.KeyShell:main(java.lang.String[]),552,555,"/**
 * Executes a KeyShell tool with given arguments and prints the result.
 */","* main() entry point for the KeyShell.  While strictly speaking the
   * return is void, it will System.exit() with a return code: 0 is for
   * success and 1 for failure.
   *
   * @param args Command line arguments.
   * @throws Exception raised on errors performing I/O.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,run,"org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.util.Tool,java.lang.String[])",95,98,"/**
* Delegates to a private method with tool's result and arguments.
*/","* Runs the <code>Tool</code> with its <code>Configuration</code>.
   * 
   * Equivalent to <code>run(tool.getConf(), tool, args)</code>.
   * 
   * @param tool <code>Tool</code> to run.
   * @param args command-line arguments to the tool.
   * @return exit code of the {@link Tool#run(String[])} method.
   * @throws Exception exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,extractCommandOptions,"org.apache.hadoop.service.launcher.ServiceLauncher:extractCommandOptions(org.apache.hadoop.conf.Configuration,java.util.List)",896,905,"/**
 * Processes arguments, extracts a subset, and applies further logic.
 * @param conf Configuration object. @param args Input list of strings.
 */
","* Extract the command options and apply them to the configuration,
   * building an array of processed arguments to hand down to the service.
   *
   * @param conf configuration to update.
   * @param args main arguments. {@code args[0]}is assumed to be
   * the service classname and is skipped.
   * @return the remaining arguments
   * @throws ExitUtil.ExitException if JVM exiting is disabled.",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,main,org.apache.hadoop.security.KDiag:main(java.lang.String[]),1062,1072,"/**
 * Processes command-line arguments and handles exceptions.
 * @param argv Command-line arguments passed to the program.
 */
","* Main entry point.
   * @param argv args list",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,main,org.apache.hadoop.fs.FsShell:main(java.lang.String[]),383,395,"/**
 * Executes a Hadoop FsShell command with provided arguments.
 * @param argv command-line arguments for the FsShell
 * @throws Exception if an error occurs during execution
 */","* main() has some simple utility methods
   * @param argv the command and its arguments
   * @throws Exception upon error",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,main,org.apache.hadoop.log.LogLevel:main(java.lang.String[]),73,76,"/**
 * Parses CLI arguments and executes the CLI application.
 * @param args Command-line arguments passed to the application.
 */","* A command line implementation
   * @param args input args.
   * @throws Exception exception.",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,main,org.apache.hadoop.util.FindClass:main(java.lang.String[]),378,386,"/**
* Runs the FindClass tool with provided arguments.
* Handles exceptions and prints error messages.
*/
","* Main entry point. 
   * Runs the class via the {@link ToolRunner}, then
   * exits with an appropriate exit code. 
   * @param args argument list",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchServiceAndExit,org.apache.hadoop.service.launcher.ServiceLauncher:launchServiceAndExit(java.util.List),281,315,"/**
 * Processes arguments, configures, and executes a task, handling exceptions.
 */","* Launch the service and exit.
   *
   * <ol>
   * <li>Parse the command line.</li> 
   * <li>Build the service configuration from it.</li>
   * <li>Start the service.</li>
   * <li>If it is a {@link LaunchableService}: execute it</li>
   * <li>Otherwise: wait for it to finish.</li>
   * <li>Exit passing the status code to the {@link #exit(int, String)}
   * method.</li>
   * </ol>
   * @param args arguments to the service. {@code arg[0]} is 
   * assumed to be the service classname.",,,True,42
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceMain,org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.util.List),1064,1073,"/**
 * Launches a service based on argsList.
 * If argsList.m1() is true, m4() is called.
 */",,,,True,43
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,main,org.apache.hadoop.service.launcher.ServiceLauncher:main(java.lang.String[]),1043,1045,"/**
* Calls m2 with the result of Arrays.m1 applied to args.
*/","* This is the JVM entry point for the service launcher.
   *
   * Converts the arguments to a list, then invokes {@link #serviceMain(List)}
   * @param args command line arguments.",,,True,44
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceMain,org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.lang.String[]),1052,1054,"/**
* Delegates to m2 with an array of strings.
* @param args Variable number of string arguments.
*/","* Varargs version of the entry point for testing and other in-JVM use.
   * Hands off to {@link #serviceMain(List)}
   * @param args command line arguments.",,,True,44
